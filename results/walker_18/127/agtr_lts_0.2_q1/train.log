{"episode_reward": 0.0, "episode": 1.0, "duration": 25.428672790527344, "step": 1000}
{"episode_reward": 28.002640152663393, "episode": 2.0, "duration": 2.070359468460083, "step": 2000}
{"episode_reward": 481.4321918933315, "episode": 3.0, "batch_reward": 0.24896480060092765, "critic_loss": 0.6597502107777647, "actor_loss": -82.11453333774917, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 65.96263337135315, "step": 3000}
{"episode_reward": 166.10082619989606, "episode": 4.0, "batch_reward": 0.20541310910880567, "critic_loss": 0.6062437261641026, "actor_loss": -79.53506945800781, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.768808841705322, "step": 4000}
{"episode_reward": 57.75928886455367, "episode": 5.0, "batch_reward": 0.17838267347216605, "critic_loss": 0.6408848156332969, "actor_loss": -77.90715869140625, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.613173007965088, "step": 5000}
{"episode_reward": 162.69839760206384, "episode": 6.0, "batch_reward": 0.18383560011535882, "critic_loss": 0.726589440047741, "actor_loss": -76.00622331237793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.959505796432495, "step": 6000}
{"episode_reward": 221.83106581897303, "episode": 7.0, "batch_reward": 0.17937943635880946, "critic_loss": 0.45887986160814764, "actor_loss": -73.93987292480469, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.264705657958984, "step": 7000}
{"episode_reward": 65.65113890728303, "episode": 8.0, "batch_reward": 0.16147111555933952, "critic_loss": 0.3842337777167559, "actor_loss": -71.90550367736816, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.403602361679077, "step": 8000}
{"episode_reward": 29.78619469274109, "episode": 9.0, "batch_reward": 0.15297281520813705, "critic_loss": 0.3636424488127232, "actor_loss": -69.95334411621094, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.47372603416443, "step": 9000}
{"episode_reward": 220.02156620187802, "episode": 10.0, "batch_reward": 0.16733310525119305, "critic_loss": 0.48885879455506803, "actor_loss": -69.03594794464111, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.589906692504883, "step": 10000}
{"episode_reward": 376.08248232418174, "episode": 11.0, "batch_reward": 0.19147105045616628, "critic_loss": 0.5599494231790304, "actor_loss": -67.90720133209228, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 44.190248012542725, "step": 11000}
{"episode_reward": 384.08654637658157, "episode": 12.0, "batch_reward": 0.1917482428252697, "critic_loss": 0.4902702040672302, "actor_loss": -66.74884159851074, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.340715169906616, "step": 12000}
{"episode_reward": 24.58943997290583, "episode": 13.0, "batch_reward": 0.1945325467288494, "critic_loss": 0.6555385146141052, "actor_loss": -64.68625267791748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.73029637336731, "step": 13000}
{"episode_reward": 468.51544442968355, "episode": 14.0, "batch_reward": 0.21887880444526672, "critic_loss": 0.8531068639755249, "actor_loss": -64.98613878631592, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.685949325561523, "step": 14000}
{"episode_reward": 571.8806314300515, "episode": 15.0, "batch_reward": 0.2398115447461605, "critic_loss": 1.0267412320375442, "actor_loss": -64.85743203735352, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.493128061294556, "step": 15000}
{"episode_reward": 488.0951418487401, "episode": 16.0, "batch_reward": 0.24257407121360303, "critic_loss": 0.96657605946064, "actor_loss": -64.17824770355224, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.00899600982666, "step": 16000}
{"episode_reward": 15.885719950599622, "episode": 17.0, "batch_reward": 0.2291742979735136, "critic_loss": 0.8736513788998127, "actor_loss": -63.04073829650879, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.083202600479126, "step": 17000}
{"episode_reward": 18.29811458364608, "episode": 18.0, "batch_reward": 0.2267868636995554, "critic_loss": 0.9360097644031048, "actor_loss": -61.725431602478025, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.608120918273926, "step": 18000}
