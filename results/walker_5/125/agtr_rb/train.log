{"episode": 1.0, "duration": 13.960334300994873, "episode_reward": 59.660092495208936, "step": 1000}
{"episode": 2.0, "duration": 1.2218351364135742, "episode_reward": 890.1817750618746, "step": 2000}
{"episode": 3.0, "duration": 1.2162635326385498, "episode_reward": 964.7892114091037, "step": 3000}
{"episode": 4.0, "duration": 1.1924428939819336, "episode_reward": 877.6006010287016, "step": 4000}
{"episode": 5.0, "duration": 1.205186128616333, "episode_reward": 951.010148846849, "step": 5000}
{"episode": 6.0, "batch_reward": 0.7452633252677034, "actor_loss": -92.63181409585057, "actor_target_entropy": -6.0, "alpha_value": 0.0033185846155122127, "duration": 372.8234465122223, "episode_reward": 25.99829992109026, "step": 6000}
{"episode": 7.0, "batch_reward": 0.5841627674102783, "actor_loss": -89.20035426330567, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.1118426322937, "episode_reward": 47.72378223466838, "step": 7000}
{"episode": 8.0, "batch_reward": 0.5107286886274814, "actor_loss": -88.02375361633301, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.184959411621094, "episode_reward": 24.729801386525896, "step": 8000}
{"episode": 9.0, "batch_reward": 0.45506236413121226, "actor_loss": -86.98172450256348, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.016822576522827, "episode_reward": 46.41431964615626, "step": 9000}
{"episode": 10.0, "batch_reward": 0.411197850972414, "actor_loss": -82.47067590332031, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 5054.040947198868, "episode_reward": 26.406486191315583, "step": 10000}
{"episode": 11.0, "batch_reward": 0.37607352086901663, "actor_loss": -81.64366088867187, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.442777633666992, "episode_reward": 24.53658884578097, "step": 11000}
{"episode": 12.0, "batch_reward": 0.34363090243935585, "actor_loss": -80.76821559143066, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.8372802734375, "episode_reward": 11.618836111843086, "step": 12000}
{"episode": 13.0, "batch_reward": 0.31851353104412555, "actor_loss": -79.60814532470702, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.877604246139526, "episode_reward": 11.818587252200187, "step": 13000}
{"episode": 14.0, "batch_reward": 0.29389335826039314, "actor_loss": -77.55736605834961, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 552.6264936923981, "episode_reward": 50.099120006282284, "step": 14000}
{"episode": 15.0, "batch_reward": 0.2768501366972923, "actor_loss": -77.40576608276368, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.229443073272705, "episode_reward": 53.78552709328286, "step": 15000}
{"episode": 16.0, "batch_reward": 0.26506746172904966, "actor_loss": -77.59593269348144, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.8179953098297, "episode_reward": 45.591092447881124, "step": 16000}
{"episode": 17.0, "batch_reward": 0.24906023612618447, "actor_loss": -77.5190592956543, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.21000099182129, "episode_reward": 45.52005887776595, "step": 17000}
{"episode": 18.0, "batch_reward": 0.23947904013097288, "actor_loss": -76.18125259399415, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 547.897590637207, "episode_reward": 44.583226919278864, "step": 18000}
{"episode": 19.0, "batch_reward": 0.22829968746006488, "actor_loss": -76.23369749450684, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.050400257110596, "episode_reward": 44.36876347414343, "step": 19000}
{"episode": 20.0, "batch_reward": 0.21723519332706928, "actor_loss": -76.91009361267089, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 547.626487493515, "episode_reward": 25.284301211123175, "step": 20000}
{"episode": 21.0, "batch_reward": 0.20932080782204868, "actor_loss": -76.23499291992188, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.17449688911438, "episode_reward": 10.992018408935058, "step": 21000}
{"episode": 22.0, "batch_reward": 0.20027178179472685, "actor_loss": -75.71681095886231, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.9110198020935, "episode_reward": 45.5806949743907, "step": 22000}
{"episode": 23.0, "batch_reward": 0.19448566365987063, "actor_loss": -75.63260736083984, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.32090973854065, "episode_reward": 25.55440625684732, "step": 23000}
{"episode": 24.0, "batch_reward": 0.1872899785414338, "actor_loss": -74.29378540039062, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.1490032672882, "episode_reward": 60.37695874175146, "step": 24000}
{"episode": 25.0, "batch_reward": 0.18253199105709791, "actor_loss": -74.19037797546386, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.135172367095947, "episode_reward": 26.292081770797463, "step": 25000}
{"episode": 26.0, "batch_reward": 0.17518359251320362, "actor_loss": -75.44211602783203, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.9833805561066, "episode_reward": 25.745705147771453, "step": 26000}
{"episode": 27.0, "batch_reward": 0.16897806157171727, "actor_loss": -75.30905935668946, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.357914447784424, "episode_reward": 51.17222728566999, "step": 27000}
{"episode": 28.0, "batch_reward": 0.16498294243216516, "actor_loss": -73.87456861877442, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.4401478767395, "episode_reward": 24.5054119676094, "step": 28000}
{"episode": 29.0, "batch_reward": 0.1581237606331706, "actor_loss": -73.57789526367188, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.509481191635132, "episode_reward": 25.86317560934724, "step": 29000}
{"episode": 30.0, "batch_reward": 0.1556949945539236, "actor_loss": -73.69059133911132, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.2907259464264, "episode_reward": 44.66688752353812, "step": 30000}
{"episode": 31.0, "batch_reward": 0.15201805894076825, "actor_loss": -73.7069063720703, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.758025884628296, "episode_reward": 25.586008729950546, "step": 31000}
{"episode": 32.0, "batch_reward": 0.1483498065955937, "actor_loss": -72.97978619384766, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.159334897995, "episode_reward": 44.057793790947144, "step": 32000}
{"episode": 33.0, "batch_reward": 0.14493629944324493, "actor_loss": -73.0805756072998, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.21475911140442, "episode_reward": 45.71329625918584, "step": 33000}
{"episode": 34.0, "batch_reward": 0.14149816855043174, "actor_loss": -74.17170391845703, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.4734706878662, "episode_reward": 27.971677092094577, "step": 34000}
{"episode": 35.0, "batch_reward": 0.1394480003863573, "actor_loss": -74.13526249694824, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.320689916610718, "episode_reward": 43.40504933075056, "step": 35000}
{"episode": 36.0, "batch_reward": 0.1371117246672511, "actor_loss": -73.04595062255859, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.2490968704224, "episode_reward": 24.76562021586583, "step": 36000}
{"episode": 37.0, "batch_reward": 0.13267282440513373, "actor_loss": -72.95784312438965, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.92966079711914, "episode_reward": 25.196654328640975, "step": 37000}
{"episode": 38.0, "batch_reward": 0.12853757914155722, "actor_loss": -74.92382734680176, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 551.8535633087158, "episode_reward": 28.316956838207087, "step": 38000}
{"episode": 39.0, "batch_reward": 0.12592811468243598, "actor_loss": -74.95376774597167, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.96078848838806, "episode_reward": 45.546611516320745, "step": 39000}
{"episode": 40.0, "batch_reward": 0.12587957219779491, "actor_loss": -73.20447352600098, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.1695683002472, "episode_reward": 59.81174712511709, "step": 40000}
{"episode": 41.0, "batch_reward": 0.12317917931079865, "actor_loss": -73.05836093139648, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.613844871520996, "episode_reward": 11.479655197265531, "step": 41000}
{"episode": 42.0, "batch_reward": 0.12077427886798978, "actor_loss": -72.79470960998535, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 537.0579121112823, "episode_reward": 26.14099003129372, "step": 42000}
{"episode": 43.0, "batch_reward": 0.11897397294640541, "actor_loss": -72.70896980285644, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.221179008483887, "episode_reward": 44.897211441102456, "step": 43000}
{"episode": 44.0, "batch_reward": 0.11744569353386759, "actor_loss": -72.83819418334961, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.1986956596375, "episode_reward": 47.13508010061006, "step": 44000}
{"episode": 45.0, "batch_reward": 0.11513481573015451, "actor_loss": -72.87313688659668, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.29388451576233, "episode_reward": 24.582212045712794, "step": 45000}
{"episode": 46.0, "batch_reward": 0.11229911721870303, "actor_loss": -72.83231007385254, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.1497530937195, "episode_reward": 25.499583489724735, "step": 46000}
{"episode": 47.0, "batch_reward": 0.11142114560678601, "actor_loss": -72.83820794677735, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.994731426239014, "episode_reward": 25.232094836700472, "step": 47000}
{"episode": 48.0, "batch_reward": 0.10948945498466492, "actor_loss": -72.45179667663574, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.1141471862793, "episode_reward": 70.84835901911862, "step": 48000}
{"episode": 49.0, "batch_reward": 0.10922275471687316, "actor_loss": -72.64323915100098, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.93121600151062, "episode_reward": 63.8360921260631, "step": 49000}
{"episode": 50.0, "batch_reward": 0.10905503321811556, "actor_loss": -72.69430172729493, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.4565110206604, "episode_reward": 50.054446222497944, "step": 50000}
{"episode": 51.0, "batch_reward": 0.10680828253924847, "actor_loss": -72.71799658203125, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.51787281036377, "episode_reward": 46.621719890970915, "step": 51000}
{"episode": 52.0, "batch_reward": 0.10499792746454477, "actor_loss": -73.4725909576416, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.504686832428, "episode_reward": 25.2841794238364, "step": 52000}
{"episode": 53.0, "batch_reward": 0.1033161426447332, "actor_loss": -73.49746766662598, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.89076542854309, "episode_reward": 44.09092094359634, "step": 53000}
{"episode": 54.0, "batch_reward": 0.10292838462069631, "actor_loss": -72.23782165527344, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.8437955379486, "episode_reward": 46.02135796044963, "step": 54000}
{"episode": 55.0, "batch_reward": 0.10210711281374096, "actor_loss": -72.10375482177734, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.717206954956055, "episode_reward": 28.580622252707776, "step": 55000}
{"episode": 56.0, "batch_reward": 0.10043467433005572, "actor_loss": -71.88523974609375, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.3949506282806, "episode_reward": 49.6250147219195, "step": 56000}
{"episode": 57.0, "batch_reward": 0.09899365509673953, "actor_loss": -71.93584692382812, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.87858772277832, "episode_reward": 25.248071763428722, "step": 57000}
{"episode": 58.0, "batch_reward": 0.09892523075640201, "actor_loss": -70.74474884033204, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.2633194923401, "episode_reward": 26.00097503618746, "step": 58000}
{"episode": 59.0, "batch_reward": 0.0970205115787685, "actor_loss": -70.6535265197754, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.936379432678223, "episode_reward": 25.457767116893343, "step": 59000}
{"episode": 60.0, "batch_reward": 0.09571817132085561, "actor_loss": -71.00396798706055, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 547.9540438652039, "episode_reward": 45.636008188973, "step": 60000}
{"episode": 61.0, "batch_reward": 0.09574986160174012, "actor_loss": -71.01952784729004, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.573589324951172, "episode_reward": 25.839946026353275, "step": 61000}
{"episode": 62.0, "batch_reward": 0.09303798661381006, "actor_loss": -70.8515538482666, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.501188993454, "episode_reward": 24.64616476955456, "step": 62000}
{"episode": 63.0, "batch_reward": 0.09364060679450631, "actor_loss": -70.86818647766113, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.87708806991577, "episode_reward": 25.26988269704913, "step": 63000}
{"episode": 64.0, "batch_reward": 0.09175297301262617, "actor_loss": -70.33889973449708, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 551.5933339595795, "episode_reward": 60.209003465054316, "step": 64000}
{"episode": 65.0, "batch_reward": 0.09122845314070582, "actor_loss": -70.35830926513673, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.64784073829651, "episode_reward": 56.96074696895, "step": 65000}
{"episode": 66.0, "batch_reward": 0.09066305939480662, "actor_loss": -71.49789167785644, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.6498982906342, "episode_reward": 12.419362904041932, "step": 66000}
{"episode": 67.0, "batch_reward": 0.09149525155499577, "actor_loss": -71.40977404785156, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.99114441871643, "episode_reward": 44.01780439230977, "step": 67000}
{"episode": 68.0, "batch_reward": 0.08934087828546762, "actor_loss": -69.9390391998291, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.8216180801392, "episode_reward": 26.262282909914344, "step": 68000}
{"episode": 69.0, "batch_reward": 0.08870180462673306, "actor_loss": -69.94731399536133, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.05093240737915, "episode_reward": 50.01075503327223, "step": 69000}
{"episode": 70.0, "batch_reward": 0.08770471486449241, "actor_loss": -71.36143827819824, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 547.4874396324158, "episode_reward": 43.95494716379882, "step": 70000}
{"episode": 71.0, "batch_reward": 0.08635770389437676, "actor_loss": -71.63882257080078, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.341222047805786, "episode_reward": 31.835009518548343, "step": 71000}
{"episode": 72.0, "batch_reward": 0.0855174527913332, "actor_loss": -70.77211865234375, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 546.5348963737488, "episode_reward": 23.30248133553358, "step": 72000}
{"episode": 73.0, "batch_reward": 0.0860088844075799, "actor_loss": -70.88640637207031, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.96601963043213, "episode_reward": 45.54371592565645, "step": 73000}
{"episode": 74.0, "batch_reward": 0.08440586544200777, "actor_loss": -70.93029238891602, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.3575768470764, "episode_reward": 46.212841548686754, "step": 74000}
{"episode": 75.0, "batch_reward": 0.08396601730957627, "actor_loss": -70.99010874938965, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.816271781921387, "episode_reward": 25.35789236244105, "step": 75000}
{"episode": 76.0, "batch_reward": 0.08420215364918113, "actor_loss": -70.78834915161133, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.8994886875153, "episode_reward": 46.743975976062856, "step": 76000}
{"episode": 77.0, "batch_reward": 0.08225087172910571, "actor_loss": -70.75977958679199, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.767515897750854, "episode_reward": 25.730068766539883, "step": 77000}
{"episode": 78.0, "batch_reward": 0.08292542034387589, "actor_loss": -71.02574987792968, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.8335828781128, "episode_reward": 26.084304298899443, "step": 78000}
{"episode": 79.0, "batch_reward": 0.08084005906805396, "actor_loss": -71.0960343322754, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.989721059799194, "episode_reward": 42.221465796706305, "step": 79000}
{"episode": 80.0, "batch_reward": 0.08133812775090336, "actor_loss": -71.54760234069825, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.9666814804077, "episode_reward": 26.298889667840584, "step": 80000}
{"episode": 81.0, "batch_reward": 0.0801322708427906, "actor_loss": -71.67777752685546, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.26495385169983, "episode_reward": 46.59692236439895, "step": 81000}
{"episode": 82.0, "batch_reward": 0.08049258707463741, "actor_loss": -71.77057412719726, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.9440693855286, "episode_reward": 25.646428560336695, "step": 82000}
{"episode": 83.0, "batch_reward": 0.07840842558816076, "actor_loss": -71.74869657897949, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.690184593200684, "episode_reward": 11.792026173865965, "step": 83000}
{"episode": 84.0, "batch_reward": 0.0774100767634809, "actor_loss": -69.9501674194336, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.9703664779663, "episode_reward": 48.85120076516869, "step": 84000}
{"episode": 85.0, "batch_reward": 0.07920155672356487, "actor_loss": -69.99599699401855, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.691162109375, "episode_reward": 27.05254715323047, "step": 85000}
{"episode": 86.0, "batch_reward": 0.07763567420095205, "actor_loss": -70.7849164428711, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.0538818836212, "episode_reward": 26.172783062454986, "step": 86000}
{"episode": 87.0, "batch_reward": 0.07728721137717366, "actor_loss": -70.74628332519531, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.739973545074463, "episode_reward": 62.46036714609585, "step": 87000}
{"episode": 88.0, "batch_reward": 0.07699223769828677, "actor_loss": -70.9688924407959, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 552.3289804458618, "episode_reward": 67.26095606833927, "step": 88000}
{"episode": 89.0, "batch_reward": 0.07601565783843398, "actor_loss": -71.05624098205567, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.84133267402649, "episode_reward": 45.4322442318162, "step": 89000}
{"episode": 90.0, "batch_reward": 0.0754780364073813, "actor_loss": -71.58409648132324, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.833756685257, "episode_reward": 25.72159470632449, "step": 90000}
{"episode": 91.0, "batch_reward": 0.07545233907178045, "actor_loss": -71.55496731567384, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 30.921692609786987, "episode_reward": 46.2913587601502, "step": 91000}
{"episode": 92.0, "batch_reward": 0.07524142365157604, "actor_loss": -72.09832707214356, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 538.148823261261, "episode_reward": 25.26700662302597, "step": 92000}
{"episode": 93.0, "batch_reward": 0.07502929671108723, "actor_loss": -72.16353656005859, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.99156355857849, "episode_reward": 72.90835555579892, "step": 93000}
{"episode": 94.0, "batch_reward": 0.07464535626769066, "actor_loss": -71.12041545104981, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.3955018520355, "episode_reward": 25.923908429776024, "step": 94000}
{"episode": 95.0, "batch_reward": 0.0745741478614509, "actor_loss": -71.08920135498047, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.09536385536194, "episode_reward": 25.45851119137464, "step": 95000}
{"episode": 96.0, "batch_reward": 0.0738845260553062, "actor_loss": -72.24963572692872, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 546.2569687366486, "episode_reward": 25.52402009343661, "step": 96000}
{"episode": 97.0, "batch_reward": 0.07312924715131522, "actor_loss": -72.2439728088379, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.670485019683838, "episode_reward": 24.565823186749522, "step": 97000}
{"episode": 98.0, "batch_reward": 0.07268604006618261, "actor_loss": -71.15244833374024, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.8219046592712, "episode_reward": 70.60163539817475, "step": 98000}
{"episode": 99.0, "batch_reward": 0.07252910711616278, "actor_loss": -71.26205824279785, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.786664724349976, "episode_reward": 21.055290532007085, "step": 99000}
{"episode": 100.0, "batch_reward": 0.0725173794440925, "actor_loss": -70.57876376342773, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.0478990077972, "episode_reward": 47.409955222401585, "step": 100000}
{"episode": 101.0, "batch_reward": 0.07231939928606153, "actor_loss": -70.60948663330078, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.739030122756958, "episode_reward": 64.46839076707288, "step": 101000}
{"episode": 102.0, "batch_reward": 0.07269573662802577, "actor_loss": -70.27907182312012, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.5797829627991, "episode_reward": 41.83738593528922, "step": 102000}
{"episode": 103.0, "batch_reward": 0.07091754543036223, "actor_loss": -70.26277156066895, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.96074604988098, "episode_reward": 25.598522944899994, "step": 103000}
{"episode": 104.0, "batch_reward": 0.07116611972823739, "actor_loss": -70.82665135192872, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 551.0681896209717, "episode_reward": 26.61014328418041, "step": 104000}
{"episode": 105.0, "batch_reward": 0.0715187357775867, "actor_loss": -70.82531324768067, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.971930503845215, "episode_reward": 42.50638564706969, "step": 105000}
{"episode": 106.0, "batch_reward": 0.07118356058001518, "actor_loss": -71.12317530822754, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.6775662899017, "episode_reward": 39.43959544615971, "step": 106000}
{"episode": 107.0, "batch_reward": 0.07063599251955748, "actor_loss": -71.09137443542481, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.920283794403076, "episode_reward": 26.274037817330505, "step": 107000}
{"episode": 108.0, "batch_reward": 0.06940627418830991, "actor_loss": -69.76786059570313, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.6271312236786, "episode_reward": 24.562171895408834, "step": 108000}
{"episode": 109.0, "batch_reward": 0.06853322548046709, "actor_loss": -69.78590585327149, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.65272855758667, "episode_reward": 54.79511740757296, "step": 109000}
{"episode": 110.0, "batch_reward": 0.06847411599755288, "actor_loss": -69.49911175537109, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.1107318401337, "episode_reward": 13.011789642574158, "step": 110000}
{"episode": 111.0, "batch_reward": 0.06837064867466688, "actor_loss": -69.4060301208496, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.24043893814087, "episode_reward": 11.377612034959142, "step": 111000}
{"episode": 112.0, "batch_reward": 0.06849965137988329, "actor_loss": -69.13655235290527, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.2885212898254, "episode_reward": 46.7172768781695, "step": 112000}
{"episode": 113.0, "batch_reward": 0.06750841322541237, "actor_loss": -69.16404396057129, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.80166983604431, "episode_reward": 25.803182558001634, "step": 113000}
{"episode": 114.0, "batch_reward": 0.06750128397345542, "actor_loss": -68.05434510803222, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.1377303600311, "episode_reward": 34.66149772859488, "step": 114000}
{"episode": 115.0, "batch_reward": 0.06735655533894896, "actor_loss": -68.07866427612305, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.84170889854431, "episode_reward": 18.868125410840662, "step": 115000}
{"episode": 116.0, "batch_reward": 0.06662503229826688, "actor_loss": -70.36331507873535, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 547.1450161933899, "episode_reward": 25.853245098440773, "step": 116000}
{"episode": 117.0, "batch_reward": 0.06805880007892848, "actor_loss": -70.41868548583984, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.019174098968506, "episode_reward": 26.572606077462336, "step": 117000}
{"episode": 118.0, "batch_reward": 0.06603119206428527, "actor_loss": -70.76617753601074, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.4627721309662, "episode_reward": 46.0304567975502, "step": 118000}
{"episode": 119.0, "batch_reward": 0.06560536241158843, "actor_loss": -70.8508983001709, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.677998542785645, "episode_reward": 25.927613052321913, "step": 119000}
{"episode": 120.0, "batch_reward": 0.06519132464751601, "actor_loss": -70.02045434570313, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 543.9002830982208, "episode_reward": 43.80792257355632, "step": 120000}
{"episode": 121.0, "batch_reward": 0.06640797222405673, "actor_loss": -70.10876205444336, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.165023803710938, "episode_reward": 49.127849518710676, "step": 121000}
{"episode": 122.0, "batch_reward": 0.0653147535994649, "actor_loss": -70.39594674682617, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 547.4605135917664, "episode_reward": 26.20964796711171, "step": 122000}
{"episode": 123.0, "batch_reward": 0.0660454477481544, "actor_loss": -70.37064695739745, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.798897743225098, "episode_reward": 24.84773862115125, "step": 123000}
{"episode": 124.0, "batch_reward": 0.0654002358019352, "actor_loss": -69.21103001403809, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.6030533313751, "episode_reward": 44.968884520475626, "step": 124000}
{"episode": 125.0, "batch_reward": 0.0640989703182131, "actor_loss": -69.28176679992676, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.723318099975586, "episode_reward": 54.72518057119686, "step": 125000}
{"episode": 126.0, "batch_reward": 0.0647591643333435, "actor_loss": -69.50522776794433, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 547.9427947998047, "episode_reward": 47.77355379115794, "step": 126000}
{"episode": 127.0, "batch_reward": 0.06440873727202416, "actor_loss": -69.48237187194825, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.983949899673462, "episode_reward": 45.260662970001825, "step": 127000}
{"episode": 128.0, "batch_reward": 0.06362723671086133, "actor_loss": -70.39344000244141, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.7281694412231, "episode_reward": 24.433110084383237, "step": 128000}
{"episode": 129.0, "batch_reward": 0.06379962988942861, "actor_loss": -70.47096719360351, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.734294891357422, "episode_reward": 29.979620711403296, "step": 129000}
{"episode": 130.0, "batch_reward": 0.06419616185128689, "actor_loss": -70.71565435791015, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.610800743103, "episode_reward": 43.38504572263283, "step": 130000}
{"episode": 131.0, "batch_reward": 0.06305437044799328, "actor_loss": -70.75090608215332, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.32221269607544, "episode_reward": 44.28538992061166, "step": 131000}
{"episode": 132.0, "batch_reward": 0.0627085579521954, "actor_loss": -70.58744819641113, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.6773653030396, "episode_reward": 47.20046013439256, "step": 132000}
{"episode": 133.0, "batch_reward": 0.0628607107270509, "actor_loss": -70.52390107727051, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.13552474975586, "episode_reward": 25.68917515938928, "step": 133000}
{"episode": 134.0, "batch_reward": 0.06265367489680648, "actor_loss": -69.7717410583496, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 551.0843341350555, "episode_reward": 72.5255719467694, "step": 134000}
{"episode": 135.0, "batch_reward": 0.06361762192845345, "actor_loss": -69.82075796508789, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.947718381881714, "episode_reward": 38.45727678449705, "step": 135000}
{"episode": 136.0, "batch_reward": 0.06302372044697403, "actor_loss": -70.9094034729004, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 550.4032187461853, "episode_reward": 53.81904375425925, "step": 136000}
{"episode": 137.0, "batch_reward": 0.06323203423060476, "actor_loss": -70.89167417907714, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.89748978614807, "episode_reward": 32.36476569904478, "step": 137000}
{"episode": 138.0, "batch_reward": 0.06236550787836313, "actor_loss": -70.39166653442383, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.0742146968842, "episode_reward": 25.11218353355547, "step": 138000}
{"episode": 139.0, "batch_reward": 0.062104841325432064, "actor_loss": -70.38178204345704, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 17.01692533493042, "episode_reward": 27.246526300833313, "step": 139000}
{"episode": 140.0, "batch_reward": 0.062457212898880246, "actor_loss": -69.66144882202148, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.9010279178619, "episode_reward": 24.869176197562915, "step": 140000}
{"episode": 141.0, "batch_reward": 0.061271790374070406, "actor_loss": -69.69084147644043, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 31.111976861953735, "episode_reward": 24.71988622718704, "step": 141000}
{"episode": 142.0, "batch_reward": 0.06211961254477501, "actor_loss": -70.845576171875, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 539.3465695381165, "episode_reward": 74.47959984727775, "step": 142000}
{"episode": 143.0, "batch_reward": 0.06135935621336103, "actor_loss": -71.24517657470703, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.86404538154602, "episode_reward": 25.79469765567853, "step": 143000}
{"episode": 144.0, "batch_reward": 0.06171088724583387, "actor_loss": -70.98706858825683, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 548.7152757644653, "episode_reward": 26.480946579213434, "step": 144000}
{"episode": 145.0, "batch_reward": 0.06041471832990646, "actor_loss": -71.21714973449707, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.56199336051941, "episode_reward": 34.94348223480725, "step": 145000}
{"episode": 146.0, "batch_reward": 0.06061838657036424, "actor_loss": -70.2538602142334, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 549.1309368610382, "episode_reward": 25.013175847117143, "step": 146000}
{"episode": 147.0, "batch_reward": 0.06023090266808868, "actor_loss": -70.1446979675293, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.94577646255493, "episode_reward": 26.57450882058199, "step": 147000}
{"episode": 148.0, "batch_reward": 0.0601191559061408, "actor_loss": -70.96798582458496, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 545.6536211967468, "episode_reward": 15.166105600855266, "step": 148000}
{"episode": 149.0, "batch_reward": 0.059387796442955734, "actor_loss": -70.93363650512696, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.92233657836914, "episode_reward": 23.98916293008995, "step": 149000}
{"episode": 150.0, "batch_reward": 0.059876901805400846, "actor_loss": -70.78091693115235, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "step": 150000}
