{"episode": 1.0, "duration": 23.7333664894104, "episode_reward": 59.660092495208936, "step": 1000}
{"episode": 2.0, "duration": 2.196570634841919, "episode_reward": 890.1817750618746, "step": 2000}
{"episode": 3.0, "batch_reward": 0.473593510699773, "actor_loss": -84.90927831093374, "actor_target_entropy": -6.0, "alpha_value": 0.0033185846155115383, "duration": 65.42991399765015, "episode_reward": 365.0690703991874, "step": 3000}
{"episode": 4.0, "batch_reward": 0.48134776547551156, "actor_loss": -84.7882707977295, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.400681972503662, "episode_reward": 737.0713941146595, "step": 4000}
{"episode": 5.0, "batch_reward": 0.5269426458775998, "actor_loss": -85.85545558166504, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.087066173553467, "episode_reward": 726.899571675455, "step": 5000}
{"episode": 6.0, "batch_reward": 0.557353207141161, "actor_loss": -86.58652857971191, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.86507773399353, "episode_reward": 565.3388613068219, "step": 6000}
{"episode": 7.0, "batch_reward": 0.5643172934651375, "actor_loss": -86.91744563293457, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.972163438796997, "episode_reward": 778.8472879590158, "step": 7000}
{"episode": 8.0, "batch_reward": 0.6002545759081841, "actor_loss": -87.73046769714355, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.43704319000244, "episode_reward": 839.2770131159411, "step": 8000}
{"episode": 9.0, "batch_reward": 0.617582167506218, "actor_loss": -88.22815991210938, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.161078691482544, "episode_reward": 655.871287725986, "step": 9000}
{"episode": 10.0, "batch_reward": 0.6295566893219948, "actor_loss": -88.56813983154296, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.1762273311615, "episode_reward": 705.5497268811221, "step": 10000}
{"episode": 11.0, "batch_reward": 0.6424557436108589, "actor_loss": -88.92016903686523, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 45.31572961807251, "episode_reward": 941.7413051320012, "step": 11000}
{"episode": 12.0, "batch_reward": 0.6671633017063141, "actor_loss": -89.46231213378907, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.251866102218628, "episode_reward": 923.5338764306765, "step": 12000}
{"episode": 13.0, "batch_reward": 0.683057280421257, "actor_loss": -89.823919921875, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.608051538467407, "episode_reward": 720.0832210154949, "step": 13000}
{"episode": 14.0, "batch_reward": 0.693960118830204, "actor_loss": -90.10570762634278, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.20905113220215, "episode_reward": 948.469645837642, "step": 14000}
{"episode": 15.0, "batch_reward": 0.7092473586797714, "actor_loss": -90.44916159057617, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.19259762763977, "episode_reward": 906.5798102653685, "step": 15000}
{"episode": 16.0, "batch_reward": 0.7166146112084388, "actor_loss": -90.69403089904785, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.954920768737793, "episode_reward": 652.0616525253051, "step": 16000}
{"episode": 17.0, "batch_reward": 0.7176415985822677, "actor_loss": -90.6184978942871, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.965935707092285, "episode_reward": 883.23904892322, "step": 17000}
{"episode": 18.0, "batch_reward": 0.723097070813179, "actor_loss": -90.76733433532715, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.307401657104492, "episode_reward": 701.2488934487403, "step": 18000}
{"episode": 19.0, "batch_reward": 0.7266721887588501, "actor_loss": -90.7921770477295, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.96034073829651, "episode_reward": 915.7769375752239, "step": 19000}
{"episode": 20.0, "batch_reward": 0.7355309730768204, "actor_loss": -90.97106941223144, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.087175369262695, "episode_reward": 905.0923438248102, "step": 20000}
{"episode": 21.0, "batch_reward": 0.7428196432590485, "actor_loss": -91.08346272277832, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 43.98136925697327, "episode_reward": 833.4652317195696, "step": 21000}
{"episode": 22.0, "batch_reward": 0.7496003001332283, "actor_loss": -91.20649319458008, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.183757305145264, "episode_reward": 937.1820170318975, "step": 22000}
{"episode": 23.0, "batch_reward": 0.7556127585768699, "actor_loss": -91.42871754455567, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.06456208229065, "episode_reward": 811.9603040176128, "step": 23000}
{"episode": 24.0, "batch_reward": 0.7583239378333092, "actor_loss": -91.47119471740723, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.540321588516235, "episode_reward": 902.2256306181608, "step": 24000}
{"episode": 25.0, "batch_reward": 0.7600757381916046, "actor_loss": -91.36809689331055, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.967987537384033, "episode_reward": 795.6121713013615, "step": 25000}
{"episode": 26.0, "batch_reward": 0.7637463837265969, "actor_loss": -91.47904333496093, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.946460008621216, "episode_reward": 895.267060397404, "step": 26000}
{"episode": 27.0, "batch_reward": 0.7729996259212494, "actor_loss": -91.78494555664062, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.53527069091797, "episode_reward": 977.063813683937, "step": 27000}
{"episode": 28.0, "batch_reward": 0.778649155497551, "actor_loss": -91.93761848449707, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.091334581375122, "episode_reward": 840.5575234890069, "step": 28000}
{"episode": 29.0, "batch_reward": 0.7807717048525811, "actor_loss": -91.85249317932129, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.359000205993652, "episode_reward": 852.0308403157203, "step": 29000}
{"episode": 30.0, "batch_reward": 0.7809057834148407, "actor_loss": -91.88005215454102, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.30820655822754, "episode_reward": 816.8085543563592, "step": 30000}
{"episode": 31.0, "batch_reward": 0.7843507387638092, "actor_loss": -91.99996321105957, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 44.79916191101074, "episode_reward": 953.2519342151605, "step": 31000}
{"episode": 32.0, "batch_reward": 0.7907722030878067, "actor_loss": -92.18179957580567, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.210699796676636, "episode_reward": 902.5165670668028, "step": 32000}
{"episode": 33.0, "batch_reward": 0.7920632456541061, "actor_loss": -92.19955340576172, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.422658443450928, "episode_reward": 875.8441058415101, "step": 33000}
{"episode": 34.0, "batch_reward": 0.7940591762065887, "actor_loss": -92.35196072387696, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.059632301330566, "episode_reward": 941.6596542405478, "step": 34000}
{"episode": 35.0, "batch_reward": 0.7969269654154778, "actor_loss": -92.42725119018554, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.088085889816284, "episode_reward": 852.6085978731434, "step": 35000}
{"episode": 36.0, "batch_reward": 0.8000337127447128, "actor_loss": -92.47558013916016, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.20461130142212, "episode_reward": 923.0405977532573, "step": 36000}
{"episode": 37.0, "batch_reward": 0.8012025339007378, "actor_loss": -92.44294660949707, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.203932762145996, "episode_reward": 810.6419331491284, "step": 37000}
{"episode": 38.0, "batch_reward": 0.8054552338719368, "actor_loss": -92.58660404968262, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.464966535568237, "episode_reward": 878.0036035786147, "step": 38000}
{"episode": 39.0, "batch_reward": 0.8057091474533081, "actor_loss": -92.61255097961426, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.27138662338257, "episode_reward": 943.7135864180485, "step": 39000}
{"episode": 40.0, "batch_reward": 0.809624983727932, "actor_loss": -92.69846578979492, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.0451979637146, "episode_reward": 912.5534944742054, "step": 40000}
{"episode": 41.0, "batch_reward": 0.8113118680119514, "actor_loss": -92.74989460754395, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 44.273082971572876, "episode_reward": 861.7793680558843, "step": 41000}
{"episode": 42.0, "batch_reward": 0.8142529310584068, "actor_loss": -92.82303962707519, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.977500915527344, "episode_reward": 894.6696784601405, "step": 42000}
{"episode": 43.0, "batch_reward": 0.8137303609848022, "actor_loss": -92.87595640563964, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.009461641311646, "episode_reward": 759.3549700659864, "step": 43000}
{"episode": 44.0, "batch_reward": 0.813830650985241, "actor_loss": -92.83277998352051, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.75110411643982, "episode_reward": 867.696278781001, "step": 44000}
{"episode": 45.0, "batch_reward": 0.8140055962204933, "actor_loss": -92.83758204650879, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.276251554489136, "episode_reward": 857.3987107308648, "step": 45000}
{"episode": 46.0, "batch_reward": 0.8171964904665947, "actor_loss": -92.97139442443847, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.03953790664673, "episode_reward": 905.3090870549261, "step": 46000}
{"episode": 47.0, "batch_reward": 0.8160938372612, "actor_loss": -93.00073590087891, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.426417350769043, "episode_reward": 725.8616659400917, "step": 47000}
{"episode": 48.0, "batch_reward": 0.8147122392058372, "actor_loss": -92.99711267089843, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.951749801635742, "episode_reward": 839.8360406838426, "step": 48000}
{"episode": 49.0, "batch_reward": 0.8178694860339165, "actor_loss": -93.02051252746583, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.300842761993408, "episode_reward": 897.9961363494357, "step": 49000}
{"episode": 50.0, "batch_reward": 0.8200853688120842, "actor_loss": -93.10857781982422, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.34439730644226, "episode_reward": 867.8290561712705, "step": 50000}
{"episode": 51.0, "batch_reward": 0.8201510283350945, "actor_loss": -93.09846954345703, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 44.12300109863281, "episode_reward": 969.9757687594239, "step": 51000}
{"episode": 52.0, "batch_reward": 0.8223882784843445, "actor_loss": -93.09418205261231, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.563537120819092, "episode_reward": 952.0416380579824, "step": 52000}
{"episode": 53.0, "batch_reward": 0.8236775025129318, "actor_loss": -93.22460090637207, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.165117979049683, "episode_reward": 932.421386156894, "step": 53000}
{"episode": 54.0, "batch_reward": 0.8262759039402008, "actor_loss": -93.19510348510742, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.18535304069519, "episode_reward": 721.7166843508721, "step": 54000}
{"episode": 55.0, "batch_reward": 0.8255135982632636, "actor_loss": -93.24437055969238, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.37771987915039, "episode_reward": 917.0810596357966, "step": 55000}
{"episode": 56.0, "batch_reward": 0.8257114517688752, "actor_loss": -93.26963385009766, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.064039707183838, "episode_reward": 817.1280177492984, "step": 56000}
{"episode": 57.0, "batch_reward": 0.8272982512116432, "actor_loss": -93.28078472900391, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.480318069458008, "episode_reward": 865.323481808534, "step": 57000}
{"episode": 58.0, "batch_reward": 0.8286952428817749, "actor_loss": -93.33719456481934, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.95120859146118, "episode_reward": 973.1738624526919, "step": 58000}
{"episode": 59.0, "batch_reward": 0.8313352845907211, "actor_loss": -93.34838125610352, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.132376432418823, "episode_reward": 896.9854901002165, "step": 59000}
{"episode": 60.0, "batch_reward": 0.8321277350187302, "actor_loss": -93.38010192871094, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.957521438598633, "episode_reward": 916.0245608262442, "step": 60000}
{"episode": 61.0, "batch_reward": 0.8327793952226639, "actor_loss": -93.35805032348632, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 44.698861837387085, "episode_reward": 908.2831630206773, "step": 61000}
{"episode": 62.0, "batch_reward": 0.8332078924775124, "actor_loss": -93.40887359619141, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.22479009628296, "episode_reward": 957.6276543522071, "step": 62000}
{"episode": 63.0, "batch_reward": 0.833795035123825, "actor_loss": -93.41998695373535, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.336989164352417, "episode_reward": 832.3083033882424, "step": 63000}
{"episode": 64.0, "batch_reward": 0.8348887233138085, "actor_loss": -93.50546423339844, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.30255913734436, "episode_reward": 917.0681068488035, "step": 64000}
{"episode": 65.0, "batch_reward": 0.8358747831583023, "actor_loss": -93.54242500305176, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.49547553062439, "episode_reward": 857.1785084999715, "step": 65000}
{"episode": 66.0, "batch_reward": 0.8368948156833649, "actor_loss": -93.51283253479004, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.95842480659485, "episode_reward": 901.5234451510132, "step": 66000}
{"episode": 67.0, "batch_reward": 0.8378187716603279, "actor_loss": -93.56931825256348, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.000516414642334, "episode_reward": 929.2129094043722, "step": 67000}
{"episode": 68.0, "batch_reward": 0.838842390358448, "actor_loss": -93.5999645690918, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.836594343185425, "episode_reward": 946.8216991055415, "step": 68000}
{"episode": 69.0, "batch_reward": 0.8408738654851914, "actor_loss": -93.63628300476074, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.428252458572388, "episode_reward": 915.8428480625686, "step": 69000}
{"episode": 70.0, "batch_reward": 0.8425182014107704, "actor_loss": -93.70783306884766, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.082263231277466, "episode_reward": 868.2265293443431, "step": 70000}
{"episode": 71.0, "batch_reward": 0.841750350356102, "actor_loss": -93.6487486114502, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 43.87648391723633, "episode_reward": 872.9527081318462, "step": 71000}
{"episode": 72.0, "batch_reward": 0.8440845701694488, "actor_loss": -93.76918641662597, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.350016117095947, "episode_reward": 927.249705491414, "step": 72000}
{"episode": 73.0, "batch_reward": 0.8428175064325333, "actor_loss": -93.68795013427734, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.96693205833435, "episode_reward": 918.1596495718734, "step": 73000}
{"episode": 74.0, "batch_reward": 0.8452273043394088, "actor_loss": -93.73082734680176, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.857980966567993, "episode_reward": 932.0426145816981, "step": 74000}
{"episode": 75.0, "batch_reward": 0.8480203642845153, "actor_loss": -93.8064881591797, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.15831470489502, "episode_reward": 901.2618027735648, "step": 75000}
{"episode": 76.0, "batch_reward": 0.846575072824955, "actor_loss": -93.72073582458496, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.09336757659912, "episode_reward": 860.0354737651479, "step": 76000}
{"episode": 77.0, "batch_reward": 0.8485237230658531, "actor_loss": -93.84838020324707, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.31386947631836, "episode_reward": 942.0027356940652, "step": 77000}
{"episode": 78.0, "batch_reward": 0.8475073805451393, "actor_loss": -93.79468659973145, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.6992130279541, "episode_reward": 954.9033850468816, "step": 78000}
{"episode": 79.0, "batch_reward": 0.8505642790794372, "actor_loss": -93.82642419433594, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.04956579208374, "episode_reward": 894.8386713058777, "step": 79000}
{"episode": 80.0, "batch_reward": 0.8498642854094506, "actor_loss": -93.84354542541504, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.39008378982544, "episode_reward": 818.6026239583789, "step": 80000}
{"episode": 81.0, "batch_reward": 0.8491499230861664, "actor_loss": -93.84827098083495, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 42.981205701828, "episode_reward": 891.6556544304403, "step": 81000}
{"episode": 82.0, "batch_reward": 0.8507659155726432, "actor_loss": -93.90568676757813, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.119439363479614, "episode_reward": 876.9800071704377, "step": 82000}
{"episode": 83.0, "batch_reward": 0.8496248181462288, "actor_loss": -93.8259308013916, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.47442626953125, "episode_reward": 921.0427590672818, "step": 83000}
{"episode": 84.0, "batch_reward": 0.8526766352057457, "actor_loss": -93.96021495056152, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.40744161605835, "episode_reward": 983.2898859666029, "step": 84000}
{"episode": 85.0, "batch_reward": 0.851845134973526, "actor_loss": -93.91194352722168, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.806994676589966, "episode_reward": 873.6204012709347, "step": 85000}
{"episode": 86.0, "batch_reward": 0.8513000814914703, "actor_loss": -93.91666539001464, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.986302614212036, "episode_reward": 685.8943688868834, "step": 86000}
{"episode": 87.0, "batch_reward": 0.8503796813488007, "actor_loss": -93.88974076843262, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.084074020385742, "episode_reward": 805.5315110057294, "step": 87000}
{"episode": 88.0, "batch_reward": 0.8495381556153297, "actor_loss": -93.90025987243652, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.32885718345642, "episode_reward": 730.683129973174, "step": 88000}
{"episode": 89.0, "batch_reward": 0.8490460600256919, "actor_loss": -93.90627577209473, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.31224298477173, "episode_reward": 938.5191787905006, "step": 89000}
{"episode": 90.0, "batch_reward": 0.8494471799135208, "actor_loss": -93.85239213562012, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.402764320373535, "episode_reward": 720.1414808200553, "step": 90000}
{"episode": 91.0, "batch_reward": 0.8492224506735802, "actor_loss": -93.81226686096191, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 42.67780685424805, "episode_reward": 919.3296983464178, "step": 91000}
{"episode": 92.0, "batch_reward": 0.8499286989569664, "actor_loss": -93.87885218811036, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.20142388343811, "episode_reward": 723.2346091823277, "step": 92000}
{"episode": 93.0, "batch_reward": 0.8491744232177735, "actor_loss": -93.8480284576416, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.62654972076416, "episode_reward": 923.2903622053738, "step": 93000}
{"episode": 94.0, "batch_reward": 0.8474323695898056, "actor_loss": -93.78167016601563, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.55331039428711, "episode_reward": 778.1438633178964, "step": 94000}
{"episode": 95.0, "batch_reward": 0.8476483824253083, "actor_loss": -93.82013160705566, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.11023449897766, "episode_reward": 861.7152137248429, "step": 95000}
{"episode": 96.0, "batch_reward": 0.8467035014033317, "actor_loss": -93.76043347167969, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.28350257873535, "episode_reward": 806.511793344257, "step": 96000}
{"episode": 97.0, "batch_reward": 0.8477922891378402, "actor_loss": -93.76496340942383, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.245727062225342, "episode_reward": 841.0277736038473, "step": 97000}
{"episode": 98.0, "batch_reward": 0.8490541575551033, "actor_loss": -93.80496464538574, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.980377435684204, "episode_reward": 873.4508149865231, "step": 98000}
{"episode": 99.0, "batch_reward": 0.8485362857580185, "actor_loss": -93.87084103393555, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.558286905288696, "episode_reward": 945.312900818083, "step": 99000}
{"episode": 100.0, "batch_reward": 0.848260399878025, "actor_loss": -93.84460777282715, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.56037664413452, "episode_reward": 862.5344699287605, "step": 100000}
{"episode": 101.0, "batch_reward": 0.8523355909585952, "actor_loss": -93.87254647827149, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 42.25296998023987, "episode_reward": 911.937087981215, "step": 101000}
{"episode": 102.0, "batch_reward": 0.8502548647522926, "actor_loss": -93.86620196533202, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.623443126678467, "episode_reward": 841.0349702372395, "step": 102000}
{"episode": 103.0, "batch_reward": 0.8494001023173332, "actor_loss": -93.84320031738281, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.128544330596924, "episode_reward": 859.0536805219956, "step": 103000}
{"episode": 104.0, "batch_reward": 0.8490856034159661, "actor_loss": -93.86230192565918, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.157512187957764, "episode_reward": 945.6935831363201, "step": 104000}
{"episode": 105.0, "batch_reward": 0.8507369098067283, "actor_loss": -93.86815760803222, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.902652978897095, "episode_reward": 874.3669244116749, "step": 105000}
{"episode": 106.0, "batch_reward": 0.851652378320694, "actor_loss": -93.87933627319336, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.342926025390625, "episode_reward": 938.5987421606341, "step": 106000}
{"episode": 107.0, "batch_reward": 0.8503280785083771, "actor_loss": -93.86513934326172, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.871241092681885, "episode_reward": 773.4198977640835, "step": 107000}
{"episode": 108.0, "batch_reward": 0.8503850657939911, "actor_loss": -93.86116627502442, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.79852271080017, "episode_reward": 854.409020134444, "step": 108000}
{"episode": 109.0, "batch_reward": 0.8523189210891724, "actor_loss": -93.85981118774414, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.264468908309937, "episode_reward": 897.6711613947713, "step": 109000}
{"episode": 110.0, "batch_reward": 0.8510560759305954, "actor_loss": -93.89296003723145, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.324551582336426, "episode_reward": 853.9926232042537, "step": 110000}
{"episode": 111.0, "batch_reward": 0.8511054084897042, "actor_loss": -93.82719752502442, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 41.64294719696045, "episode_reward": 820.610267677881, "step": 111000}
{"episode": 112.0, "batch_reward": 0.85101398396492, "actor_loss": -93.7585044708252, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.393676280975342, "episode_reward": 895.9552241675623, "step": 112000}
{"episode": 113.0, "batch_reward": 0.8513294554948807, "actor_loss": -93.85125067138672, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.207653760910034, "episode_reward": 852.6389779571517, "step": 113000}
{"episode": 114.0, "batch_reward": 0.8524667288661003, "actor_loss": -93.88323141479492, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.910993814468384, "episode_reward": 966.7715332087396, "step": 114000}
{"episode": 115.0, "batch_reward": 0.8534987881183624, "actor_loss": -93.90139489746093, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.016931533813477, "episode_reward": 739.0086907365504, "step": 115000}
{"episode": 116.0, "batch_reward": 0.8518641008734703, "actor_loss": -93.82261772155762, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.330350875854492, "episode_reward": 927.3393694332333, "step": 116000}
{"episode": 117.0, "batch_reward": 0.8520242931246758, "actor_loss": -93.83891780090332, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.872563123703003, "episode_reward": 776.4057159484285, "step": 117000}
{"episode": 118.0, "batch_reward": 0.8520850193500519, "actor_loss": -93.8654471130371, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.512460947036743, "episode_reward": 881.9816142360822, "step": 118000}
{"episode": 119.0, "batch_reward": 0.8519018364548683, "actor_loss": -93.88104582214355, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.215744256973267, "episode_reward": 923.9330200216967, "step": 119000}
{"episode": 120.0, "batch_reward": 0.8525046680569649, "actor_loss": -93.93851344299317, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.293743133544922, "episode_reward": 896.0986570016873, "step": 120000}
{"episode": 121.0, "batch_reward": 0.8510250790715218, "actor_loss": -93.89407788085937, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 40.92165923118591, "episode_reward": 874.3497084399635, "step": 121000}
{"episode": 122.0, "batch_reward": 0.8528823605179787, "actor_loss": -93.91946774291992, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 20.996968746185303, "episode_reward": 751.523427051752, "step": 122000}
{"episode": 123.0, "batch_reward": 0.8509457357525826, "actor_loss": -93.84953427124023, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.037344217300415, "episode_reward": 810.5454475419757, "step": 123000}
{"episode": 124.0, "batch_reward": 0.852090591609478, "actor_loss": -93.82769354248047, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.306480407714844, "episode_reward": 883.1750792399963, "step": 124000}
{"episode": 125.0, "batch_reward": 0.8529831602573394, "actor_loss": -93.87119590759278, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.144458532333374, "episode_reward": 855.001179323565, "step": 125000}
{"episode": 126.0, "batch_reward": 0.8533115125894547, "actor_loss": -93.84371894836426, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.27617120742798, "episode_reward": 938.8732174921215, "step": 126000}
{"episode": 127.0, "batch_reward": 0.8529208501577378, "actor_loss": -93.89724870300293, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.11894202232361, "episode_reward": 852.3839800286463, "step": 127000}
{"episode": 128.0, "batch_reward": 0.8513746808767318, "actor_loss": -93.84274891662598, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.344057083129883, "episode_reward": 861.1268864778737, "step": 128000}
{"episode": 129.0, "batch_reward": 0.8528018237948418, "actor_loss": -93.8472982788086, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.72240710258484, "episode_reward": 892.3939468514908, "step": 129000}
{"episode": 130.0, "batch_reward": 0.8542839630842208, "actor_loss": -93.87064907836914, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.429710388183594, "episode_reward": 921.6357376769301, "step": 130000}
{"episode": 131.0, "batch_reward": 0.8542325230836868, "actor_loss": -93.87852816772461, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 40.46618986129761, "episode_reward": 882.3405310354354, "step": 131000}
{"episode": 132.0, "batch_reward": 0.8557656053304672, "actor_loss": -93.9624330291748, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.24414348602295, "episode_reward": 937.8778334045574, "step": 132000}
{"episode": 133.0, "batch_reward": 0.8553120222091675, "actor_loss": -94.0068228149414, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.276710033416748, "episode_reward": 840.1588225799001, "step": 133000}
{"episode": 134.0, "batch_reward": 0.85288930529356, "actor_loss": -93.8771372680664, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.459760904312134, "episode_reward": 919.129704294509, "step": 134000}
{"episode": 135.0, "batch_reward": 0.8553452340364456, "actor_loss": -93.90022149658203, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.644603967666626, "episode_reward": 968.7616195088016, "step": 135000}
{"episode": 136.0, "batch_reward": 0.8566737894415856, "actor_loss": -93.97619563293458, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.51972770690918, "episode_reward": 954.6279152243526, "step": 136000}
{"episode": 137.0, "batch_reward": 0.8557920738458633, "actor_loss": -93.98181661987304, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.156890630722046, "episode_reward": 898.598957218797, "step": 137000}
{"episode": 138.0, "batch_reward": 0.8569862129092216, "actor_loss": -93.99033586120605, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.659618139266968, "episode_reward": 896.2230491468587, "step": 138000}
{"episode": 139.0, "batch_reward": 0.8566802775859833, "actor_loss": -93.99930561828613, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 21.87346863746643, "episode_reward": 853.4670504673072, "step": 139000}
{"episode": 140.0, "batch_reward": 0.8561152412295342, "actor_loss": -93.9614326019287, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.118218183517456, "episode_reward": 710.5586318768993, "step": 140000}
{"episode": 141.0, "batch_reward": 0.8576938878893853, "actor_loss": -94.00135974121093, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 37.13217520713806, "episode_reward": 909.0636991152516, "step": 141000}
{"episode": 142.0, "batch_reward": 0.8571454588770866, "actor_loss": -93.97299726867676, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 16.858754873275757, "episode_reward": 977.6644471314112, "step": 142000}
{"episode": 143.0, "batch_reward": 0.8566570407748222, "actor_loss": -93.95056228637695, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.854299783706665, "episode_reward": 761.3907222643248, "step": 143000}
{"episode": 144.0, "batch_reward": 0.8575258132815361, "actor_loss": -94.03535917663574, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.598454236984253, "episode_reward": 920.8061431185525, "step": 144000}
{"episode": 145.0, "batch_reward": 0.8582661569714546, "actor_loss": -94.03053244018555, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.03178119659424, "episode_reward": 958.9359437957312, "step": 145000}
{"episode": 146.0, "batch_reward": 0.8587299242019654, "actor_loss": -94.08245245361329, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.074204444885254, "episode_reward": 951.4969758791901, "step": 146000}
{"episode": 147.0, "batch_reward": 0.8577194575071335, "actor_loss": -94.04540100097657, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 22.281617879867554, "episode_reward": 874.5907800747879, "step": 147000}
{"episode": 148.0, "batch_reward": 0.8590829562544823, "actor_loss": -94.05200941467285, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.667026042938232, "episode_reward": 982.2963704122958, "step": 148000}
{"episode": 149.0, "batch_reward": 0.8608498421907425, "actor_loss": -94.07831147766113, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "duration": 19.349913358688354, "episode_reward": 905.790803565963, "step": 149000}
{"episode": 150.0, "batch_reward": 0.8595976550579071, "actor_loss": -94.13739811706543, "actor_target_entropy": -6.0, "alpha_value": 0.003318584615511434, "step": 150000}
