{"episode": 1.0, "duration": 15.756237745285034, "episode_reward": 58.61337562337205, "step": 1000}
{"episode": 2.0, "duration": 1.3989739418029785, "episode_reward": 632.427646246406, "step": 2000}
{"episode": 3.0, "duration": 1.36506986618042, "episode_reward": 603.104995919503, "step": 3000}
{"episode": 4.0, "duration": 1.384953260421753, "episode_reward": 685.8214873689033, "step": 4000}
{"episode": 5.0, "duration": 1.3725204467773438, "episode_reward": 662.1010530184953, "step": 5000}
{"episode": 6.0, "batch_reward": 0.5271158297150821, "actor_loss": -73.95721141126273, "actor_target_entropy": -6.0, "alpha_value": 0.004365629073062549, "duration": 360.7606158256531, "episode_reward": 236.9462451475943, "step": 6000}
{"episode": 7.0, "batch_reward": 0.4526822000443935, "actor_loss": -70.61919610595703, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.760812520980835, "episode_reward": 147.69139491650697, "step": 7000}
{"episode": 8.0, "batch_reward": 0.4201824907064438, "actor_loss": -69.49747512817383, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.911247730255127, "episode_reward": 168.25672180110985, "step": 8000}
{"episode": 9.0, "batch_reward": 0.38393008649349214, "actor_loss": -68.52625245666503, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.957392454147339, "episode_reward": 203.15980121544249, "step": 9000}
{"episode": 10.0, "batch_reward": 0.36869312095642087, "actor_loss": -63.999051055908204, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 4900.444657087326, "episode_reward": 168.92699377731745, "step": 10000}
{"episode": 11.0, "batch_reward": 0.363417333394289, "actor_loss": -64.07112796783447, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.183175086975098, "episode_reward": 462.14069650765623, "step": 11000}
{"episode": 12.0, "batch_reward": 0.36387727949023246, "actor_loss": -61.26172539520264, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.4747231006622, "episode_reward": 275.1413291105401, "step": 12000}
{"episode": 13.0, "batch_reward": 0.3481805736720562, "actor_loss": -61.63302975463867, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.53872013092041, "episode_reward": 192.08756872485182, "step": 13000}
{"episode": 14.0, "batch_reward": 0.3470249156206846, "actor_loss": -60.18268476104736, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 533.3857262134552, "episode_reward": 311.54507472346745, "step": 14000}
{"episode": 15.0, "batch_reward": 0.3476975163221359, "actor_loss": -60.62478058624268, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.864418029785156, "episode_reward": 441.1979481665858, "step": 15000}
{"episode": 16.0, "batch_reward": 0.3529915650486946, "actor_loss": -59.59007680511475, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.0684516429901, "episode_reward": 534.8857947651811, "step": 16000}
{"episode": 17.0, "batch_reward": 0.3686467626988888, "actor_loss": -60.3274965133667, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.85027813911438, "episode_reward": 731.1275468856819, "step": 17000}
{"episode": 18.0, "batch_reward": 0.3744935106933117, "actor_loss": -60.06200511932373, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.940426826477, "episode_reward": 20.792806617479798, "step": 18000}
{"episode": 19.0, "batch_reward": 0.35485003632307055, "actor_loss": -59.23969593811035, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.90024185180664, "episode_reward": 21.667722682846108, "step": 19000}
{"episode": 20.0, "batch_reward": 0.3510061812996864, "actor_loss": -59.0592504196167, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.2832005023956, "episode_reward": 642.0101244470028, "step": 20000}
{"episode": 21.0, "batch_reward": 0.3639376236051321, "actor_loss": -59.56263786315918, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 29.848872661590576, "episode_reward": 597.2726271552456, "step": 21000}
{"episode": 22.0, "batch_reward": 0.37297425447404386, "actor_loss": -58.25797145080566, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.3006265163422, "episode_reward": 422.37466170963484, "step": 22000}
{"episode": 23.0, "batch_reward": 0.3795329000353813, "actor_loss": -58.488073287963864, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.715990543365479, "episode_reward": 655.099031830463, "step": 23000}
{"episode": 24.0, "batch_reward": 0.3884376280605793, "actor_loss": -58.707204887390134, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.243105173111, "episode_reward": 580.3013776445854, "step": 24000}
{"episode": 25.0, "batch_reward": 0.39990768596529963, "actor_loss": -59.355074432373044, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.543900966644287, "episode_reward": 702.3062931321243, "step": 25000}
{"episode": 26.0, "batch_reward": 0.4104533041119576, "actor_loss": -59.369951721191406, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 528.9685096740723, "episode_reward": 752.4432343281477, "step": 26000}
{"episode": 27.0, "batch_reward": 0.4190424241125584, "actor_loss": -59.82963874053955, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.58841586112976, "episode_reward": 654.9921144022646, "step": 27000}
{"episode": 28.0, "batch_reward": 0.43345937687158587, "actor_loss": -60.261524604797366, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.4952750205994, "episode_reward": 633.0224730273609, "step": 28000}
{"episode": 29.0, "batch_reward": 0.44034369322657585, "actor_loss": -60.64055242919922, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.654093027114868, "episode_reward": 723.0429578847703, "step": 29000}
{"episode": 30.0, "batch_reward": 0.45029291784763337, "actor_loss": -58.614179260253906, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.5490937232971, "episode_reward": 728.5006829008421, "step": 30000}
{"episode": 31.0, "batch_reward": 0.4611762607097626, "actor_loss": -59.25874160003662, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.014644145965576, "episode_reward": 735.9701491150597, "step": 31000}
{"episode": 32.0, "batch_reward": 0.466902288377285, "actor_loss": -58.80986737823486, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 528.5973646640778, "episode_reward": 634.3700734364326, "step": 32000}
{"episode": 33.0, "batch_reward": 0.4757865425646305, "actor_loss": -59.204287170410154, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.865196704864502, "episode_reward": 802.5372953100012, "step": 33000}
{"episode": 34.0, "batch_reward": 0.4819230390787125, "actor_loss": -58.41538696289062, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 528.7886481285095, "episode_reward": 695.8225955684247, "step": 34000}
{"episode": 35.0, "batch_reward": 0.48788676255941393, "actor_loss": -58.83142338562012, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.621500492095947, "episode_reward": 721.2891890213103, "step": 35000}
{"episode": 36.0, "batch_reward": 0.4972128726243973, "actor_loss": -60.67823179626465, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.8957886695862, "episode_reward": 820.1093279589448, "step": 36000}
{"episode": 37.0, "batch_reward": 0.503328890234232, "actor_loss": -60.9410842666626, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.73419737815857, "episode_reward": 726.275640491144, "step": 37000}
{"episode": 38.0, "batch_reward": 0.5132083191275597, "actor_loss": -59.66520619964599, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.5033230781555, "episode_reward": 780.8731043292536, "step": 38000}
{"episode": 39.0, "batch_reward": 0.5145781030356884, "actor_loss": -59.79710074615478, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.88379168510437, "episode_reward": 707.1140233850972, "step": 39000}
{"episode": 40.0, "batch_reward": 0.5182595183253288, "actor_loss": -59.85660334777832, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.7320444583893, "episode_reward": 563.5706655197196, "step": 40000}
{"episode": 41.0, "batch_reward": 0.5237542127370834, "actor_loss": -60.38072975158691, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.044183492660522, "episode_reward": 782.3062293430497, "step": 41000}
{"episode": 42.0, "batch_reward": 0.5277257865965367, "actor_loss": -60.66639493560791, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 521.7124679088593, "episode_reward": 687.7416567803664, "step": 42000}
{"episode": 43.0, "batch_reward": 0.5339599807858467, "actor_loss": -60.96170363616943, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.640554189682007, "episode_reward": 707.0860246719438, "step": 43000}
{"episode": 44.0, "batch_reward": 0.538267567962408, "actor_loss": -62.17536039733887, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.0114703178406, "episode_reward": 750.7070997860549, "step": 44000}
{"episode": 45.0, "batch_reward": 0.5422335702478885, "actor_loss": -62.49770518493652, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.81897783279419, "episode_reward": 793.1234534969158, "step": 45000}
{"episode": 46.0, "batch_reward": 0.5453144566714764, "actor_loss": -62.36779505157471, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.3601861000061, "episode_reward": 795.1578643791113, "step": 46000}
{"episode": 47.0, "batch_reward": 0.5523195869624614, "actor_loss": -62.609224983215334, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.730641603469849, "episode_reward": 794.3356980847589, "step": 47000}
{"episode": 48.0, "batch_reward": 0.5567723841369152, "actor_loss": -63.081161933898926, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.5184075832367, "episode_reward": 845.5149596848316, "step": 48000}
{"episode": 49.0, "batch_reward": 0.565461612045765, "actor_loss": -63.45524473571778, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.951872110366821, "episode_reward": 732.2052652843784, "step": 49000}
{"episode": 50.0, "batch_reward": 0.5664129649698735, "actor_loss": -63.70898960876465, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.6651129722595, "episode_reward": 805.173254750517, "step": 50000}
{"episode": 51.0, "batch_reward": 0.5719366124272347, "actor_loss": -63.95135753631592, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.338492155075073, "episode_reward": 807.087890608289, "step": 51000}
{"episode": 52.0, "batch_reward": 0.5765388726592063, "actor_loss": -63.3149235458374, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.217523097992, "episode_reward": 745.8366250146772, "step": 52000}
{"episode": 53.0, "batch_reward": 0.5807777978777885, "actor_loss": -63.52918817901611, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.858661413192749, "episode_reward": 722.5459537483481, "step": 53000}
{"episode": 54.0, "batch_reward": 0.5825235019028187, "actor_loss": -64.75131716156005, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 528.6758584976196, "episode_reward": 783.4523549638604, "step": 54000}
{"episode": 55.0, "batch_reward": 0.5862587185800076, "actor_loss": -64.96063439178467, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.851404905319214, "episode_reward": 837.2029742512268, "step": 55000}
{"episode": 56.0, "batch_reward": 0.5897948454618454, "actor_loss": -65.70552392578125, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.0011923313141, "episode_reward": 764.1920884416651, "step": 56000}
{"episode": 57.0, "batch_reward": 0.5946867910921574, "actor_loss": -65.91662567138673, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.090083599090576, "episode_reward": 779.8638493842725, "step": 57000}
{"episode": 58.0, "batch_reward": 0.5993260816037654, "actor_loss": -66.75906502532959, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 533.1830093860626, "episode_reward": 857.172595316211, "step": 58000}
{"episode": 59.0, "batch_reward": 0.6015680369138717, "actor_loss": -66.82387001037597, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.11540126800537, "episode_reward": 783.5318026929712, "step": 59000}
{"episode": 60.0, "batch_reward": 0.6057316070199013, "actor_loss": -66.39630922698974, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.0099222660065, "episode_reward": 872.2992820244771, "step": 60000}
{"episode": 61.0, "batch_reward": 0.6110954678058624, "actor_loss": -66.58150652313232, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.181833505630493, "episode_reward": 833.6128355341663, "step": 61000}
{"episode": 62.0, "batch_reward": 0.6112594273686409, "actor_loss": -67.02608763122558, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.3120176792145, "episode_reward": 796.4536874979026, "step": 62000}
{"episode": 63.0, "batch_reward": 0.6156926975250244, "actor_loss": -67.15666035461426, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.793702602386475, "episode_reward": 783.3368027372981, "step": 63000}
{"episode": 64.0, "batch_reward": 0.6202921427488327, "actor_loss": -67.75112211608887, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 533.1607434749603, "episode_reward": 845.8460435375988, "step": 64000}
{"episode": 65.0, "batch_reward": 0.6236362223029137, "actor_loss": -67.87177250671387, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.725977420806885, "episode_reward": 817.8534760890082, "step": 65000}
{"episode": 66.0, "batch_reward": 0.6246423505544663, "actor_loss": -67.5619274597168, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.3364460468292, "episode_reward": 851.9501049855912, "step": 66000}
{"episode": 67.0, "batch_reward": 0.6281168726682663, "actor_loss": -67.77828858947754, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.28665590286255, "episode_reward": 793.848063818744, "step": 67000}
{"episode": 68.0, "batch_reward": 0.6319922501444817, "actor_loss": -68.23796940612793, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.2674188613892, "episode_reward": 805.7813066121266, "step": 68000}
{"episode": 69.0, "batch_reward": 0.6328593806028366, "actor_loss": -68.29558309936523, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.737220525741577, "episode_reward": 814.2999810539461, "step": 69000}
{"episode": 70.0, "batch_reward": 0.6362156480550766, "actor_loss": -68.31695181274414, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 528.4818651676178, "episode_reward": 833.9741146613316, "step": 70000}
{"episode": 71.0, "batch_reward": 0.6378661210536957, "actor_loss": -68.44714135742187, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 29.87129044532776, "episode_reward": 744.5590474631904, "step": 71000}
{"episode": 72.0, "batch_reward": 0.6381085314154625, "actor_loss": -68.29128889465332, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.9138851165771, "episode_reward": 754.6766516749154, "step": 72000}
{"episode": 73.0, "batch_reward": 0.6419454960227012, "actor_loss": -68.44701266479493, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.731528043746948, "episode_reward": 775.6883282105724, "step": 73000}
{"episode": 74.0, "batch_reward": 0.6445267233848572, "actor_loss": -68.60304223632812, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.8763344287872, "episode_reward": 842.508956483002, "step": 74000}
{"episode": 75.0, "batch_reward": 0.6456958360671997, "actor_loss": -68.72821237182617, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.936983585357666, "episode_reward": 808.4734810961203, "step": 75000}
{"episode": 76.0, "batch_reward": 0.6472563669085503, "actor_loss": -68.83967483520507, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.5179512500763, "episode_reward": 766.0385154965711, "step": 76000}
{"episode": 77.0, "batch_reward": 0.6495472459793091, "actor_loss": -68.93986891174316, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.688360452651978, "episode_reward": 806.0733209154927, "step": 77000}
{"episode": 78.0, "batch_reward": 0.6520579170584678, "actor_loss": -69.07999961853028, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.2094068527222, "episode_reward": 809.8696492209043, "step": 78000}
{"episode": 79.0, "batch_reward": 0.6547357720136643, "actor_loss": -69.19756195068359, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.742547750473022, "episode_reward": 843.4213001049073, "step": 79000}
{"episode": 80.0, "batch_reward": 0.6552241191864013, "actor_loss": -69.14207852172852, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.3137757778168, "episode_reward": 813.9312614361834, "step": 80000}
{"episode": 81.0, "batch_reward": 0.6575264023542404, "actor_loss": -69.27532135009766, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 29.95063352584839, "episode_reward": 783.6549911595548, "step": 81000}
{"episode": 82.0, "batch_reward": 0.6592502798438072, "actor_loss": -69.11829463195801, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.4685750007629, "episode_reward": 816.6257294156911, "step": 82000}
{"episode": 83.0, "batch_reward": 0.6611198943853378, "actor_loss": -69.1937629852295, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.072555541992188, "episode_reward": 817.2435727250921, "step": 83000}
{"episode": 84.0, "batch_reward": 0.6630048370361328, "actor_loss": -68.98773629760743, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.082380771637, "episode_reward": 868.2703207557859, "step": 84000}
{"episode": 85.0, "batch_reward": 0.6669212815761566, "actor_loss": -69.11194068908691, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.900544166564941, "episode_reward": 842.1911279457099, "step": 85000}
{"episode": 86.0, "batch_reward": 0.6684362155199051, "actor_loss": -69.11029997253418, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.5563263893127, "episode_reward": 789.936706617254, "step": 86000}
{"episode": 87.0, "batch_reward": 0.66962317943573, "actor_loss": -69.22398620605469, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.110296964645386, "episode_reward": 853.8098569278671, "step": 87000}
{"episode": 88.0, "batch_reward": 0.6728151786327362, "actor_loss": -69.22406098937988, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.9101965427399, "episode_reward": 868.8171151847703, "step": 88000}
{"episode": 89.0, "batch_reward": 0.6729527465105056, "actor_loss": -69.29386614990234, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.087663650512695, "episode_reward": 796.143789433803, "step": 89000}
{"episode": 90.0, "batch_reward": 0.6742413106560707, "actor_loss": -69.28259632873535, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.7066421508789, "episode_reward": 734.4398035173199, "step": 90000}
{"episode": 91.0, "batch_reward": 0.676633740246296, "actor_loss": -69.29880030822754, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.545602798461914, "episode_reward": 719.3165643094438, "step": 91000}
{"episode": 92.0, "batch_reward": 0.6755065752267837, "actor_loss": -69.3469382019043, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 520.5305898189545, "episode_reward": 840.2533329521787, "step": 92000}
{"episode": 93.0, "batch_reward": 0.6780543620586396, "actor_loss": -69.36624348449708, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.06804609298706, "episode_reward": 798.9345811832407, "step": 93000}
{"episode": 94.0, "batch_reward": 0.6798285650610923, "actor_loss": -69.38642515563964, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 528.5648503303528, "episode_reward": 809.9819393191026, "step": 94000}
{"episode": 95.0, "batch_reward": 0.6790038458704949, "actor_loss": -69.36518888854981, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.763813257217407, "episode_reward": 738.0922036154284, "step": 95000}
{"episode": 96.0, "batch_reward": 0.6823550419807434, "actor_loss": -69.32042100524902, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.952241897583, "episode_reward": 818.5581610150248, "step": 96000}
{"episode": 97.0, "batch_reward": 0.6831863438487052, "actor_loss": -69.25575067138672, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.978511571884155, "episode_reward": 793.2954628985026, "step": 97000}
{"episode": 98.0, "batch_reward": 0.684363873064518, "actor_loss": -69.64310084533692, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.0455403327942, "episode_reward": 788.5099659236321, "step": 98000}
{"episode": 99.0, "batch_reward": 0.6859077360630036, "actor_loss": -69.7167052154541, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.29740262031555, "episode_reward": 851.4340009914108, "step": 99000}
{"episode": 100.0, "batch_reward": 0.6867982676029205, "actor_loss": -69.46438116455079, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.491297006607, "episode_reward": 844.6565516174961, "step": 100000}
{"episode": 101.0, "batch_reward": 0.6868966727852821, "actor_loss": -69.4441096496582, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.302471160888672, "episode_reward": 807.0030858998731, "step": 101000}
{"episode": 102.0, "batch_reward": 0.6899033424854278, "actor_loss": -69.50758738708497, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.3757064342499, "episode_reward": 868.5826514235855, "step": 102000}
{"episode": 103.0, "batch_reward": 0.6928716000914573, "actor_loss": -69.57483055114746, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.04742932319641, "episode_reward": 877.1929150645335, "step": 103000}
{"episode": 104.0, "batch_reward": 0.6927392727136612, "actor_loss": -69.89255693054199, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.9638388156891, "episode_reward": 829.157438017679, "step": 104000}
{"episode": 105.0, "batch_reward": 0.6949684289097786, "actor_loss": -70.1086026763916, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.969676971435547, "episode_reward": 847.3646578123607, "step": 105000}
{"episode": 106.0, "batch_reward": 0.6963894166350365, "actor_loss": -70.10058357238769, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.6907517910004, "episode_reward": 833.0623979819592, "step": 106000}
{"episode": 107.0, "batch_reward": 0.6971994941830635, "actor_loss": -70.23388069152833, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.140379428863525, "episode_reward": 846.2453529185117, "step": 107000}
{"episode": 108.0, "batch_reward": 0.6970694546699524, "actor_loss": -70.50299920654297, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 533.7849485874176, "episode_reward": 833.4773143969742, "step": 108000}
{"episode": 109.0, "batch_reward": 0.6998941345810891, "actor_loss": -70.57544744873047, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.75013780593872, "episode_reward": 822.5685598586132, "step": 109000}
{"episode": 110.0, "batch_reward": 0.7017426111102104, "actor_loss": -70.48322241210937, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 529.965873003006, "episode_reward": 822.4775561151806, "step": 110000}
{"episode": 111.0, "batch_reward": 0.7008487492799759, "actor_loss": -70.5154487915039, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.002859115600586, "episode_reward": 786.1106766826425, "step": 111000}
{"episode": 112.0, "batch_reward": 0.7028816848993301, "actor_loss": -70.27811112976075, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.7457361221313, "episode_reward": 791.9368482923844, "step": 112000}
{"episode": 113.0, "batch_reward": 0.7029563811421394, "actor_loss": -70.3120245513916, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.68553376197815, "episode_reward": 837.8819119350234, "step": 113000}
{"episode": 114.0, "batch_reward": 0.7059435351490975, "actor_loss": -70.5403930053711, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.36505818367, "episode_reward": 843.2910338241163, "step": 114000}
{"episode": 115.0, "batch_reward": 0.7064181344509125, "actor_loss": -70.60637313842773, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.156699657440186, "episode_reward": 864.5047368201829, "step": 115000}
{"episode": 116.0, "batch_reward": 0.7073920251727104, "actor_loss": -69.92774606323242, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.8738758563995, "episode_reward": 773.9354163816691, "step": 116000}
{"episode": 117.0, "batch_reward": 0.7096792836785316, "actor_loss": -70.15176570129394, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.82386565208435, "episode_reward": 780.8701503579379, "step": 117000}
{"episode": 118.0, "batch_reward": 0.7086442596912385, "actor_loss": -70.3170400390625, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.4217483997345, "episode_reward": 854.753071751807, "step": 118000}
{"episode": 119.0, "batch_reward": 0.7118985453248023, "actor_loss": -70.47045930480957, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.85769772529602, "episode_reward": 895.8746071858484, "step": 119000}
{"episode": 120.0, "batch_reward": 0.7105242930650711, "actor_loss": -70.27346786499024, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.2570652961731, "episode_reward": 818.7881454860039, "step": 120000}
{"episode": 121.0, "batch_reward": 0.7126463416218758, "actor_loss": -70.34048262023926, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.42075276374817, "episode_reward": 817.8793832507471, "step": 121000}
{"episode": 122.0, "batch_reward": 0.7127506830692292, "actor_loss": -70.07415797424316, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.4496490955353, "episode_reward": 774.529670957626, "step": 122000}
{"episode": 123.0, "batch_reward": 0.7128329497575759, "actor_loss": -70.12649447631836, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.838935136795044, "episode_reward": 714.6969413405923, "step": 123000}
{"episode": 124.0, "batch_reward": 0.714144752562046, "actor_loss": -70.72526420593262, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.9494831562042, "episode_reward": 826.1577152044379, "step": 124000}
{"episode": 125.0, "batch_reward": 0.7137084794640541, "actor_loss": -70.72067417907715, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.282240390777588, "episode_reward": 733.771948281353, "step": 125000}
{"episode": 126.0, "batch_reward": 0.7166960670351982, "actor_loss": -70.73400129699706, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 533.1273460388184, "episode_reward": 816.8313738426409, "step": 126000}
{"episode": 127.0, "batch_reward": 0.7165353953242302, "actor_loss": -70.75729501342774, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.80040955543518, "episode_reward": 840.259895833427, "step": 127000}
{"episode": 128.0, "batch_reward": 0.7156041892170906, "actor_loss": -70.97824195861817, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.9406633377075, "episode_reward": 839.5436413185593, "step": 128000}
{"episode": 129.0, "batch_reward": 0.7157031031250953, "actor_loss": -70.94589059448242, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.128497838974, "episode_reward": 25.844013300073886, "step": 129000}
{"episode": 130.0, "batch_reward": 0.7117056420445442, "actor_loss": -70.37813694763183, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.1909980773926, "episode_reward": 710.8486646370429, "step": 130000}
{"episode": 131.0, "batch_reward": 0.7125065577626228, "actor_loss": -70.47378134155274, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.106030225753784, "episode_reward": 757.6790919982159, "step": 131000}
{"episode": 132.0, "batch_reward": 0.7130544841885567, "actor_loss": -70.66291979980468, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.0260331630707, "episode_reward": 805.0077657553772, "step": 132000}
{"episode": 133.0, "batch_reward": 0.7138334032893181, "actor_loss": -70.68797180175781, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.916335344314575, "episode_reward": 666.6306673622186, "step": 133000}
{"episode": 134.0, "batch_reward": 0.712976127922535, "actor_loss": -70.33616603088379, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.0068738460541, "episode_reward": 826.7390467186115, "step": 134000}
{"episode": 135.0, "batch_reward": 0.713409685432911, "actor_loss": -70.45470512390136, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 15.976814031600952, "episode_reward": 841.281612095258, "step": 135000}
{"episode": 136.0, "batch_reward": 0.7153226619362831, "actor_loss": -70.22808450317383, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.3492209911346, "episode_reward": 821.5693459347212, "step": 136000}
{"episode": 137.0, "batch_reward": 0.7155177198052406, "actor_loss": -70.23123663330078, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.290443658828735, "episode_reward": 870.2280599115012, "step": 137000}
{"episode": 138.0, "batch_reward": 0.7169956722259522, "actor_loss": -70.3970322418213, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 533.403618812561, "episode_reward": 811.6546242567368, "step": 138000}
{"episode": 139.0, "batch_reward": 0.7172682725787163, "actor_loss": -70.57122369384766, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.004437685012817, "episode_reward": 774.5213854084466, "step": 139000}
{"episode": 140.0, "batch_reward": 0.7186555958390236, "actor_loss": -70.3951333618164, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 528.8859696388245, "episode_reward": 826.4656381650316, "step": 140000}
{"episode": 141.0, "batch_reward": 0.7178025065660477, "actor_loss": -70.38170010375977, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 30.67385697364807, "episode_reward": 864.8441318058572, "step": 141000}
{"episode": 142.0, "batch_reward": 0.7205602396726608, "actor_loss": -70.25047555541992, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 517.3567569255829, "episode_reward": 836.2301311580106, "step": 142000}
{"episode": 143.0, "batch_reward": 0.7201033252477645, "actor_loss": -70.17040100097657, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.055415153503418, "episode_reward": 872.629136578815, "step": 143000}
{"episode": 144.0, "batch_reward": 0.7208438931703568, "actor_loss": -70.33110910034179, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 532.3212716579437, "episode_reward": 799.871702712412, "step": 144000}
{"episode": 145.0, "batch_reward": 0.7214853100776673, "actor_loss": -70.39303009033203, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.510878801345825, "episode_reward": 836.6129825423468, "step": 145000}
{"episode": 146.0, "batch_reward": 0.7225548793077469, "actor_loss": -69.96239289855957, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 530.3538539409637, "episode_reward": 841.3786253406092, "step": 146000}
{"episode": 147.0, "batch_reward": 0.7237564108371735, "actor_loss": -70.07537867736816, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.506683588027954, "episode_reward": 723.790896706251, "step": 147000}
{"episode": 148.0, "batch_reward": 0.7244329106211662, "actor_loss": -70.05117604064941, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 531.8301992416382, "episode_reward": 842.2053367931973, "step": 148000}
{"episode": 149.0, "batch_reward": 0.7233398512005806, "actor_loss": -70.12465837097167, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "duration": 16.132567167282104, "episode_reward": 837.4829604196558, "step": 149000}
{"episode": 150.0, "batch_reward": 0.7258789314031601, "actor_loss": -70.09551023864746, "actor_target_entropy": -6.0, "alpha_value": 0.0043656290730604555, "step": 150000}
