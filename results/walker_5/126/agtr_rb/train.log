{"episode": 1.0, "duration": 14.701914072036743, "episode_reward": 51.16415125024753, "step": 1000}
{"episode": 2.0, "duration": 1.2700650691986084, "episode_reward": 519.9299312670827, "step": 2000}
{"episode": 3.0, "duration": 1.29622483253479, "episode_reward": 452.5114175168042, "step": 3000}
{"episode": 4.0, "duration": 1.2974557876586914, "episode_reward": 406.89477731055035, "step": 4000}
{"episode": 5.0, "duration": 1.305042028427124, "episode_reward": 590.4248088220485, "step": 5000}
{"episode": 6.0, "batch_reward": 0.4027369721189068, "actor_loss": -73.36873417541722, "actor_target_entropy": -6.0, "alpha_value": 0.0038911237795434947, "duration": 370.9636673927307, "episode_reward": 12.289292837830521, "step": 6000}
{"episode": 7.0, "batch_reward": 0.3166945530325174, "actor_loss": -69.98666607666016, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.396127223968506, "episode_reward": 43.905383851751424, "step": 7000}
{"episode": 8.0, "batch_reward": 0.28174298648536206, "actor_loss": -69.30778453063965, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.38067650794983, "episode_reward": 69.76253751081347, "step": 8000}
{"episode": 9.0, "batch_reward": 0.2581321924626827, "actor_loss": -68.39323576354981, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.468257904052734, "episode_reward": 70.49322002484658, "step": 9000}
{"episode": 10.0, "batch_reward": 0.23953660108149052, "actor_loss": -64.71686179351806, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 5046.769531488419, "episode_reward": 78.45787686527885, "step": 10000}
{"episode": 11.0, "batch_reward": 0.2197255375534296, "actor_loss": -65.03877377319336, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.635387897491455, "episode_reward": 35.09131951283205, "step": 11000}
{"episode": 12.0, "batch_reward": 0.20397892349958419, "actor_loss": -63.12785481262207, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 552.0474274158478, "episode_reward": 26.926973598414044, "step": 12000}
{"episode": 13.0, "batch_reward": 0.18852343820035458, "actor_loss": -63.01165740203857, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.561517238616943, "episode_reward": 35.173579639271765, "step": 13000}
{"episode": 14.0, "batch_reward": 0.18258531588315963, "actor_loss": -61.513114791870116, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 551.3439483642578, "episode_reward": 104.17424683629658, "step": 14000}
{"episode": 15.0, "batch_reward": 0.17690491022914648, "actor_loss": -61.36597808837891, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.795182704925537, "episode_reward": 69.35297427070381, "step": 15000}
{"episode": 16.0, "batch_reward": 0.1700340350419283, "actor_loss": -60.552026237487794, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 550.4115753173828, "episode_reward": 72.08158963641301, "step": 16000}
{"episode": 17.0, "batch_reward": 0.1623918314501643, "actor_loss": -60.4330807800293, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.598294019699097, "episode_reward": 92.45775962115448, "step": 17000}
{"episode": 18.0, "batch_reward": 0.15643651621043683, "actor_loss": -60.37777333831787, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 543.7666616439819, "episode_reward": 17.17244200673379, "step": 18000}
{"episode": 19.0, "batch_reward": 0.14952723751217126, "actor_loss": -60.49868633270264, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 17.05899953842163, "episode_reward": 67.96447539347562, "step": 19000}
{"episode": 20.0, "batch_reward": 0.1477941197901964, "actor_loss": -59.28763690185547, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.768009185791, "episode_reward": 104.03093201963613, "step": 20000}
{"episode": 21.0, "batch_reward": 0.14647092037647963, "actor_loss": -59.46909368896485, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.0821795463562, "episode_reward": 114.62352920593833, "step": 21000}
{"episode": 22.0, "batch_reward": 0.1465204348862171, "actor_loss": -59.30805451965332, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.4175870418549, "episode_reward": 265.72770871663477, "step": 22000}
{"episode": 23.0, "batch_reward": 0.14825752828270197, "actor_loss": -59.50299080657959, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.55966806411743, "episode_reward": 139.76025153472722, "step": 23000}
{"episode": 24.0, "batch_reward": 0.15134375290572644, "actor_loss": -59.80641777038574, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.9748582839966, "episode_reward": 270.35699729402427, "step": 24000}
{"episode": 25.0, "batch_reward": 0.15629618810862303, "actor_loss": -59.9673399810791, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.764456748962402, "episode_reward": 273.5973578347449, "step": 25000}
{"episode": 26.0, "batch_reward": 0.1636846532598138, "actor_loss": -60.5641890335083, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.4322793483734, "episode_reward": 321.7233824839676, "step": 26000}
{"episode": 27.0, "batch_reward": 0.16174447668343783, "actor_loss": -60.539044204711914, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.537879943847656, "episode_reward": 101.65316354072712, "step": 27000}
{"episode": 28.0, "batch_reward": 0.1670578513890505, "actor_loss": -60.4968996963501, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 550.0233745574951, "episode_reward": 339.0165586623258, "step": 28000}
{"episode": 29.0, "batch_reward": 0.17688609655201434, "actor_loss": -60.88906336975098, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.287827730178833, "episode_reward": 656.6606057946871, "step": 29000}
{"episode": 30.0, "batch_reward": 0.19168852208554746, "actor_loss": -61.863215370178224, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.5835580825806, "episode_reward": 455.813523847329, "step": 30000}
{"episode": 31.0, "batch_reward": 0.20176926617324353, "actor_loss": -62.1278154373169, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.238770961761475, "episode_reward": 624.7618623195433, "step": 31000}
{"episode": 32.0, "batch_reward": 0.21301721933484077, "actor_loss": -61.971519882202145, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.4052278995514, "episode_reward": 446.9107939118097, "step": 32000}
{"episode": 33.0, "batch_reward": 0.21872333985567094, "actor_loss": -62.09680715179444, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.7743399143219, "episode_reward": 516.7600708516004, "step": 33000}
{"episode": 34.0, "batch_reward": 0.2327379312813282, "actor_loss": -62.58714092254639, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.2066328525543, "episode_reward": 698.1558565721848, "step": 34000}
{"episode": 35.0, "batch_reward": 0.24367037412524223, "actor_loss": -62.822230270385745, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.542446613311768, "episode_reward": 597.3656426186633, "step": 35000}
{"episode": 36.0, "batch_reward": 0.2517584936916828, "actor_loss": -63.15613902282715, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.0466403961182, "episode_reward": 468.5702111149368, "step": 36000}
{"episode": 37.0, "batch_reward": 0.2607478145509958, "actor_loss": -63.5606965713501, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.332010507583618, "episode_reward": 653.3088724404673, "step": 37000}
{"episode": 38.0, "batch_reward": 0.27176370461285115, "actor_loss": -64.31848783874511, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.1651103496552, "episode_reward": 652.3121214459769, "step": 38000}
{"episode": 39.0, "batch_reward": 0.28282506677508357, "actor_loss": -64.74356508636474, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.557320594787598, "episode_reward": 680.8606545159099, "step": 39000}
{"episode": 40.0, "batch_reward": 0.2913955774605274, "actor_loss": -65.43496495056152, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.1759614944458, "episode_reward": 581.5162171068126, "step": 40000}
{"episode": 41.0, "batch_reward": 0.2984546872675419, "actor_loss": -65.8185684890747, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.331092357635498, "episode_reward": 717.088985735237, "step": 41000}
{"episode": 42.0, "batch_reward": 0.30805007918179034, "actor_loss": -65.58182551574707, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 537.0513942241669, "episode_reward": 597.0046076623944, "step": 42000}
{"episode": 43.0, "batch_reward": 0.31536392351984976, "actor_loss": -65.8532950668335, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.557897329330444, "episode_reward": 657.5419922251523, "step": 43000}
{"episode": 44.0, "batch_reward": 0.3235801503956318, "actor_loss": -65.71077167510987, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.3488030433655, "episode_reward": 565.3166229959468, "step": 44000}
{"episode": 45.0, "batch_reward": 0.3281861808001995, "actor_loss": -65.76579347991944, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.559769868850708, "episode_reward": 703.9879807836232, "step": 45000}
{"episode": 46.0, "batch_reward": 0.3381710833907127, "actor_loss": -66.29616804504394, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.0697588920593, "episode_reward": 609.5408001271597, "step": 46000}
{"episode": 47.0, "batch_reward": 0.3427554726600647, "actor_loss": -66.45292561340332, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.41087508201599, "episode_reward": 344.1345380977946, "step": 47000}
{"episode": 48.0, "batch_reward": 0.342665804207325, "actor_loss": -65.88821513366699, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.9265789985657, "episode_reward": 666.2299688630787, "step": 48000}
{"episode": 49.0, "batch_reward": 0.34719168636202813, "actor_loss": -66.09636125183106, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.957760334014893, "episode_reward": 434.6584436939517, "step": 49000}
{"episode": 50.0, "batch_reward": 0.3513058059215546, "actor_loss": -66.30041487121582, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.6427867412567, "episode_reward": 620.8245035842723, "step": 50000}
{"episode": 51.0, "batch_reward": 0.36033819857239724, "actor_loss": -66.6374859008789, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.45059847831726, "episode_reward": 783.6766319094768, "step": 51000}
{"episode": 52.0, "batch_reward": 0.364360543102026, "actor_loss": -66.81913821411133, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.7845222949982, "episode_reward": 632.0718288051852, "step": 52000}
{"episode": 53.0, "batch_reward": 0.36947640010714533, "actor_loss": -66.91289900207519, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.839295625686646, "episode_reward": 654.2028676605777, "step": 53000}
{"episode": 54.0, "batch_reward": 0.37371747189760207, "actor_loss": -66.88639981842041, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.5261342525482, "episode_reward": 530.9674308787768, "step": 54000}
{"episode": 55.0, "batch_reward": 0.3768729031085968, "actor_loss": -66.91046598815917, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.88753318786621, "episode_reward": 594.0343903647612, "step": 55000}
{"episode": 56.0, "batch_reward": 0.38150457382202146, "actor_loss": -66.85499507904052, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 549.4047076702118, "episode_reward": 741.8213160695528, "step": 56000}
{"episode": 57.0, "batch_reward": 0.38862946021556855, "actor_loss": -67.15891123199462, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.704041719436646, "episode_reward": 672.6116589577043, "step": 57000}
{"episode": 58.0, "batch_reward": 0.39442369931936266, "actor_loss": -67.47712828063965, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.5923111438751, "episode_reward": 676.6553367210062, "step": 58000}
{"episode": 59.0, "batch_reward": 0.3998007736504078, "actor_loss": -67.56945971679687, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.4319326877594, "episode_reward": 692.7310113375505, "step": 59000}
{"episode": 60.0, "batch_reward": 0.40075323233008386, "actor_loss": -67.58520092773438, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.4785108566284, "episode_reward": 540.7818258767865, "step": 60000}
{"episode": 61.0, "batch_reward": 0.4058198875486851, "actor_loss": -67.73912759399414, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.61682939529419, "episode_reward": 651.2953151502041, "step": 61000}
{"episode": 62.0, "batch_reward": 0.4089618532061577, "actor_loss": -67.96790243530273, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.959157705307, "episode_reward": 767.0517477247436, "step": 62000}
{"episode": 63.0, "batch_reward": 0.4164893397986889, "actor_loss": -68.21153659057617, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.622318506240845, "episode_reward": 728.6241263510071, "step": 63000}
{"episode": 64.0, "batch_reward": 0.4190385218858719, "actor_loss": -68.11939692687989, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.8932085037231, "episode_reward": 678.1486578208911, "step": 64000}
{"episode": 65.0, "batch_reward": 0.4242489037513733, "actor_loss": -68.2431143951416, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.677287578582764, "episode_reward": 621.1879603262, "step": 65000}
{"episode": 66.0, "batch_reward": 0.42769479084014894, "actor_loss": -68.79797157287598, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.9009246826172, "episode_reward": 669.0835808588145, "step": 66000}
{"episode": 67.0, "batch_reward": 0.42995593696832657, "actor_loss": -68.90389872741699, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.491066217422485, "episode_reward": 633.2168552283108, "step": 67000}
{"episode": 68.0, "batch_reward": 0.43439930990338327, "actor_loss": -69.21735165405273, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.8915221691132, "episode_reward": 551.854384505764, "step": 68000}
{"episode": 69.0, "batch_reward": 0.4352229531407356, "actor_loss": -69.31060879516602, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.96798300743103, "episode_reward": 637.6758701532336, "step": 69000}
{"episode": 70.0, "batch_reward": 0.43793368205428124, "actor_loss": -69.56140867614747, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.9960741996765, "episode_reward": 679.6965833672955, "step": 70000}
{"episode": 71.0, "batch_reward": 0.4420608262717724, "actor_loss": -69.69558000183106, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.194166898727417, "episode_reward": 676.2813266875781, "step": 71000}
{"episode": 72.0, "batch_reward": 0.4459200205206871, "actor_loss": -69.46628253173829, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.1304335594177, "episode_reward": 640.4624646385003, "step": 72000}
{"episode": 73.0, "batch_reward": 0.4456611970961094, "actor_loss": -69.43058168029785, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.671138048171997, "episode_reward": 662.7222784077007, "step": 73000}
{"episode": 74.0, "batch_reward": 0.4521019806265831, "actor_loss": -69.23250877380372, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.38578748703, "episode_reward": 719.4713289899227, "step": 74000}
{"episode": 75.0, "batch_reward": 0.45535391065478326, "actor_loss": -69.3265456085205, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.623876571655273, "episode_reward": 720.7555731453516, "step": 75000}
{"episode": 76.0, "batch_reward": 0.45661967489123345, "actor_loss": -69.58723358154298, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.253796339035, "episode_reward": 557.6163410487059, "step": 76000}
{"episode": 77.0, "batch_reward": 0.4592080750465393, "actor_loss": -69.93233963012695, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.890074253082275, "episode_reward": 653.9877226804159, "step": 77000}
{"episode": 78.0, "batch_reward": 0.4615762994587421, "actor_loss": -69.67523526000977, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 550.6221992969513, "episode_reward": 688.3023666545893, "step": 78000}
{"episode": 79.0, "batch_reward": 0.46302616205811503, "actor_loss": -69.7899048614502, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.590717554092407, "episode_reward": 628.8106576331608, "step": 79000}
{"episode": 80.0, "batch_reward": 0.46873717674612997, "actor_loss": -69.91720771789551, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.1066393852234, "episode_reward": 691.2812955119573, "step": 80000}
{"episode": 81.0, "batch_reward": 0.4679956814944744, "actor_loss": -69.96342701721191, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.06711745262146, "episode_reward": 382.4002508330683, "step": 81000}
{"episode": 82.0, "batch_reward": 0.4676321074962616, "actor_loss": -70.41462214660645, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.6031184196472, "episode_reward": 531.9398381456097, "step": 82000}
{"episode": 83.0, "batch_reward": 0.47009722715616226, "actor_loss": -70.47652026367187, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.953008890151978, "episode_reward": 700.9329600011803, "step": 83000}
{"episode": 84.0, "batch_reward": 0.47106998932361605, "actor_loss": -70.07191659545899, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.4561996459961, "episode_reward": 657.4735646878564, "step": 84000}
{"episode": 85.0, "batch_reward": 0.4738965996801853, "actor_loss": -70.24694409179688, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.948890686035156, "episode_reward": 630.8060214853391, "step": 85000}
{"episode": 86.0, "batch_reward": 0.4751122392416, "actor_loss": -70.04910098266602, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.0027029514313, "episode_reward": 593.3082046265703, "step": 86000}
{"episode": 87.0, "batch_reward": 0.47787536239624023, "actor_loss": -70.16220234680176, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.765897512435913, "episode_reward": 618.6852115235027, "step": 87000}
{"episode": 88.0, "batch_reward": 0.47881837978959085, "actor_loss": -70.19030134582519, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.954523563385, "episode_reward": 682.6464509721989, "step": 88000}
{"episode": 89.0, "batch_reward": 0.48333624497056005, "actor_loss": -70.44287846374512, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.734402656555176, "episode_reward": 791.4575861617535, "step": 89000}
{"episode": 90.0, "batch_reward": 0.484143779784441, "actor_loss": -70.45511369323731, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 544.2144000530243, "episode_reward": 595.5799395274436, "step": 90000}
{"episode": 91.0, "batch_reward": 0.48507750084996226, "actor_loss": -70.50859442138672, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.515089750289917, "episode_reward": 576.2037241737397, "step": 91000}
{"episode": 92.0, "batch_reward": 0.48731146976351736, "actor_loss": -70.75957275390626, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 537.0619764328003, "episode_reward": 719.4472761854638, "step": 92000}
{"episode": 93.0, "batch_reward": 0.4911346663236618, "actor_loss": -70.98228300476075, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.47724199295044, "episode_reward": 697.2775947198802, "step": 93000}
{"episode": 94.0, "batch_reward": 0.49417867839336393, "actor_loss": -70.70013468933105, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.896285533905, "episode_reward": 678.3822333903589, "step": 94000}
{"episode": 95.0, "batch_reward": 0.4948153569996357, "actor_loss": -70.70001518249512, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.550872802734375, "episode_reward": 619.659624316778, "step": 95000}
{"episode": 96.0, "batch_reward": 0.49495109272003174, "actor_loss": -70.37720985412598, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.1435568332672, "episode_reward": 764.3396337560092, "step": 96000}
{"episode": 97.0, "batch_reward": 0.49866460222005843, "actor_loss": -70.63234951782226, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.614571571350098, "episode_reward": 505.11397151760065, "step": 97000}
{"episode": 98.0, "batch_reward": 0.49811803421378137, "actor_loss": -70.47849853515625, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.156033039093, "episode_reward": 703.97990119525, "step": 98000}
{"episode": 99.0, "batch_reward": 0.4997942610681057, "actor_loss": -70.47908102416991, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.321544408798218, "episode_reward": 623.3896135570498, "step": 99000}
{"episode": 100.0, "batch_reward": 0.5005130415260792, "actor_loss": -70.38480612182617, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.6799051761627, "episode_reward": 610.0989622873365, "step": 100000}
{"episode": 101.0, "batch_reward": 0.5032304927706719, "actor_loss": -70.61954121398925, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.20377230644226, "episode_reward": 709.8592417308142, "step": 101000}
{"episode": 102.0, "batch_reward": 0.5036206685006619, "actor_loss": -71.19936656188965, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.8630411624908, "episode_reward": 578.2077980220597, "step": 102000}
{"episode": 103.0, "batch_reward": 0.5020286203920841, "actor_loss": -71.23902987670898, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.913957118988037, "episode_reward": 636.9876739157621, "step": 103000}
{"episode": 104.0, "batch_reward": 0.506670041769743, "actor_loss": -71.59418566894531, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.0575094223022, "episode_reward": 786.4770524110158, "step": 104000}
{"episode": 105.0, "batch_reward": 0.5093320116996766, "actor_loss": -71.68033164978027, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.621609449386597, "episode_reward": 684.0783931817878, "step": 105000}
{"episode": 106.0, "batch_reward": 0.5122611506581306, "actor_loss": -71.28282489013672, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.4614255428314, "episode_reward": 657.5772988940197, "step": 106000}
{"episode": 107.0, "batch_reward": 0.5138112848103047, "actor_loss": -71.48259146118164, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.789571523666382, "episode_reward": 622.6811951183, "step": 107000}
{"episode": 108.0, "batch_reward": 0.5139351325631142, "actor_loss": -71.19185548400878, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.2670500278473, "episode_reward": 652.7191678125139, "step": 108000}
{"episode": 109.0, "batch_reward": 0.5138784302771091, "actor_loss": -71.23421165466308, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.995829582214355, "episode_reward": 647.3804456592417, "step": 109000}
{"episode": 110.0, "batch_reward": 0.5143167486488819, "actor_loss": -70.79011688232421, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.716117143631, "episode_reward": 571.0716688865673, "step": 110000}
{"episode": 111.0, "batch_reward": 0.5144801303446292, "actor_loss": -70.9693660736084, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.47121500968933, "episode_reward": 747.7693144519798, "step": 111000}
{"episode": 112.0, "batch_reward": 0.5191517798900604, "actor_loss": -71.65345602416993, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.0289797782898, "episode_reward": 740.7316545906331, "step": 112000}
{"episode": 113.0, "batch_reward": 0.5203671515583992, "actor_loss": -71.71269329833984, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.732167959213257, "episode_reward": 640.8326963336486, "step": 113000}
{"episode": 114.0, "batch_reward": 0.5192540137767792, "actor_loss": -71.92503433227539, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.8583707809448, "episode_reward": 686.9314831906611, "step": 114000}
{"episode": 115.0, "batch_reward": 0.52349498462677, "actor_loss": -72.10967535400391, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.656424283981323, "episode_reward": 666.6914906370324, "step": 115000}
{"episode": 116.0, "batch_reward": 0.5265016869008541, "actor_loss": -71.66704623413086, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.0490720272064, "episode_reward": 577.8604527446604, "step": 116000}
{"episode": 117.0, "batch_reward": 0.5219209110438824, "actor_loss": -71.56418928527832, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.927926778793335, "episode_reward": 621.7952805450303, "step": 117000}
{"episode": 118.0, "batch_reward": 0.5280215223729611, "actor_loss": -71.89133378601075, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.7125566005707, "episode_reward": 660.9325031909003, "step": 118000}
{"episode": 119.0, "batch_reward": 0.5259150836169719, "actor_loss": -71.85989668273926, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.68867778778076, "episode_reward": 491.43187314408215, "step": 119000}
{"episode": 120.0, "batch_reward": 0.5257336531281471, "actor_loss": -71.68382260131835, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.5685741901398, "episode_reward": 772.3598515683748, "step": 120000}
{"episode": 121.0, "batch_reward": 0.5281980476379394, "actor_loss": -71.73178477478028, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.40289306640625, "episode_reward": 478.97194458792734, "step": 121000}
{"episode": 122.0, "batch_reward": 0.5263286580145359, "actor_loss": -71.33244720458984, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.2006826400757, "episode_reward": 629.4885435217602, "step": 122000}
{"episode": 123.0, "batch_reward": 0.5276972524821758, "actor_loss": -71.4383267364502, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 17.096771001815796, "episode_reward": 618.2101631166349, "step": 123000}
{"episode": 124.0, "batch_reward": 0.5288689891695977, "actor_loss": -71.32158732604981, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 548.9074518680573, "episode_reward": 585.5855265755008, "step": 124000}
{"episode": 125.0, "batch_reward": 0.531098425924778, "actor_loss": -71.38504951477051, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.64655089378357, "episode_reward": 615.6869932122049, "step": 125000}
{"episode": 126.0, "batch_reward": 0.529620703369379, "actor_loss": -71.3954877319336, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.9251391887665, "episode_reward": 600.6369696033706, "step": 126000}
{"episode": 127.0, "batch_reward": 0.5335510035455227, "actor_loss": -71.61543443298339, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.663738012313843, "episode_reward": 775.3222748595805, "step": 127000}
{"episode": 128.0, "batch_reward": 0.5315325920283794, "actor_loss": -71.26669851684571, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.7717950344086, "episode_reward": 684.0014165535935, "step": 128000}
{"episode": 129.0, "batch_reward": 0.5353909803628921, "actor_loss": -71.39106239318848, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.23075532913208, "episode_reward": 692.3388026132884, "step": 129000}
{"episode": 130.0, "batch_reward": 0.5329696710705757, "actor_loss": -71.62097581481933, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.6239426136017, "episode_reward": 555.7213705552764, "step": 130000}
{"episode": 131.0, "batch_reward": 0.5336722706854343, "actor_loss": -71.61752398681641, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 30.435417413711548, "episode_reward": 595.2098045103942, "step": 131000}
{"episode": 132.0, "batch_reward": 0.5360478253662586, "actor_loss": -71.67280714416503, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 549.0705173015594, "episode_reward": 701.947875016794, "step": 132000}
{"episode": 133.0, "batch_reward": 0.5373362383544446, "actor_loss": -71.66136909484864, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.802350521087646, "episode_reward": 693.0391165490455, "step": 133000}
{"episode": 134.0, "batch_reward": 0.5370604239106178, "actor_loss": -72.21136822509766, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 549.6849458217621, "episode_reward": 676.6831098006504, "step": 134000}
{"episode": 135.0, "batch_reward": 0.5384954207241536, "actor_loss": -72.3215887298584, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.914299249649048, "episode_reward": 683.8139761250644, "step": 135000}
{"episode": 136.0, "batch_reward": 0.5409831806123256, "actor_loss": -71.86059907531738, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 543.1476984024048, "episode_reward": 711.1160729114021, "step": 136000}
{"episode": 137.0, "batch_reward": 0.5445980882942677, "actor_loss": -71.96055128479004, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 17.036123037338257, "episode_reward": 736.4599638086203, "step": 137000}
{"episode": 138.0, "batch_reward": 0.5443838945627213, "actor_loss": -71.90320315551757, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.0945489406586, "episode_reward": 750.5097363600535, "step": 138000}
{"episode": 139.0, "batch_reward": 0.5434298048317432, "actor_loss": -71.86612805175781, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.545122861862183, "episode_reward": 647.0468983493884, "step": 139000}
{"episode": 140.0, "batch_reward": 0.5453857147395611, "actor_loss": -71.80237654113769, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.5987479686737, "episode_reward": 762.1541074683809, "step": 140000}
{"episode": 141.0, "batch_reward": 0.5462196422219276, "actor_loss": -71.92040559387208, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 29.910577297210693, "episode_reward": 788.7816018204255, "step": 141000}
{"episode": 142.0, "batch_reward": 0.5493957452178001, "actor_loss": -72.00855039978028, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 537.3787508010864, "episode_reward": 751.7480525202869, "step": 142000}
{"episode": 143.0, "batch_reward": 0.5500822858512402, "actor_loss": -72.00026553344726, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.518502473831177, "episode_reward": 644.6407430609371, "step": 143000}
{"episode": 144.0, "batch_reward": 0.5505040504634381, "actor_loss": -71.99267657470703, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 547.3689670562744, "episode_reward": 528.251275466856, "step": 144000}
{"episode": 145.0, "batch_reward": 0.5510619939267636, "actor_loss": -72.05444674682617, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.766928911209106, "episode_reward": 575.0747144100287, "step": 145000}
{"episode": 146.0, "batch_reward": 0.5510541743040085, "actor_loss": -72.14873864746093, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 546.5751566886902, "episode_reward": 756.3799624631629, "step": 146000}
{"episode": 147.0, "batch_reward": 0.5525419634878636, "actor_loss": -72.14287362670899, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 16.2030131816864, "episode_reward": 711.5410978897357, "step": 147000}
{"episode": 148.0, "batch_reward": 0.5531079270541668, "actor_loss": -72.30584097290038, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 545.2356023788452, "episode_reward": 798.1934591129144, "step": 148000}
{"episode": 149.0, "batch_reward": 0.5561393136382103, "actor_loss": -72.39998243713379, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "duration": 15.931504726409912, "episode_reward": 730.7602274794152, "step": 149000}
{"episode": 150.0, "batch_reward": 0.5559340982437134, "actor_loss": -72.12825843811035, "actor_target_entropy": -6.0, "alpha_value": 0.003891123779543812, "step": 150000}
