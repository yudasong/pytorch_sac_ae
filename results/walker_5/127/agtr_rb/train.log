{"episode": 1.0, "duration": 15.766488313674927, "episode_reward": 61.50603977848499, "step": 1000}
{"episode": 2.0, "duration": 1.4048409461975098, "episode_reward": 808.9209525976895, "step": 2000}
{"episode": 3.0, "duration": 1.403702735900879, "episode_reward": 868.545874649282, "step": 3000}
{"episode": 4.0, "duration": 1.3921961784362793, "episode_reward": 882.8223713450807, "step": 4000}
{"episode": 5.0, "duration": 1.431654453277588, "episode_reward": 790.4544938923829, "step": 5000}
{"episode": 6.0, "batch_reward": 0.6800217484484355, "actor_loss": -84.03444339998862, "actor_target_entropy": -6.0, "alpha_value": 0.004839161189225159, "duration": 373.88066601753235, "episode_reward": 27.75528777505131, "step": 6000}
{"episode": 7.0, "batch_reward": 0.5323701325058937, "actor_loss": -77.86063415527343, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.57008934020996, "episode_reward": 32.27213123632609, "step": 7000}
{"episode": 8.0, "batch_reward": 0.4678281151652336, "actor_loss": -74.17498980712891, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.953464031219482, "episode_reward": 48.46932306903493, "step": 8000}
{"episode": 9.0, "batch_reward": 0.418662326335907, "actor_loss": -71.64385229492187, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.772637367248535, "episode_reward": 75.69279707934967, "step": 9000}
{"episode": 10.0, "batch_reward": 0.3816539265215397, "actor_loss": -67.52864082336426, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 5025.274208545685, "episode_reward": 32.476608714775246, "step": 10000}
{"episode": 11.0, "batch_reward": 0.3505929859280586, "actor_loss": -66.344358543396, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.144633293151855, "episode_reward": 102.80611585587427, "step": 11000}
{"episode": 12.0, "batch_reward": 0.3248805709928274, "actor_loss": -62.167118255615236, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 549.9838399887085, "episode_reward": 75.42158700412247, "step": 12000}
{"episode": 13.0, "batch_reward": 0.3045373442173004, "actor_loss": -61.83541094207764, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.837218523025513, "episode_reward": 37.766963502100474, "step": 13000}
{"episode": 14.0, "batch_reward": 0.2867716888487339, "actor_loss": -57.85096879577637, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 551.2927014827728, "episode_reward": 75.53199367367215, "step": 14000}
{"episode": 15.0, "batch_reward": 0.2736498648673296, "actor_loss": -57.71331964111328, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.756680011749268, "episode_reward": 124.96775451679525, "step": 15000}
{"episode": 16.0, "batch_reward": 0.2659055573791265, "actor_loss": -55.73000157165527, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 548.7239294052124, "episode_reward": 130.63144356088094, "step": 16000}
{"episode": 17.0, "batch_reward": 0.2565676440000534, "actor_loss": -56.337319496154784, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.63530707359314, "episode_reward": 28.814740305199663, "step": 17000}
{"episode": 18.0, "batch_reward": 0.24202961809933185, "actor_loss": -55.15522162628174, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.8153872489929, "episode_reward": 28.999315158327704, "step": 18000}
{"episode": 19.0, "batch_reward": 0.22919916792213918, "actor_loss": -55.76137370300293, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.678905248641968, "episode_reward": 28.017006423375225, "step": 19000}
{"episode": 20.0, "batch_reward": 0.2236028753966093, "actor_loss": -56.42942617797851, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.6960926055908, "episode_reward": 116.72545704258833, "step": 20000}
{"episode": 21.0, "batch_reward": 0.2157085392177105, "actor_loss": -57.06700002288818, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.401546239852905, "episode_reward": 29.07574021162085, "step": 21000}
{"episode": 22.0, "batch_reward": 0.20604459123313426, "actor_loss": -56.6344786529541, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.4519522190094, "episode_reward": 28.138682070149596, "step": 22000}
{"episode": 23.0, "batch_reward": 0.20369486436247825, "actor_loss": -57.28728849029541, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.949706077575684, "episode_reward": 294.7367663158758, "step": 23000}
{"episode": 24.0, "batch_reward": 0.2019088145568967, "actor_loss": -58.43680428314209, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.9513442516327, "episode_reward": 41.23508599167013, "step": 24000}
{"episode": 25.0, "batch_reward": 0.1947491460442543, "actor_loss": -58.96417219543457, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.874637126922607, "episode_reward": 28.585090057642248, "step": 25000}
{"episode": 26.0, "batch_reward": 0.1889221423342824, "actor_loss": -59.26833739471436, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.5758304595947, "episode_reward": 45.55438612955787, "step": 26000}
{"episode": 27.0, "batch_reward": 0.18630574217438697, "actor_loss": -59.87738549804688, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.473305702209473, "episode_reward": 87.39259662076486, "step": 27000}
{"episode": 28.0, "batch_reward": 0.1799180722683668, "actor_loss": -60.84741950988769, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.5677599906921, "episode_reward": 29.050995139266995, "step": 28000}
{"episode": 29.0, "batch_reward": 0.17347018367052078, "actor_loss": -61.01576540374756, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.9424467086792, "episode_reward": 27.9846226626344, "step": 29000}
{"episode": 30.0, "batch_reward": 0.17167455776780843, "actor_loss": -61.010158447265624, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.0630314350128, "episode_reward": 134.44005945641464, "step": 30000}
{"episode": 31.0, "batch_reward": 0.16992303228378297, "actor_loss": -61.429811531066896, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.425848722457886, "episode_reward": 87.98888229796924, "step": 31000}
{"episode": 32.0, "batch_reward": 0.1668863542973995, "actor_loss": -60.97177624511719, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.5387904644012, "episode_reward": 81.43063550380371, "step": 32000}
{"episode": 33.0, "batch_reward": 0.16390116181969644, "actor_loss": -61.27314421081543, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.21033525466919, "episode_reward": 28.63048280713794, "step": 33000}
{"episode": 34.0, "batch_reward": 0.16145175628364086, "actor_loss": -62.051550773620605, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.6120502948761, "episode_reward": 77.96645733813574, "step": 34000}
{"episode": 35.0, "batch_reward": 0.15737242235988377, "actor_loss": -62.24844418334961, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.429432153701782, "episode_reward": 53.74780421010797, "step": 35000}
{"episode": 36.0, "batch_reward": 0.1588118879944086, "actor_loss": -61.71983041381836, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.8450875282288, "episode_reward": 378.2154778613521, "step": 36000}
{"episode": 37.0, "batch_reward": 0.1608917925953865, "actor_loss": -62.0586845779419, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.893877506256104, "episode_reward": 72.01130022082707, "step": 37000}
{"episode": 38.0, "batch_reward": 0.1590891761481762, "actor_loss": -62.609059341430665, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 548.5101227760315, "episode_reward": 27.30095276864543, "step": 38000}
{"episode": 39.0, "batch_reward": 0.1532022882476449, "actor_loss": -62.810378959655765, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.101536750793457, "episode_reward": 28.952552675388873, "step": 39000}
{"episode": 40.0, "batch_reward": 0.15581200231611728, "actor_loss": -62.61838607025147, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 548.5023784637451, "episode_reward": 151.29861588762716, "step": 40000}
{"episode": 41.0, "batch_reward": 0.15225653378665446, "actor_loss": -62.87442168426514, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.29299306869507, "episode_reward": 27.141077574543342, "step": 41000}
{"episode": 42.0, "batch_reward": 0.15097132299095392, "actor_loss": -62.77329887390137, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 536.0407991409302, "episode_reward": 181.82585932160734, "step": 42000}
{"episode": 43.0, "batch_reward": 0.14848836582899094, "actor_loss": -62.90782080078125, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.42082381248474, "episode_reward": 68.6537076352568, "step": 43000}
{"episode": 44.0, "batch_reward": 0.1476135276854038, "actor_loss": -62.31101497650147, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.0494334697723, "episode_reward": 76.21522874536834, "step": 44000}
{"episode": 45.0, "batch_reward": 0.14770849349349738, "actor_loss": -62.876238594055174, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.145398378372192, "episode_reward": 162.00198276534545, "step": 45000}
{"episode": 46.0, "batch_reward": 0.1482040086016059, "actor_loss": -63.28032116699219, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.505765914917, "episode_reward": 255.41866591190384, "step": 46000}
{"episode": 47.0, "batch_reward": 0.15191935298591852, "actor_loss": -63.258159156799316, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.986276626586914, "episode_reward": 397.7076963719697, "step": 47000}
{"episode": 48.0, "batch_reward": 0.1576451955139637, "actor_loss": -62.448483909606935, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.8607845306396, "episode_reward": 412.0502336906665, "step": 48000}
{"episode": 49.0, "batch_reward": 0.16185023342818022, "actor_loss": -62.56491049194336, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.999847412109375, "episode_reward": 458.82049138580146, "step": 49000}
{"episode": 50.0, "batch_reward": 0.1695011650696397, "actor_loss": -62.57726696777344, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.7752556800842, "episode_reward": 515.4909005367184, "step": 50000}
{"episode": 51.0, "batch_reward": 0.17467368746548892, "actor_loss": -62.79681256866455, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 30.6096088886261, "episode_reward": 160.33298171930556, "step": 51000}
{"episode": 52.0, "batch_reward": 0.17604744686186313, "actor_loss": -62.45702368927002, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 543.8458683490753, "episode_reward": 364.8175001269344, "step": 52000}
{"episode": 53.0, "batch_reward": 0.17958168787509202, "actor_loss": -62.46214652252197, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.792118787765503, "episode_reward": 588.0767812197205, "step": 53000}
{"episode": 54.0, "batch_reward": 0.18673419181257486, "actor_loss": -61.98646210479736, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 543.6394681930542, "episode_reward": 357.6922389645807, "step": 54000}
{"episode": 55.0, "batch_reward": 0.18940270239114762, "actor_loss": -62.248939270019534, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.00671696662903, "episode_reward": 251.21384604297765, "step": 55000}
{"episode": 56.0, "batch_reward": 0.19327487789839506, "actor_loss": -62.446427543640134, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.3164203166962, "episode_reward": 579.0281432129292, "step": 56000}
{"episode": 57.0, "batch_reward": 0.19941332203149795, "actor_loss": -62.74854544067383, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.150652408599854, "episode_reward": 591.7459367817636, "step": 57000}
{"episode": 58.0, "batch_reward": 0.20677715339511632, "actor_loss": -62.6866653137207, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.9619133472443, "episode_reward": 481.43341982465574, "step": 58000}
{"episode": 59.0, "batch_reward": 0.20480671176314355, "actor_loss": -62.718983947753905, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.955649375915527, "episode_reward": 60.504838765571144, "step": 59000}
{"episode": 60.0, "batch_reward": 0.20503303381800653, "actor_loss": -63.43371905517578, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 548.7972967624664, "episode_reward": 115.67079587758712, "step": 60000}
{"episode": 61.0, "batch_reward": 0.20332786840200423, "actor_loss": -63.41367343139648, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.32265329360962, "episode_reward": 98.98633686774811, "step": 61000}
{"episode": 62.0, "batch_reward": 0.2065631354302168, "actor_loss": -62.726634307861325, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.0095458030701, "episode_reward": 784.1371711399835, "step": 62000}
{"episode": 63.0, "batch_reward": 0.21336742635071276, "actor_loss": -62.760673240661625, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.248750925064087, "episode_reward": 605.2642973039642, "step": 63000}
{"episode": 64.0, "batch_reward": 0.2188551242053509, "actor_loss": -62.85157044219971, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.6293025016785, "episode_reward": 329.75474967433786, "step": 64000}
{"episode": 65.0, "batch_reward": 0.21957003447413445, "actor_loss": -62.92004581451416, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.056679010391235, "episode_reward": 154.15349953969442, "step": 65000}
{"episode": 66.0, "batch_reward": 0.22253656090795995, "actor_loss": -61.728778373718264, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.7109758853912, "episode_reward": 503.1139952249722, "step": 66000}
{"episode": 67.0, "batch_reward": 0.22439190596342087, "actor_loss": -61.70840663146973, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.021800756454468, "episode_reward": 526.0979055623992, "step": 67000}
{"episode": 68.0, "batch_reward": 0.22625445273518563, "actor_loss": -61.64360758972168, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.458990573883, "episode_reward": 66.3542841810204, "step": 68000}
{"episode": 69.0, "batch_reward": 0.2277854253798723, "actor_loss": -61.793972145080566, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.982850074768066, "episode_reward": 461.80471714813166, "step": 69000}
{"episode": 70.0, "batch_reward": 0.2302351920902729, "actor_loss": -61.66728723907471, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.728404045105, "episode_reward": 566.008064038394, "step": 70000}
{"episode": 71.0, "batch_reward": 0.2383042176514864, "actor_loss": -61.97412635803223, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.500505208969116, "episode_reward": 792.9107703768736, "step": 71000}
{"episode": 72.0, "batch_reward": 0.24098767368495463, "actor_loss": -62.08442803955078, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.4100790023804, "episode_reward": 380.55917365044775, "step": 72000}
{"episode": 73.0, "batch_reward": 0.24458689412474632, "actor_loss": -62.219651245117184, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.948203802108765, "episode_reward": 703.142013851689, "step": 73000}
{"episode": 74.0, "batch_reward": 0.25220569525659087, "actor_loss": -62.07420941162109, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 548.328447341919, "episode_reward": 890.8827549150086, "step": 74000}
{"episode": 75.0, "batch_reward": 0.26254736590385436, "actor_loss": -62.53343972015381, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.834401607513428, "episode_reward": 725.9112671179961, "step": 75000}
{"episode": 76.0, "batch_reward": 0.26590930934250356, "actor_loss": -63.521364540100095, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.1330916881561, "episode_reward": 691.3334209174222, "step": 76000}
{"episode": 77.0, "batch_reward": 0.273369069442153, "actor_loss": -63.73920574188232, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.713937759399414, "episode_reward": 701.836310312155, "step": 77000}
{"episode": 78.0, "batch_reward": 0.2792674638032913, "actor_loss": -63.91344711303711, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.4992289543152, "episode_reward": 888.7500777800951, "step": 78000}
{"episode": 79.0, "batch_reward": 0.2874763617664576, "actor_loss": -64.15555355072021, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.710283756256104, "episode_reward": 709.5618132465183, "step": 79000}
{"episode": 80.0, "batch_reward": 0.2917572722584009, "actor_loss": -64.5302592010498, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.4166159629822, "episode_reward": 775.1602756155804, "step": 80000}
{"episode": 81.0, "batch_reward": 0.29479699525237085, "actor_loss": -64.57668573760986, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.375015020370483, "episode_reward": 500.36326168864076, "step": 81000}
{"episode": 82.0, "batch_reward": 0.29851653915643694, "actor_loss": -64.51243199157715, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.5453667640686, "episode_reward": 840.2468457268819, "step": 82000}
{"episode": 83.0, "batch_reward": 0.30645424059033394, "actor_loss": -64.75309337615967, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.867029905319214, "episode_reward": 880.0459330974476, "step": 83000}
{"episode": 84.0, "batch_reward": 0.3140261388272047, "actor_loss": -64.56990130615235, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 548.2852313518524, "episode_reward": 780.3354296404149, "step": 84000}
{"episode": 85.0, "batch_reward": 0.31821850667893886, "actor_loss": -64.78958891296386, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.730498790740967, "episode_reward": 833.2859043108845, "step": 85000}
{"episode": 86.0, "batch_reward": 0.3247268628627062, "actor_loss": -64.32853904724121, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.1738965511322, "episode_reward": 683.4616575548233, "step": 86000}
{"episode": 87.0, "batch_reward": 0.32904339711368086, "actor_loss": -64.29884262847901, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.67254066467285, "episode_reward": 786.1651476288209, "step": 87000}
{"episode": 88.0, "batch_reward": 0.3348271015286446, "actor_loss": -65.06749137878418, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.0025713443756, "episode_reward": 875.6983846734481, "step": 88000}
{"episode": 89.0, "batch_reward": 0.3394135679602623, "actor_loss": -65.24480278778076, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.588396310806274, "episode_reward": 759.1675714005098, "step": 89000}
{"episode": 90.0, "batch_reward": 0.346538424000144, "actor_loss": -65.11193064117431, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.9755008220673, "episode_reward": 829.7533038687839, "step": 90000}
{"episode": 91.0, "batch_reward": 0.350421314150095, "actor_loss": -65.31830113220215, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.519272089004517, "episode_reward": 896.0311469647344, "step": 91000}
{"episode": 92.0, "batch_reward": 0.35668908002972605, "actor_loss": -64.46297251892089, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 532.715576171875, "episode_reward": 615.279086748163, "step": 92000}
{"episode": 93.0, "batch_reward": 0.35937207040190694, "actor_loss": -64.45305247497559, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.158443212509155, "episode_reward": 722.7427448962868, "step": 93000}
{"episode": 94.0, "batch_reward": 0.3634259207993746, "actor_loss": -63.57543887329101, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.975355386734, "episode_reward": 733.7701267240828, "step": 94000}
{"episode": 95.0, "batch_reward": 0.36663397249579427, "actor_loss": -63.67264932250976, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.863781690597534, "episode_reward": 756.9020440403498, "step": 95000}
{"episode": 96.0, "batch_reward": 0.3701972980201244, "actor_loss": -62.84143239593506, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.8395264148712, "episode_reward": 895.3881064967328, "step": 96000}
{"episode": 97.0, "batch_reward": 0.3767986153364182, "actor_loss": -63.30240022277832, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.874512195587158, "episode_reward": 850.2429681865584, "step": 97000}
{"episode": 98.0, "batch_reward": 0.381540623486042, "actor_loss": -62.942537353515625, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.757963180542, "episode_reward": 869.3060199746367, "step": 98000}
{"episode": 99.0, "batch_reward": 0.38604943719506263, "actor_loss": -63.157566062927245, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.77350354194641, "episode_reward": 795.8321352993127, "step": 99000}
{"episode": 100.0, "batch_reward": 0.3900994472503662, "actor_loss": -62.284966758728025, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.0822186470032, "episode_reward": 656.3927324157636, "step": 100000}
{"episode": 101.0, "batch_reward": 0.3917863158583641, "actor_loss": -62.285135597229, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.177423000335693, "episode_reward": 719.3504802125121, "step": 101000}
{"episode": 102.0, "batch_reward": 0.3968199207186699, "actor_loss": -62.865392890930174, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.665197134018, "episode_reward": 865.8063152696276, "step": 102000}
{"episode": 103.0, "batch_reward": 0.40247681468725205, "actor_loss": -63.088689094543454, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.721195220947266, "episode_reward": 878.7514706466342, "step": 103000}
{"episode": 104.0, "batch_reward": 0.4051791609823704, "actor_loss": -62.1015643157959, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.1950056552887, "episode_reward": 882.3845576471257, "step": 104000}
{"episode": 105.0, "batch_reward": 0.4094734901487827, "actor_loss": -62.19177912902832, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.985596179962158, "episode_reward": 744.0884940081874, "step": 105000}
{"episode": 106.0, "batch_reward": 0.41250200390815733, "actor_loss": -62.95233046722412, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.1797709465027, "episode_reward": 892.469087082403, "step": 106000}
{"episode": 107.0, "batch_reward": 0.4178541995286941, "actor_loss": -63.09192869567871, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.989288806915283, "episode_reward": 727.1612091498771, "step": 107000}
{"episode": 108.0, "batch_reward": 0.42147842586040496, "actor_loss": -63.24419095611572, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.7839481830597, "episode_reward": 807.1223637499543, "step": 108000}
{"episode": 109.0, "batch_reward": 0.42575855138897895, "actor_loss": -63.393064193725586, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.605207443237305, "episode_reward": 825.5268263293424, "step": 109000}
{"episode": 110.0, "batch_reward": 0.42839206421375275, "actor_loss": -63.10096570587158, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.5735440254211, "episode_reward": 751.7932410633651, "step": 110000}
{"episode": 111.0, "batch_reward": 0.431561036080122, "actor_loss": -63.12284478759766, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 30.744224071502686, "episode_reward": 716.8456215828835, "step": 111000}
{"episode": 112.0, "batch_reward": 0.4323815402686596, "actor_loss": -63.90852355957031, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.7415215969086, "episode_reward": 893.6282838864785, "step": 112000}
{"episode": 113.0, "batch_reward": 0.43757033050060273, "actor_loss": -64.21957306671142, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 17.068191528320312, "episode_reward": 892.6625103546269, "step": 113000}
{"episode": 114.0, "batch_reward": 0.4411110805571079, "actor_loss": -65.20333893585205, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.2107136249542, "episode_reward": 768.9355701033737, "step": 114000}
{"episode": 115.0, "batch_reward": 0.4463431686758995, "actor_loss": -65.4317618560791, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.707699060440063, "episode_reward": 906.9967865072803, "step": 115000}
{"episode": 116.0, "batch_reward": 0.44747299337387086, "actor_loss": -64.41561861419677, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.7412281036377, "episode_reward": 711.0814312171548, "step": 116000}
{"episode": 117.0, "batch_reward": 0.449945079177618, "actor_loss": -64.66995333862305, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.752144813537598, "episode_reward": 748.2957382593177, "step": 117000}
{"episode": 118.0, "batch_reward": 0.45291974958777426, "actor_loss": -65.31735417175292, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.4830346107483, "episode_reward": 672.6474213391011, "step": 118000}
{"episode": 119.0, "batch_reward": 0.4543264750540256, "actor_loss": -65.3008469543457, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.57798147201538, "episode_reward": 659.6337075088874, "step": 119000}
{"episode": 120.0, "batch_reward": 0.45862111389636995, "actor_loss": -65.53666576385498, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 542.2672672271729, "episode_reward": 870.9998252292219, "step": 120000}
{"episode": 121.0, "batch_reward": 0.45863501691818237, "actor_loss": -65.47810552978515, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 31.319379329681396, "episode_reward": 629.306771775968, "step": 121000}
{"episode": 122.0, "batch_reward": 0.4628870381116867, "actor_loss": -65.38089065551758, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.4538493156433, "episode_reward": 813.1770154567648, "step": 122000}
{"episode": 123.0, "batch_reward": 0.46522177413105964, "actor_loss": -65.36654749298096, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.60407853126526, "episode_reward": 852.4858998051398, "step": 123000}
{"episode": 124.0, "batch_reward": 0.46695308029651644, "actor_loss": -65.97275601196289, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.2137296199799, "episode_reward": 718.7698571611035, "step": 124000}
{"episode": 125.0, "batch_reward": 0.47234602478146553, "actor_loss": -65.96836609649658, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.937369346618652, "episode_reward": 679.2541778077416, "step": 125000}
{"episode": 126.0, "batch_reward": 0.4701435834467411, "actor_loss": -67.11623296356201, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.5924451351166, "episode_reward": 765.1449059596812, "step": 126000}
{"episode": 127.0, "batch_reward": 0.4739513732492924, "actor_loss": -67.42410565948487, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.35962176322937, "episode_reward": 796.3813936459594, "step": 127000}
{"episode": 128.0, "batch_reward": 0.4746027079820633, "actor_loss": -66.82194633483887, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.3777105808258, "episode_reward": 830.6245212134808, "step": 128000}
{"episode": 129.0, "batch_reward": 0.4802456564605236, "actor_loss": -67.00047716522216, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.73695182800293, "episode_reward": 640.361206189574, "step": 129000}
{"episode": 130.0, "batch_reward": 0.47898835384845734, "actor_loss": -66.54785372924805, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.6457538604736, "episode_reward": 724.7519865911354, "step": 130000}
{"episode": 131.0, "batch_reward": 0.479953931748867, "actor_loss": -66.4790908203125, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 30.97467875480652, "episode_reward": 762.8704872846482, "step": 131000}
{"episode": 132.0, "batch_reward": 0.4826284702718258, "actor_loss": -67.03376406860352, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.1026570796967, "episode_reward": 668.5979546483585, "step": 132000}
{"episode": 133.0, "batch_reward": 0.4866671145558357, "actor_loss": -67.13896652984619, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.571836709976196, "episode_reward": 755.130101503542, "step": 133000}
{"episode": 134.0, "batch_reward": 0.48754069951176643, "actor_loss": -67.75306875610352, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.2637739181519, "episode_reward": 782.4910328705369, "step": 134000}
{"episode": 135.0, "batch_reward": 0.4891660372018814, "actor_loss": -67.60588357543945, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.811758279800415, "episode_reward": 780.8748443089439, "step": 135000}
{"episode": 136.0, "batch_reward": 0.4913239968717098, "actor_loss": -67.92416519165039, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.973874092102, "episode_reward": 677.5392330304002, "step": 136000}
{"episode": 137.0, "batch_reward": 0.49219665628671644, "actor_loss": -67.93742626953124, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.947877168655396, "episode_reward": 743.484459541519, "step": 137000}
{"episode": 138.0, "batch_reward": 0.4927969726920128, "actor_loss": -67.77592393493653, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 547.3753640651703, "episode_reward": 404.5599790684178, "step": 138000}
{"episode": 139.0, "batch_reward": 0.49473319929838183, "actor_loss": -67.9668321838379, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.53954529762268, "episode_reward": 831.9033026597692, "step": 139000}
{"episode": 140.0, "batch_reward": 0.4966743270456791, "actor_loss": -67.84634564208984, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 543.5635595321655, "episode_reward": 796.5443075092937, "step": 140000}
{"episode": 141.0, "batch_reward": 0.49836500531435013, "actor_loss": -67.78758375549316, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 30.995945930480957, "episode_reward": 784.5514787277081, "step": 141000}
{"episode": 142.0, "batch_reward": 0.4997246501147747, "actor_loss": -67.60377435302735, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 534.4104573726654, "episode_reward": 755.054967195133, "step": 142000}
{"episode": 143.0, "batch_reward": 0.5026659169495106, "actor_loss": -67.73785038757325, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.844162225723267, "episode_reward": 698.2574499382213, "step": 143000}
{"episode": 144.0, "batch_reward": 0.5026067161262036, "actor_loss": -67.15505947875977, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 545.3284010887146, "episode_reward": 821.7051855537944, "step": 144000}
{"episode": 145.0, "batch_reward": 0.5062181305289268, "actor_loss": -67.31157566070557, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.959784507751465, "episode_reward": 873.8730609879556, "step": 145000}
{"episode": 146.0, "batch_reward": 0.507688388377428, "actor_loss": -67.56045378112793, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 544.1884942054749, "episode_reward": 757.1780192782859, "step": 146000}
{"episode": 147.0, "batch_reward": 0.5105792954266072, "actor_loss": -67.58838775634766, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.697444677352905, "episode_reward": 773.3824975685961, "step": 147000}
{"episode": 148.0, "batch_reward": 0.513883390635252, "actor_loss": -67.8630616607666, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 546.2548549175262, "episode_reward": 805.8431066438955, "step": 148000}
{"episode": 149.0, "batch_reward": 0.5147832865417004, "actor_loss": -67.76321965026855, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "duration": 16.308916568756104, "episode_reward": 831.2921348307323, "step": 149000}
{"episode": 150.0, "batch_reward": 0.5163632333278656, "actor_loss": -67.98540620422364, "actor_target_entropy": -6.0, "alpha_value": 0.0048391611892268475, "step": 150000}
