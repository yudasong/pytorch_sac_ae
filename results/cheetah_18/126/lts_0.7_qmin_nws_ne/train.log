{"episode_reward": 0.0, "episode": 1.0, "duration": 19.04590654373169, "step": 1000}
{"episode_reward": 4.231880753996205, "episode": 2.0, "duration": 1.748105525970459, "step": 2000}
{"episode_reward": 395.1450543749875, "episode": 3.0, "batch_reward": 0.18721398602886652, "critic_loss": 0.019753620731878793, "actor_loss": -29.458985896968915, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 66.93954300880432, "step": 3000}
{"episode_reward": 2.273398260939352, "episode": 4.0, "batch_reward": 0.11639363999664784, "critic_loss": 0.010694507466163487, "actor_loss": -25.694818776130678, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.945670127868652, "step": 4000}
{"episode_reward": 2.4156262316942985, "episode": 5.0, "batch_reward": 0.09112498272210359, "critic_loss": 0.015166023884434252, "actor_loss": -24.16419576215744, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.44048523902893, "step": 5000}
{"episode_reward": 3.499472875993578, "episode": 6.0, "batch_reward": 0.07496359705552459, "critic_loss": 0.010550867704907433, "actor_loss": -23.352045982837677, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.1635582447052, "step": 6000}
{"episode_reward": 3.1099252132223736, "episode": 7.0, "batch_reward": 0.0641656311750412, "critic_loss": 0.017238988767145202, "actor_loss": -22.635813004016875, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.858897924423218, "step": 7000}
{"episode_reward": 4.122897414870252, "episode": 8.0, "batch_reward": 0.055955600133165716, "critic_loss": 0.01651865834230557, "actor_loss": -24.190309740066528, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.686476469039917, "step": 8000}
{"episode_reward": 3.572413652557391, "episode": 9.0, "batch_reward": 0.049148023925721644, "critic_loss": 0.017521666725631803, "actor_loss": -22.598162492752074, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.73471975326538, "step": 9000}
{"episode_reward": 3.412686223969405, "episode": 10.0, "batch_reward": 0.044948638133704664, "critic_loss": 0.025459330438636244, "actor_loss": -21.70077117109299, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.942408561706543, "step": 10000}
{"episode_reward": 2.6921149797397548, "episode": 11.0, "batch_reward": 0.04060905367974192, "critic_loss": 0.02208688242943026, "actor_loss": -21.33365647315979, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.497801542282104, "step": 11000}
{"episode_reward": 3.7973884899731982, "episode": 12.0, "batch_reward": 0.03785481489915401, "critic_loss": 0.02160206641093828, "actor_loss": -20.96036379337311, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.809333324432373, "step": 12000}
{"episode_reward": 4.000456481501105, "episode": 13.0, "batch_reward": 0.03520362652558833, "critic_loss": 0.013503987900912761, "actor_loss": -20.78857751607895, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.78217315673828, "step": 13000}
{"episode_reward": 3.818239495963247, "episode": 14.0, "batch_reward": 0.032082087236922234, "critic_loss": 0.010167090611299499, "actor_loss": -20.313520924568177, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.166229248046875, "step": 14000}
{"episode_reward": 3.4353653038417327, "episode": 15.0, "batch_reward": 0.030505677436944097, "critic_loss": 0.008619197103427723, "actor_loss": -18.643481342315674, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.312142610549927, "step": 15000}
{"episode_reward": 3.549897130114035, "episode": 16.0, "batch_reward": 0.028364803777076304, "critic_loss": 0.006319884731434286, "actor_loss": -20.54415719604492, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.995302200317383, "step": 16000}
{"episode_reward": 2.908530686593857, "episode": 17.0, "batch_reward": 0.026955029733944685, "critic_loss": 0.005520868637831882, "actor_loss": -19.80689548969269, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.868727684020996, "step": 17000}
{"episode_reward": 3.5658975751251494, "episode": 18.0, "batch_reward": 0.025490773610770703, "critic_loss": 0.005512103450717404, "actor_loss": -19.82195829105377, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.888001680374146, "step": 18000}
{"episode_reward": 2.997643888268387, "episode": 19.0, "batch_reward": 0.02493792019574903, "critic_loss": 0.006913286749622784, "actor_loss": -19.941215869903566, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.00797414779663, "step": 19000}
{"episode_reward": 2.929124585987554, "episode": 20.0, "batch_reward": 0.023745933742728084, "critic_loss": 0.006070391348097473, "actor_loss": -18.593158187866212, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.66932439804077, "step": 20000}
{"episode_reward": 3.4972346054593872, "episode": 21.0, "batch_reward": 0.022033102323301138, "critic_loss": 0.006499016364279669, "actor_loss": -18.922315133571626, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.720463037490845, "step": 21000}
{"episode_reward": 3.4709666652927584, "episode": 22.0, "batch_reward": 0.02135587429453153, "critic_loss": 0.00651849661488086, "actor_loss": -18.689704563856125, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.31794285774231, "step": 22000}
{"episode_reward": 4.011557639888812, "episode": 23.0, "batch_reward": 0.020810170624405145, "critic_loss": 0.005170337392832153, "actor_loss": -18.9725511367321, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.323837995529175, "step": 23000}
{"episode_reward": 3.8014200066132777, "episode": 24.0, "batch_reward": 0.02023637932469137, "critic_loss": 0.005013526711787563, "actor_loss": -19.221081440925598, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.743881225585938, "step": 24000}
{"episode_reward": 4.2470939857140335, "episode": 25.0, "batch_reward": 0.019504557389533148, "critic_loss": 0.0056320019552367736, "actor_loss": -18.71318676161766, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.71531629562378, "step": 25000}
{"episode_reward": 6.374678233464279, "episode": 26.0, "batch_reward": 0.019270055572269483, "critic_loss": 0.006711713528202381, "actor_loss": -18.60818238401413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.940338373184204, "step": 26000}
{"episode_reward": 3.330388774950997, "episode": 27.0, "batch_reward": 0.018692108293296768, "critic_loss": 0.006653406748897396, "actor_loss": -18.07050593519211, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.735824823379517, "step": 27000}
{"episode_reward": 4.575920796117123, "episode": 28.0, "batch_reward": 0.017505548449698836, "critic_loss": 0.006381944213062525, "actor_loss": -18.689429988861082, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.764381170272827, "step": 28000}
{"episode_reward": 6.354833006756918, "episode": 29.0, "batch_reward": 0.017096631367923693, "critic_loss": 0.005761159648245666, "actor_loss": -17.883046561479567, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.036818742752075, "step": 29000}
{"episode_reward": 3.5219151152031665, "episode": 30.0, "batch_reward": 0.017287078818539157, "critic_loss": 0.005480114669597242, "actor_loss": -17.78530884385109, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.298146963119507, "step": 30000}
{"episode_reward": 3.5066286136455047, "episode": 31.0, "batch_reward": 0.0164391361954622, "critic_loss": 0.004654075118160108, "actor_loss": -18.393877066373825, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.84329009056091, "step": 31000}
{"episode_reward": 3.2082045877086407, "episode": 32.0, "batch_reward": 0.016070384052116422, "critic_loss": 0.004854301423591096, "actor_loss": -18.100734624624252, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.145657539367676, "step": 32000}
{"episode_reward": 3.2518467223888106, "episode": 33.0, "batch_reward": 0.01607009069318883, "critic_loss": 0.00478594604958198, "actor_loss": -17.70090079379082, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.865866899490356, "step": 33000}
{"episode_reward": 4.910501965367833, "episode": 34.0, "batch_reward": 0.015266346813878045, "critic_loss": 0.003682520404632669, "actor_loss": -18.40546647620201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.014017343521118, "step": 34000}
{"episode_reward": 3.543084224071135, "episode": 35.0, "batch_reward": 0.01501088865858037, "critic_loss": 0.0030228207781910895, "actor_loss": -17.543771255254747, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.8013973236084, "step": 35000}
{"episode_reward": 3.2574957003798333, "episode": 36.0, "batch_reward": 0.014668518155114725, "critic_loss": 0.0034577012440131514, "actor_loss": -18.841466900110245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.274657726287842, "step": 36000}
{"episode_reward": 3.4545796970733225, "episode": 37.0, "batch_reward": 0.014372068090713582, "critic_loss": 0.0033281907751807013, "actor_loss": -18.06490133559704, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.519307136535645, "step": 37000}
{"episode_reward": 2.9236446906936093, "episode": 38.0, "batch_reward": 0.014133250184822827, "critic_loss": 0.004217206030327361, "actor_loss": -17.189130269885062, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.56609272956848, "step": 38000}
{"episode_reward": 2.7233098779391147, "episode": 39.0, "batch_reward": 0.014040258870460093, "critic_loss": 0.003758507871243637, "actor_loss": -17.97556523692608, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.186336278915405, "step": 39000}
{"episode_reward": 2.659975165724926, "episode": 40.0, "batch_reward": 0.013378065186319872, "critic_loss": 0.0035689045417821035, "actor_loss": -18.04968785429001, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.593353748321533, "step": 40000}
{"episode_reward": 2.323912167685072, "episode": 41.0, "batch_reward": 0.01330577777302824, "critic_loss": 0.002963724834204186, "actor_loss": -18.554433636307717, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.260414361953735, "step": 41000}
{"episode_reward": 3.1963493945372594, "episode": 42.0, "batch_reward": 0.012995215125847608, "critic_loss": 0.0034533530453045385, "actor_loss": -17.5788313035965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.07626175880432, "step": 42000}
{"episode_reward": 4.428263571588142, "episode": 43.0, "batch_reward": 0.012789701086934656, "critic_loss": 0.002754653746553231, "actor_loss": -18.154366797804833, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.67479920387268, "step": 43000}
{"episode_reward": 7.536379312858002, "episode": 44.0, "batch_reward": 0.01250267554144375, "critic_loss": 0.002537718602688983, "actor_loss": -17.43416969060898, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.173547744750977, "step": 44000}
{"episode_reward": 3.630473367061728, "episode": 45.0, "batch_reward": 0.012424789569573477, "critic_loss": 0.002710910927664372, "actor_loss": -17.163541411757468, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.07812213897705, "step": 45000}
{"episode_reward": 3.496391808267509, "episode": 46.0, "batch_reward": 0.012373407565057278, "critic_loss": 0.0024185398979170714, "actor_loss": -17.69558169621229, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.509632110595703, "step": 46000}
{"episode_reward": 4.230965367390606, "episode": 47.0, "batch_reward": 0.01211096272827126, "critic_loss": 0.002304343670984963, "actor_loss": -17.969137435793876, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.992353439331055, "step": 47000}
{"episode_reward": 3.151935354760209, "episode": 48.0, "batch_reward": 0.011846611449029296, "critic_loss": 0.002064463800212252, "actor_loss": -17.788837530732156, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.715681076049805, "step": 48000}
{"episode_reward": 4.068032896555979, "episode": 49.0, "batch_reward": 0.012032612355425954, "critic_loss": 0.002453809683996951, "actor_loss": -17.827266626119613, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.53173279762268, "step": 49000}
{"episode_reward": 3.6386186887434917, "episode": 50.0, "batch_reward": 0.011723019198514522, "critic_loss": 0.002723696832370479, "actor_loss": -17.176158437609672, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.01299810409546, "step": 50000}
{"episode_reward": 3.3957052377267547, "episode": 51.0, "batch_reward": 0.011457231959793717, "critic_loss": 0.002450789715781866, "actor_loss": -17.465529854536058, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.30552577972412, "step": 51000}
{"episode_reward": 4.313828822378433, "episode": 52.0, "batch_reward": 0.011374573761131615, "critic_loss": 0.002364417063756264, "actor_loss": -17.942203890621663, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.244281768798828, "step": 52000}
{"episode_reward": 3.0691215649275114, "episode": 53.0, "batch_reward": 0.011258451185305603, "critic_loss": 0.0025800231551693286, "actor_loss": -16.504797538161277, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.611537218093872, "step": 53000}
{"episode_reward": 3.9920901402102937, "episode": 54.0, "batch_reward": 0.011320172142470256, "critic_loss": 0.0022865442556358174, "actor_loss": -18.6317112146616, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.16653299331665, "step": 54000}
{"episode_reward": 3.2413694672634907, "episode": 55.0, "batch_reward": 0.010772867986001074, "critic_loss": 0.002079743170288566, "actor_loss": -18.027021491229533, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.605482578277588, "step": 55000}
{"episode_reward": 2.5831182476721555, "episode": 56.0, "batch_reward": 0.010583913692738861, "critic_loss": 0.0018392419861411327, "actor_loss": -16.783280834436418, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.968475103378296, "step": 56000}
{"episode_reward": 3.0357494156090556, "episode": 57.0, "batch_reward": 0.010796403414336965, "critic_loss": 0.0019468751347158104, "actor_loss": -17.761562488853933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.840604066848755, "step": 57000}
{"episode_reward": 6.612593215599484, "episode": 58.0, "batch_reward": 0.010586585876066238, "critic_loss": 0.00192183463172114, "actor_loss": -17.50286399370432, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.777137279510498, "step": 58000}
{"episode_reward": 3.6791030113748393, "episode": 59.0, "batch_reward": 0.010282310804817826, "critic_loss": 0.0022267910864320582, "actor_loss": -17.861740803062915, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.880871534347534, "step": 59000}
{"episode_reward": 5.403159306103707, "episode": 60.0, "batch_reward": 0.010325709050288423, "critic_loss": 0.001736510781396646, "actor_loss": -17.073147403687237, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.77048921585083, "step": 60000}
{"episode_reward": 4.342852040088077, "episode": 61.0, "batch_reward": 0.010139584845630451, "critic_loss": 0.0015785468716348987, "actor_loss": -17.87702998805046, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.68153500556946, "step": 61000}
{"episode_reward": 3.8618357428751535, "episode": 62.0, "batch_reward": 0.009884678072063252, "critic_loss": 0.0016651319369411794, "actor_loss": -16.674440894812346, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.771093606948853, "step": 62000}
{"episode_reward": 4.010548103103694, "episode": 63.0, "batch_reward": 0.010051392799359746, "critic_loss": 0.0014084917718646466, "actor_loss": -17.05310198149085, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.801427125930786, "step": 63000}
{"episode_reward": 6.452597795779403, "episode": 64.0, "batch_reward": 0.009826680747093633, "critic_loss": 0.0015884146310272626, "actor_loss": -17.530245783537627, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.88797378540039, "step": 64000}
{"episode_reward": 3.427305756516329, "episode": 65.0, "batch_reward": 0.00995921806199476, "critic_loss": 0.0017196821372417617, "actor_loss": -16.597324057877064, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.715749740600586, "step": 65000}
{"episode_reward": 5.025067411565401, "episode": 66.0, "batch_reward": 0.009575177306542173, "critic_loss": 0.0017249553868459771, "actor_loss": -16.872086003690956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.038339614868164, "step": 66000}
{"episode_reward": 3.391491962179365, "episode": 67.0, "batch_reward": 0.009586828928091564, "critic_loss": 0.0016159896370481874, "actor_loss": -16.463233891814948, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.267587661743164, "step": 67000}
{"episode_reward": 3.802991225241612, "episode": 68.0, "batch_reward": 0.009668244024040177, "critic_loss": 0.0018523528351943241, "actor_loss": -17.597291623979807, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.847124099731445, "step": 68000}
{"episode_reward": 3.900727593190103, "episode": 69.0, "batch_reward": 0.009644423364195973, "critic_loss": 0.001677874606786645, "actor_loss": -17.30329812321067, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.84657645225525, "step": 69000}
{"episode_reward": 3.6366021165982882, "episode": 70.0, "batch_reward": 0.00933811601018533, "critic_loss": 0.0013819202510640025, "actor_loss": -17.860312356054784, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.140347957611084, "step": 70000}
{"episode_reward": 3.4832566554253064, "episode": 71.0, "batch_reward": 0.009297689330065624, "critic_loss": 0.0016067052895450615, "actor_loss": -16.045014150828123, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.340699911117554, "step": 71000}
{"episode_reward": 3.3822234379196434, "episode": 72.0, "batch_reward": 0.00913878802745603, "critic_loss": 0.0014103914081206312, "actor_loss": -17.247891762554644, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.477910041809082, "step": 72000}
{"episode_reward": 3.6796593156642157, "episode": 73.0, "batch_reward": 0.009290136144962161, "critic_loss": 0.0014157229274205747, "actor_loss": -17.352758541628717, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.721336841583252, "step": 73000}
{"episode_reward": 2.9947811236322863, "episode": 74.0, "batch_reward": 0.009055863566929475, "critic_loss": 0.001517802576214308, "actor_loss": -17.176591906383635, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.57017683982849, "step": 74000}
{"episode_reward": 2.974944680861305, "episode": 75.0, "batch_reward": 0.009196295602014288, "critic_loss": 0.0013545321988931392, "actor_loss": -17.46717385840416, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.72613549232483, "step": 75000}
{"episode_reward": 3.00480053191357, "episode": 76.0, "batch_reward": 0.008871110552921892, "critic_loss": 0.0010828259276131574, "actor_loss": -17.199729386404158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.16736388206482, "step": 76000}
{"episode_reward": 2.674348401204619, "episode": 77.0, "batch_reward": 0.008690495649119839, "critic_loss": 0.0012047652124238084, "actor_loss": -16.77737520211935, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.471333026885986, "step": 77000}
{"episode_reward": 4.284406592013947, "episode": 78.0, "batch_reward": 0.008876063560135662, "critic_loss": 0.001266115466154588, "actor_loss": -17.042780059501528, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.64740252494812, "step": 78000}
{"episode_reward": 3.3012467989207366, "episode": 79.0, "batch_reward": 0.008617623816477136, "critic_loss": 0.0012978405404937803, "actor_loss": -16.802825367331504, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.884984493255615, "step": 79000}
{"episode_reward": 3.0537162161135463, "episode": 80.0, "batch_reward": 0.008658424974535592, "critic_loss": 0.0016794891372846905, "actor_loss": -16.965648731961846, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.39324688911438, "step": 80000}
{"episode_reward": 3.77739285197715, "episode": 81.0, "batch_reward": 0.008747195859439671, "critic_loss": 0.0011712203704337299, "actor_loss": -16.252598332718016, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.003520011901855, "step": 81000}
{"episode_reward": 5.54085313501991, "episode": 82.0, "batch_reward": 0.008353083987953142, "critic_loss": 0.001483970555698761, "actor_loss": -16.467420675352216, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.62385058403015, "step": 82000}
{"episode_reward": 2.9311123633352305, "episode": 83.0, "batch_reward": 0.008444689223309978, "critic_loss": 0.0013992746659059775, "actor_loss": -17.671267281726003, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.005118370056152, "step": 83000}
{"episode_reward": 6.006795187532479, "episode": 84.0, "batch_reward": 0.008264137292979284, "critic_loss": 0.0014376288777166338, "actor_loss": -17.725279560752213, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.75773310661316, "step": 84000}
{"episode_reward": 3.5267132396270693, "episode": 85.0, "batch_reward": 0.008598397647845559, "critic_loss": 0.0013941232539218618, "actor_loss": -16.406346229106187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.19920587539673, "step": 85000}
{"episode_reward": 3.3757902378926277, "episode": 86.0, "batch_reward": 0.008339725096244364, "critic_loss": 0.0014944863609198365, "actor_loss": -16.775380143888295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.76496434211731, "step": 86000}
{"episode_reward": 5.135318951221214, "episode": 87.0, "batch_reward": 0.008648678387631663, "critic_loss": 0.0010548322374743294, "actor_loss": -17.010899223349988, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.33448338508606, "step": 87000}
{"episode_reward": 6.6180647403165676, "episode": 88.0, "batch_reward": 0.00849663515528664, "critic_loss": 0.001340229169538361, "actor_loss": -17.41293230391294, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.512423515319824, "step": 88000}
{"episode_reward": 6.429026953782233, "episode": 89.0, "batch_reward": 0.008314589109038935, "critic_loss": 0.0013262392896176607, "actor_loss": -16.432213254623115, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.101263284683228, "step": 89000}
{"episode_reward": 3.322790631676652, "episode": 90.0, "batch_reward": 0.008107055913773366, "critic_loss": 0.0011839909490990977, "actor_loss": -17.13468226341903, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.54397177696228, "step": 90000}
{"episode_reward": 4.5748088748804605, "episode": 91.0, "batch_reward": 0.008176194121828302, "critic_loss": 0.0012792993219081837, "actor_loss": -16.604755324535073, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.5570113658905, "step": 91000}
{"episode_reward": 3.683879107880494, "episode": 92.0, "batch_reward": 0.008054070577025414, "critic_loss": 0.0011397175547317603, "actor_loss": -16.918731058120727, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.043996572494507, "step": 92000}
{"episode_reward": 5.687763023770895, "episode": 93.0, "batch_reward": 0.008117278738413006, "critic_loss": 0.0009694242399637006, "actor_loss": -17.20316277974844, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.253310441970825, "step": 93000}
{"episode_reward": 3.8262687505710553, "episode": 94.0, "batch_reward": 0.007949103215127252, "critic_loss": 0.0013348595318821027, "actor_loss": -18.02843030804023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.977770805358887, "step": 94000}
{"episode_reward": 6.418224653262338, "episode": 95.0, "batch_reward": 0.00796182691771537, "critic_loss": 0.0010269021247040655, "actor_loss": -17.895384096007795, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.426333904266357, "step": 95000}
{"episode_reward": 4.332805958995239, "episode": 96.0, "batch_reward": 0.008087447527563199, "critic_loss": 0.0012805196276858624, "actor_loss": -17.50620358574763, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.743947982788086, "step": 96000}
{"episode_reward": 3.2212320627229114, "episode": 97.0, "batch_reward": 0.007888466095668263, "critic_loss": 0.0009644598843005952, "actor_loss": -17.396137045603247, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.352195501327515, "step": 97000}
{"episode_reward": 5.9227862684248995, "episode": 98.0, "batch_reward": 0.007887052801903337, "critic_loss": 0.001238252955419739, "actor_loss": -17.06211637496203, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.661322116851807, "step": 98000}
{"episode_reward": 3.780657950580535, "episode": 99.0, "batch_reward": 0.008032085723709314, "critic_loss": 0.0010986184811445127, "actor_loss": -17.43160643348843, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.161108016967773, "step": 99000}
{"episode_reward": 3.612092726462971, "episode": 100.0, "batch_reward": 0.007733059424674138, "critic_loss": 0.0010181176780461102, "actor_loss": -17.494475509606303, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.919440507888794, "step": 100000}
{"episode_reward": 3.388017622378017, "episode": 101.0, "batch_reward": 0.007687908464111388, "critic_loss": 0.0010039106869371608, "actor_loss": -17.222493915116413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.085877656936646, "step": 101000}
{"episode_reward": 3.2590704364862635, "episode": 102.0, "batch_reward": 0.0076221025816630575, "critic_loss": 0.0014536050559254363, "actor_loss": -18.42682820051536, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.94598889350891, "step": 102000}
{"episode_reward": 2.8393977261758616, "episode": 103.0, "batch_reward": 0.007857713616453111, "critic_loss": 0.0013314748411321488, "actor_loss": -17.036450810931623, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.616441249847412, "step": 103000}
{"episode_reward": 3.432806550944478, "episode": 104.0, "batch_reward": 0.007851356561877765, "critic_loss": 0.000973776248843933, "actor_loss": -17.709541174277664, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.966615200042725, "step": 104000}
{"episode_reward": 6.460026311826957, "episode": 105.0, "batch_reward": 0.007538261105306447, "critic_loss": 0.0010685625003934547, "actor_loss": -16.759639290106485, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.81376552581787, "step": 105000}
{"episode_reward": 2.9345438541608964, "episode": 106.0, "batch_reward": 0.007784366317559034, "critic_loss": 0.0010068520490422088, "actor_loss": -18.17282102288492, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.595520973205566, "step": 106000}
{"episode_reward": 5.9225653728451, "episode": 107.0, "batch_reward": 0.007553034462966025, "critic_loss": 0.0008739742504803871, "actor_loss": -17.428893976856024, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.87101149559021, "step": 107000}
{"episode_reward": 3.850157741348297, "episode": 108.0, "batch_reward": 0.007399852235568687, "critic_loss": 0.0010556399124034214, "actor_loss": -16.395672490505035, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.890238285064697, "step": 108000}
{"episode_reward": 4.271435782397016, "episode": 109.0, "batch_reward": 0.007541107190889306, "critic_loss": 0.0010439872110291617, "actor_loss": -17.82809144236706, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.422648191452026, "step": 109000}
{"episode_reward": 3.3639292938581478, "episode": 110.0, "batch_reward": 0.007438723356230185, "critic_loss": 0.0009604668869396847, "actor_loss": -17.086528526580892, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.84513211250305, "step": 110000}
{"episode_reward": 3.4524392702092133, "episode": 111.0, "batch_reward": 0.007436296592350118, "critic_loss": 0.0010864596894134592, "actor_loss": -16.569237595306245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.22332406044006, "step": 111000}
{"episode_reward": 4.153525956343094, "episode": 112.0, "batch_reward": 0.007539613189175725, "critic_loss": 0.0010280724568765436, "actor_loss": -17.580272702937947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.284575700759888, "step": 112000}
{"episode_reward": 7.4800177001718655, "episode": 113.0, "batch_reward": 0.007488264781422913, "critic_loss": 0.0011108614333606966, "actor_loss": -17.317394189585933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.70360565185547, "step": 113000}
{"episode_reward": 3.0986015195645376, "episode": 114.0, "batch_reward": 0.00749062542791944, "critic_loss": 0.0010634374220462631, "actor_loss": -17.997670279269574, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.595601797103882, "step": 114000}
{"episode_reward": 3.215264870127837, "episode": 115.0, "batch_reward": 0.0072141536890994755, "critic_loss": 0.0013043128658173373, "actor_loss": -17.214622762969697, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.467823266983032, "step": 115000}
{"episode_reward": 4.470151251523035, "episode": 116.0, "batch_reward": 0.007291349534178153, "critic_loss": 0.0010163563214555325, "actor_loss": -17.40093900434277, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.434910535812378, "step": 116000}
{"episode_reward": 4.345499977439743, "episode": 117.0, "batch_reward": 0.007586419957573525, "critic_loss": 0.0011485035169862385, "actor_loss": -16.444254225076175, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.74338936805725, "step": 117000}
{"episode_reward": 3.4747668524437785, "episode": 118.0, "batch_reward": 0.007185838587349281, "critic_loss": 0.0010469685789867072, "actor_loss": -17.06456705932971, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.383294343948364, "step": 118000}
{"episode_reward": 2.919837728176277, "episode": 119.0, "batch_reward": 0.007210925057763233, "critic_loss": 0.0010357465669585508, "actor_loss": -17.238134744487006, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.00358510017395, "step": 119000}
{"episode_reward": 3.7626161540193706, "episode": 120.0, "batch_reward": 0.007190108824754134, "critic_loss": 0.0014893370300305834, "actor_loss": -16.617381528827362, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.829306602478027, "step": 120000}
{"episode_reward": 4.344783611567065, "episode": 121.0, "batch_reward": 0.007169267992605455, "critic_loss": 0.0009503061850300583, "actor_loss": -16.480932285272516, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.227622270584106, "step": 121000}
{"episode_reward": 4.091788114524206, "episode": 122.0, "batch_reward": 0.007231607905589044, "critic_loss": 0.0011596608588843082, "actor_loss": -16.955868414006662, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.31527304649353, "step": 122000}
{"episode_reward": 3.430677885998077, "episode": 123.0, "batch_reward": 0.0069764046701602635, "critic_loss": 0.000988422208596603, "actor_loss": -17.914171691783004, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.727595806121826, "step": 123000}
{"episode_reward": 4.028598126154127, "episode": 124.0, "batch_reward": 0.007062700363690965, "critic_loss": 0.000913463052627776, "actor_loss": -17.572555657891556, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.012630701065063, "step": 124000}
{"episode_reward": 5.62011335934538, "episode": 125.0, "batch_reward": 0.00711816266993992, "critic_loss": 0.001081951141877653, "actor_loss": -17.28102279342292, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.382226943969727, "step": 125000}
{"episode_reward": 4.476574093890274, "episode": 126.0, "batch_reward": 0.0072458314395044, "critic_loss": 0.0014182782621683146, "actor_loss": -16.75843410922261, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.529880046844482, "step": 126000}
{"episode_reward": 3.967342137355711, "episode": 127.0, "batch_reward": 0.007097318819491193, "critic_loss": 0.000978917922690016, "actor_loss": -16.790356927486602, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.274227380752563, "step": 127000}
{"episode_reward": 3.9641316251443426, "episode": 128.0, "batch_reward": 0.006719698642846197, "critic_loss": 0.000928846055190661, "actor_loss": -17.269450154762946, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.648512840270996, "step": 128000}
{"episode_reward": 3.959977613565445, "episode": 129.0, "batch_reward": 0.007143107394687831, "critic_loss": 0.001214354747611651, "actor_loss": -16.698983640931313, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.261882066726685, "step": 129000}
{"episode_reward": 3.199262203546727, "episode": 130.0, "batch_reward": 0.006743726512766443, "critic_loss": 0.0009549560372724955, "actor_loss": -16.72362056333199, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.153194665908813, "step": 130000}
{"episode_reward": 4.345925549664771, "episode": 131.0, "batch_reward": 0.00689200108894147, "critic_loss": 0.001197872826232924, "actor_loss": -16.993873999787496, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.37794876098633, "step": 131000}
{"episode_reward": 3.2868533421961454, "episode": 132.0, "batch_reward": 0.006854355745599605, "critic_loss": 0.0010212515522507602, "actor_loss": -17.49898261881224, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.503414630889893, "step": 132000}
{"episode_reward": 3.0551015548720457, "episode": 133.0, "batch_reward": 0.006768559120246209, "critic_loss": 0.0007310936426729313, "actor_loss": -17.048574276087106, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.84486961364746, "step": 133000}
{"episode_reward": 2.7557076383263492, "episode": 134.0, "batch_reward": 0.0069791496918769555, "critic_loss": 0.001027979264716123, "actor_loss": -16.552933701137547, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.207521677017212, "step": 134000}
{"episode_reward": 6.251132399482149, "episode": 135.0, "batch_reward": 0.006799854204058647, "critic_loss": 0.0008650311348174, "actor_loss": -17.00456179881422, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.273048639297485, "step": 135000}
{"episode_reward": 2.7188199438830547, "episode": 136.0, "batch_reward": 0.0067077125557698306, "critic_loss": 0.0009630598085568635, "actor_loss": -15.55687313067331, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.694811582565308, "step": 136000}
{"episode_reward": 3.4720120592262433, "episode": 137.0, "batch_reward": 0.0067499709121184425, "critic_loss": 0.0007452968156394491, "actor_loss": -17.333911447212333, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.448450326919556, "step": 137000}
{"episode_reward": 3.5716699538331422, "episode": 138.0, "batch_reward": 0.0068184754222165795, "critic_loss": 0.001024580877758126, "actor_loss": -17.66985262191435, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.095309495925903, "step": 138000}
{"episode_reward": 4.973939919434144, "episode": 139.0, "batch_reward": 0.006715633773012087, "critic_loss": 0.000862422538408282, "actor_loss": -17.037131531264166, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.842760801315308, "step": 139000}
{"episode_reward": 4.3347176240788645, "episode": 140.0, "batch_reward": 0.00660113611863926, "critic_loss": 0.0008633339505759068, "actor_loss": -16.7495082376895, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.77515697479248, "step": 140000}
{"episode_reward": 3.7636244992315255, "episode": 141.0, "batch_reward": 0.006925945326220245, "critic_loss": 0.0010559524831733143, "actor_loss": -16.996580348080954, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.1916868686676, "step": 141000}
{"episode_reward": 3.536734306491452, "episode": 142.0, "batch_reward": 0.006730305499164388, "critic_loss": 0.0008910053026411333, "actor_loss": -16.034000953904354, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.830492973327637, "step": 142000}
{"episode_reward": 5.154301492341382, "episode": 143.0, "batch_reward": 0.00678932298941072, "critic_loss": 0.0008487037131199031, "actor_loss": -16.56304726247536, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.711562156677246, "step": 143000}
{"episode_reward": 4.871880186226065, "episode": 144.0, "batch_reward": 0.006446400608168915, "critic_loss": 0.0009494036056348705, "actor_loss": -16.78546043639537, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.292388916015625, "step": 144000}
{"episode_reward": 2.798700165402143, "episode": 145.0, "batch_reward": 0.0065419899952830745, "critic_loss": 0.000835738306010171, "actor_loss": -17.37041783292382, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.770859241485596, "step": 145000}
{"episode_reward": 3.201494411052279, "episode": 146.0, "batch_reward": 0.006656707690097392, "critic_loss": 0.0008758755736416788, "actor_loss": -17.47276931606303, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.225679874420166, "step": 146000}
{"episode_reward": 3.2657437315000313, "episode": 147.0, "batch_reward": 0.006568580390419811, "critic_loss": 0.0007609722994238836, "actor_loss": -17.23552776249568, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.406245708465576, "step": 147000}
{"episode_reward": 6.397807381447365, "episode": 148.0, "batch_reward": 0.006639140028739348, "critic_loss": 0.000975079921878205, "actor_loss": -17.66399124926352, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.720361948013306, "step": 148000}
{"episode_reward": 3.1059168609207113, "episode": 149.0, "batch_reward": 0.00657675050618127, "critic_loss": 0.0009163901344363694, "actor_loss": -16.551728460124227, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.406776189804077, "step": 149000}
{"episode_reward": 4.233278067808789, "episode": 150.0, "batch_reward": 0.0069417052130447705, "critic_loss": 0.000916132536567602, "actor_loss": -17.14128281688737, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
