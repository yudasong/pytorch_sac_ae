{"episode_reward": 0.0, "episode": 1.0, "duration": 19.65215229988098, "step": 1000}
{"episode_reward": 4.231880753996205, "episode": 2.0, "duration": 1.7520253658294678, "step": 2000}
{"episode_reward": 395.1450543749875, "episode": 3.0, "batch_reward": 0.18815593139484907, "critic_loss": 0.022149649800073758, "actor_loss": -12.615728855028706, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 70.29485249519348, "step": 3000}
{"episode_reward": 31.959326526645178, "episode": 4.0, "batch_reward": 0.12571078922599555, "critic_loss": 0.021167619707994162, "actor_loss": -11.007228940844536, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.370153188705444, "step": 4000}
{"episode_reward": 7.25239818137339, "episode": 5.0, "batch_reward": 0.09947997361049056, "critic_loss": 0.01926892381394282, "actor_loss": -11.796807797789574, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.86961841583252, "step": 5000}
{"episode_reward": 8.485508951376078, "episode": 6.0, "batch_reward": 0.0837437304109335, "critic_loss": 0.019197789873927833, "actor_loss": -10.086715104460716, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.656917572021484, "step": 6000}
{"episode_reward": 18.381632992157385, "episode": 7.0, "batch_reward": 0.08293830355629325, "critic_loss": 0.04013721467461437, "actor_loss": -10.12493795579672, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.838047742843628, "step": 7000}
{"episode_reward": 116.86939094469682, "episode": 8.0, "batch_reward": 0.08022529964148999, "critic_loss": 0.04021982599422336, "actor_loss": -10.976574827671051, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.315938472747803, "step": 8000}
{"episode_reward": 52.91746041951585, "episode": 9.0, "batch_reward": 0.08041618458554149, "critic_loss": 0.06177727264165878, "actor_loss": -10.803903530597687, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.657063245773315, "step": 9000}
{"episode_reward": 67.73224093021418, "episode": 10.0, "batch_reward": 0.07812874020263552, "critic_loss": 0.06325706179440022, "actor_loss": -11.20051969385147, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.461114406585693, "step": 10000}
{"episode_reward": 43.97470395215315, "episode": 11.0, "batch_reward": 0.07528032389655709, "critic_loss": 0.06781793219968676, "actor_loss": -10.049834943771362, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.7639377117157, "step": 11000}
{"episode_reward": 64.86279467148496, "episode": 12.0, "batch_reward": 0.07279024763032794, "critic_loss": 0.07224060945212842, "actor_loss": -11.36206802225113, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.653883695602417, "step": 12000}
{"episode_reward": 20.855905936430307, "episode": 13.0, "batch_reward": 0.07011810139939188, "critic_loss": 0.07682559693418443, "actor_loss": -10.047987149238587, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.617145538330078, "step": 13000}
{"episode_reward": 45.480056775903236, "episode": 14.0, "batch_reward": 0.06819980547577142, "critic_loss": 0.08890058198012411, "actor_loss": -10.20949717950821, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.677542686462402, "step": 14000}
{"episode_reward": 58.146173216335676, "episode": 15.0, "batch_reward": 0.06856932398676872, "critic_loss": 0.11554396108537912, "actor_loss": -9.328288423538208, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.002484798431396, "step": 15000}
{"episode_reward": 69.16789970108061, "episode": 16.0, "batch_reward": 0.06700406995788216, "critic_loss": 0.1192478647455573, "actor_loss": -11.568969641685486, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.62382435798645, "step": 16000}
{"episode_reward": 46.404239589100264, "episode": 17.0, "batch_reward": 0.06629717373102903, "critic_loss": 0.12354322472587227, "actor_loss": -11.646315722465514, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.645666360855103, "step": 17000}
{"episode_reward": 48.154672162350025, "episode": 18.0, "batch_reward": 0.06588639599457383, "critic_loss": 0.14618095261976122, "actor_loss": -11.48019239616394, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.003525018692017, "step": 18000}
{"episode_reward": 77.17400169369735, "episode": 19.0, "batch_reward": 0.06822412587702274, "critic_loss": 0.16833173640817403, "actor_loss": -11.937319732666015, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.67324447631836, "step": 19000}
{"episode_reward": 148.38492056680468, "episode": 20.0, "batch_reward": 0.07007253985852004, "critic_loss": 0.1770398803129792, "actor_loss": -11.639454197883605, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.203136682510376, "step": 20000}
{"episode_reward": 44.87542787172599, "episode": 21.0, "batch_reward": 0.07186402984336018, "critic_loss": 0.1795709162056446, "actor_loss": -12.521038167953492, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.87110114097595, "step": 21000}
{"episode_reward": 213.73066085912572, "episode": 22.0, "batch_reward": 0.0795502821803093, "critic_loss": 0.23912222117185591, "actor_loss": -12.698435046195984, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.122514247894287, "step": 22000}
{"episode_reward": 191.9401505972756, "episode": 23.0, "batch_reward": 0.08226441680639983, "critic_loss": 0.2358346560969949, "actor_loss": -13.860215320587159, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.05731511116028, "step": 23000}
{"episode_reward": 111.4846120785747, "episode": 24.0, "batch_reward": 0.08532855328917503, "critic_loss": 0.208837215192616, "actor_loss": -14.425341329574586, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.419087648391724, "step": 24000}
{"episode_reward": 189.11891817562366, "episode": 25.0, "batch_reward": 0.09005326309800148, "critic_loss": 0.2126985748410225, "actor_loss": -15.651253406524658, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.62479281425476, "step": 25000}
{"episode_reward": 290.9849911281117, "episode": 26.0, "batch_reward": 0.0981572213023901, "critic_loss": 0.2133029853105545, "actor_loss": -16.57391991233826, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.05886697769165, "step": 26000}
{"episode_reward": 306.66920867147996, "episode": 27.0, "batch_reward": 0.10585098361223937, "critic_loss": 0.21285020730644463, "actor_loss": -17.315130966186523, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.192726135253906, "step": 27000}
{"episode_reward": 209.88540918743757, "episode": 28.0, "batch_reward": 0.10797419416159391, "critic_loss": 0.2058400841206312, "actor_loss": -17.85209820175171, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.77208161354065, "step": 28000}
{"episode_reward": 152.8519821425432, "episode": 29.0, "batch_reward": 0.11222652471810579, "critic_loss": 0.19691237454861402, "actor_loss": -17.3353358001709, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.201789617538452, "step": 29000}
{"episode_reward": 337.9836945562289, "episode": 30.0, "batch_reward": 0.1165476952791214, "critic_loss": 0.20343194723129274, "actor_loss": -18.161456714630127, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.368420600891113, "step": 30000}
{"episode_reward": 72.38090303945471, "episode": 31.0, "batch_reward": 0.11850938571244478, "critic_loss": 0.20460585546493532, "actor_loss": -18.381080558776855, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.20577144622803, "step": 31000}
{"episode_reward": 273.12018893255976, "episode": 32.0, "batch_reward": 0.12319782606512308, "critic_loss": 0.20482443338632583, "actor_loss": -18.451993160247802, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.062412977218628, "step": 32000}
{"episode_reward": 261.80529853942124, "episode": 33.0, "batch_reward": 0.12752369708567857, "critic_loss": 0.1905967632085085, "actor_loss": -18.48208563041687, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.027470588684082, "step": 33000}
{"episode_reward": 330.01230044167494, "episode": 34.0, "batch_reward": 0.1315287682786584, "critic_loss": 0.19648952874541284, "actor_loss": -19.145294343948365, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.682819843292236, "step": 34000}
{"episode_reward": 128.85739899980274, "episode": 35.0, "batch_reward": 0.1313769116550684, "critic_loss": 0.20253925903886558, "actor_loss": -18.62564264678955, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.01363205909729, "step": 35000}
{"episode_reward": 154.9247054015206, "episode": 36.0, "batch_reward": 0.13366762725263834, "critic_loss": 0.21178096232563257, "actor_loss": -18.947434656143187, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.297037601470947, "step": 36000}
{"episode_reward": 210.20254639158756, "episode": 37.0, "batch_reward": 0.13319489877671004, "critic_loss": 0.20552247904241086, "actor_loss": -18.93480638694763, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.47569251060486, "step": 37000}
{"episode_reward": 76.19441345860075, "episode": 38.0, "batch_reward": 0.13258138932287694, "critic_loss": 0.20011361923068763, "actor_loss": -18.21151738548279, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.252405405044556, "step": 38000}
{"episode_reward": 108.61041131852498, "episode": 39.0, "batch_reward": 0.13178366639465094, "critic_loss": 0.209395179964602, "actor_loss": -18.351242036819457, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.867632389068604, "step": 39000}
{"episode_reward": 106.3176449392203, "episode": 40.0, "batch_reward": 0.13025954644382, "critic_loss": 0.2103388421162963, "actor_loss": -18.343168054580687, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.69351053237915, "step": 40000}
{"episode_reward": 89.93432076668054, "episode": 41.0, "batch_reward": 0.12859704095870256, "critic_loss": 0.2088679674193263, "actor_loss": -18.244579048156737, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.08231282234192, "step": 41000}
{"episode_reward": 29.329763866355776, "episode": 42.0, "batch_reward": 0.1295655559822917, "critic_loss": 0.22614080292731523, "actor_loss": -18.138007860183716, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.987176656723022, "step": 42000}
{"episode_reward": 352.87746849288345, "episode": 43.0, "batch_reward": 0.13539754458516837, "critic_loss": 0.21312187781184913, "actor_loss": -18.638959329605104, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.72753381729126, "step": 43000}
{"episode_reward": 343.5636281487761, "episode": 44.0, "batch_reward": 0.13954475270956754, "critic_loss": 0.22080227465927602, "actor_loss": -18.827060165405275, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.23559260368347, "step": 44000}
{"episode_reward": 335.31742210305055, "episode": 45.0, "batch_reward": 0.14404567603766918, "critic_loss": 0.20237166165560483, "actor_loss": -18.888390872955323, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.394968271255493, "step": 45000}
{"episode_reward": 349.8728002922981, "episode": 46.0, "batch_reward": 0.14896078631281853, "critic_loss": 0.2107726897224784, "actor_loss": -19.44321479034424, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.835373640060425, "step": 46000}
{"episode_reward": 309.9051248893317, "episode": 47.0, "batch_reward": 0.1518392040207982, "critic_loss": 0.20826693788915873, "actor_loss": -19.996929815292358, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.30220627784729, "step": 47000}
{"episode_reward": 347.5193662193394, "episode": 48.0, "batch_reward": 0.1573391696214676, "critic_loss": 0.19019584757089614, "actor_loss": -19.743142164230346, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.296109676361084, "step": 48000}
{"episode_reward": 384.88225942111046, "episode": 49.0, "batch_reward": 0.16170038464665412, "critic_loss": 0.19593675412982703, "actor_loss": -20.2505544052124, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.089761972427368, "step": 49000}
{"episode_reward": 366.0557805687627, "episode": 50.0, "batch_reward": 0.1656149687245488, "critic_loss": 0.179457403101027, "actor_loss": -20.875015560150146, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.00347876548767, "step": 50000}
{"episode_reward": 370.68656898503156, "episode": 51.0, "batch_reward": 0.1676012025848031, "critic_loss": 0.19547484555095435, "actor_loss": -20.40868674850464, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 43.88151288032532, "step": 51000}
{"episode_reward": 76.53992793478533, "episode": 52.0, "batch_reward": 0.1676892739161849, "critic_loss": 0.20739671953767538, "actor_loss": -20.996864866256715, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.73403549194336, "step": 52000}
{"episode_reward": 338.0592354900799, "episode": 53.0, "batch_reward": 0.17004614212363958, "critic_loss": 0.2040469364672899, "actor_loss": -20.651546115875245, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.766844034194946, "step": 53000}
{"episode_reward": 230.4190504296551, "episode": 54.0, "batch_reward": 0.17060634414851666, "critic_loss": 0.24262289560586214, "actor_loss": -21.00514610862732, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.16261887550354, "step": 54000}
{"episode_reward": 86.17875643319437, "episode": 55.0, "batch_reward": 0.16995363719761372, "critic_loss": 0.2287549034357071, "actor_loss": -21.054560176849364, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.44637441635132, "step": 55000}
{"episode_reward": 243.211098704467, "episode": 56.0, "batch_reward": 0.17122195765376091, "critic_loss": 0.22599183865636588, "actor_loss": -20.82966560935974, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.666242837905884, "step": 56000}
{"episode_reward": 376.7692701151684, "episode": 57.0, "batch_reward": 0.17545311222970486, "critic_loss": 0.22412210219353437, "actor_loss": -21.259893016815184, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.151169300079346, "step": 57000}
{"episode_reward": 275.6011329648582, "episode": 58.0, "batch_reward": 0.17556329756975175, "critic_loss": 0.22416767345368863, "actor_loss": -20.893454082489015, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.41917085647583, "step": 58000}
{"episode_reward": 107.00287049728189, "episode": 59.0, "batch_reward": 0.17522813385725022, "critic_loss": 0.2216251223757863, "actor_loss": -21.0878961391449, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.628663778305054, "step": 59000}
{"episode_reward": 210.9022605367375, "episode": 60.0, "batch_reward": 0.1764162886738777, "critic_loss": 0.2286374664902687, "actor_loss": -21.4906395816803, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.98950743675232, "step": 60000}
{"episode_reward": 354.65979897381726, "episode": 61.0, "batch_reward": 0.18011602340638638, "critic_loss": 0.23856721630692482, "actor_loss": -21.606247341156006, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.41908240318298, "step": 61000}
{"episode_reward": 420.58194148215233, "episode": 62.0, "batch_reward": 0.18366764825582504, "critic_loss": 0.24556229666620494, "actor_loss": -21.558234409332275, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.8641836643219, "step": 62000}
{"episode_reward": 355.21024387395397, "episode": 63.0, "batch_reward": 0.18577155642211438, "critic_loss": 0.2546609394699335, "actor_loss": -21.724599269866943, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.280186891555786, "step": 63000}
{"episode_reward": 410.0864876637776, "episode": 64.0, "batch_reward": 0.1901229062527418, "critic_loss": 0.23731920621544123, "actor_loss": -22.56112010192871, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.409257888793945, "step": 64000}
{"episode_reward": 388.3362774400836, "episode": 65.0, "batch_reward": 0.19322186082601547, "critic_loss": 0.22923671753704547, "actor_loss": -22.638547382354737, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.945806741714478, "step": 65000}
{"episode_reward": 416.4211190745947, "episode": 66.0, "batch_reward": 0.19605098113417627, "critic_loss": 0.24319048218429087, "actor_loss": -23.04971362686157, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.20297622680664, "step": 66000}
{"episode_reward": 404.2851411665342, "episode": 67.0, "batch_reward": 0.19955465708673, "critic_loss": 0.24456142888963223, "actor_loss": -23.173744384765627, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.634953260421753, "step": 67000}
{"episode_reward": 391.2228014265871, "episode": 68.0, "batch_reward": 0.20260606990754604, "critic_loss": 0.24427991526573897, "actor_loss": -23.73739499282837, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.54774045944214, "step": 68000}
{"episode_reward": 390.7675383668872, "episode": 69.0, "batch_reward": 0.2052643777281046, "critic_loss": 0.2504593122974038, "actor_loss": -23.750626377105714, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.27761745452881, "step": 69000}
{"episode_reward": 413.09683694670986, "episode": 70.0, "batch_reward": 0.20853692838549615, "critic_loss": 0.23135463315993549, "actor_loss": -24.477079669952392, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.39192295074463, "step": 70000}
{"episode_reward": 431.2555943884959, "episode": 71.0, "batch_reward": 0.21040396055579186, "critic_loss": 0.23735271475464106, "actor_loss": -24.312672454833983, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.647045850753784, "step": 71000}
{"episode_reward": 416.41678801021516, "episode": 72.0, "batch_reward": 0.2139232600033283, "critic_loss": 0.22737051436305047, "actor_loss": -24.70649892807007, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.05016541481018, "step": 72000}
{"episode_reward": 392.79926663841934, "episode": 73.0, "batch_reward": 0.21648781730234623, "critic_loss": 0.22580870515853166, "actor_loss": -25.034793781280516, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.033436059951782, "step": 73000}
{"episode_reward": 337.91440260967283, "episode": 74.0, "batch_reward": 0.2183618821501732, "critic_loss": 0.23971316814422608, "actor_loss": -25.404675827026367, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.3741774559021, "step": 74000}
{"episode_reward": 422.4745710822519, "episode": 75.0, "batch_reward": 0.22143088318407536, "critic_loss": 0.22498690592497586, "actor_loss": -25.563589729309083, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.062357664108276, "step": 75000}
{"episode_reward": 269.08463636273854, "episode": 76.0, "batch_reward": 0.22198531864583493, "critic_loss": 0.24539612316712736, "actor_loss": -25.80265184402466, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.76566195487976, "step": 76000}
{"episode_reward": 417.9111570393501, "episode": 77.0, "batch_reward": 0.22366321410238743, "critic_loss": 0.23901098228991033, "actor_loss": -25.4660517616272, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.917190313339233, "step": 77000}
{"episode_reward": 308.41684885818034, "episode": 78.0, "batch_reward": 0.22552322117984294, "critic_loss": 0.23776767323166131, "actor_loss": -25.71860135269165, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.03693199157715, "step": 78000}
{"episode_reward": 454.60174498190025, "episode": 79.0, "batch_reward": 0.2284162287712097, "critic_loss": 0.21885759685188533, "actor_loss": -25.978339660644533, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.867539882659912, "step": 79000}
{"episode_reward": 454.1080558183795, "episode": 80.0, "batch_reward": 0.23110592551529408, "critic_loss": 0.23730384947359562, "actor_loss": -26.54751502227783, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.12646222114563, "step": 80000}
{"episode_reward": 447.94534630413534, "episode": 81.0, "batch_reward": 0.23445442740619182, "critic_loss": 0.23643980594724417, "actor_loss": -26.531088329315185, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.65416765213013, "step": 81000}
{"episode_reward": 456.06864242067616, "episode": 82.0, "batch_reward": 0.23605226258933545, "critic_loss": 0.2447974636480212, "actor_loss": -26.80693867111206, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.55089545249939, "step": 82000}
{"episode_reward": 433.55970876524856, "episode": 83.0, "batch_reward": 0.23858578260242938, "critic_loss": 0.23919554442167282, "actor_loss": -27.274700565338136, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.267465829849243, "step": 83000}
{"episode_reward": 434.430075919872, "episode": 84.0, "batch_reward": 0.2408360330313444, "critic_loss": 0.21935471066087484, "actor_loss": -27.42182695388794, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.5149507522583, "step": 84000}
{"episode_reward": 448.4771484831601, "episode": 85.0, "batch_reward": 0.24390393057465554, "critic_loss": 0.2403888161405921, "actor_loss": -27.40826399230957, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.696816444396973, "step": 85000}
{"episode_reward": 475.3427621031266, "episode": 86.0, "batch_reward": 0.24730553114414214, "critic_loss": 0.2250083644464612, "actor_loss": -28.085185291290284, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.964274168014526, "step": 86000}
{"episode_reward": 476.20502195433215, "episode": 87.0, "batch_reward": 0.24924893951416016, "critic_loss": 0.22368991217017173, "actor_loss": -28.155761772155763, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.52992820739746, "step": 87000}
{"episode_reward": 468.4014251143017, "episode": 88.0, "batch_reward": 0.25248661264777184, "critic_loss": 0.23236721032857896, "actor_loss": -28.51459633255005, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.841935634613037, "step": 88000}
{"episode_reward": 475.9094562060198, "episode": 89.0, "batch_reward": 0.25458133782446385, "critic_loss": 0.22618320355564356, "actor_loss": -28.543326679229736, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.05171537399292, "step": 89000}
{"episode_reward": 481.94110500306806, "episode": 90.0, "batch_reward": 0.2564966107159853, "critic_loss": 0.2164022925272584, "actor_loss": -28.592451736450194, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.36826515197754, "step": 90000}
{"episode_reward": 468.58223921421654, "episode": 91.0, "batch_reward": 0.2595661893635988, "critic_loss": 0.21853444025665522, "actor_loss": -28.81082722091675, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.77795124053955, "step": 91000}
{"episode_reward": 491.37112808522465, "episode": 92.0, "batch_reward": 0.26086635145545006, "critic_loss": 0.2226504955366254, "actor_loss": -29.230273666381837, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.715399980545044, "step": 92000}
{"episode_reward": 455.13843099152007, "episode": 93.0, "batch_reward": 0.264179090872407, "critic_loss": 0.21064697653055192, "actor_loss": -29.444826152801514, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.772950410842896, "step": 93000}
{"episode_reward": 463.7653801893257, "episode": 94.0, "batch_reward": 0.2663529364168644, "critic_loss": 0.21309154487401247, "actor_loss": -29.78927791595459, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.64486050605774, "step": 94000}
{"episode_reward": 470.4080713688162, "episode": 95.0, "batch_reward": 0.26816951094567776, "critic_loss": 0.2157806207984686, "actor_loss": -29.460815647125244, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.87178945541382, "step": 95000}
{"episode_reward": 474.29001108465803, "episode": 96.0, "batch_reward": 0.2705646337270737, "critic_loss": 0.22904601904004812, "actor_loss": -30.59960578918457, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.751741647720337, "step": 96000}
{"episode_reward": 471.5410033800419, "episode": 97.0, "batch_reward": 0.2720957570374012, "critic_loss": 0.21817056049406527, "actor_loss": -30.50505094909668, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.213335514068604, "step": 97000}
{"episode_reward": 500.9397116735785, "episode": 98.0, "batch_reward": 0.27504616267979143, "critic_loss": 0.20416187192499638, "actor_loss": -30.315375247955323, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.915648221969604, "step": 98000}
{"episode_reward": 474.51546168615675, "episode": 99.0, "batch_reward": 0.27741634991765024, "critic_loss": 0.20826824775338174, "actor_loss": -31.317144020080566, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.451316833496094, "step": 99000}
{"episode_reward": 476.1731909362602, "episode": 100.0, "batch_reward": 0.27795274575054646, "critic_loss": 0.2191697313785553, "actor_loss": -31.143165790557862, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.614152193069458, "step": 100000}
{"episode_reward": 434.54892752923126, "episode": 101.0, "batch_reward": 0.2804178199917078, "critic_loss": 0.21082642072439195, "actor_loss": -31.534051750183107, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.53470039367676, "step": 101000}
{"episode_reward": 468.68802653860655, "episode": 102.0, "batch_reward": 0.2827509500384331, "critic_loss": 0.21530636092275382, "actor_loss": -31.539550605773925, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.733585357666016, "step": 102000}
{"episode_reward": 477.74109354845376, "episode": 103.0, "batch_reward": 0.28380744372308253, "critic_loss": 0.21358664067834615, "actor_loss": -31.61768690109253, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.74801993370056, "step": 103000}
{"episode_reward": 474.18507064985647, "episode": 104.0, "batch_reward": 0.28514994238317015, "critic_loss": 0.22008838330954314, "actor_loss": -31.955686923980714, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.577856302261353, "step": 104000}
{"episode_reward": 467.9728331970594, "episode": 105.0, "batch_reward": 0.28836836938560007, "critic_loss": 0.2220463110357523, "actor_loss": -32.245809688568116, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.517704010009766, "step": 105000}
{"episode_reward": 475.12871063143155, "episode": 106.0, "batch_reward": 0.28891160260140897, "critic_loss": 0.2075879884287715, "actor_loss": -32.60425896453857, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.843864917755127, "step": 106000}
{"episode_reward": 468.6059114970356, "episode": 107.0, "batch_reward": 0.2911012788116932, "critic_loss": 0.21048605880141258, "actor_loss": -32.66674961090088, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.192485094070435, "step": 107000}
{"episode_reward": 463.4860341093968, "episode": 108.0, "batch_reward": 0.29175996567308904, "critic_loss": 0.2212996625006199, "actor_loss": -32.2862116355896, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.257225036621094, "step": 108000}
{"episode_reward": 462.75311968068524, "episode": 109.0, "batch_reward": 0.2944532592743635, "critic_loss": 0.20127040231972934, "actor_loss": -32.97538104629517, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.845471620559692, "step": 109000}
{"episode_reward": 475.7285425042115, "episode": 110.0, "batch_reward": 0.29625579392910006, "critic_loss": 0.22020857597887517, "actor_loss": -33.20656115722656, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.992730379104614, "step": 110000}
{"episode_reward": 431.98391177245543, "episode": 111.0, "batch_reward": 0.2971753178834915, "critic_loss": 0.19626624432951212, "actor_loss": -32.67236325836182, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 44.009411573410034, "step": 111000}
{"episode_reward": 460.7643962340866, "episode": 112.0, "batch_reward": 0.29850218807160855, "critic_loss": 0.2028250071182847, "actor_loss": -33.54239426803589, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.70995807647705, "step": 112000}
{"episode_reward": 484.8377313721013, "episode": 113.0, "batch_reward": 0.300412892729044, "critic_loss": 0.20603157737106084, "actor_loss": -33.42261103439331, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.364593267440796, "step": 113000}
{"episode_reward": 475.6753911755746, "episode": 114.0, "batch_reward": 0.3018556367456913, "critic_loss": 0.21143666608631612, "actor_loss": -33.58022290039062, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.027348518371582, "step": 114000}
{"episode_reward": 476.54476215828515, "episode": 115.0, "batch_reward": 0.3028581147938967, "critic_loss": 0.20654744580388068, "actor_loss": -33.47796182632446, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.516663551330566, "step": 115000}
{"episode_reward": 479.5878878454637, "episode": 116.0, "batch_reward": 0.30515764504671095, "critic_loss": 0.2056334532946348, "actor_loss": -33.5979638710022, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.67105460166931, "step": 116000}
{"episode_reward": 467.1869077391507, "episode": 117.0, "batch_reward": 0.3061863460242748, "critic_loss": 0.21367660522460938, "actor_loss": -33.391025344848636, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.917528867721558, "step": 117000}
{"episode_reward": 484.18162329700937, "episode": 118.0, "batch_reward": 0.30771167880296707, "critic_loss": 0.2124928489997983, "actor_loss": -33.922811710357664, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.663002967834473, "step": 118000}
{"episode_reward": 481.26096299280135, "episode": 119.0, "batch_reward": 0.3097825264334679, "critic_loss": 0.2087594083994627, "actor_loss": -34.09112629699707, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.99452543258667, "step": 119000}
{"episode_reward": 482.8898926921696, "episode": 120.0, "batch_reward": 0.30941431736946107, "critic_loss": 0.2193002999871969, "actor_loss": -33.78716577911377, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.05881953239441, "step": 120000}
{"episode_reward": 471.82909837665636, "episode": 121.0, "batch_reward": 0.3114113792777061, "critic_loss": 0.20654567290097475, "actor_loss": -34.445094230651854, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.61852502822876, "step": 121000}
{"episode_reward": 476.97786411198194, "episode": 122.0, "batch_reward": 0.3140525996685028, "critic_loss": 0.20964919394254686, "actor_loss": -34.45374343490601, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.434283018112183, "step": 122000}
{"episode_reward": 466.3767762241852, "episode": 123.0, "batch_reward": 0.3150444635152817, "critic_loss": 0.21425749400258065, "actor_loss": -34.698544738769534, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.223140239715576, "step": 123000}
{"episode_reward": 469.4513594496188, "episode": 124.0, "batch_reward": 0.31531753060221673, "critic_loss": 0.20632752557098866, "actor_loss": -34.842912773132326, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.04255771636963, "step": 124000}
{"episode_reward": 480.6484643189664, "episode": 125.0, "batch_reward": 0.31759012591838837, "critic_loss": 0.20786964189261198, "actor_loss": -34.8476254196167, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.771175861358643, "step": 125000}
{"episode_reward": 477.6215557150964, "episode": 126.0, "batch_reward": 0.31898176699876785, "critic_loss": 0.21781969663500786, "actor_loss": -34.81794917678833, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.8008234500885, "step": 126000}
{"episode_reward": 474.4065525510451, "episode": 127.0, "batch_reward": 0.31869063711166384, "critic_loss": 0.21358656850457192, "actor_loss": -35.264290958404544, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.06240439414978, "step": 127000}
{"episode_reward": 458.88382704085774, "episode": 128.0, "batch_reward": 0.3199971470236778, "critic_loss": 0.19498851075023413, "actor_loss": -35.409338466644286, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.703417539596558, "step": 128000}
{"episode_reward": 496.1595352896602, "episode": 129.0, "batch_reward": 0.32203997632861137, "critic_loss": 0.2076754217669368, "actor_loss": -35.3333522529602, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.20320153236389, "step": 129000}
{"episode_reward": 477.83847767966466, "episode": 130.0, "batch_reward": 0.3242607200145721, "critic_loss": 0.22550213818997145, "actor_loss": -35.59837813568115, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.52181386947632, "step": 130000}
{"episode_reward": 486.05563192954713, "episode": 131.0, "batch_reward": 0.32485833814740184, "critic_loss": 0.21323756477236747, "actor_loss": -35.42589950180054, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 43.094496726989746, "step": 131000}
{"episode_reward": 480.7447680266395, "episode": 132.0, "batch_reward": 0.3248034791946411, "critic_loss": 0.22075587432831525, "actor_loss": -35.69945422744751, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.847466707229614, "step": 132000}
{"episode_reward": 454.79847579948074, "episode": 133.0, "batch_reward": 0.3276020076870918, "critic_loss": 0.21286345981061458, "actor_loss": -35.55119691848755, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.037062644958496, "step": 133000}
{"episode_reward": 500.49542244471326, "episode": 134.0, "batch_reward": 0.32817762303352355, "critic_loss": 0.21464073160290717, "actor_loss": -35.70023762130737, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.71100425720215, "step": 134000}
{"episode_reward": 503.74333502649984, "episode": 135.0, "batch_reward": 0.33064650458097455, "critic_loss": 0.22927826748788357, "actor_loss": -36.41430271530152, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.99318289756775, "step": 135000}
{"episode_reward": 512.7362067062746, "episode": 136.0, "batch_reward": 0.33044614747166634, "critic_loss": 0.21060415221750736, "actor_loss": -35.79910649871826, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.331480026245117, "step": 136000}
{"episode_reward": 492.54110519451393, "episode": 137.0, "batch_reward": 0.3313217316865921, "critic_loss": 0.24172319275140763, "actor_loss": -36.55407120895386, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.088960647583008, "step": 137000}
{"episode_reward": 481.9993547527253, "episode": 138.0, "batch_reward": 0.3341221958398819, "critic_loss": 0.25390796445310115, "actor_loss": -37.062291889190675, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.918544054031372, "step": 138000}
{"episode_reward": 504.0740922547667, "episode": 139.0, "batch_reward": 0.3350883902311325, "critic_loss": 0.22712515379488468, "actor_loss": -36.794264736175535, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.800915479660034, "step": 139000}
{"episode_reward": 479.25806048107074, "episode": 140.0, "batch_reward": 0.33601335719227793, "critic_loss": 0.2213626222535968, "actor_loss": -37.13454972839355, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.036566972732544, "step": 140000}
{"episode_reward": 478.31945319657746, "episode": 141.0, "batch_reward": 0.3357600490748882, "critic_loss": 0.2047324492558837, "actor_loss": -37.33327447128296, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.81566119194031, "step": 141000}
{"episode_reward": 471.74421905448713, "episode": 142.0, "batch_reward": 0.33631282207369806, "critic_loss": 0.20206176883727311, "actor_loss": -37.148707015991214, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.878220081329346, "step": 142000}
{"episode_reward": 503.45380386511124, "episode": 143.0, "batch_reward": 0.3381293361186981, "critic_loss": 0.21316841249912977, "actor_loss": -37.23243050765991, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.073911666870117, "step": 143000}
{"episode_reward": 496.436290764186, "episode": 144.0, "batch_reward": 0.3397038141787052, "critic_loss": 0.19020891825854777, "actor_loss": -37.64007550430298, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.766812086105347, "step": 144000}
{"episode_reward": 478.993217589679, "episode": 145.0, "batch_reward": 0.3414027804434299, "critic_loss": 0.19949083329737186, "actor_loss": -37.82948117828369, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.326924324035645, "step": 145000}
{"episode_reward": 480.994214593063, "episode": 146.0, "batch_reward": 0.3416442418992519, "critic_loss": 0.20887862247228622, "actor_loss": -38.009346168518064, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.683544874191284, "step": 146000}
{"episode_reward": 488.906989238207, "episode": 147.0, "batch_reward": 0.34219660529494283, "critic_loss": 0.19295064949244262, "actor_loss": -37.778600112915036, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.930281400680542, "step": 147000}
{"episode_reward": 492.58992761644686, "episode": 148.0, "batch_reward": 0.34415973734855654, "critic_loss": 0.20440359851717949, "actor_loss": -37.9955474395752, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.386693239212036, "step": 148000}
{"episode_reward": 491.05597805681464, "episode": 149.0, "batch_reward": 0.34447867020964623, "critic_loss": 0.21464602081477643, "actor_loss": -37.87964316558838, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.88162875175476, "step": 149000}
{"episode_reward": 483.0391498951884, "episode": 150.0, "batch_reward": 0.34638074001669883, "critic_loss": 0.20030337335169315, "actor_loss": -38.04004864501953, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
