{"episode_reward": 0.0, "episode": 1.0, "duration": 21.036271572113037, "step": 1000}
{"episode_reward": 4.231880753996205, "episode": 2.0, "duration": 1.5972816944122314, "step": 2000}
{"episode_reward": 395.1450543749875, "episode": 3.0, "batch_reward": 0.1872139861456312, "critic_loss": 0.020174656962033667, "actor_loss": -22.20250572314506, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 68.79881834983826, "step": 3000}
{"episode_reward": 2.2733994121535033, "episode": 4.0, "batch_reward": 0.11639364042133094, "critic_loss": 0.00963749509654008, "actor_loss": -19.001347024917603, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.833080053329468, "step": 4000}
{"episode_reward": 2.4156261277207323, "episode": 5.0, "batch_reward": 0.09112498300150036, "critic_loss": 0.014435222214786336, "actor_loss": -19.318262926101685, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.100459814071655, "step": 5000}
{"episode_reward": 3.4994729025064575, "episode": 6.0, "batch_reward": 0.07496359739080072, "critic_loss": 0.011324263170594349, "actor_loss": -17.410844132900237, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.453203201293945, "step": 6000}
{"episode_reward": 3.109926216808554, "episode": 7.0, "batch_reward": 0.06409042761102318, "critic_loss": 0.014004073987714947, "actor_loss": -17.189601295471192, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.970935344696045, "step": 7000}
{"episode_reward": 2.759899928212282, "episode": 8.0, "batch_reward": 0.055767735999077556, "critic_loss": 0.02027849855367094, "actor_loss": -18.573972412109374, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.066877126693726, "step": 8000}
{"episode_reward": 3.572464544170223, "episode": 9.0, "batch_reward": 0.04898803446255624, "critic_loss": 0.022551107632461934, "actor_loss": -17.930161946296693, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.368632793426514, "step": 9000}
{"episode_reward": 3.412686223969405, "episode": 10.0, "batch_reward": 0.04481002400070429, "critic_loss": 0.03050887313764542, "actor_loss": -17.37391024017334, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.361998796463013, "step": 10000}
{"episode_reward": 2.6921149797397548, "episode": 11.0, "batch_reward": 0.0404754796018824, "critic_loss": 0.02862050619116053, "actor_loss": -15.798081809043884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.319135904312134, "step": 11000}
{"episode_reward": 3.7973884899731982, "episode": 12.0, "batch_reward": 0.03773445430956781, "critic_loss": 0.02471830300288275, "actor_loss": -16.192646760463713, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.447262048721313, "step": 12000}
{"episode_reward": 4.000456481501105, "episode": 13.0, "batch_reward": 0.03509850107319653, "critic_loss": 0.01895600134530105, "actor_loss": -14.916983408927917, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.574580430984497, "step": 13000}
{"episode_reward": 3.818239495963247, "episode": 14.0, "batch_reward": 0.03199525243183598, "critic_loss": 0.015308094679145142, "actor_loss": -15.291465099573136, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.396767139434814, "step": 14000}
{"episode_reward": 3.4353653038417327, "episode": 15.0, "batch_reward": 0.030405971322208642, "critic_loss": 0.013027835628250614, "actor_loss": -13.643819190263748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.246378660202026, "step": 15000}
{"episode_reward": 3.549897130114035, "episode": 16.0, "batch_reward": 0.028280469125136733, "critic_loss": 0.01012259567459114, "actor_loss": -15.965160089492798, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.248494625091553, "step": 16000}
{"episode_reward": 2.908530686593857, "episode": 17.0, "batch_reward": 0.02687498862296343, "critic_loss": 0.010663292171200737, "actor_loss": -15.270754422664643, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.574644804000854, "step": 17000}
{"episode_reward": 3.5658975751251494, "episode": 18.0, "batch_reward": 0.025405877389013766, "critic_loss": 0.00986058207298629, "actor_loss": -15.751908547878266, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.40988564491272, "step": 18000}
{"episode_reward": 2.997643888268387, "episode": 19.0, "batch_reward": 0.024862563064787537, "critic_loss": 0.009917517994064838, "actor_loss": -15.38763250041008, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.676498413085938, "step": 19000}
{"episode_reward": 2.929124585987554, "episode": 20.0, "batch_reward": 0.02367930159252137, "critic_loss": 0.008763787023723126, "actor_loss": -13.695865967988968, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.278429985046387, "step": 20000}
{"episode_reward": 3.4972346054593872, "episode": 21.0, "batch_reward": 0.021964808349497617, "critic_loss": 0.009796535318950191, "actor_loss": -14.66924322938919, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.54772925376892, "step": 21000}
{"episode_reward": 3.4709666652927584, "episode": 22.0, "batch_reward": 0.021298105573165232, "critic_loss": 0.010307717184070498, "actor_loss": -14.239027090311051, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.80027151107788, "step": 22000}
{"episode_reward": 4.011557639888812, "episode": 23.0, "batch_reward": 0.020752677697921173, "critic_loss": 0.009438159796525724, "actor_loss": -13.822591716289521, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.80043625831604, "step": 23000}
{"episode_reward": 3.8014200066132777, "episode": 24.0, "batch_reward": 0.02017729775724001, "critic_loss": 0.008081124808872119, "actor_loss": -13.773160340547562, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.705835103988647, "step": 24000}
{"episode_reward": 4.2470939857140335, "episode": 25.0, "batch_reward": 0.019447210824582725, "critic_loss": 0.007148535606684163, "actor_loss": -13.550153483629227, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.4892635345459, "step": 25000}
{"episode_reward": 6.374678233464279, "episode": 26.0, "batch_reward": 0.019214121558819897, "critic_loss": 0.008619777300627901, "actor_loss": -14.183026675224305, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.98587393760681, "step": 26000}
{"episode_reward": 3.330388774950997, "episode": 27.0, "batch_reward": 0.01863698306772858, "critic_loss": 0.009042745106271469, "actor_loss": -13.055142800569534, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.334784984588623, "step": 27000}
{"episode_reward": 4.575920796117123, "episode": 28.0, "batch_reward": 0.017462084520375357, "critic_loss": 0.007865141254966148, "actor_loss": -14.232514038085938, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.646620988845825, "step": 28000}
{"episode_reward": 6.354833006756918, "episode": 29.0, "batch_reward": 0.017047684114659206, "critic_loss": 0.007953189888619818, "actor_loss": -12.816744903087615, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.029509782791138, "step": 29000}
{"episode_reward": 3.5219151152031665, "episode": 30.0, "batch_reward": 0.01723424734175205, "critic_loss": 0.008056402810267174, "actor_loss": -12.927109381198884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.506038188934326, "step": 30000}
{"episode_reward": 3.5066286136455047, "episode": 31.0, "batch_reward": 0.01639383730571717, "critic_loss": 0.006787002552358899, "actor_loss": -13.731703888773918, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.72542762756348, "step": 31000}
{"episode_reward": 3.2082045877086407, "episode": 32.0, "batch_reward": 0.016023052304517477, "critic_loss": 0.006389954282843974, "actor_loss": -13.658958658099175, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.393737077713013, "step": 32000}
{"episode_reward": 3.2518467223888106, "episode": 33.0, "batch_reward": 0.016029167230939494, "critic_loss": 0.005570134092005901, "actor_loss": -12.155085485100747, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.275570154190063, "step": 33000}
{"episode_reward": 4.910501965367833, "episode": 34.0, "batch_reward": 0.015224608046701178, "critic_loss": 0.004373117529205046, "actor_loss": -13.248989045381546, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.173805236816406, "step": 34000}
{"episode_reward": 3.543084224071135, "episode": 35.0, "batch_reward": 0.014968062295927667, "critic_loss": 0.0032064602690079482, "actor_loss": -13.513190899729729, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.405994653701782, "step": 35000}
{"episode_reward": 3.2574957003798333, "episode": 36.0, "batch_reward": 0.01463037013192661, "critic_loss": 0.003099248528626049, "actor_loss": -13.963073974967003, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.789012670516968, "step": 36000}
{"episode_reward": 3.4545796970733225, "episode": 37.0, "batch_reward": 0.014332579861860721, "critic_loss": 0.003234111066471087, "actor_loss": -13.375777400016785, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.29300856590271, "step": 37000}
{"episode_reward": 2.9236446906936093, "episode": 38.0, "batch_reward": 0.014094495529308915, "critic_loss": 0.0035024264612293337, "actor_loss": -12.111010388851167, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.232840538024902, "step": 38000}
{"episode_reward": 2.7233098779391147, "episode": 39.0, "batch_reward": 0.014003859651042148, "critic_loss": 0.00339522412547376, "actor_loss": -12.84046225297451, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.239230155944824, "step": 39000}
{"episode_reward": 2.659975165724926, "episode": 40.0, "batch_reward": 0.013343365744920448, "critic_loss": 0.003809340784981032, "actor_loss": -13.433625041544438, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.75756049156189, "step": 40000}
{"episode_reward": 2.323912167685072, "episode": 41.0, "batch_reward": 0.013273238313384355, "critic_loss": 0.003059212414489593, "actor_loss": -13.964914451062679, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.88578820228577, "step": 41000}
{"episode_reward": 3.1963493945372594, "episode": 42.0, "batch_reward": 0.012965709006879478, "critic_loss": 0.0037548234775313175, "actor_loss": -12.630471141934395, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.954017877578735, "step": 42000}
{"episode_reward": 4.428263571588142, "episode": 43.0, "batch_reward": 0.01275643891398795, "critic_loss": 0.002900351551084896, "actor_loss": -13.130809585571289, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.294787168502808, "step": 43000}
{"episode_reward": 7.53637930394307, "episode": 44.0, "batch_reward": 0.012469803062034771, "critic_loss": 0.0029498149906721665, "actor_loss": -12.371782580256463, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.4760901927948, "step": 44000}
{"episode_reward": 3.630473367061728, "episode": 45.0, "batch_reward": 0.012398053496610374, "critic_loss": 0.0026239273797982604, "actor_loss": -11.790283827066421, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.822521924972534, "step": 45000}
{"episode_reward": 3.496391808267509, "episode": 46.0, "batch_reward": 0.012343413958791643, "critic_loss": 0.002552831991633866, "actor_loss": -13.11107625156641, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.72255253791809, "step": 46000}
{"episode_reward": 4.230965367390606, "episode": 47.0, "batch_reward": 0.012078729158150963, "critic_loss": 0.002646778644702863, "actor_loss": -13.491437460899354, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.52317237854004, "step": 47000}
{"episode_reward": 3.151935354760209, "episode": 48.0, "batch_reward": 0.011819945250404999, "critic_loss": 0.0021447192539926618, "actor_loss": -12.658607133448124, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.417059659957886, "step": 48000}
{"episode_reward": 4.068032896555979, "episode": 49.0, "batch_reward": 0.012005589995067567, "critic_loss": 0.00239237037699786, "actor_loss": -13.09313317501545, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.083887100219727, "step": 49000}
{"episode_reward": 3.6386186887434917, "episode": 50.0, "batch_reward": 0.011693392897723243, "critic_loss": 0.0025920751632438625, "actor_loss": -12.287504204213619, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.038248300552368, "step": 50000}
{"episode_reward": 3.3957052377267547, "episode": 51.0, "batch_reward": 0.011429558050818741, "critic_loss": 0.0020603065327450165, "actor_loss": -11.210204611837863, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.949217319488525, "step": 51000}
{"episode_reward": 4.313828822378433, "episode": 52.0, "batch_reward": 0.011348826453089714, "critic_loss": 0.0022273237163899467, "actor_loss": -13.524070384174586, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.2697856426239, "step": 52000}
{"episode_reward": 3.0691215649275114, "episode": 53.0, "batch_reward": 0.011233863545348867, "critic_loss": 0.0019635874181913094, "actor_loss": -11.19491526028514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.420538902282715, "step": 53000}
{"episode_reward": 3.9920901402102937, "episode": 54.0, "batch_reward": 0.011291763245826586, "critic_loss": 0.0022598995091102554, "actor_loss": -13.632587029397488, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.068387269973755, "step": 54000}
{"episode_reward": 3.2413694672634907, "episode": 55.0, "batch_reward": 0.010748407628387214, "critic_loss": 0.0019657139858682057, "actor_loss": -12.912166299790144, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.996069192886353, "step": 55000}
{"episode_reward": 2.5831182476721555, "episode": 56.0, "batch_reward": 0.010559784607496113, "critic_loss": 0.0018369006239954615, "actor_loss": -11.709199641048908, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.80287766456604, "step": 56000}
{"episode_reward": 3.0357494156090556, "episode": 57.0, "batch_reward": 0.010770813604118303, "critic_loss": 0.0020615974490792725, "actor_loss": -12.140254436939955, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.624040126800537, "step": 57000}
{"episode_reward": 6.61259321065944, "episode": 58.0, "batch_reward": 0.010561770024243742, "critic_loss": 0.0019716618759412084, "actor_loss": -11.505471055060625, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.09284520149231, "step": 58000}
{"episode_reward": 3.6791030113748393, "episode": 59.0, "batch_reward": 0.010261314060539007, "critic_loss": 0.0022535286943384564, "actor_loss": -12.413423415213824, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.259512186050415, "step": 59000}
{"episode_reward": 5.403159306103707, "episode": 60.0, "batch_reward": 0.010299951515858993, "critic_loss": 0.0018583598141995025, "actor_loss": -13.23918658363819, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.319560050964355, "step": 60000}
{"episode_reward": 4.342852040088077, "episode": 61.0, "batch_reward": 0.010119475417071953, "critic_loss": 0.0018861903983051888, "actor_loss": -12.779720108300447, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.5056095123291, "step": 61000}
{"episode_reward": 3.8618357428751535, "episode": 62.0, "batch_reward": 0.009867378821596504, "critic_loss": 0.001840371556714672, "actor_loss": -11.275133064806461, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.164454698562622, "step": 62000}
{"episode_reward": 4.010548103103694, "episode": 63.0, "batch_reward": 0.010029459442012011, "critic_loss": 0.0014594093111954863, "actor_loss": -11.47851517316699, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.057507753372192, "step": 63000}
{"episode_reward": 6.452597795779403, "episode": 64.0, "batch_reward": 0.009809924938366748, "critic_loss": 0.0016628762510154046, "actor_loss": -11.955637147456407, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.392931938171387, "step": 64000}
{"episode_reward": 3.427305756516329, "episode": 65.0, "batch_reward": 0.009940852076280862, "critic_loss": 0.0018485128079119022, "actor_loss": -11.708360164672136, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.54617691040039, "step": 65000}
{"episode_reward": 5.025067411565401, "episode": 66.0, "batch_reward": 0.009558940359856933, "critic_loss": 0.001768960067536682, "actor_loss": -11.832238915503025, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.647488832473755, "step": 66000}
{"episode_reward": 3.391491962179365, "episode": 67.0, "batch_reward": 0.009565101920510643, "critic_loss": 0.0017139940932538592, "actor_loss": -11.032525422215462, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.66043758392334, "step": 67000}
{"episode_reward": 3.802991225241612, "episode": 68.0, "batch_reward": 0.00964846495562233, "critic_loss": 0.0017851985891975345, "actor_loss": -12.022032624870539, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.504830598831177, "step": 68000}
{"episode_reward": 3.900727593190103, "episode": 69.0, "batch_reward": 0.009621711659245193, "critic_loss": 0.0014854771889877157, "actor_loss": -12.063287026122213, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.241792917251587, "step": 69000}
{"episode_reward": 3.6366021165982882, "episode": 70.0, "batch_reward": 0.009316386190243066, "critic_loss": 0.0016011133251013234, "actor_loss": -12.589727369010449, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.79141116142273, "step": 70000}
{"episode_reward": 3.4832566554253064, "episode": 71.0, "batch_reward": 0.009277623888105154, "critic_loss": 0.001982340950489743, "actor_loss": -12.534171690404415, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.204450368881226, "step": 71000}
{"episode_reward": 3.3822234379196434, "episode": 72.0, "batch_reward": 0.009121909972745925, "critic_loss": 0.0014026140415735427, "actor_loss": -12.21820143175125, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.641544818878174, "step": 72000}
{"episode_reward": 3.6796593156642157, "episode": 73.0, "batch_reward": 0.009270182200474665, "critic_loss": 0.001510103270622494, "actor_loss": -12.02681102269888, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.084912300109863, "step": 73000}
{"episode_reward": 2.9947811236322863, "episode": 74.0, "batch_reward": 0.009037974975421094, "critic_loss": 0.0014191408801125362, "actor_loss": -12.351157052457333, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.39398765563965, "step": 74000}
{"episode_reward": 2.974944680861305, "episode": 75.0, "batch_reward": 0.009178336488083005, "critic_loss": 0.001348917065639398, "actor_loss": -12.398030540794133, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.196109533309937, "step": 75000}
{"episode_reward": 3.00480053191357, "episode": 76.0, "batch_reward": 0.00885956278652884, "critic_loss": 0.001241049206306343, "actor_loss": -12.730570713579654, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.120277881622314, "step": 76000}
{"episode_reward": 2.674348401204619, "episode": 77.0, "batch_reward": 0.008672603713814168, "critic_loss": 0.0014107860693329712, "actor_loss": -12.153291618779301, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.41070294380188, "step": 77000}
{"episode_reward": 4.284406592013947, "episode": 78.0, "batch_reward": 0.008861394905019552, "critic_loss": 0.0015740254411502974, "actor_loss": -12.136445731297135, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.026432514190674, "step": 78000}
{"episode_reward": 3.3012467989207366, "episode": 79.0, "batch_reward": 0.00860343566723168, "critic_loss": 0.001259128281060839, "actor_loss": -11.99649998614192, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.96300768852234, "step": 79000}
{"episode_reward": 3.0537162161135463, "episode": 80.0, "batch_reward": 0.008642891634372063, "critic_loss": 0.0017332895636864123, "actor_loss": -12.30312679465115, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.529216051101685, "step": 80000}
{"episode_reward": 3.77739285197715, "episode": 81.0, "batch_reward": 0.008732084390125237, "critic_loss": 0.0014064049505614093, "actor_loss": -11.978695670902729, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.016477823257446, "step": 81000}
{"episode_reward": 5.54085313501991, "episode": 82.0, "batch_reward": 0.008335574024356902, "critic_loss": 0.0013462706322134182, "actor_loss": -11.623885715246201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.1921706199646, "step": 82000}
{"episode_reward": 2.9311123633352305, "episode": 83.0, "batch_reward": 0.008429447943344713, "critic_loss": 0.0016184029266732977, "actor_loss": -12.833319810703397, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.651794910430908, "step": 83000}
{"episode_reward": 6.006795187532479, "episode": 84.0, "batch_reward": 0.008247114765224979, "critic_loss": 0.0014386179899738636, "actor_loss": -12.969628357857466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.79636573791504, "step": 84000}
{"episode_reward": 3.5267132396270693, "episode": 85.0, "batch_reward": 0.008581841358100063, "critic_loss": 0.0013740068193073965, "actor_loss": -11.607867750078439, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.989499807357788, "step": 85000}
{"episode_reward": 3.3757902378926277, "episode": 86.0, "batch_reward": 0.008322234718129038, "critic_loss": 0.0015161559038169797, "actor_loss": -12.275447394907475, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.37787175178528, "step": 86000}
{"episode_reward": 5.135318951221214, "episode": 87.0, "batch_reward": 0.00863276692351792, "critic_loss": 0.0010013450516962621, "actor_loss": -12.89477910964936, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.60342526435852, "step": 87000}
{"episode_reward": 6.6180647403165676, "episode": 88.0, "batch_reward": 0.008483889725059271, "critic_loss": 0.0014552372406687938, "actor_loss": -12.917685898385942, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.55962562561035, "step": 88000}
{"episode_reward": 6.429026953782233, "episode": 89.0, "batch_reward": 0.008299667432904244, "critic_loss": 0.0013976075195896555, "actor_loss": -11.875969608999789, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.6222505569458, "step": 89000}
{"episode_reward": 3.322790631676652, "episode": 90.0, "batch_reward": 0.008090776441502384, "critic_loss": 0.0011702506926158094, "actor_loss": -12.10040141607076, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.297078132629395, "step": 90000}
{"episode_reward": 4.5748088748804605, "episode": 91.0, "batch_reward": 0.00815898413513787, "critic_loss": 0.0015405433331834501, "actor_loss": -11.861840465143324, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.106441020965576, "step": 91000}
{"episode_reward": 3.683879107880494, "episode": 92.0, "batch_reward": 0.008042567642172798, "critic_loss": 0.001193586284403864, "actor_loss": -12.037968826502562, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.70652437210083, "step": 92000}
{"episode_reward": 5.687763023770895, "episode": 93.0, "batch_reward": 0.008105920199071989, "critic_loss": 0.0010760934393474599, "actor_loss": -12.12220118048042, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.911747694015503, "step": 93000}
{"episode_reward": 3.8262687505710553, "episode": 94.0, "batch_reward": 0.007935016472358256, "critic_loss": 0.001476364071117132, "actor_loss": -12.122119227170945, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.4900004863739, "step": 94000}
{"episode_reward": 6.418224653262338, "episode": 95.0, "batch_reward": 0.007948103401111439, "critic_loss": 0.0009219214913755423, "actor_loss": -12.47934443231672, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.860038995742798, "step": 95000}
{"episode_reward": 4.332805958995239, "episode": 96.0, "batch_reward": 0.008070401618257166, "critic_loss": 0.0013862821266884566, "actor_loss": -12.468159421019257, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.109936475753784, "step": 96000}
{"episode_reward": 3.2212320627229114, "episode": 97.0, "batch_reward": 0.007873268121737055, "critic_loss": 0.0011298207209256362, "actor_loss": -12.270160495646298, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.30948519706726, "step": 97000}
{"episode_reward": 5.9227862684248995, "episode": 98.0, "batch_reward": 0.007871850969502702, "critic_loss": 0.001403202979512571, "actor_loss": -11.579923853643239, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.995142698287964, "step": 98000}
{"episode_reward": 3.780657950580535, "episode": 99.0, "batch_reward": 0.008017967855324968, "critic_loss": 0.0011746702434247708, "actor_loss": -12.926245821923018, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.153637170791626, "step": 99000}
{"episode_reward": 3.612092726462971, "episode": 100.0, "batch_reward": 0.007717511150287465, "critic_loss": 0.0013815513190129423, "actor_loss": -12.551411358818411, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.816089630126953, "step": 100000}
{"episode_reward": 3.388017622378017, "episode": 101.0, "batch_reward": 0.0076767179773887615, "critic_loss": 0.0011380490300434759, "actor_loss": -12.525829145118594, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.90407633781433, "step": 101000}
{"episode_reward": 3.2590704364862635, "episode": 102.0, "batch_reward": 0.007605818446259946, "critic_loss": 0.0014425939621723956, "actor_loss": -12.617542435638606, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.12605881690979, "step": 102000}
{"episode_reward": 2.8393977261758616, "episode": 103.0, "batch_reward": 0.007839573826175183, "critic_loss": 0.0012323410605313257, "actor_loss": -12.336309210762382, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.466684103012085, "step": 103000}
{"episode_reward": 3.432806550944478, "episode": 104.0, "batch_reward": 0.00783889264788013, "critic_loss": 0.0012482032766019983, "actor_loss": -13.063142035681754, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.041535139083862, "step": 104000}
{"episode_reward": 6.460026311826957, "episode": 105.0, "batch_reward": 0.007526355852605775, "critic_loss": 0.0014763210015116784, "actor_loss": -12.369172431774437, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.369407892227173, "step": 105000}
{"episode_reward": 2.9345438541608964, "episode": 106.0, "batch_reward": 0.0077705960129387675, "critic_loss": 0.0011693612174203736, "actor_loss": -12.954005451276899, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.99854326248169, "step": 106000}
{"episode_reward": 5.9225653728451, "episode": 107.0, "batch_reward": 0.007540532501065172, "critic_loss": 0.0010233466647405294, "actor_loss": -12.698067869994789, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.733672618865967, "step": 107000}
{"episode_reward": 3.850157741348297, "episode": 108.0, "batch_reward": 0.007387413342250511, "critic_loss": 0.0013000622544241196, "actor_loss": -11.378607924047857, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.98133420944214, "step": 108000}
{"episode_reward": 4.271435782397016, "episode": 109.0, "batch_reward": 0.007531626302283257, "critic_loss": 0.001146599360188702, "actor_loss": -13.335845227811486, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.854201078414917, "step": 109000}
{"episode_reward": 3.3639292938581478, "episode": 110.0, "batch_reward": 0.007426353965653106, "critic_loss": 0.0010783922200353119, "actor_loss": -13.320119231306016, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.673418760299683, "step": 110000}
{"episode_reward": 3.4524392702092133, "episode": 111.0, "batch_reward": 0.0074221596873831, "critic_loss": 0.001276941044601699, "actor_loss": -11.643895483452827, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.90677857398987, "step": 111000}
{"episode_reward": 4.153525956343094, "episode": 112.0, "batch_reward": 0.007529372503748163, "critic_loss": 0.0011068101877390291, "actor_loss": -13.663520661596209, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.085477590560913, "step": 112000}
{"episode_reward": 7.480017695758852, "episode": 113.0, "batch_reward": 0.007475489086005837, "critic_loss": 0.0013275408062690985, "actor_loss": -12.588733190454542, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.256307363510132, "step": 113000}
{"episode_reward": 3.0986015195645376, "episode": 114.0, "batch_reward": 0.007478668816038408, "critic_loss": 0.0014509627336665289, "actor_loss": -12.74167378316447, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.219853401184082, "step": 114000}
{"episode_reward": 3.215264870127837, "episode": 115.0, "batch_reward": 0.007200671284459531, "critic_loss": 0.0013166731657438505, "actor_loss": -12.685282552413643, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.370280504226685, "step": 115000}
{"episode_reward": 4.470151251523035, "episode": 116.0, "batch_reward": 0.007281492140609771, "critic_loss": 0.001024663186432008, "actor_loss": -12.77472534738481, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.81436514854431, "step": 116000}
{"episode_reward": 4.345499977439743, "episode": 117.0, "batch_reward": 0.007574687957763672, "critic_loss": 0.0012801268607727252, "actor_loss": -10.995544680543244, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.525737285614014, "step": 117000}
{"episode_reward": 3.4747668524437785, "episode": 118.0, "batch_reward": 0.0071760996266966685, "critic_loss": 0.0010215314576780656, "actor_loss": -11.94720056675002, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.423319101333618, "step": 118000}
{"episode_reward": 2.919837728176277, "episode": 119.0, "batch_reward": 0.007201165319653228, "critic_loss": 0.0010709393314209592, "actor_loss": -12.382855095684528, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.98343062400818, "step": 119000}
{"episode_reward": 3.7626161540193706, "episode": 120.0, "batch_reward": 0.0071799737948458646, "critic_loss": 0.0013564970581574015, "actor_loss": -11.556823728259653, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.581465482711792, "step": 120000}
{"episode_reward": 4.344783611567065, "episode": 121.0, "batch_reward": 0.007158131197909825, "critic_loss": 0.0009455887305630313, "actor_loss": -11.573950312223285, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.72497272491455, "step": 121000}
{"episode_reward": 4.091788114524206, "episode": 122.0, "batch_reward": 0.007222411633469164, "critic_loss": 0.0013679023463773774, "actor_loss": -11.99279761462286, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.23999810218811, "step": 122000}
{"episode_reward": 3.430677885998077, "episode": 123.0, "batch_reward": 0.006963085097260773, "critic_loss": 0.0011720528000332706, "actor_loss": -12.46797950386256, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.371558666229248, "step": 123000}
{"episode_reward": 4.028598126154127, "episode": 124.0, "batch_reward": 0.007053924984415062, "critic_loss": 0.0011462530977769348, "actor_loss": -12.413958463648335, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.50969433784485, "step": 124000}
{"episode_reward": 5.62011335934538, "episode": 125.0, "batch_reward": 0.007103388157440349, "critic_loss": 0.001117187579140591, "actor_loss": -12.422650156259536, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.402241945266724, "step": 125000}
{"episode_reward": 4.476574093890274, "episode": 126.0, "batch_reward": 0.007234296669019386, "critic_loss": 0.0014848440884888986, "actor_loss": -11.792688083257525, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.026858806610107, "step": 126000}
{"episode_reward": 3.967342137355711, "episode": 127.0, "batch_reward": 0.007085455978056416, "critic_loss": 0.0012912238716708089, "actor_loss": -12.938481898210943, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.448647022247314, "step": 127000}
{"episode_reward": 3.9641316251443426, "episode": 128.0, "batch_reward": 0.006710208498174324, "critic_loss": 0.001256861473157187, "actor_loss": -12.257449516233057, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.50869345664978, "step": 128000}
{"episode_reward": 3.959977613565445, "episode": 129.0, "batch_reward": 0.007132962678326294, "critic_loss": 0.0015480929212390037, "actor_loss": -11.735400401026011, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.535324096679688, "step": 129000}
{"episode_reward": 3.199262203546727, "episode": 130.0, "batch_reward": 0.006730740302242339, "critic_loss": 0.0008923882192393649, "actor_loss": -11.718752816673367, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.14009714126587, "step": 130000}
{"episode_reward": 4.345925549664771, "episode": 131.0, "batch_reward": 0.006881313538644463, "critic_loss": 0.0011666406166332308, "actor_loss": -12.512459462074563, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.09267616271973, "step": 131000}
{"episode_reward": 3.2868533421961454, "episode": 132.0, "batch_reward": 0.0068420443512732165, "critic_loss": 0.0012309498729991901, "actor_loss": -12.689693828867748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.12485909461975, "step": 132000}
{"episode_reward": 3.0551015548720457, "episode": 133.0, "batch_reward": 0.006758624602924101, "critic_loss": 0.0009343139808843261, "actor_loss": -11.666497301021591, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.166895866394043, "step": 133000}
{"episode_reward": 2.7557076383263492, "episode": 134.0, "batch_reward": 0.006969268304645084, "critic_loss": 0.001334762095582846, "actor_loss": -11.151078387822956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.76953434944153, "step": 134000}
{"episode_reward": 6.251132399482149, "episode": 135.0, "batch_reward": 0.006794114662101492, "critic_loss": 0.0011583808888972272, "actor_loss": -12.867262340236454, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.40831732749939, "step": 135000}
{"episode_reward": 2.7188199438830547, "episode": 136.0, "batch_reward": 0.006695973625872285, "critic_loss": 0.0010980500616351492, "actor_loss": -10.163831940991804, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.59268856048584, "step": 136000}
{"episode_reward": 3.4720120592262433, "episode": 137.0, "batch_reward": 0.006737514664186164, "critic_loss": 0.001090354662755999, "actor_loss": -12.176965748591348, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.741621017456055, "step": 137000}
{"episode_reward": 3.5716699538331422, "episode": 138.0, "batch_reward": 0.006808474178425968, "critic_loss": 0.0012368317722684877, "actor_loss": -12.034352757411078, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.594238996505737, "step": 138000}
{"episode_reward": 4.973939919434144, "episode": 139.0, "batch_reward": 0.006700848031323403, "critic_loss": 0.000991358620045503, "actor_loss": -12.036002791076898, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.633946180343628, "step": 139000}
{"episode_reward": 4.3347176240788645, "episode": 140.0, "batch_reward": 0.006590666596894152, "critic_loss": 0.0011247902987852284, "actor_loss": -12.230010125530884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.780073881149292, "step": 140000}
{"episode_reward": 3.7636244992315255, "episode": 141.0, "batch_reward": 0.00691626871842891, "critic_loss": 0.00128729243206908, "actor_loss": -12.433509059622883, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.191593647003174, "step": 141000}
{"episode_reward": 3.536734306491452, "episode": 142.0, "batch_reward": 0.006720389827154577, "critic_loss": 0.0010755579237556958, "actor_loss": -11.076856863763183, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.926087379455566, "step": 142000}
{"episode_reward": 5.154301492341382, "episode": 143.0, "batch_reward": 0.006778190533746965, "critic_loss": 0.0010293931705564318, "actor_loss": -10.929190112834796, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.157179355621338, "step": 143000}
{"episode_reward": 4.871880186226065, "episode": 144.0, "batch_reward": 0.006437573663890362, "critic_loss": 0.0010799902450926312, "actor_loss": -12.609677460062317, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.165833473205566, "step": 144000}
{"episode_reward": 2.798700165402143, "episode": 145.0, "batch_reward": 0.006534403730067424, "critic_loss": 0.0011114952862990321, "actor_loss": -12.315833680288867, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.23836374282837, "step": 145000}
{"episode_reward": 3.201494411052279, "episode": 146.0, "batch_reward": 0.006647816505050286, "critic_loss": 0.0010236042984579398, "actor_loss": -12.37926825161092, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.194623947143555, "step": 146000}
{"episode_reward": 3.2657437315000313, "episode": 147.0, "batch_reward": 0.0065575184386689215, "critic_loss": 0.0008328520023314922, "actor_loss": -12.422345319993793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.12956213951111, "step": 147000}
{"episode_reward": 6.39780737653215, "episode": 148.0, "batch_reward": 0.006630678964080289, "critic_loss": 0.0011108547944604652, "actor_loss": -12.031548819232732, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.80087113380432, "step": 148000}
{"episode_reward": 3.1059168609207113, "episode": 149.0, "batch_reward": 0.006565164911095053, "critic_loss": 0.0012633848966943334, "actor_loss": -12.122642172886058, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.213335037231445, "step": 149000}
{"episode_reward": 4.233278067808789, "episode": 150.0, "batch_reward": 0.006932070698356256, "critic_loss": 0.0012377619551116367, "actor_loss": -12.475010637484491, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
