{"episode_reward": 0.0, "episode": 1.0, "duration": 19.243581533432007, "step": 1000}
{"episode_reward": 4.231880753996205, "episode": 2.0, "duration": 1.741149663925171, "step": 2000}
{"episode_reward": 395.1450543749875, "episode": 3.0, "batch_reward": 0.18721398641891024, "critic_loss": 0.019954650982389573, "actor_loss": -18.29594877708544, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 69.14844369888306, "step": 3000}
{"episode_reward": 2.2734008472274745, "episode": 4.0, "batch_reward": 0.11639364097267389, "critic_loss": 0.01076679620728828, "actor_loss": -16.214459819793703, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.25147533416748, "step": 4000}
{"episode_reward": 2.4156260426321614, "episode": 5.0, "batch_reward": 0.09112498321011663, "critic_loss": 0.01465138961514458, "actor_loss": -16.021526176929473, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.75221610069275, "step": 5000}
{"episode_reward": 3.499472901460872, "episode": 6.0, "batch_reward": 0.07496359745413064, "critic_loss": 0.011182541931979359, "actor_loss": -14.66956829071045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.840433835983276, "step": 6000}
{"episode_reward": 3.109925912549609, "episode": 7.0, "batch_reward": 0.06409310296550393, "critic_loss": 0.012358075950527564, "actor_loss": -14.459182246685028, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.225386142730713, "step": 7000}
{"episode_reward": 3.068079003161561, "episode": 8.0, "batch_reward": 0.05585655191820115, "critic_loss": 0.017307442974299193, "actor_loss": -14.542963452339173, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.64757800102234, "step": 8000}
{"episode_reward": 4.727662127927882, "episode": 9.0, "batch_reward": 0.04918814017623663, "critic_loss": 0.016152661149855703, "actor_loss": -15.038859091281891, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.669685125350952, "step": 9000}
{"episode_reward": 3.7689116767388713, "episode": 10.0, "batch_reward": 0.04506655486673117, "critic_loss": 0.019178828721866013, "actor_loss": -14.793699342250823, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.118549823760986, "step": 10000}
{"episode_reward": 3.802524697694739, "episode": 11.0, "batch_reward": 0.04067460705060512, "critic_loss": 0.01572231558058411, "actor_loss": -13.403773458957673, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 44.40417718887329, "step": 11000}
{"episode_reward": 3.5481261921234957, "episode": 12.0, "batch_reward": 0.03790259304549545, "critic_loss": 0.0207666484254878, "actor_loss": -14.28847607421875, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.348742485046387, "step": 12000}
{"episode_reward": 3.7123213636273107, "episode": 13.0, "batch_reward": 0.03520190542563796, "critic_loss": 0.016719270211877302, "actor_loss": -12.97402041053772, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.265048027038574, "step": 13000}
{"episode_reward": 3.7192360571368885, "episode": 14.0, "batch_reward": 0.032172350162640216, "critic_loss": 0.013725886755040847, "actor_loss": -13.101148170471191, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.163256645202637, "step": 14000}
{"episode_reward": 3.493259869744762, "episode": 15.0, "batch_reward": 0.030602857273072003, "critic_loss": 0.009271506658231374, "actor_loss": -11.306613095283508, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.711267232894897, "step": 15000}
{"episode_reward": 4.070902039334523, "episode": 16.0, "batch_reward": 0.028510476063005625, "critic_loss": 0.011924003318417817, "actor_loss": -14.40229948592186, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.040207386016846, "step": 16000}
{"episode_reward": 4.858528396976178, "episode": 17.0, "batch_reward": 0.02719723512325436, "critic_loss": 0.00956450515112374, "actor_loss": -13.861037023067475, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.27401614189148, "step": 17000}
{"episode_reward": 4.3655378800277616, "episode": 18.0, "batch_reward": 0.025751724580768496, "critic_loss": 0.008511479762557428, "actor_loss": -13.59990027475357, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.01058864593506, "step": 18000}
{"episode_reward": 3.849523845386237, "episode": 19.0, "batch_reward": 0.025238992337835952, "critic_loss": 0.00907018510188209, "actor_loss": -13.628941599845886, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.146877765655518, "step": 19000}
{"episode_reward": 3.8648105779321407, "episode": 20.0, "batch_reward": 0.0240543130482547, "critic_loss": 0.007428742848074762, "actor_loss": -12.396036750555039, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.31378173828125, "step": 20000}
{"episode_reward": 3.356569966506698, "episode": 21.0, "batch_reward": 0.022355783286504446, "critic_loss": 0.006745073804340791, "actor_loss": -13.147055727481842, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.15006470680237, "step": 21000}
{"episode_reward": 3.8910106687321946, "episode": 22.0, "batch_reward": 0.021661389071727172, "critic_loss": 0.008027866957243533, "actor_loss": -11.480558356523513, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.707687854766846, "step": 22000}
{"episode_reward": 4.178596806834469, "episode": 23.0, "batch_reward": 0.021079383921111004, "critic_loss": 0.006743365624715807, "actor_loss": -12.420587476730347, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.687268495559692, "step": 23000}
{"episode_reward": 3.7523887190152863, "episode": 24.0, "batch_reward": 0.020483899026643486, "critic_loss": 0.005652629349206109, "actor_loss": -11.822942643404007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.746574878692627, "step": 24000}
{"episode_reward": 3.620669250083263, "episode": 25.0, "batch_reward": 0.019660692365141584, "critic_loss": 0.005459734289703192, "actor_loss": -12.084889168739318, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.86860418319702, "step": 25000}
{"episode_reward": 4.379144222935512, "episode": 26.0, "batch_reward": 0.019444288008613513, "critic_loss": 0.004677636329637608, "actor_loss": -12.625473870873451, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.19399356842041, "step": 26000}
{"episode_reward": 4.277031263103414, "episode": 27.0, "batch_reward": 0.018803116122027858, "critic_loss": 0.004863055303692818, "actor_loss": -11.102938516259194, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.496630430221558, "step": 27000}
{"episode_reward": 3.8291817797670675, "episode": 28.0, "batch_reward": 0.01752903735009022, "critic_loss": 0.005686957130557857, "actor_loss": -12.513910602211952, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.006531238555908, "step": 28000}
{"episode_reward": 3.7784985866473395, "episode": 29.0, "batch_reward": 0.017138810218311845, "critic_loss": 0.00451497627448407, "actor_loss": -11.124534044027328, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.407362461090088, "step": 29000}
{"episode_reward": 3.733995300595865, "episode": 30.0, "batch_reward": 0.017322791375918314, "critic_loss": 0.006334481368219713, "actor_loss": -11.640367172956466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.779072999954224, "step": 30000}
{"episode_reward": 4.637622317333474, "episode": 31.0, "batch_reward": 0.016554346635937692, "critic_loss": 0.003746143455253332, "actor_loss": -12.619294619321822, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.86940383911133, "step": 31000}
{"episode_reward": 4.112052607644333, "episode": 32.0, "batch_reward": 0.016169963180786, "critic_loss": 0.005764028483637958, "actor_loss": -11.771834245562554, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.178125858306885, "step": 32000}
{"episode_reward": 3.3736313915761125, "episode": 33.0, "batch_reward": 0.01617707714694552, "critic_loss": 0.005262181987462099, "actor_loss": -10.512959215641022, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.81318974494934, "step": 33000}
{"episode_reward": 4.0103588306371165, "episode": 34.0, "batch_reward": 0.01531204152177088, "critic_loss": 0.003727858979458688, "actor_loss": -12.113661437869071, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.606884717941284, "step": 34000}
{"episode_reward": 3.5187726781015103, "episode": 35.0, "batch_reward": 0.015092956901760771, "critic_loss": 0.003655160084468662, "actor_loss": -12.258301557540893, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.09782075881958, "step": 35000}
{"episode_reward": 3.621921554169297, "episode": 36.0, "batch_reward": 0.014801247214432807, "critic_loss": 0.004016212225673371, "actor_loss": -12.010288105130195, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.027687788009644, "step": 36000}
{"episode_reward": 4.143238589507888, "episode": 37.0, "batch_reward": 0.01446999157895334, "critic_loss": 0.0035518427836796038, "actor_loss": -11.973379929661752, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.816272020339966, "step": 37000}
{"episode_reward": 3.1898350798721413, "episode": 38.0, "batch_reward": 0.01424400846217759, "critic_loss": 0.004317149675218388, "actor_loss": -10.804089616894721, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.249546766281128, "step": 38000}
{"episode_reward": 3.296985403495561, "episode": 39.0, "batch_reward": 0.014152400265447795, "critic_loss": 0.003953310709737707, "actor_loss": -11.30901643061638, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.35533356666565, "step": 39000}
{"episode_reward": 2.9821470101767744, "episode": 40.0, "batch_reward": 0.013460818264633417, "critic_loss": 0.004555806052914704, "actor_loss": -11.99539452445507, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.030839920043945, "step": 40000}
{"episode_reward": 2.9969810573317, "episode": 41.0, "batch_reward": 0.01344292221055366, "critic_loss": 0.002732997236555093, "actor_loss": -12.69495656490326, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 44.067941188812256, "step": 41000}
{"episode_reward": 5.102975073608349, "episode": 42.0, "batch_reward": 0.01313044100231491, "critic_loss": 0.0036060059336741686, "actor_loss": -11.665287092208862, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.019729614257812, "step": 42000}
{"episode_reward": 3.6190087189446984, "episode": 43.0, "batch_reward": 0.012839967038715257, "critic_loss": 0.0038325897236791205, "actor_loss": -11.592196197390557, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.156769514083862, "step": 43000}
{"episode_reward": 3.8018259248640396, "episode": 44.0, "batch_reward": 0.01251842620037496, "critic_loss": 0.002741352587545407, "actor_loss": -11.238036421358585, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.62723207473755, "step": 44000}
{"episode_reward": 3.4717315198110343, "episode": 45.0, "batch_reward": 0.012478656830033288, "critic_loss": 0.004104128129882156, "actor_loss": -10.460725429952145, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.94085121154785, "step": 45000}
{"episode_reward": 4.030859313470437, "episode": 46.0, "batch_reward": 0.012415065784240141, "critic_loss": 0.0028669194406538735, "actor_loss": -11.258329874098301, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.486549854278564, "step": 46000}
{"episode_reward": 4.490950810263049, "episode": 47.0, "batch_reward": 0.012212450121063739, "critic_loss": 0.0041388125507655785, "actor_loss": -12.243957313597202, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.134007215499878, "step": 47000}
{"episode_reward": 3.892459891512523, "episode": 48.0, "batch_reward": 0.011932290483498946, "critic_loss": 0.0031928198134701232, "actor_loss": -11.00556440883875, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.954607725143433, "step": 48000}
{"episode_reward": 3.5503207942243167, "episode": 49.0, "batch_reward": 0.012139776316238568, "critic_loss": 0.0035947771683204335, "actor_loss": -11.11146682792902, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.1622531414032, "step": 49000}
{"episode_reward": 4.04240506522976, "episode": 50.0, "batch_reward": 0.011825466965325177, "critic_loss": 0.0031868480008197366, "actor_loss": -11.794755633473397, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.565165758132935, "step": 50000}
{"episode_reward": 4.2197270994326415, "episode": 51.0, "batch_reward": 0.011548311229795218, "critic_loss": 0.0027185222925108976, "actor_loss": -9.75026924198866, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.79791712760925, "step": 51000}
{"episode_reward": 4.236837763237261, "episode": 52.0, "batch_reward": 0.011547901315381751, "critic_loss": 0.0030456430446356535, "actor_loss": -12.311617145597936, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.051037311553955, "step": 52000}
{"episode_reward": 4.080403242726235, "episode": 53.0, "batch_reward": 0.011380462401546538, "critic_loss": 0.0034660286870712297, "actor_loss": -10.120827058434486, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.337405920028687, "step": 53000}
{"episode_reward": 3.9132398911700803, "episode": 54.0, "batch_reward": 0.011443469116464258, "critic_loss": 0.0027584827259779557, "actor_loss": -11.956634268522263, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.248213291168213, "step": 54000}
{"episode_reward": 4.4902823174268685, "episode": 55.0, "batch_reward": 0.010924062152160331, "critic_loss": 0.0026214424262871036, "actor_loss": -11.605061326622963, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.423530340194702, "step": 55000}
{"episode_reward": 3.481074286072291, "episode": 56.0, "batch_reward": 0.010748792425729334, "critic_loss": 0.0024147202649983227, "actor_loss": -10.22387104165554, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.668951511383057, "step": 56000}
{"episode_reward": 3.31552554360378, "episode": 57.0, "batch_reward": 0.010821248688269407, "critic_loss": 0.0033465605293749832, "actor_loss": -10.807212434709072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.010635375976562, "step": 57000}
{"episode_reward": 3.6276037429055066, "episode": 58.0, "batch_reward": 0.010622479879530147, "critic_loss": 0.0031009320426019257, "actor_loss": -10.150960878610611, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.88584613800049, "step": 58000}
{"episode_reward": 4.0061991026168835, "episode": 59.0, "batch_reward": 0.010358861040789634, "critic_loss": 0.0028132709711426286, "actor_loss": -11.439665728032589, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.690556526184082, "step": 59000}
{"episode_reward": 4.37760881577244, "episode": 60.0, "batch_reward": 0.010435282605933025, "critic_loss": 0.00234769857901847, "actor_loss": -12.146597215861082, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.488999128341675, "step": 60000}
{"episode_reward": 3.9628591312586265, "episode": 61.0, "batch_reward": 0.010179588519269601, "critic_loss": 0.0020087308662041325, "actor_loss": -11.146237063109874, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.92418909072876, "step": 61000}
{"episode_reward": 4.434956220224983, "episode": 62.0, "batch_reward": 0.009901121069677174, "critic_loss": 0.0031201247214048635, "actor_loss": -9.809777584582568, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.532084941864014, "step": 62000}
{"episode_reward": 3.9622230931985376, "episode": 63.0, "batch_reward": 0.010013445587130264, "critic_loss": 0.002469123030146875, "actor_loss": -9.698712099671363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.732271909713745, "step": 63000}
{"episode_reward": 4.084724620052628, "episode": 64.0, "batch_reward": 0.009846410122700036, "critic_loss": 0.002311738420787151, "actor_loss": -10.952777197122574, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.33127999305725, "step": 64000}
{"episode_reward": 4.407018563933911, "episode": 65.0, "batch_reward": 0.010008475687587634, "critic_loss": 0.002634010036534164, "actor_loss": -10.783605026990175, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.5084228515625, "step": 65000}
{"episode_reward": 4.676597031874457, "episode": 66.0, "batch_reward": 0.009639147028559818, "critic_loss": 0.0025273014859558316, "actor_loss": -10.216518252104521, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.625764846801758, "step": 66000}
{"episode_reward": 3.3812578106327686, "episode": 67.0, "batch_reward": 0.00959260645112954, "critic_loss": 0.002715867045837513, "actor_loss": -10.587135504126548, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.676921367645264, "step": 67000}
{"episode_reward": 3.942759085341038, "episode": 68.0, "batch_reward": 0.009740746819647029, "critic_loss": 0.003087095015835075, "actor_loss": -11.257991328030824, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.730717420578003, "step": 68000}
{"episode_reward": 3.760652873608559, "episode": 69.0, "batch_reward": 0.009723146597854794, "critic_loss": 0.0022394544496928573, "actor_loss": -10.749223865002394, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.201215267181396, "step": 69000}
{"episode_reward": 4.114999428159966, "episode": 70.0, "batch_reward": 0.009434308174764738, "critic_loss": 0.0019720954628719483, "actor_loss": -11.668254045367242, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.7093026638031, "step": 70000}
{"episode_reward": 4.019545673816387, "episode": 71.0, "batch_reward": 0.0094274586008396, "critic_loss": 0.0030232587791251715, "actor_loss": -11.340408732682466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.96033215522766, "step": 71000}
{"episode_reward": 3.430259045046076, "episode": 72.0, "batch_reward": 0.009241180351236835, "critic_loss": 0.0022084167023058397, "actor_loss": -10.838085788220168, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.71061682701111, "step": 72000}
{"episode_reward": 4.007844532595559, "episode": 73.0, "batch_reward": 0.009301762635586783, "critic_loss": 0.002114754275371524, "actor_loss": -10.828798887521028, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.64229440689087, "step": 73000}
{"episode_reward": 4.159031796515806, "episode": 74.0, "batch_reward": 0.009116359933977946, "critic_loss": 0.0017325594054273096, "actor_loss": -11.801985966086388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.41532611846924, "step": 74000}
{"episode_reward": 4.739327536433323, "episode": 75.0, "batch_reward": 0.009315612475620582, "critic_loss": 0.0019496628826746018, "actor_loss": -11.423386955350637, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.14645218849182, "step": 75000}
{"episode_reward": 3.4019746616550015, "episode": 76.0, "batch_reward": 0.008905705581186339, "critic_loss": 0.0015002554182210587, "actor_loss": -12.227584525793791, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.53706192970276, "step": 76000}
{"episode_reward": 3.4025220324611847, "episode": 77.0, "batch_reward": 0.008778489620890468, "critic_loss": 0.001981344180578162, "actor_loss": -10.568361244797707, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.833454608917236, "step": 77000}
{"episode_reward": 3.7617350128665383, "episode": 78.0, "batch_reward": 0.009032693616813048, "critic_loss": 0.0020497698812396267, "actor_loss": -10.800453386247158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.027209043502808, "step": 78000}
{"episode_reward": 3.465825387056695, "episode": 79.0, "batch_reward": 0.008736966870725156, "critic_loss": 0.0015997895417458495, "actor_loss": -10.448226577818394, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.87710952758789, "step": 79000}
{"episode_reward": 3.539872850059088, "episode": 80.0, "batch_reward": 0.008759931286796927, "critic_loss": 0.003186608104922925, "actor_loss": -11.384093245983124, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.2333025932312, "step": 80000}
{"episode_reward": 4.048425145663683, "episode": 81.0, "batch_reward": 0.008812406165990978, "critic_loss": 0.002083261806415976, "actor_loss": -10.6836326110363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.514063358306885, "step": 81000}
{"episode_reward": 4.233175188679052, "episode": 82.0, "batch_reward": 0.008511660146759824, "critic_loss": 0.002502835387895175, "actor_loss": -10.675537346839905, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.60775899887085, "step": 82000}
{"episode_reward": 3.6996751855866825, "episode": 83.0, "batch_reward": 0.008460534499492496, "critic_loss": 0.0028344617212569573, "actor_loss": -11.65003360952437, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.527639865875244, "step": 83000}
{"episode_reward": 3.0314328400095203, "episode": 84.0, "batch_reward": 0.008256380716338753, "critic_loss": 0.0019429559309064644, "actor_loss": -11.806208516091107, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.815826416015625, "step": 84000}
{"episode_reward": 3.1340738038702964, "episode": 85.0, "batch_reward": 0.008665548653807491, "critic_loss": 0.002783130457653897, "actor_loss": -10.281214919582009, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.518179416656494, "step": 85000}
{"episode_reward": 3.6942634606476377, "episode": 86.0, "batch_reward": 0.00843894022819586, "critic_loss": 0.0026153601977202926, "actor_loss": -11.152052247583866, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.895219326019287, "step": 86000}
{"episode_reward": 4.262882382966037, "episode": 87.0, "batch_reward": 0.00863227102928795, "critic_loss": 0.001740319265292783, "actor_loss": -11.253833160579205, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.404123306274414, "step": 87000}
{"episode_reward": 3.816167063957014, "episode": 88.0, "batch_reward": 0.008484526715474203, "critic_loss": 0.001604079382675991, "actor_loss": -11.421778919830919, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.59029531478882, "step": 88000}
{"episode_reward": 3.8177056330623973, "episode": 89.0, "batch_reward": 0.008326172790490092, "critic_loss": 0.002519737482383789, "actor_loss": -10.755945833623409, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.73949909210205, "step": 89000}
{"episode_reward": 4.045992589957152, "episode": 90.0, "batch_reward": 0.008119079458294437, "critic_loss": 0.001788222982911975, "actor_loss": -10.851868904963135, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.40655517578125, "step": 90000}
{"episode_reward": 3.0303495116408428, "episode": 91.0, "batch_reward": 0.008164892232511193, "critic_loss": 0.0021335659207979917, "actor_loss": -10.747492853909732, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.77760648727417, "step": 91000}
{"episode_reward": 3.6778850609721037, "episode": 92.0, "batch_reward": 0.008060644118580967, "critic_loss": 0.0016861778998463707, "actor_loss": -11.020443417936564, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.661454916000366, "step": 92000}
{"episode_reward": 3.354445124296742, "episode": 93.0, "batch_reward": 0.008076564451213925, "critic_loss": 0.0011611620612129627, "actor_loss": -10.892481381818651, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.549174308776855, "step": 93000}
{"episode_reward": 3.5817696753051274, "episode": 94.0, "batch_reward": 0.007868516888236627, "critic_loss": 0.002258476266506477, "actor_loss": -11.219085505872965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.04269027709961, "step": 94000}
{"episode_reward": 3.115137122451614, "episode": 95.0, "batch_reward": 0.007937110961647705, "critic_loss": 0.0012735317928090808, "actor_loss": -10.675238043516874, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.2728214263916, "step": 95000}
{"episode_reward": 4.24825287713137, "episode": 96.0, "batch_reward": 0.007921751769725233, "critic_loss": 0.0020125523411916217, "actor_loss": -11.977797859072686, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.193726062774658, "step": 96000}
{"episode_reward": 4.3669104474224785, "episode": 97.0, "batch_reward": 0.007779256324050948, "critic_loss": 0.0018629446235572686, "actor_loss": -10.880255937054754, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.412626028060913, "step": 97000}
{"episode_reward": 3.1151513393588752, "episode": 98.0, "batch_reward": 0.007775175311835482, "critic_loss": 0.0018335502011104837, "actor_loss": -10.341220593452453, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.215450525283813, "step": 98000}
{"episode_reward": 3.8368091560296236, "episode": 99.0, "batch_reward": 0.007898813501698897, "critic_loss": 0.0016338870171020972, "actor_loss": -12.075388847470283, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.51611304283142, "step": 99000}
{"episode_reward": 3.7215762451807475, "episode": 100.0, "batch_reward": 0.007616642283275723, "critic_loss": 0.0020824590834163247, "actor_loss": -11.203656246215106, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.703023672103882, "step": 100000}
{"episode_reward": 2.9995980156291213, "episode": 101.0, "batch_reward": 0.007601222241297364, "critic_loss": 0.0014652475740513182, "actor_loss": -11.413944621875881, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.53480100631714, "step": 101000}
{"episode_reward": 2.7079316253376007, "episode": 102.0, "batch_reward": 0.007522560690529644, "critic_loss": 0.001579953303975344, "actor_loss": -10.969955992102623, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.36202096939087, "step": 102000}
{"episode_reward": 3.401366508797917, "episode": 103.0, "batch_reward": 0.007713260245975107, "critic_loss": 0.0023740388940932462, "actor_loss": -10.822630843147635, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.43091344833374, "step": 103000}
{"episode_reward": 3.395236172610845, "episode": 104.0, "batch_reward": 0.007818021427141502, "critic_loss": 0.0015398130458524975, "actor_loss": -11.638655199930072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.152303457260132, "step": 104000}
{"episode_reward": 3.3628037472826158, "episode": 105.0, "batch_reward": 0.007471007389947772, "critic_loss": 0.0018793186873772357, "actor_loss": -11.670862735673785, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.99877643585205, "step": 105000}
{"episode_reward": 4.033572177068699, "episode": 106.0, "batch_reward": 0.007609915544046089, "critic_loss": 0.0017030459604648058, "actor_loss": -11.73828723229468, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.065688133239746, "step": 106000}
{"episode_reward": 4.3873031408742245, "episode": 107.0, "batch_reward": 0.007460702896583825, "critic_loss": 0.0010395676637817814, "actor_loss": -11.880042895764113, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.980294227600098, "step": 107000}
{"episode_reward": 3.508822710642636, "episode": 108.0, "batch_reward": 0.0072261611563153565, "critic_loss": 0.0019837771180318668, "actor_loss": -10.297233331754803, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.07299041748047, "step": 108000}
{"episode_reward": 4.17858429362067, "episode": 109.0, "batch_reward": 0.00741420705919154, "critic_loss": 0.0012946948475691896, "actor_loss": -12.106622843325137, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.67848563194275, "step": 109000}
{"episode_reward": 3.2127749000739962, "episode": 110.0, "batch_reward": 0.0072711926631163805, "critic_loss": 0.0011014999993458332, "actor_loss": -12.097007382661104, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.22555708885193, "step": 110000}
{"episode_reward": 3.419646802327184, "episode": 111.0, "batch_reward": 0.007331658988492563, "critic_loss": 0.001448756503948971, "actor_loss": -10.253151168644429, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.06696653366089, "step": 111000}
{"episode_reward": 4.5450314259358215, "episode": 112.0, "batch_reward": 0.007341052440926432, "critic_loss": 0.00141945608844253, "actor_loss": -12.809885821446777, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.72121787071228, "step": 112000}
{"episode_reward": 3.393444219416092, "episode": 113.0, "batch_reward": 0.007354997797403485, "critic_loss": 0.001429234397412074, "actor_loss": -11.394052130922676, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.39399766921997, "step": 113000}
{"episode_reward": 3.800642288973558, "episode": 114.0, "batch_reward": 0.007297605991596356, "critic_loss": 0.0020494530907708393, "actor_loss": -11.661591541394591, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.998836994171143, "step": 114000}
{"episode_reward": 4.445696766175153, "episode": 115.0, "batch_reward": 0.007077991634141654, "critic_loss": 0.002135782214681967, "actor_loss": -11.438168752253056, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.21963119506836, "step": 115000}
{"episode_reward": 4.434062545043438, "episode": 116.0, "batch_reward": 0.007101675080135464, "critic_loss": 0.0015747777889409918, "actor_loss": -11.05159695609659, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.357763051986694, "step": 116000}
{"episode_reward": 4.747830478353679, "episode": 117.0, "batch_reward": 0.0074830221184529365, "critic_loss": 0.0019359241535021282, "actor_loss": -9.818404934309424, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.957791328430176, "step": 117000}
{"episode_reward": 4.328158862911097, "episode": 118.0, "batch_reward": 0.007065408691065386, "critic_loss": 0.0013532901340186072, "actor_loss": -10.662249179393053, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.03649115562439, "step": 118000}
{"episode_reward": 3.7838514580577463, "episode": 119.0, "batch_reward": 0.007032840925035999, "critic_loss": 0.001729512228997919, "actor_loss": -10.661770335689187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.961281538009644, "step": 119000}
{"episode_reward": 3.5630081843032273, "episode": 120.0, "batch_reward": 0.0070317117671947925, "critic_loss": 0.0019298319977897336, "actor_loss": -9.839893695563077, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.62029004096985, "step": 120000}
{"episode_reward": 4.305595020281561, "episode": 121.0, "batch_reward": 0.007044616610044614, "critic_loss": 0.0012262374983401968, "actor_loss": -10.72856741258502, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.910120725631714, "step": 121000}
{"episode_reward": 3.777068250685887, "episode": 122.0, "batch_reward": 0.007035314409062267, "critic_loss": 0.001798470955589437, "actor_loss": -10.667064170010388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.58680248260498, "step": 122000}
{"episode_reward": 4.728767870706445, "episode": 123.0, "batch_reward": 0.006806419325293973, "critic_loss": 0.0014546710185895791, "actor_loss": -11.31600752492994, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.16501784324646, "step": 123000}
{"episode_reward": 4.558835433297415, "episode": 124.0, "batch_reward": 0.007033875721041113, "critic_loss": 0.0012581312464899383, "actor_loss": -11.414870740547777, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.243862628936768, "step": 124000}
{"episode_reward": 4.117179873758931, "episode": 125.0, "batch_reward": 0.007024109941208735, "critic_loss": 0.0014541607394821768, "actor_loss": -11.209769391097128, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.81140899658203, "step": 125000}
{"episode_reward": 4.205890898830737, "episode": 126.0, "batch_reward": 0.007057824268238619, "critic_loss": 0.0019123589866649126, "actor_loss": -10.389824428267778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.34914255142212, "step": 126000}
{"episode_reward": 3.8241713019286587, "episode": 127.0, "batch_reward": 0.006951624934095889, "critic_loss": 0.002059640371837304, "actor_loss": -12.28554941689223, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.97124719619751, "step": 127000}
{"episode_reward": 3.4754525389073914, "episode": 128.0, "batch_reward": 0.0065994597703684125, "critic_loss": 0.0017344345670098845, "actor_loss": -11.034140385046602, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.69976305961609, "step": 128000}
{"episode_reward": 3.807395403588711, "episode": 129.0, "batch_reward": 0.006983466853853315, "critic_loss": 0.0023188900812456266, "actor_loss": -10.382844060018659, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.75613832473755, "step": 129000}
{"episode_reward": 3.672648264867888, "episode": 130.0, "batch_reward": 0.006748641441576183, "critic_loss": 0.001353093222285679, "actor_loss": -10.698506606191398, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.027743101119995, "step": 130000}
{"episode_reward": 3.4943550135669765, "episode": 131.0, "batch_reward": 0.006828374044038355, "critic_loss": 0.0013081591700029095, "actor_loss": -11.209744276612996, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.73714876174927, "step": 131000}
{"episode_reward": 3.4968575626070657, "episode": 132.0, "batch_reward": 0.006725837290287018, "critic_loss": 0.0016838089713455701, "actor_loss": -11.684054110169411, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.26692223548889, "step": 132000}
{"episode_reward": 3.281657346554504, "episode": 133.0, "batch_reward": 0.006611968045355752, "critic_loss": 0.0010845827892771922, "actor_loss": -10.420021013200284, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.461483001708984, "step": 133000}
{"episode_reward": 3.6723733490947006, "episode": 134.0, "batch_reward": 0.006853681419743225, "critic_loss": 0.00167493151727831, "actor_loss": -10.006049856752158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.436432600021362, "step": 134000}
{"episode_reward": 4.049949209786522, "episode": 135.0, "batch_reward": 0.006693577652098611, "critic_loss": 0.001095343673350726, "actor_loss": -11.678894140049815, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.887778520584106, "step": 135000}
{"episode_reward": 3.3203360842893286, "episode": 136.0, "batch_reward": 0.006580197549425066, "critic_loss": 0.0010570426363847218, "actor_loss": -9.11445023201406, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.373429775238037, "step": 136000}
{"episode_reward": 3.806753001352084, "episode": 137.0, "batch_reward": 0.006605131837306545, "critic_loss": 0.0007742964193676017, "actor_loss": -11.015747894309461, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.159096717834473, "step": 137000}
{"episode_reward": 4.140533723324997, "episode": 138.0, "batch_reward": 0.006671371304662898, "critic_loss": 0.001240683168227406, "actor_loss": -11.12646814315766, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.421415328979492, "step": 138000}
{"episode_reward": 4.013258791691102, "episode": 139.0, "batch_reward": 0.006580259134061635, "critic_loss": 0.0013111686463416845, "actor_loss": -10.582096565760672, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.362047910690308, "step": 139000}
{"episode_reward": 3.083968682181613, "episode": 140.0, "batch_reward": 0.006534551757620647, "critic_loss": 0.0010905459409259492, "actor_loss": -11.29084831134975, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.724792957305908, "step": 140000}
{"episode_reward": 3.8996565961778944, "episode": 141.0, "batch_reward": 0.0067195589616894725, "critic_loss": 0.0010077341255637293, "actor_loss": -11.39950163013488, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.257230043411255, "step": 141000}
{"episode_reward": 4.2766003715679215, "episode": 142.0, "batch_reward": 0.006577859919751063, "critic_loss": 0.001009851240680291, "actor_loss": -10.530270980983973, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.186023950576782, "step": 142000}
{"episode_reward": 3.652042112039389, "episode": 143.0, "batch_reward": 0.00663788925902918, "critic_loss": 0.0009205548204299702, "actor_loss": -10.256648747138678, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.96945309638977, "step": 143000}
{"episode_reward": 4.076172472809038, "episode": 144.0, "batch_reward": 0.006384079658426345, "critic_loss": 0.0014239070731891844, "actor_loss": -10.912158491373063, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.852789878845215, "step": 144000}
{"episode_reward": 3.9733040566695905, "episode": 145.0, "batch_reward": 0.00646037766803056, "critic_loss": 0.0010029450425390678, "actor_loss": -11.171513391755521, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.03816294670105, "step": 145000}
{"episode_reward": 3.967617701938627, "episode": 146.0, "batch_reward": 0.006530910927336663, "critic_loss": 0.0007289085836928279, "actor_loss": -11.34608001781255, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.694216012954712, "step": 146000}
{"episode_reward": 3.882645116030891, "episode": 147.0, "batch_reward": 0.006442144565517083, "critic_loss": 0.0010100747200813202, "actor_loss": -10.796396817058325, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.22795271873474, "step": 147000}
{"episode_reward": 3.6271436395012255, "episode": 148.0, "batch_reward": 0.006503470771480352, "critic_loss": 0.0008933132742786256, "actor_loss": -11.071990771502257, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.0267972946167, "step": 148000}
{"episode_reward": 4.234128101213929, "episode": 149.0, "batch_reward": 0.006462644529994577, "critic_loss": 0.001300810693675885, "actor_loss": -11.173357753418387, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.733084201812744, "step": 149000}
{"episode_reward": 3.2933895054248863, "episode": 150.0, "batch_reward": 0.006719684135867283, "critic_loss": 0.0012236765363195445, "actor_loss": -11.61345958890021, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
