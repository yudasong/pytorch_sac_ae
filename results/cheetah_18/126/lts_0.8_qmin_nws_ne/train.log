{"episode_reward": 0.0, "episode": 1.0, "duration": 20.150065898895264, "step": 1000}
{"episode_reward": 4.231880753996205, "episode": 2.0, "duration": 1.8102493286132812, "step": 2000}
{"episode_reward": 395.1450543749875, "episode": 3.0, "batch_reward": 0.1872139859245236, "critic_loss": 0.019256618284136207, "actor_loss": -32.9768895670703, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 69.42624497413635, "step": 3000}
{"episode_reward": 2.2733972005045104, "episode": 4.0, "batch_reward": 0.11639363971352577, "critic_loss": 0.010234065857715904, "actor_loss": -28.93005237007141, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.839152097702026, "step": 4000}
{"episode_reward": 2.4156262744564554, "episode": 5.0, "batch_reward": 0.09112498251348734, "critic_loss": 0.012685735229868442, "actor_loss": -27.46212962770462, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.99655818939209, "step": 5000}
{"episode_reward": 3.4994728797564885, "episode": 6.0, "batch_reward": 0.0749635968990624, "critic_loss": 0.009564203753136099, "actor_loss": -26.311740195274353, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.792476415634155, "step": 6000}
{"episode_reward": 3.1099248754410977, "episode": 7.0, "batch_reward": 0.06423208725452423, "critic_loss": 0.014750043906271458, "actor_loss": -26.21698807525635, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.432962656021118, "step": 7000}
{"episode_reward": 3.9324075603323903, "episode": 8.0, "batch_reward": 0.05593690099194646, "critic_loss": 0.01356973389792256, "actor_loss": -26.792848818778992, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.937744617462158, "step": 8000}
{"episode_reward": 3.572413652557391, "episode": 9.0, "batch_reward": 0.04913129989802838, "critic_loss": 0.016867978977039455, "actor_loss": -25.11163760519028, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.359758377075195, "step": 9000}
{"episode_reward": 3.412686223969405, "episode": 10.0, "batch_reward": 0.044931545559316874, "critic_loss": 0.027704451525583863, "actor_loss": -24.607068445682525, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.96732211112976, "step": 10000}
{"episode_reward": 2.6921149797397548, "episode": 11.0, "batch_reward": 0.04059505733475089, "critic_loss": 0.02209085000725463, "actor_loss": -23.899152701854707, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.89476275444031, "step": 11000}
{"episode_reward": 3.7973884899731982, "episode": 12.0, "batch_reward": 0.03784946264140308, "critic_loss": 0.01997519446676597, "actor_loss": -23.653079439640045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.24536418914795, "step": 12000}
{"episode_reward": 4.000456481501105, "episode": 13.0, "batch_reward": 0.035185648078098895, "critic_loss": 0.016297800992848353, "actor_loss": -23.21868629550934, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.193377017974854, "step": 13000}
{"episode_reward": 3.818239495963247, "episode": 14.0, "batch_reward": 0.03207582866726443, "critic_loss": 0.011985462448326871, "actor_loss": -23.145656394958497, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.20620083808899, "step": 14000}
{"episode_reward": 3.4353653038417327, "episode": 15.0, "batch_reward": 0.030475979049224406, "critic_loss": 0.010901513108517974, "actor_loss": -21.56726497745514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.917572736740112, "step": 15000}
{"episode_reward": 3.549897130114035, "episode": 16.0, "batch_reward": 0.028347841456532477, "critic_loss": 0.008810867138905451, "actor_loss": -22.748577762126924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.14544677734375, "step": 16000}
{"episode_reward": 2.908530686593857, "episode": 17.0, "batch_reward": 0.02695722037833184, "critic_loss": 0.007607012761174701, "actor_loss": -22.13977058815956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.28835368156433, "step": 17000}
{"episode_reward": 3.5658975751251494, "episode": 18.0, "batch_reward": 0.025470480107702316, "critic_loss": 0.006954794610268437, "actor_loss": -22.207740322589874, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.173415422439575, "step": 18000}
{"episode_reward": 2.997643888268387, "episode": 19.0, "batch_reward": 0.024929099189583213, "critic_loss": 0.0078304181342246, "actor_loss": -22.32137122988701, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.399662971496582, "step": 19000}
{"episode_reward": 2.929124585987554, "episode": 20.0, "batch_reward": 0.023734784114873037, "critic_loss": 0.006013869873364456, "actor_loss": -21.221657933712006, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.408949851989746, "step": 20000}
{"episode_reward": 3.4972346054593872, "episode": 21.0, "batch_reward": 0.022015702628763394, "critic_loss": 0.00533356802421622, "actor_loss": -21.30835306215286, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.01792120933533, "step": 21000}
{"episode_reward": 3.4709666652927584, "episode": 22.0, "batch_reward": 0.02134497130382806, "critic_loss": 0.0059004882630542856, "actor_loss": -20.86969964647293, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.265531301498413, "step": 22000}
{"episode_reward": 4.011557639888812, "episode": 23.0, "batch_reward": 0.020804294022265822, "critic_loss": 0.004179644700954668, "actor_loss": -21.24319768333435, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.40801692008972, "step": 23000}
{"episode_reward": 3.8014200066132777, "episode": 24.0, "batch_reward": 0.020220742853125556, "critic_loss": 0.004527406910201534, "actor_loss": -20.904390750408172, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.53715491294861, "step": 24000}
{"episode_reward": 4.2470939857140335, "episode": 25.0, "batch_reward": 0.01950500646070577, "critic_loss": 0.004575086069409736, "actor_loss": -21.615317235469817, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.00652265548706, "step": 25000}
{"episode_reward": 6.374678233464279, "episode": 26.0, "batch_reward": 0.019263272690819577, "critic_loss": 0.005235446130100172, "actor_loss": -20.779232605695725, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.724513292312622, "step": 26000}
{"episode_reward": 3.330388774950997, "episode": 27.0, "batch_reward": 0.018679334688233212, "critic_loss": 0.00502179512695875, "actor_loss": -20.77583092021942, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.402828216552734, "step": 27000}
{"episode_reward": 4.575920796117123, "episode": 28.0, "batch_reward": 0.017508935732534155, "critic_loss": 0.004546955813595559, "actor_loss": -21.00628475499153, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.927143335342407, "step": 28000}
{"episode_reward": 6.354833006756918, "episode": 29.0, "batch_reward": 0.017093865782022478, "critic_loss": 0.004670363549201284, "actor_loss": -20.034868908882142, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.365147590637207, "step": 29000}
{"episode_reward": 3.5219151152031665, "episode": 30.0, "batch_reward": 0.017278668604791166, "critic_loss": 0.0038887287736870347, "actor_loss": -19.980659550189973, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.640949249267578, "step": 30000}
{"episode_reward": 3.5066286136455047, "episode": 31.0, "batch_reward": 0.016431622656295077, "critic_loss": 0.0035717364971060307, "actor_loss": -21.01185342359543, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.911816358566284, "step": 31000}
{"episode_reward": 3.2082045877086407, "episode": 32.0, "batch_reward": 0.01606483395281248, "critic_loss": 0.00271115730603924, "actor_loss": -20.561952731251715, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.103926420211792, "step": 32000}
{"episode_reward": 3.2518467223888106, "episode": 33.0, "batch_reward": 0.01607219273177907, "critic_loss": 0.003133237308822572, "actor_loss": -20.562629253149034, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.74644136428833, "step": 33000}
{"episode_reward": 4.910501965367833, "episode": 34.0, "batch_reward": 0.015257272068876773, "critic_loss": 0.0025235493928776123, "actor_loss": -20.531253218650818, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.961616277694702, "step": 34000}
{"episode_reward": 3.543084224071135, "episode": 35.0, "batch_reward": 0.015007359695388004, "critic_loss": 0.0025872166885528712, "actor_loss": -19.95225697696209, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.86951208114624, "step": 35000}
{"episode_reward": 3.2574957003798333, "episode": 36.0, "batch_reward": 0.014665439117932692, "critic_loss": 0.002927725648507476, "actor_loss": -21.29252291405201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.641618013381958, "step": 36000}
{"episode_reward": 3.4545796970733225, "episode": 37.0, "batch_reward": 0.014362918748054654, "critic_loss": 0.002596628603467252, "actor_loss": -20.517951042890548, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.125181913375854, "step": 37000}
{"episode_reward": 2.9236446906936093, "episode": 38.0, "batch_reward": 0.014132366790436208, "critic_loss": 0.002737019692271133, "actor_loss": -19.734106224298475, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.714115619659424, "step": 38000}
{"episode_reward": 2.7233098779391147, "episode": 39.0, "batch_reward": 0.014030739304609597, "critic_loss": 0.002741929729498224, "actor_loss": -20.720250620126723, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.630377292633057, "step": 39000}
{"episode_reward": 2.659975165724926, "episode": 40.0, "batch_reward": 0.01337082878802903, "critic_loss": 0.0027178585650544846, "actor_loss": -20.647552454948425, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.79319953918457, "step": 40000}
{"episode_reward": 2.323912167685072, "episode": 41.0, "batch_reward": 0.013302651779958978, "critic_loss": 0.0021994283884123433, "actor_loss": -21.20146981227398, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.835901498794556, "step": 41000}
{"episode_reward": 3.1963493945372594, "episode": 42.0, "batch_reward": 0.01298754946468398, "critic_loss": 0.0025822452781285392, "actor_loss": -19.55062820363045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.08745503425598, "step": 42000}
{"episode_reward": 4.428263571588142, "episode": 43.0, "batch_reward": 0.012778595884796231, "critic_loss": 0.002458691995758272, "actor_loss": -20.582940941214563, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.283154249191284, "step": 43000}
{"episode_reward": 7.536379312858002, "episode": 44.0, "batch_reward": 0.012502085986780003, "critic_loss": 0.0024674002897954778, "actor_loss": -20.45120080256462, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.002392768859863, "step": 44000}
{"episode_reward": 3.630473367061728, "episode": 45.0, "batch_reward": 0.012428656252101064, "critic_loss": 0.0024846744799433508, "actor_loss": -20.131934089422227, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.861577033996582, "step": 45000}
{"episode_reward": 3.496391808267509, "episode": 46.0, "batch_reward": 0.012370816174661741, "critic_loss": 0.002320040248669102, "actor_loss": -19.934096678435804, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.709036827087402, "step": 46000}
{"episode_reward": 4.230965367390606, "episode": 47.0, "batch_reward": 0.012116146334912628, "critic_loss": 0.002425469225316192, "actor_loss": -20.465560658335686, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.254865169525146, "step": 47000}
{"episode_reward": 3.151935354760209, "episode": 48.0, "batch_reward": 0.011840317664900795, "critic_loss": 0.0018881250877602724, "actor_loss": -20.326332228183745, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.044519186019897, "step": 48000}
{"episode_reward": 4.068032896555979, "episode": 49.0, "batch_reward": 0.012023063390515745, "critic_loss": 0.0018689465431671124, "actor_loss": -20.518205056130885, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.303706645965576, "step": 49000}
{"episode_reward": 3.6386186887434917, "episode": 50.0, "batch_reward": 0.011716455073561519, "critic_loss": 0.002240480196676799, "actor_loss": -19.753646089851856, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.355306148529053, "step": 50000}
{"episode_reward": 3.3957052377267547, "episode": 51.0, "batch_reward": 0.01145603855396621, "critic_loss": 0.0017114377913385397, "actor_loss": -19.759594922065734, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.388290882110596, "step": 51000}
{"episode_reward": 4.313828822378433, "episode": 52.0, "batch_reward": 0.011371031197020784, "critic_loss": 0.0019530070686450927, "actor_loss": -20.487135014235974, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.4718656539917, "step": 52000}
{"episode_reward": 3.0691215649275114, "episode": 53.0, "batch_reward": 0.0112555112906266, "critic_loss": 0.0018736167750757885, "actor_loss": -19.15830245757103, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.343281745910645, "step": 53000}
{"episode_reward": 3.9920901402102937, "episode": 54.0, "batch_reward": 0.011304961191257462, "critic_loss": 0.0020329685701144626, "actor_loss": -20.690671475708484, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.935070991516113, "step": 54000}
{"episode_reward": 3.2413694672634907, "episode": 55.0, "batch_reward": 0.010765819810563699, "critic_loss": 0.002172695113171358, "actor_loss": -20.140291909813882, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.252557516098022, "step": 55000}
{"episode_reward": 2.5831182476721555, "episode": 56.0, "batch_reward": 0.010588442260865123, "critic_loss": 0.0018395169167706627, "actor_loss": -19.081321168363093, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.38716435432434, "step": 56000}
{"episode_reward": 3.0357494156090556, "episode": 57.0, "batch_reward": 0.010790926974965259, "critic_loss": 0.0018926097894945996, "actor_loss": -20.250842874318362, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.6081280708313, "step": 57000}
{"episode_reward": 6.612593215599484, "episode": 58.0, "batch_reward": 0.010585588982328772, "critic_loss": 0.0018668644333156407, "actor_loss": -19.61127997094393, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.035918474197388, "step": 58000}
{"episode_reward": 3.6791030113748393, "episode": 59.0, "batch_reward": 0.010278015324147419, "critic_loss": 0.001981246956398536, "actor_loss": -19.686706217318772, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.718952894210815, "step": 59000}
{"episode_reward": 5.403159306103707, "episode": 60.0, "batch_reward": 0.01032201884360984, "critic_loss": 0.0016433260512931157, "actor_loss": -19.62609586650133, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.368892192840576, "step": 60000}
{"episode_reward": 4.342852040088077, "episode": 61.0, "batch_reward": 0.01013872227142565, "critic_loss": 0.001610715219467238, "actor_loss": -20.134986649781464, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.119903564453125, "step": 61000}
{"episode_reward": 3.8618357428751535, "episode": 62.0, "batch_reward": 0.009884684167336672, "critic_loss": 0.0015714417877752567, "actor_loss": -19.23676694688201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.53949475288391, "step": 62000}
{"episode_reward": 4.010548103103694, "episode": 63.0, "batch_reward": 0.010046367491129787, "critic_loss": 0.0011928929751957185, "actor_loss": -19.527496165543795, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.509660243988037, "step": 63000}
{"episode_reward": 6.452597795779403, "episode": 64.0, "batch_reward": 0.009821678645093925, "critic_loss": 0.00157028415442619, "actor_loss": -19.561816691190003, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.313488483428955, "step": 64000}
{"episode_reward": 3.427305756516329, "episode": 65.0, "batch_reward": 0.00995283071603626, "critic_loss": 0.0014862509743106785, "actor_loss": -19.254993718385695, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.152328968048096, "step": 65000}
{"episode_reward": 5.025067411565401, "episode": 66.0, "batch_reward": 0.00957791421120055, "critic_loss": 0.0013379833655126276, "actor_loss": -19.34924914225936, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.061582803726196, "step": 66000}
{"episode_reward": 3.391491962179365, "episode": 67.0, "batch_reward": 0.009580443731858394, "critic_loss": 0.0014128676115869893, "actor_loss": -18.884404584020377, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.678005695343018, "step": 67000}
{"episode_reward": 3.802991225241612, "episode": 68.0, "batch_reward": 0.009658870911458508, "critic_loss": 0.0016287198853533482, "actor_loss": -19.712966963142158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.329500436782837, "step": 68000}
{"episode_reward": 3.900727593190103, "episode": 69.0, "batch_reward": 0.00965073696291074, "critic_loss": 0.0014240571457849, "actor_loss": -19.766720734328032, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.259270429611206, "step": 69000}
{"episode_reward": 3.6366021165982882, "episode": 70.0, "batch_reward": 0.009338600733084604, "critic_loss": 0.0012219744119356618, "actor_loss": -19.739153184115885, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.19083285331726, "step": 70000}
{"episode_reward": 3.4832566554253064, "episode": 71.0, "batch_reward": 0.00929716137307696, "critic_loss": 0.0015483894324061112, "actor_loss": -18.95315397666395, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.74990129470825, "step": 71000}
{"episode_reward": 3.3822234379196434, "episode": 72.0, "batch_reward": 0.009140449082711711, "critic_loss": 0.0012470837775690597, "actor_loss": -19.911001002714038, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.311999559402466, "step": 72000}
{"episode_reward": 3.6796593156642157, "episode": 73.0, "batch_reward": 0.0092906705213245, "critic_loss": 0.0013766737391997595, "actor_loss": -19.720129756689072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.255257844924927, "step": 73000}
{"episode_reward": 2.9947811236322863, "episode": 74.0, "batch_reward": 0.00905548715707846, "critic_loss": 0.0012347673010008294, "actor_loss": -19.20025733654201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.356639862060547, "step": 74000}
{"episode_reward": 2.974944680861305, "episode": 75.0, "batch_reward": 0.009188623817637563, "critic_loss": 0.0011729629650362766, "actor_loss": -19.83341801273823, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.875392198562622, "step": 75000}
{"episode_reward": 3.00480053191357, "episode": 76.0, "batch_reward": 0.008872551632113755, "critic_loss": 0.0011633601438225014, "actor_loss": -19.368774151206015, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.784687757492065, "step": 76000}
{"episode_reward": 2.674348401204619, "episode": 77.0, "batch_reward": 0.008689199992688372, "critic_loss": 0.001195516994503123, "actor_loss": -19.153757565394045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.709723234176636, "step": 77000}
{"episode_reward": 4.284406592013947, "episode": 78.0, "batch_reward": 0.00888121988682542, "critic_loss": 0.0011611791976974927, "actor_loss": -18.781649326711893, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.865264415740967, "step": 78000}
{"episode_reward": 3.3012467989207366, "episode": 79.0, "batch_reward": 0.00861624012887478, "critic_loss": 0.0011062371757689106, "actor_loss": -19.365124901428818, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.309060096740723, "step": 79000}
{"episode_reward": 3.0537162161135463, "episode": 80.0, "batch_reward": 0.008658695704536512, "critic_loss": 0.0015483023121014411, "actor_loss": -19.42662901765108, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.570717573165894, "step": 80000}
{"episode_reward": 3.77739285197715, "episode": 81.0, "batch_reward": 0.008751061706920154, "critic_loss": 0.0012049075505165092, "actor_loss": -19.39320203359425, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.9957435131073, "step": 81000}
{"episode_reward": 5.54085313501991, "episode": 82.0, "batch_reward": 0.008345626243739388, "critic_loss": 0.0012733969052605972, "actor_loss": -19.179139700859785, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.70206880569458, "step": 82000}
{"episode_reward": 2.9311123633352305, "episode": 83.0, "batch_reward": 0.008443283818894998, "critic_loss": 0.0011693216614912672, "actor_loss": -19.25689832903445, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.150293588638306, "step": 83000}
{"episode_reward": 6.006795187532479, "episode": 84.0, "batch_reward": 0.008254341292544268, "critic_loss": 0.0009635421979692182, "actor_loss": -20.091972669437528, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.328333139419556, "step": 84000}
{"episode_reward": 3.5267132396270693, "episode": 85.0, "batch_reward": 0.008602059551980347, "critic_loss": 0.0009675638340995647, "actor_loss": -19.407549645870922, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.400004863739014, "step": 85000}
{"episode_reward": 3.3757902378926277, "episode": 86.0, "batch_reward": 0.008332921038847417, "critic_loss": 0.0014173038658300357, "actor_loss": -19.77593211591989, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.187891483306885, "step": 86000}
{"episode_reward": 5.135318951221214, "episode": 87.0, "batch_reward": 0.008645209791488015, "critic_loss": 0.0007960501486086286, "actor_loss": -19.72006800726801, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.936246156692505, "step": 87000}
{"episode_reward": 6.6180647403165676, "episode": 88.0, "batch_reward": 0.00849488264915999, "critic_loss": 0.0011452330386418907, "actor_loss": -19.870108658224343, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.529554843902588, "step": 88000}
{"episode_reward": 6.429026953782233, "episode": 89.0, "batch_reward": 0.008315508265513926, "critic_loss": 0.0010422222703746228, "actor_loss": -19.228594874575734, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.337757110595703, "step": 89000}
{"episode_reward": 3.322790631676652, "episode": 90.0, "batch_reward": 0.00810897518065758, "critic_loss": 0.001081846516197402, "actor_loss": -19.36577666913718, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.187758207321167, "step": 90000}
{"episode_reward": 4.5748088748804605, "episode": 91.0, "batch_reward": 0.008171848140656949, "critic_loss": 0.0013389080794768233, "actor_loss": -19.02684818024188, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.81266784667969, "step": 91000}
{"episode_reward": 3.683879107880494, "episode": 92.0, "batch_reward": 0.008058005482656882, "critic_loss": 0.000913715438782674, "actor_loss": -19.147669082209468, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.37847924232483, "step": 92000}
{"episode_reward": 5.687763023770895, "episode": 93.0, "batch_reward": 0.008120338605018332, "critic_loss": 0.0009772207722562598, "actor_loss": -19.864596941970287, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.847859382629395, "step": 93000}
{"episode_reward": 3.8262687505710553, "episode": 94.0, "batch_reward": 0.007943566590431146, "critic_loss": 0.0015187492135810316, "actor_loss": -19.813296909965576, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.251067399978638, "step": 94000}
{"episode_reward": 6.418224653262338, "episode": 95.0, "batch_reward": 0.007962221384048462, "critic_loss": 0.0008643826823936251, "actor_loss": -19.819753244355322, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.850420475006104, "step": 95000}
{"episode_reward": 4.332805958995239, "episode": 96.0, "batch_reward": 0.008075802931096405, "critic_loss": 0.0012913944348074437, "actor_loss": -19.385781159728765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.138257265090942, "step": 96000}
{"episode_reward": 3.2212320627229114, "episode": 97.0, "batch_reward": 0.007885960512445308, "critic_loss": 0.001129816119268071, "actor_loss": -19.762378115460276, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.88246440887451, "step": 97000}
{"episode_reward": 5.9227862684248995, "episode": 98.0, "batch_reward": 0.007883438265300356, "critic_loss": 0.0011532955690272502, "actor_loss": -19.813695767693222, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.45326042175293, "step": 98000}
{"episode_reward": 3.780657950580535, "episode": 99.0, "batch_reward": 0.008025933685014024, "critic_loss": 0.0009884379521536174, "actor_loss": -19.457794077195228, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.466341972351074, "step": 99000}
{"episode_reward": 3.612092726462971, "episode": 100.0, "batch_reward": 0.00772914865007624, "critic_loss": 0.0011518807092215865, "actor_loss": -20.10426229469478, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.884711980819702, "step": 100000}
{"episode_reward": 3.388017622378017, "episode": 101.0, "batch_reward": 0.007683804682223126, "critic_loss": 0.0009895714234080515, "actor_loss": -19.872894618973135, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.91442584991455, "step": 101000}
{"episode_reward": 3.2590704364862635, "episode": 102.0, "batch_reward": 0.007612734147114679, "critic_loss": 0.0011601569767408363, "actor_loss": -20.35200656822324, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.641623497009277, "step": 102000}
{"episode_reward": 2.8393977261758616, "episode": 103.0, "batch_reward": 0.00785144696268253, "critic_loss": 0.0011480057217522698, "actor_loss": -18.96495525904745, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.247178554534912, "step": 103000}
{"episode_reward": 3.432806550944478, "episode": 104.0, "batch_reward": 0.007851736775715835, "critic_loss": 0.0010384886031715723, "actor_loss": -19.633351068601012, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.11310648918152, "step": 104000}
{"episode_reward": 6.460026311826957, "episode": 105.0, "batch_reward": 0.007529391228104942, "critic_loss": 0.0010469783904118232, "actor_loss": -19.175053209386768, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.66647243499756, "step": 105000}
{"episode_reward": 2.9345438541608964, "episode": 106.0, "batch_reward": 0.007785629674093798, "critic_loss": 0.001029964585788548, "actor_loss": -20.24174902150035, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.677579164505005, "step": 106000}
{"episode_reward": 5.9225653728451, "episode": 107.0, "batch_reward": 0.007555070107337088, "critic_loss": 0.0010162705907860072, "actor_loss": -20.178832974683495, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.22222924232483, "step": 107000}
{"episode_reward": 3.850157741348297, "episode": 108.0, "batch_reward": 0.007395141076529398, "critic_loss": 0.0012477236094855472, "actor_loss": -19.202403250321744, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.64972233772278, "step": 108000}
{"episode_reward": 4.271435782397016, "episode": 109.0, "batch_reward": 0.007544704588712193, "critic_loss": 0.001088442655673134, "actor_loss": -19.898114679027348, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.479673862457275, "step": 109000}
{"episode_reward": 3.3639292938581478, "episode": 110.0, "batch_reward": 0.007436955900164321, "critic_loss": 0.0010497298001464514, "actor_loss": -19.590719078205527, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.453042030334473, "step": 110000}
{"episode_reward": 3.4524392702092133, "episode": 111.0, "batch_reward": 0.00743405326968059, "critic_loss": 0.001277511104624864, "actor_loss": -19.271134544456377, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.355942249298096, "step": 111000}
{"episode_reward": 4.153525956343094, "episode": 112.0, "batch_reward": 0.007538761195028201, "critic_loss": 0.001082141129725642, "actor_loss": -19.847664671473204, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.020780563354492, "step": 112000}
{"episode_reward": 7.4800177001718655, "episode": 113.0, "batch_reward": 0.007485838376218453, "critic_loss": 0.0012938670807270682, "actor_loss": -20.018756090573966, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.372356414794922, "step": 113000}
{"episode_reward": 3.0986015195645376, "episode": 114.0, "batch_reward": 0.0074892944508465, "critic_loss": 0.0011287676944302803, "actor_loss": -20.113261584602295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.744731903076172, "step": 114000}
{"episode_reward": 3.215264870127837, "episode": 115.0, "batch_reward": 0.0072067294903099535, "critic_loss": 0.0014238277577242116, "actor_loss": -19.37732109943405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.553832530975342, "step": 115000}
{"episode_reward": 4.470151251523035, "episode": 116.0, "batch_reward": 0.007291006431914866, "critic_loss": 0.0011309144621991435, "actor_loss": -19.671326066892593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.29362988471985, "step": 116000}
{"episode_reward": 4.345499977439743, "episode": 117.0, "batch_reward": 0.007581164250848814, "critic_loss": 0.0012482512843016594, "actor_loss": -18.6755846308209, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.06684637069702, "step": 117000}
{"episode_reward": 3.4747668524437785, "episode": 118.0, "batch_reward": 0.007184234101092443, "critic_loss": 0.0010349038639687932, "actor_loss": -19.188963772207497, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.33924436569214, "step": 118000}
{"episode_reward": 2.919837728176277, "episode": 119.0, "batch_reward": 0.007209494384704158, "critic_loss": 0.001058352546475362, "actor_loss": -19.221419564299286, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.535861492156982, "step": 119000}
{"episode_reward": 3.7626161540193706, "episode": 120.0, "batch_reward": 0.0071879588377196345, "critic_loss": 0.001217071473809483, "actor_loss": -19.181366111673416, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.556274890899658, "step": 120000}
{"episode_reward": 4.344783611567065, "episode": 121.0, "batch_reward": 0.007167630257550627, "critic_loss": 0.0009079613623289333, "actor_loss": -19.189969780329616, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 44.95040011405945, "step": 121000}
{"episode_reward": 4.091788114524206, "episode": 122.0, "batch_reward": 0.00723319640615955, "critic_loss": 0.0013073108081007377, "actor_loss": -19.808410281479357, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.281436920166016, "step": 122000}
{"episode_reward": 3.430677885998077, "episode": 123.0, "batch_reward": 0.006973719746805727, "critic_loss": 0.0010843613323631872, "actor_loss": -20.177962324755267, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.866650104522705, "step": 123000}
{"episode_reward": 4.028598126154127, "episode": 124.0, "batch_reward": 0.007067194962408394, "critic_loss": 0.000956345921567845, "actor_loss": -19.980478314796464, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.33550214767456, "step": 124000}
{"episode_reward": 5.62011335934538, "episode": 125.0, "batch_reward": 0.0071122487040702256, "critic_loss": 0.0008493323969305493, "actor_loss": -19.78055095891282, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.975866317749023, "step": 125000}
{"episode_reward": 4.476574093890274, "episode": 126.0, "batch_reward": 0.007243210311979055, "critic_loss": 0.001548682889486372, "actor_loss": -19.26299029050395, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.95651936531067, "step": 126000}
{"episode_reward": 3.967342137355711, "episode": 127.0, "batch_reward": 0.00709621614869684, "critic_loss": 0.001077113932249631, "actor_loss": -18.96359503052756, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.489256620407104, "step": 127000}
{"episode_reward": 3.9641316251443426, "episode": 128.0, "batch_reward": 0.0067157525192014875, "critic_loss": 0.001122224170354457, "actor_loss": -19.142555838909, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.113706588745117, "step": 128000}
{"episode_reward": 3.959977613565445, "episode": 129.0, "batch_reward": 0.007139340445864946, "critic_loss": 0.0013297021901053085, "actor_loss": -19.495130396541207, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.62067484855652, "step": 129000}
{"episode_reward": 3.199262203546727, "episode": 130.0, "batch_reward": 0.0067437858351040636, "critic_loss": 0.0009333109547806089, "actor_loss": -19.03097738798708, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.997682332992554, "step": 130000}
{"episode_reward": 4.345925549664771, "episode": 131.0, "batch_reward": 0.0068868637136183675, "critic_loss": 0.001221316472197941, "actor_loss": -19.69393901972845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.78272795677185, "step": 131000}
{"episode_reward": 3.2868533421961454, "episode": 132.0, "batch_reward": 0.006856039587059058, "critic_loss": 0.0010563369209194208, "actor_loss": -20.14770142635517, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.1239275932312, "step": 132000}
{"episode_reward": 3.0551015548720457, "episode": 133.0, "batch_reward": 0.006767121852608398, "critic_loss": 0.0007518280017393408, "actor_loss": -19.69674563315697, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.835986137390137, "step": 133000}
{"episode_reward": 2.7557076383263492, "episode": 134.0, "batch_reward": 0.0069788419349351895, "critic_loss": 0.0012112072751260712, "actor_loss": -19.301132420834154, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.13220238685608, "step": 134000}
{"episode_reward": 6.251132399482149, "episode": 135.0, "batch_reward": 0.006800018191803247, "critic_loss": 0.0009328035958678811, "actor_loss": -19.02517886089906, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.828471660614014, "step": 135000}
{"episode_reward": 2.7188199438830547, "episode": 136.0, "batch_reward": 0.006706127888988703, "critic_loss": 0.0011464137275288523, "actor_loss": -18.594837460516022, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.95150113105774, "step": 136000}
{"episode_reward": 3.4720120592262433, "episode": 137.0, "batch_reward": 0.006749669689801521, "critic_loss": 0.001058075991259102, "actor_loss": -19.98438734434731, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.372982025146484, "step": 137000}
{"episode_reward": 3.5716699538331422, "episode": 138.0, "batch_reward": 0.006812841685954482, "critic_loss": 0.0012292353500488388, "actor_loss": -19.545321173597127, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.31355881690979, "step": 138000}
{"episode_reward": 4.973939919434144, "episode": 139.0, "batch_reward": 0.006708814338315278, "critic_loss": 0.0010192284201712028, "actor_loss": -19.244233338326215, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.85489535331726, "step": 139000}
{"episode_reward": 4.3347176240788645, "episode": 140.0, "batch_reward": 0.006598995794658549, "critic_loss": 0.0009203893205340137, "actor_loss": -19.206229071564973, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.68101954460144, "step": 140000}
{"episode_reward": 3.7636244992315255, "episode": 141.0, "batch_reward": 0.006926244796719402, "critic_loss": 0.0013105880770999648, "actor_loss": -19.73987237604335, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.060713052749634, "step": 141000}
{"episode_reward": 3.536734306491452, "episode": 142.0, "batch_reward": 0.006724583697738126, "critic_loss": 0.0012095166327271726, "actor_loss": -18.29523491935432, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.291611671447754, "step": 142000}
{"episode_reward": 5.154301492341382, "episode": 143.0, "batch_reward": 0.0067836104050511496, "critic_loss": 0.0010289749472503899, "actor_loss": -18.68435940299928, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.751503705978394, "step": 143000}
{"episode_reward": 4.871880186226065, "episode": 144.0, "batch_reward": 0.006444934287341312, "critic_loss": 0.001003381755646842, "actor_loss": -19.292303301725536, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.05366539955139, "step": 144000}
{"episode_reward": 2.798700165402143, "episode": 145.0, "batch_reward": 0.00654408693977166, "critic_loss": 0.0010400317254607217, "actor_loss": -19.97383692483604, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.45811700820923, "step": 145000}
{"episode_reward": 3.201494411052279, "episode": 146.0, "batch_reward": 0.0066591071370057766, "critic_loss": 0.0008118738063694764, "actor_loss": -19.730277724802495, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.61667227745056, "step": 146000}
{"episode_reward": 3.2657437315000313, "episode": 147.0, "batch_reward": 0.0065608020052313806, "critic_loss": 0.0008100947707280284, "actor_loss": -19.06271002645418, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.520663738250732, "step": 147000}
{"episode_reward": 6.397807381447365, "episode": 148.0, "batch_reward": 0.006636796054430306, "critic_loss": 0.0008334972133488918, "actor_loss": -19.923997241909614, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.65073847770691, "step": 148000}
{"episode_reward": 3.1059168609207113, "episode": 149.0, "batch_reward": 0.006571764111984521, "critic_loss": 0.001019773900938162, "actor_loss": -18.85509004403278, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.111536026000977, "step": 149000}
{"episode_reward": 4.233278067808789, "episode": 150.0, "batch_reward": 0.0069427475010743365, "critic_loss": 0.0011545460677189112, "actor_loss": -19.828342793103307, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
