{"episode_reward": 0.0, "episode": 1.0, "duration": 17.577966690063477, "step": 1000}
{"episode_reward": 7.051849146674556, "episode": 2.0, "duration": 1.5196471214294434, "step": 2000}
{"episode_reward": 365.4711839680396, "episode": 3.0, "batch_reward": 0.17479153766000854, "critic_loss": 0.01837315641234282, "actor_loss": -26.65391642728201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 64.24555563926697, "step": 3000}
{"episode_reward": 2.4688016806327227, "episode": 4.0, "batch_reward": 0.10852946462482214, "critic_loss": 0.009065899012377485, "actor_loss": -24.110096074581147, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.570989847183228, "step": 4000}
{"episode_reward": 1.817528240366142, "episode": 5.0, "batch_reward": 0.08437088498473168, "critic_loss": 0.0060457181016681715, "actor_loss": -22.872488910913468, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.40429663658142, "step": 5000}
{"episode_reward": 1.7289443739685044, "episode": 6.0, "batch_reward": 0.0685224218852818, "critic_loss": 0.006763633442926221, "actor_loss": -23.167165648460387, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.197646141052246, "step": 6000}
{"episode_reward": 1.8330429405170197, "episode": 7.0, "batch_reward": 0.05861303733102977, "critic_loss": 0.0067510798064759, "actor_loss": -22.42889940738678, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.695268154144287, "step": 7000}
{"episode_reward": 2.492023335293318, "episode": 8.0, "batch_reward": 0.05150196251831949, "critic_loss": 0.006779203151178081, "actor_loss": -22.52455939364433, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.538915634155273, "step": 8000}
{"episode_reward": 3.219378032558036, "episode": 9.0, "batch_reward": 0.0455406287945807, "critic_loss": 0.005046342111832928, "actor_loss": -22.243558000326157, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.692781448364258, "step": 9000}
{"episode_reward": 2.4397796527085718, "episode": 10.0, "batch_reward": 0.041203706744126975, "critic_loss": 0.005870771905465517, "actor_loss": -23.09966857123375, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.92963743209839, "step": 10000}
{"episode_reward": 2.258088357578092, "episode": 11.0, "batch_reward": 0.03783415236510337, "critic_loss": 0.0053636286848923196, "actor_loss": -22.047618942975998, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.2418577671051, "step": 11000}
{"episode_reward": 2.5078446703536748, "episode": 12.0, "batch_reward": 0.03398797255195677, "critic_loss": 0.004527199466712773, "actor_loss": -22.22678692126274, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.263705492019653, "step": 12000}
{"episode_reward": 2.854697737487604, "episode": 13.0, "batch_reward": 0.031556744336150584, "critic_loss": 0.004491844400938134, "actor_loss": -21.641819120764733, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.35775065422058, "step": 13000}
{"episode_reward": 2.7566488597221928, "episode": 14.0, "batch_reward": 0.029767399745527653, "critic_loss": 0.0040658092732774095, "actor_loss": -21.392616431832312, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.58721113204956, "step": 14000}
{"episode_reward": 2.8001848724249445, "episode": 15.0, "batch_reward": 0.027509957192, "critic_loss": 0.0037014948052819818, "actor_loss": -21.60779108774662, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.50543761253357, "step": 15000}
{"episode_reward": 2.0300075323361852, "episode": 16.0, "batch_reward": 0.026078206604812294, "critic_loss": 0.0030234286389895716, "actor_loss": -22.217238334536553, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.85237145423889, "step": 16000}
{"episode_reward": 2.742772766859356, "episode": 17.0, "batch_reward": 0.02456147461081855, "critic_loss": 0.003870464460720541, "actor_loss": -20.87275677084923, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.330214738845825, "step": 17000}
{"episode_reward": 1.7547701170288281, "episode": 18.0, "batch_reward": 0.023874041515402497, "critic_loss": 0.0030763122866774213, "actor_loss": -21.233738892674445, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.55766463279724, "step": 18000}
{"episode_reward": 3.488025098966465, "episode": 19.0, "batch_reward": 0.02250797980511561, "critic_loss": 0.0032922466193849686, "actor_loss": -21.20105877113342, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.950241804122925, "step": 19000}
{"episode_reward": 2.859163909335769, "episode": 20.0, "batch_reward": 0.021350076452828944, "critic_loss": 0.0032937652961263666, "actor_loss": -22.62333664059639, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.16253399848938, "step": 20000}
{"episode_reward": 2.1989933593587168, "episode": 21.0, "batch_reward": 0.01996053260518238, "critic_loss": 0.003335201778245391, "actor_loss": -20.710072254657746, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.513954162597656, "step": 21000}
{"episode_reward": 2.4289430812420156, "episode": 22.0, "batch_reward": 0.01959804023196921, "critic_loss": 0.0037569065630523256, "actor_loss": -21.80897419607639, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.35425090789795, "step": 22000}
{"episode_reward": 3.597864439432772, "episode": 23.0, "batch_reward": 0.018895594244124368, "critic_loss": 0.002463828528954764, "actor_loss": -20.837125002980233, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.705915212631226, "step": 23000}
{"episode_reward": 2.7164219815394226, "episode": 24.0, "batch_reward": 0.018033444829750805, "critic_loss": 0.0029640866966510657, "actor_loss": -21.58021347641945, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.483534336090088, "step": 24000}
{"episode_reward": 2.640200185785778, "episode": 25.0, "batch_reward": 0.01771943881874904, "critic_loss": 0.002118896173873509, "actor_loss": -22.277748083889485, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.799520254135132, "step": 25000}
{"episode_reward": 2.792624867566312, "episode": 26.0, "batch_reward": 0.01700293017551303, "critic_loss": 0.0021980104935792042, "actor_loss": -21.179968760848045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.48039746284485, "step": 26000}
{"episode_reward": 2.5710646558965924, "episode": 27.0, "batch_reward": 0.01677254336490296, "critic_loss": 0.0025950002381796367, "actor_loss": -21.09806848323345, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.85575246810913, "step": 27000}
{"episode_reward": 2.663092035506632, "episode": 28.0, "batch_reward": 0.016122662764042617, "critic_loss": 0.0016218535043371958, "actor_loss": -20.955803440511225, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.585530042648315, "step": 28000}
{"episode_reward": 2.605387261789293, "episode": 29.0, "batch_reward": 0.015293758237734437, "critic_loss": 0.002490372640771966, "actor_loss": -21.16376213210821, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.649917602539062, "step": 29000}
{"episode_reward": 2.2945831974686937, "episode": 30.0, "batch_reward": 0.01504644775716588, "critic_loss": 0.0023831668475904734, "actor_loss": -20.6952728459239, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.90224599838257, "step": 30000}
{"episode_reward": 2.4219213436824063, "episode": 31.0, "batch_reward": 0.014494262643624098, "critic_loss": 0.0014010557739748037, "actor_loss": -20.493889068841934, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.191386461257935, "step": 31000}
{"episode_reward": 2.1435049772193517, "episode": 32.0, "batch_reward": 0.01425230360403657, "critic_loss": 0.0018588434692283044, "actor_loss": -20.719364711105822, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.040149450302124, "step": 32000}
{"episode_reward": 2.5680668124232184, "episode": 33.0, "batch_reward": 0.013777825807919726, "critic_loss": 0.0018666232348259655, "actor_loss": -21.10321237909794, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.481204986572266, "step": 33000}
{"episode_reward": 2.290325844083749, "episode": 34.0, "batch_reward": 0.013714305828791112, "critic_loss": 0.0016638700313851586, "actor_loss": -20.977774614453317, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.565958976745605, "step": 34000}
{"episode_reward": 3.1085421242473568, "episode": 35.0, "batch_reward": 0.012689299975754693, "critic_loss": 0.0014542667550922488, "actor_loss": -21.30183605182171, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.822369813919067, "step": 35000}
{"episode_reward": 2.6485595708834215, "episode": 36.0, "batch_reward": 0.0125504029734293, "critic_loss": 0.0012684173478810408, "actor_loss": -22.05563081717491, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.26327610015869, "step": 36000}
{"episode_reward": 2.3665330478612256, "episode": 37.0, "batch_reward": 0.012635105362394825, "critic_loss": 0.0017215223276652977, "actor_loss": -20.474079393029214, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.880377292633057, "step": 37000}
{"episode_reward": 2.482254518810039, "episode": 38.0, "batch_reward": 0.012297299322206526, "critic_loss": 0.0016484891969812453, "actor_loss": -19.808836985170842, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.58014440536499, "step": 38000}
{"episode_reward": 2.198824856157678, "episode": 39.0, "batch_reward": 0.012131480897311121, "critic_loss": 0.001545330091365031, "actor_loss": -20.639274734437464, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.51437258720398, "step": 39000}
{"episode_reward": 2.5284578938947844, "episode": 40.0, "batch_reward": 0.011834980610292405, "critic_loss": 0.0017300452395138564, "actor_loss": -20.91119748353958, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.97475576400757, "step": 40000}
{"episode_reward": 2.5441308010680364, "episode": 41.0, "batch_reward": 0.011506882277317346, "critic_loss": 0.0013918428764300187, "actor_loss": -20.37467456406355, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.234858751297, "step": 41000}
{"episode_reward": 2.605564311247226, "episode": 42.0, "batch_reward": 0.011337624164298177, "critic_loss": 0.001329728876131412, "actor_loss": -20.42561733800173, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.004070043563843, "step": 42000}
{"episode_reward": 2.787803393290467, "episode": 43.0, "batch_reward": 0.011105364881106652, "critic_loss": 0.0015885665185342078, "actor_loss": -20.535030114769935, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.539177417755127, "step": 43000}
{"episode_reward": 2.2862976625570584, "episode": 44.0, "batch_reward": 0.011111621346790343, "critic_loss": 0.0017534329960835749, "actor_loss": -22.703300488948823, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.566248416900635, "step": 44000}
{"episode_reward": 2.9146047414823455, "episode": 45.0, "batch_reward": 0.0107156320650829, "critic_loss": 0.0012093702194542857, "actor_loss": -21.522782937943937, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.828666925430298, "step": 45000}
{"episode_reward": 1.9043464709289917, "episode": 46.0, "batch_reward": 0.010239003577618859, "critic_loss": 0.0012557291857592644, "actor_loss": -20.08311363378167, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.325822114944458, "step": 46000}
{"episode_reward": 2.2414579110306088, "episode": 47.0, "batch_reward": 0.01023889334849082, "critic_loss": 0.0012056304731850106, "actor_loss": -20.97075343170762, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.7203049659729, "step": 47000}
{"episode_reward": 2.744121385790527, "episode": 48.0, "batch_reward": 0.010253712047240697, "critic_loss": 0.0014255010055785533, "actor_loss": -20.5393893045187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.51151990890503, "step": 48000}
{"episode_reward": 3.051909288537618, "episode": 49.0, "batch_reward": 0.010242542930762284, "critic_loss": 0.0013751224736515723, "actor_loss": -22.088349530220032, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.77805233001709, "step": 49000}
{"episode_reward": 2.768897254674828, "episode": 50.0, "batch_reward": 0.009895776887075044, "critic_loss": 0.001717336032023013, "actor_loss": -21.020532017856837, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.22658610343933, "step": 50000}
{"episode_reward": 2.423159461072144, "episode": 51.0, "batch_reward": 0.009758886363939382, "critic_loss": 0.0010405992842206615, "actor_loss": -20.649275985598564, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.23761868476868, "step": 51000}
{"episode_reward": 2.3685970821726907, "episode": 52.0, "batch_reward": 0.00973285499587655, "critic_loss": 0.0011065502096971613, "actor_loss": -20.6495950910449, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.441874742507935, "step": 52000}
{"episode_reward": 2.393209483758558, "episode": 53.0, "batch_reward": 0.00946802301518619, "critic_loss": 0.0018998840315398412, "actor_loss": -21.581623832821847, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.105501890182495, "step": 53000}
{"episode_reward": 2.1591171469557757, "episode": 54.0, "batch_reward": 0.0093712024432607, "critic_loss": 0.0011683984885821701, "actor_loss": -22.168983959317206, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.50402307510376, "step": 54000}
{"episode_reward": 2.3310743552960744, "episode": 55.0, "batch_reward": 0.009367367197177373, "critic_loss": 0.0014350720500733588, "actor_loss": -21.41014211115241, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.6756329536438, "step": 55000}
{"episode_reward": 2.366084056250994, "episode": 56.0, "batch_reward": 0.009060736007522791, "critic_loss": 0.0010270011669344967, "actor_loss": -21.098266937881707, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.471829891204834, "step": 56000}
{"episode_reward": 2.7770556297044253, "episode": 57.0, "batch_reward": 0.00900388201046735, "critic_loss": 0.0013123034407835803, "actor_loss": -21.032096530646086, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.780922174453735, "step": 57000}
{"episode_reward": 2.604315260047617, "episode": 58.0, "batch_reward": 0.008786253007361666, "critic_loss": 0.0009685750348980946, "actor_loss": -21.034197124421596, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.555565357208252, "step": 58000}
{"episode_reward": 3.4747142017956545, "episode": 59.0, "batch_reward": 0.008781826184596867, "critic_loss": 0.0014161882819826133, "actor_loss": -20.676999399602412, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.607073545455933, "step": 59000}
{"episode_reward": 2.309201067688921, "episode": 60.0, "batch_reward": 0.008773108395049349, "critic_loss": 0.0009554275996524666, "actor_loss": -21.139311072587965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.84705686569214, "step": 60000}
{"episode_reward": 1.851156979127773, "episode": 61.0, "batch_reward": 0.008713886084384285, "critic_loss": 0.00113936028318858, "actor_loss": -20.786665928006173, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.27489733695984, "step": 61000}
{"episode_reward": 1.7733222190229285, "episode": 62.0, "batch_reward": 0.008761499287909829, "critic_loss": 0.0008458138319074351, "actor_loss": -21.13315071693063, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.6966495513916, "step": 62000}
{"episode_reward": 3.366815278312659, "episode": 63.0, "batch_reward": 0.008334866014076396, "critic_loss": 0.001277799400188087, "actor_loss": -21.248143591403963, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.76486325263977, "step": 63000}
{"episode_reward": 2.3067405640038268, "episode": 64.0, "batch_reward": 0.008215826385538093, "critic_loss": 0.0013374442539461598, "actor_loss": -21.120615238904954, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.54197335243225, "step": 64000}
{"episode_reward": 2.1776249571192565, "episode": 65.0, "batch_reward": 0.008191222771187313, "critic_loss": 0.0011772566712534172, "actor_loss": -20.591761387258767, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.89873433113098, "step": 65000}
{"episode_reward": 3.299774188700872, "episode": 66.0, "batch_reward": 0.007983862772816793, "critic_loss": 0.000844688791406952, "actor_loss": -20.876455197513103, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.455292224884033, "step": 66000}
{"episode_reward": 2.353248089360434, "episode": 67.0, "batch_reward": 0.00806111987179611, "critic_loss": 0.0013859051931394788, "actor_loss": -21.640345901057124, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.95744276046753, "step": 67000}
{"episode_reward": 2.39027516695815, "episode": 68.0, "batch_reward": 0.007957200470380486, "critic_loss": 0.0008290760719792161, "actor_loss": -22.81775399272144, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.566220998764038, "step": 68000}
{"episode_reward": 2.53089172317328, "episode": 69.0, "batch_reward": 0.007940599492751062, "critic_loss": 0.0011639031113300008, "actor_loss": -20.333303248152138, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.436609983444214, "step": 69000}
{"episode_reward": 1.819882155314453, "episode": 70.0, "batch_reward": 0.0077724752009380605, "critic_loss": 0.000914592135231942, "actor_loss": -20.979789781421424, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.249241590499878, "step": 70000}
{"episode_reward": 2.6379561995437504, "episode": 71.0, "batch_reward": 0.00781660726305563, "critic_loss": 0.0010634457272099098, "actor_loss": -20.09098171444237, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.29406213760376, "step": 71000}
{"episode_reward": 2.4731551501236093, "episode": 72.0, "batch_reward": 0.007743106393609196, "critic_loss": 0.0010907932982445346, "actor_loss": -20.74164033859968, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.919460773468018, "step": 72000}
{"episode_reward": 2.4323857842440937, "episode": 73.0, "batch_reward": 0.007715580750140362, "critic_loss": 0.0010986709147255168, "actor_loss": -20.98586909839511, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.221339225769043, "step": 73000}
{"episode_reward": 2.792605779494502, "episode": 74.0, "batch_reward": 0.0074050628151744605, "critic_loss": 0.0010898990774076082, "actor_loss": -20.323784480780365, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.617622137069702, "step": 74000}
{"episode_reward": 2.034913942555338, "episode": 75.0, "batch_reward": 0.007246002200292424, "critic_loss": 0.0009679709594456653, "actor_loss": -20.777766515627505, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.552212953567505, "step": 75000}
{"episode_reward": 2.1317297152525487, "episode": 76.0, "batch_reward": 0.007492434262647294, "critic_loss": 0.000857390827852214, "actor_loss": -20.55579121027887, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.87023377418518, "step": 76000}
{"episode_reward": 2.538719556057837, "episode": 77.0, "batch_reward": 0.007455858357483521, "critic_loss": 0.0009075075847176777, "actor_loss": -20.73708293224871, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.905622482299805, "step": 77000}
{"episode_reward": 2.499612539052745, "episode": 78.0, "batch_reward": 0.007538459263392724, "critic_loss": 0.0011053848910196394, "actor_loss": -21.31952510024607, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.56315541267395, "step": 78000}
{"episode_reward": 2.6104199564100767, "episode": 79.0, "batch_reward": 0.007140859743463806, "critic_loss": 0.0013844678623936488, "actor_loss": -21.669976494893433, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.5830979347229, "step": 79000}
{"episode_reward": 2.4938961280054346, "episode": 80.0, "batch_reward": 0.007205094850738533, "critic_loss": 0.0010780977743816038, "actor_loss": -21.314768023654818, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.053206205368042, "step": 80000}
{"episode_reward": 2.0659708924111806, "episode": 81.0, "batch_reward": 0.0069495602258248254, "critic_loss": 0.0007269059458649281, "actor_loss": -20.53059038902819, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.80991578102112, "step": 81000}
{"episode_reward": 2.520863416827245, "episode": 82.0, "batch_reward": 0.006862697987467982, "critic_loss": 0.0008231866105925292, "actor_loss": -20.24316425521672, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.566187143325806, "step": 82000}
{"episode_reward": 1.8953384666178574, "episode": 83.0, "batch_reward": 0.006901887783431448, "critic_loss": 0.0008625117526098621, "actor_loss": -20.773949822738768, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.09688663482666, "step": 83000}
{"episode_reward": 2.317841652127407, "episode": 84.0, "batch_reward": 0.0068678976783994585, "critic_loss": 0.00082794632954392, "actor_loss": -21.65367315503955, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.580933094024658, "step": 84000}
{"episode_reward": 3.0484192584010765, "episode": 85.0, "batch_reward": 0.006746381561504677, "critic_loss": 0.0006946752485127945, "actor_loss": -21.249282646551727, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.572351455688477, "step": 85000}
{"episode_reward": 3.3659194149604517, "episode": 86.0, "batch_reward": 0.00693167740444187, "critic_loss": 0.0006878606022000895, "actor_loss": -19.895996647551655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.991095781326294, "step": 86000}
{"episode_reward": 2.3082955418295965, "episode": 87.0, "batch_reward": 0.006756767420098186, "critic_loss": 0.0008692974973419041, "actor_loss": -22.190168062224984, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.37689185142517, "step": 87000}
{"episode_reward": 2.0803333061597393, "episode": 88.0, "batch_reward": 0.006428475329768844, "critic_loss": 0.0008387715065673547, "actor_loss": -20.16223717699945, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.773212671279907, "step": 88000}
{"episode_reward": 2.3673585930994605, "episode": 89.0, "batch_reward": 0.006659885619301349, "critic_loss": 0.0008693469971731247, "actor_loss": -20.349657486587763, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.55183482170105, "step": 89000}
{"episode_reward": 2.160132195666146, "episode": 90.0, "batch_reward": 0.006716054807417095, "critic_loss": 0.000981176534358383, "actor_loss": -20.356482016414404, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.590318202972412, "step": 90000}
{"episode_reward": 2.999300011013728, "episode": 91.0, "batch_reward": 0.006553564030793496, "critic_loss": 0.0008394217752684199, "actor_loss": -20.173490163370968, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.03792905807495, "step": 91000}
{"episode_reward": 2.335453569363094, "episode": 92.0, "batch_reward": 0.006487657658522949, "critic_loss": 0.0009166798563346675, "actor_loss": -21.011036731511354, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.82168745994568, "step": 92000}
{"episode_reward": 2.9880149667270923, "episode": 93.0, "batch_reward": 0.0064197014073142785, "critic_loss": 0.001117098371254542, "actor_loss": -20.05657378974557, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.392968893051147, "step": 93000}
{"episode_reward": 2.2061675461952834, "episode": 94.0, "batch_reward": 0.006316851456533186, "critic_loss": 0.0011699553245925927, "actor_loss": -20.813918252527714, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.115720748901367, "step": 94000}
{"episode_reward": 2.6865549537685425, "episode": 95.0, "batch_reward": 0.006269907531677745, "critic_loss": 0.000820624656338623, "actor_loss": -21.634145717956127, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.540546655654907, "step": 95000}
{"episode_reward": 2.5384488990566325, "episode": 96.0, "batch_reward": 0.006502484578406438, "critic_loss": 0.0015344705584284383, "actor_loss": -21.116774560630322, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.87614369392395, "step": 96000}
{"episode_reward": 1.8539370717827577, "episode": 97.0, "batch_reward": 0.006402833283413202, "critic_loss": 0.0009765374280577817, "actor_loss": -19.867081208847463, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.474702835083008, "step": 97000}
{"episode_reward": 2.3012633328096888, "episode": 98.0, "batch_reward": 0.006140122026088647, "critic_loss": 0.0008563107392546953, "actor_loss": -22.352593217641115, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.96040964126587, "step": 98000}
{"episode_reward": 2.326862020050739, "episode": 99.0, "batch_reward": 0.006187883080798201, "critic_loss": 0.0010851582239156414, "actor_loss": -20.214938587009907, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.535306692123413, "step": 99000}
{"episode_reward": 2.3907644855368626, "episode": 100.0, "batch_reward": 0.006172270059818402, "critic_loss": 0.0007759808409537072, "actor_loss": -20.211972540304064, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.600034713745117, "step": 100000}
{"episode_reward": 2.1990721475186543, "episode": 101.0, "batch_reward": 0.006146345779881813, "critic_loss": 0.0010541058539820368, "actor_loss": -20.043845278792084, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.80922532081604, "step": 101000}
{"episode_reward": 2.0231516297906675, "episode": 102.0, "batch_reward": 0.0062647078194422645, "critic_loss": 0.0010130923513461312, "actor_loss": -21.1735042078346, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.604326009750366, "step": 102000}
{"episode_reward": 2.47413025333378, "episode": 103.0, "batch_reward": 0.006228092120494693, "critic_loss": 0.0008633585158895584, "actor_loss": -20.22206511258334, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.624512672424316, "step": 103000}
{"episode_reward": 2.5048610349358382, "episode": 104.0, "batch_reward": 0.006195573118166067, "critic_loss": 0.0009375754275642976, "actor_loss": -20.15629681780189, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.24864625930786, "step": 104000}
{"episode_reward": 2.194353896211335, "episode": 105.0, "batch_reward": 0.006077561820740812, "critic_loss": 0.000888839768620528, "actor_loss": -20.626353933110835, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.564634561538696, "step": 105000}
{"episode_reward": 2.303616241178471, "episode": 106.0, "batch_reward": 0.005758314025588334, "critic_loss": 0.0005875690760003636, "actor_loss": -20.384353567883373, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.22583508491516, "step": 106000}
{"episode_reward": 2.020624704513712, "episode": 107.0, "batch_reward": 0.005913347049732692, "critic_loss": 0.0011112835291605735, "actor_loss": -19.268916671931745, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.55016040802002, "step": 107000}
{"episode_reward": 2.410231618764313, "episode": 108.0, "batch_reward": 0.0060543117406778035, "critic_loss": 0.0010239012215606635, "actor_loss": -21.28568699877709, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.589505672454834, "step": 108000}
{"episode_reward": 1.9792008912612848, "episode": 109.0, "batch_reward": 0.00573648204957135, "critic_loss": 0.0009108438770272187, "actor_loss": -20.377255130879583, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.6084144115448, "step": 109000}
{"episode_reward": 2.7951599195890315, "episode": 110.0, "batch_reward": 0.006052489745081402, "critic_loss": 0.0013864077956022812, "actor_loss": -21.393022980853914, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.00234341621399, "step": 110000}
{"episode_reward": 2.2306684360413684, "episode": 111.0, "batch_reward": 0.00573571224976331, "critic_loss": 0.0007042924542583933, "actor_loss": -19.79617787395418, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.89300322532654, "step": 111000}
{"episode_reward": 2.7895228186398167, "episode": 112.0, "batch_reward": 0.005865467903553508, "critic_loss": 0.0007036425713522476, "actor_loss": -21.330997271992267, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.563090085983276, "step": 112000}
{"episode_reward": 2.3328055621746167, "episode": 113.0, "batch_reward": 0.005734059130656533, "critic_loss": 0.0006752711301160161, "actor_loss": -20.26504517584294, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.313694953918457, "step": 113000}
{"episode_reward": 3.1123931202883526, "episode": 114.0, "batch_reward": 0.005800012418068945, "critic_loss": 0.0008101474461327597, "actor_loss": -20.857013484813272, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.353562831878662, "step": 114000}
{"episode_reward": 2.6278342239344687, "episode": 115.0, "batch_reward": 0.005502333987271413, "critic_loss": 0.0006049551077921933, "actor_loss": -20.369422136127948, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.55984115600586, "step": 115000}
{"episode_reward": 2.5751473210717504, "episode": 116.0, "batch_reward": 0.00572183112683706, "critic_loss": 0.0008083783761212544, "actor_loss": -20.97087836448103, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.940685749053955, "step": 116000}
{"episode_reward": 1.7523629075542533, "episode": 117.0, "batch_reward": 0.005611799533362501, "critic_loss": 0.0005730511900746933, "actor_loss": -19.96827451721579, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.50544285774231, "step": 117000}
{"episode_reward": 2.8077985015418907, "episode": 118.0, "batch_reward": 0.005573764001368545, "critic_loss": 0.0005937773117057077, "actor_loss": -19.84563601797819, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.914479970932007, "step": 118000}
{"episode_reward": 2.3062039292777303, "episode": 119.0, "batch_reward": 0.005690152373397723, "critic_loss": 0.0007056702418994973, "actor_loss": -20.378387833304703, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.55228877067566, "step": 119000}
{"episode_reward": 2.3345417775234987, "episode": 120.0, "batch_reward": 0.005652108930633403, "critic_loss": 0.0006473978852736764, "actor_loss": -19.67251129103452, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.14025092124939, "step": 120000}
{"episode_reward": 2.6662078850317084, "episode": 121.0, "batch_reward": 0.00545725179661531, "critic_loss": 0.0008255542701372179, "actor_loss": -20.015858022905885, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.93348288536072, "step": 121000}
{"episode_reward": 2.717593858068362, "episode": 122.0, "batch_reward": 0.005488703810609877, "critic_loss": 0.0005666517860881868, "actor_loss": -20.905821221351623, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.512498378753662, "step": 122000}
{"episode_reward": 2.041531710252014, "episode": 123.0, "batch_reward": 0.005505024216952733, "critic_loss": 0.0007264367949646839, "actor_loss": -20.734062236629427, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.220805406570435, "step": 123000}
{"episode_reward": 2.549511600167305, "episode": 124.0, "batch_reward": 0.005554661176982336, "critic_loss": 0.0008845184430138033, "actor_loss": -21.087994040459396, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.377790212631226, "step": 124000}
{"episode_reward": 3.017694912496985, "episode": 125.0, "batch_reward": 0.005345583524089307, "critic_loss": 0.0004947638527191884, "actor_loss": -20.30365430483222, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.873960256576538, "step": 125000}
{"episode_reward": 2.072128367423832, "episode": 126.0, "batch_reward": 0.005526079334318638, "critic_loss": 0.0007684028652729467, "actor_loss": -21.08188636636734, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.576416730880737, "step": 126000}
{"episode_reward": 3.1173259296122695, "episode": 127.0, "batch_reward": 0.005405314072035253, "critic_loss": 0.0008258772004774073, "actor_loss": -21.082460802771152, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.144246816635132, "step": 127000}
{"episode_reward": 2.829876846234053, "episode": 128.0, "batch_reward": 0.005378361213603057, "critic_loss": 0.0007647698009859596, "actor_loss": -20.843453042939306, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.33411145210266, "step": 128000}
{"episode_reward": 2.669493713724499, "episode": 129.0, "batch_reward": 0.005416271416004747, "critic_loss": 0.0007907728170976042, "actor_loss": -20.554390050910413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.535308599472046, "step": 129000}
{"episode_reward": 3.0096433270977476, "episode": 130.0, "batch_reward": 0.005246531661017798, "critic_loss": 0.0007867979491420556, "actor_loss": -21.312427197240293, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.50573468208313, "step": 130000}
{"episode_reward": 1.8744724207460963, "episode": 131.0, "batch_reward": 0.005275441062403843, "critic_loss": 0.0006175551187880046, "actor_loss": -19.363736932329832, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.81220197677612, "step": 131000}
{"episode_reward": 2.284352303708065, "episode": 132.0, "batch_reward": 0.005239280627807603, "critic_loss": 0.0008471857617696515, "actor_loss": -21.368662499301134, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.719494104385376, "step": 132000}
{"episode_reward": 2.747433438465469, "episode": 133.0, "batch_reward": 0.005196017695474438, "critic_loss": 0.0005463889995680802, "actor_loss": -20.132590018868445, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.57862114906311, "step": 133000}
{"episode_reward": 2.3254756636846245, "episode": 134.0, "batch_reward": 0.0053727436340413985, "critic_loss": 0.0005887463657782064, "actor_loss": -20.77385075137764, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.608940839767456, "step": 134000}
{"episode_reward": 2.8014890567369597, "episode": 135.0, "batch_reward": 0.005066038279910572, "critic_loss": 0.0005455344795464043, "actor_loss": -21.422391955427827, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.87238335609436, "step": 135000}
{"episode_reward": 2.562258110365214, "episode": 136.0, "batch_reward": 0.005281017977977171, "critic_loss": 0.0008123145472272882, "actor_loss": -20.54816816173494, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.539270162582397, "step": 136000}
{"episode_reward": 2.3266866374895905, "episode": 137.0, "batch_reward": 0.005136389811756089, "critic_loss": 0.0005920477358704375, "actor_loss": -20.839442210927604, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.52599334716797, "step": 137000}
{"episode_reward": 2.4369959225003726, "episode": 138.0, "batch_reward": 0.0051977284867316485, "critic_loss": 0.00068007515404679, "actor_loss": -20.72275525212288, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.28338861465454, "step": 138000}
{"episode_reward": 2.3771710484565753, "episode": 139.0, "batch_reward": 0.005063640559557826, "critic_loss": 0.0010300091119679563, "actor_loss": -20.186845876894893, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.421708583831787, "step": 139000}
{"episode_reward": 2.4910675779657225, "episode": 140.0, "batch_reward": 0.0052972955984296275, "critic_loss": 0.0007103947210125625, "actor_loss": -21.6697899992615, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.542808055877686, "step": 140000}
{"episode_reward": 2.666822390862336, "episode": 141.0, "batch_reward": 0.0050759780807420616, "critic_loss": 0.0006462327951303451, "actor_loss": -20.831899191610514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.09550452232361, "step": 141000}
{"episode_reward": 2.545933336106898, "episode": 142.0, "batch_reward": 0.005059316243743524, "critic_loss": 0.0008824128099950031, "actor_loss": -20.60089036116749, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.070671558380127, "step": 142000}
{"episode_reward": 2.307628659246869, "episode": 143.0, "batch_reward": 0.005169867508811876, "critic_loss": 0.0007634494481480942, "actor_loss": -20.781672976344822, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.510993003845215, "step": 143000}
{"episode_reward": 2.9734045701259157, "episode": 144.0, "batch_reward": 0.0050176296129357065, "critic_loss": 0.0005658826363287517, "actor_loss": -21.89102524368465, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.22775936126709, "step": 144000}
{"episode_reward": 2.3955625299904653, "episode": 145.0, "batch_reward": 0.0049224997722776605, "critic_loss": 0.0004401661382562452, "actor_loss": -20.95066625920683, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.10740375518799, "step": 145000}
{"episode_reward": 3.6183711215200374, "episode": 146.0, "batch_reward": 0.004958803538582288, "critic_loss": 0.0006219248790766869, "actor_loss": -19.41567911863327, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.53675150871277, "step": 146000}
{"episode_reward": 2.461315473925545, "episode": 147.0, "batch_reward": 0.005027088126167655, "critic_loss": 0.000756320361419057, "actor_loss": -20.53666969820112, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.57452893257141, "step": 147000}
{"episode_reward": 2.355287579526562, "episode": 148.0, "batch_reward": 0.004890241864370182, "critic_loss": 0.0005984497463941806, "actor_loss": -21.18186978738755, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.123873710632324, "step": 148000}
{"episode_reward": 2.844107688646543, "episode": 149.0, "batch_reward": 0.0049785009323386475, "critic_loss": 0.0007356542494690075, "actor_loss": -21.00514576549828, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.3138108253479, "step": 149000}
{"episode_reward": 2.770471075678706, "episode": 150.0, "batch_reward": 0.004778551560593769, "critic_loss": 0.0004334749002555327, "actor_loss": -21.061746337726714, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
