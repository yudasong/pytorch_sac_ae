{"episode": 1.0, "duration": 16.67339825630188, "episode_reward": 7.051849146674556, "step": 1000}
{"episode": 2.0, "duration": 1.4363617897033691, "episode_reward": 365.4711839680396, "step": 2000}
{"episode": 3.0, "batch_reward": 0.18714447248455046, "critic_loss": 0.3182376548846629, "actor_loss": -35.51597655663828, "actor_target_entropy": -6.0, "alpha_value": 0.01377813086574139, "duration": 63.40248703956604, "episode_reward": 247.66013412448362, "step": 3000}
{"episode": 4.0, "batch_reward": 0.21750445230305196, "critic_loss": 0.33509311170876027, "actor_loss": -36.67994180297852, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 20.655675411224365, "episode_reward": 307.29127007422835, "step": 4000}
{"episode": 5.0, "batch_reward": 0.23862086366117, "critic_loss": 0.37622041171789167, "actor_loss": -37.58037837982178, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 22.026512145996094, "episode_reward": 290.05087799602535, "step": 5000}
{"episode": 6.0, "batch_reward": 0.24920270183682441, "critic_loss": 0.39579244711995126, "actor_loss": -37.93205695343018, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 27.274117708206177, "episode_reward": 328.22149541281664, "step": 6000}
{"episode": 7.0, "batch_reward": 0.25699303048849104, "critic_loss": 0.43800458958745003, "actor_loss": -38.08504502105713, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 25.930606842041016, "episode_reward": 259.4024048609709, "step": 7000}
{"episode": 8.0, "batch_reward": 0.2609426877349615, "critic_loss": 0.49046199426054954, "actor_loss": -38.05233051300049, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 25.65867257118225, "episode_reward": 284.68761779187287, "step": 8000}
{"episode": 9.0, "batch_reward": 0.26166556763648985, "critic_loss": 0.5430282091498375, "actor_loss": -37.90192687225342, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 25.742676734924316, "episode_reward": 264.00655381146123, "step": 9000}
{"episode": 10.0, "batch_reward": 0.2626698267310858, "critic_loss": 0.5474910444617271, "actor_loss": -29.250125873565676, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 3568.303401708603, "episode_reward": 298.5044559495347, "step": 10000}
{"episode": 11.0, "batch_reward": 0.26522482961416244, "critic_loss": 0.4640452819168568, "actor_loss": -29.397733722686766, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 36.12524652481079, "episode_reward": 273.8949497216252, "step": 11000}
{"episode": 12.0, "batch_reward": 0.2638910195082426, "critic_loss": 0.4150407537817955, "actor_loss": -24.762773551940917, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 434.6619071960449, "episode_reward": 280.033253548719, "step": 12000}
{"episode": 13.0, "batch_reward": 0.26973135973513124, "critic_loss": 0.39852061274647715, "actor_loss": -25.08838108444214, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 20.648815631866455, "episode_reward": 353.7138007590697, "step": 13000}
{"episode": 14.0, "batch_reward": 0.27764455978572367, "critic_loss": 0.39685042433440687, "actor_loss": -23.428811023712157, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 378.4812834262848, "episode_reward": 407.99864744772333, "step": 14000}
{"episode": 15.0, "batch_reward": 0.2853136487752199, "critic_loss": 0.378092247530818, "actor_loss": -23.907324493408204, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 20.187812328338623, "episode_reward": 378.1660145395678, "step": 15000}
{"episode": 16.0, "batch_reward": 0.29233558431267737, "critic_loss": 0.35422393026947974, "actor_loss": -23.178595905303954, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 432.43377900123596, "episode_reward": 394.93432918663103, "step": 16000}
{"episode": 17.0, "batch_reward": 0.2999764728844166, "critic_loss": 0.33649549768865106, "actor_loss": -23.615052108764647, "actor_target_entropy": -6.0, "alpha_value": 0.013778130865741629, "duration": 24.878477811813354, "episode_reward": 410.0449666480592, "step": 17000}
