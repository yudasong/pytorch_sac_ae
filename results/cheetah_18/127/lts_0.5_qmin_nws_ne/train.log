{"episode_reward": 0.0, "episode": 1.0, "duration": 18.375844717025757, "step": 1000}
{"episode_reward": 7.051849146674556, "episode": 2.0, "duration": 1.5221819877624512, "step": 2000}
{"episode_reward": 365.4711839680396, "episode": 3.0, "batch_reward": 0.17479153759293098, "critic_loss": 0.01833455766260527, "actor_loss": -20.12934150613573, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 68.88016438484192, "step": 3000}
{"episode_reward": 2.4688012299490154, "episode": 4.0, "batch_reward": 0.10852946434915066, "critic_loss": 0.008691167192650028, "actor_loss": -18.050268385887147, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.334450244903564, "step": 4000}
{"episode_reward": 1.8175277648342567, "episode": 5.0, "batch_reward": 0.08437088475748897, "critic_loss": 0.006056966325850226, "actor_loss": -17.362210829257965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.486098766326904, "step": 5000}
{"episode_reward": 1.7289444567562564, "episode": 6.0, "batch_reward": 0.06852242182195187, "critic_loss": 0.007484993961115833, "actor_loss": -18.020680991888046, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.612059116363525, "step": 6000}
{"episode_reward": 1.8330429373953987, "episode": 7.0, "batch_reward": 0.05861303715966642, "critic_loss": 0.006418345466023311, "actor_loss": -16.408247717380522, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.242260694503784, "step": 7000}
{"episode_reward": 2.492022978747407, "episode": 8.0, "batch_reward": 0.051501962399110195, "critic_loss": 0.006739258385903667, "actor_loss": -16.05250025486946, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.81902265548706, "step": 8000}
{"episode_reward": 3.2193781368718875, "episode": 9.0, "batch_reward": 0.04554062863998115, "critic_loss": 0.004786922481958754, "actor_loss": -16.984150531291963, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.640175342559814, "step": 9000}
{"episode_reward": 2.439779611328899, "episode": 10.0, "batch_reward": 0.041203706643544134, "critic_loss": 0.005627469255006872, "actor_loss": -16.801879365444183, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.55286455154419, "step": 10000}
{"episode_reward": 2.2580882290714572, "episode": 11.0, "batch_reward": 0.03783415221981704, "critic_loss": 0.005359445904148743, "actor_loss": -16.282648721814155, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.90219831466675, "step": 11000}
{"episode_reward": 2.507844493759533, "episode": 12.0, "batch_reward": 0.0339879724541679, "critic_loss": 0.005476940218533855, "actor_loss": -16.104868880867958, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.128689527511597, "step": 12000}
{"episode_reward": 2.8546979497830476, "episode": 13.0, "batch_reward": 0.03155674427933991, "critic_loss": 0.00506514347056509, "actor_loss": -15.756343941569328, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.583223342895508, "step": 13000}
{"episode_reward": 2.7566489868455766, "episode": 14.0, "batch_reward": 0.029767399674281478, "critic_loss": 0.004291658733563963, "actor_loss": -15.751940311789513, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.39065456390381, "step": 14000}
{"episode_reward": 2.8001851308809425, "episode": 15.0, "batch_reward": 0.02750995712215081, "critic_loss": 0.0034614651158917696, "actor_loss": -16.48908180296421, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.1944477558136, "step": 15000}
{"episode_reward": 2.030007527687075, "episode": 16.0, "batch_reward": 0.026078206541016696, "critic_loss": 0.0032656070989032743, "actor_loss": -16.091062126517297, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.74093246459961, "step": 16000}
{"episode_reward": 2.7427728917705583, "episode": 17.0, "batch_reward": 0.024561474548187107, "critic_loss": 0.0034499426364782266, "actor_loss": -15.752277727127074, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.756353616714478, "step": 17000}
{"episode_reward": 1.7547701450611701, "episode": 18.0, "batch_reward": 0.023874041493050755, "critic_loss": 0.0029975794589408907, "actor_loss": -15.63546263229847, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.288748025894165, "step": 18000}
{"episode_reward": 3.488025142385729, "episode": 19.0, "batch_reward": 0.022507979760412127, "critic_loss": 0.0026261341352073943, "actor_loss": -15.372862103700637, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.064009428024292, "step": 19000}
{"episode_reward": 2.859163930278185, "episode": 20.0, "batch_reward": 0.021350076423957944, "critic_loss": 0.0028452160970773546, "actor_loss": -16.501688432335854, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.895005226135254, "step": 20000}
{"episode_reward": 2.198993355484605, "episode": 21.0, "batch_reward": 0.019960532563738524, "critic_loss": 0.0030796358319203137, "actor_loss": -14.11677335202694, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.28649425506592, "step": 21000}
{"episode_reward": 2.428943073321277, "episode": 22.0, "batch_reward": 0.01959804019704461, "critic_loss": 0.0030395421780267497, "actor_loss": -16.391320658802986, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.99756169319153, "step": 22000}
{"episode_reward": 3.5978644342939985, "episode": 23.0, "batch_reward": 0.01889559422247112, "critic_loss": 0.002193910752830561, "actor_loss": -15.582020380735397, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.65155601501465, "step": 23000}
{"episode_reward": 2.7164219676277837, "episode": 24.0, "batch_reward": 0.018033444812987, "critic_loss": 0.0024467856846313225, "actor_loss": -15.401047994494439, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.95257019996643, "step": 24000}
{"episode_reward": 2.6402001821956858, "episode": 25.0, "batch_reward": 0.017719438795465975, "critic_loss": 0.001962561873238883, "actor_loss": -15.904978190898895, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.85235071182251, "step": 25000}
{"episode_reward": 2.7926248632098414, "episode": 26.0, "batch_reward": 0.017002930156188086, "critic_loss": 0.0020203523656819014, "actor_loss": -15.39676278424263, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.82884645462036, "step": 26000}
{"episode_reward": 2.571064650976764, "episode": 27.0, "batch_reward": 0.016772543352330103, "critic_loss": 0.00247262163679261, "actor_loss": -14.78028730905056, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.529907941818237, "step": 27000}
{"episode_reward": 2.6630920312793607, "episode": 28.0, "batch_reward": 0.01612266273982823, "critic_loss": 0.0016613058059519973, "actor_loss": -15.528205209970475, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.522277355194092, "step": 28000}
{"episode_reward": 2.6053872616166682, "episode": 29.0, "batch_reward": 0.015293758230749518, "critic_loss": 0.002248939831231837, "actor_loss": -15.150398462712765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.884199619293213, "step": 29000}
{"episode_reward": 2.294583194773635, "episode": 30.0, "batch_reward": 0.015046447740518488, "critic_loss": 0.002590784048901696, "actor_loss": -13.92012011629343, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.984182834625244, "step": 30000}
{"episode_reward": 2.4219213437563223, "episode": 31.0, "batch_reward": 0.014494262611726299, "critic_loss": 0.0012720820795293548, "actor_loss": -14.824243945777416, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 44.604597091674805, "step": 31000}
{"episode_reward": 2.1435049756124736, "episode": 32.0, "batch_reward": 0.014252303577726708, "critic_loss": 0.00192335897700832, "actor_loss": -15.943752752125263, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.84336829185486, "step": 32000}
{"episode_reward": 2.5680668124232184, "episode": 33.0, "batch_reward": 0.013777825773693622, "critic_loss": 0.0018321527426378452, "actor_loss": -15.561830397844314, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.102597951889038, "step": 33000}
{"episode_reward": 2.290325844083749, "episode": 34.0, "batch_reward": 0.013714305792702362, "critic_loss": 0.0017463976904400623, "actor_loss": -14.494184761047363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.967679023742676, "step": 34000}
{"episode_reward": 3.1085421242473568, "episode": 35.0, "batch_reward": 0.012689299962949008, "critic_loss": 0.0017042525666329312, "actor_loss": -15.470193649172783, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.408164262771606, "step": 35000}
{"episode_reward": 2.6485595708834215, "episode": 36.0, "batch_reward": 0.01255040295375511, "critic_loss": 0.0018729718814829539, "actor_loss": -15.572501629710198, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.862303018569946, "step": 36000}
{"episode_reward": 2.3665330478612256, "episode": 37.0, "batch_reward": 0.01263510533364024, "critic_loss": 0.0014666917180074961, "actor_loss": -14.6437938054204, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.041422843933105, "step": 37000}
{"episode_reward": 2.482254518810039, "episode": 38.0, "batch_reward": 0.012297299311263487, "critic_loss": 0.0021032777783620985, "actor_loss": -14.972898365259171, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.25571918487549, "step": 38000}
{"episode_reward": 2.198824855208384, "episode": 39.0, "batch_reward": 0.012131480877986179, "critic_loss": 0.0014829888106833095, "actor_loss": -14.677790519058705, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.560056924819946, "step": 39000}
{"episode_reward": 2.5284578938947844, "episode": 40.0, "batch_reward": 0.011834980589221231, "critic_loss": 0.0016527794554131105, "actor_loss": -14.65707010614872, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.346967220306396, "step": 40000}
{"episode_reward": 2.5441308010680364, "episode": 41.0, "batch_reward": 0.011506882263347507, "critic_loss": 0.0012577617026836379, "actor_loss": -14.18298036468029, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.045775413513184, "step": 41000}
{"episode_reward": 2.605564311247226, "episode": 42.0, "batch_reward": 0.011337624148698523, "critic_loss": 0.0015103045768555604, "actor_loss": -14.460585919559001, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.140851497650146, "step": 42000}
{"episode_reward": 2.787803393290467, "episode": 43.0, "batch_reward": 0.011105364857474343, "critic_loss": 0.0017099785722311934, "actor_loss": -15.34564256361127, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.11497950553894, "step": 43000}
{"episode_reward": 2.2862976625570584, "episode": 44.0, "batch_reward": 0.011111621323507279, "critic_loss": 0.0012971554539253703, "actor_loss": -16.333888257622718, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.913723707199097, "step": 44000}
{"episode_reward": 2.9146047414823455, "episode": 45.0, "batch_reward": 0.010715632063103839, "critic_loss": 0.0012571694372418279, "actor_loss": -15.02618812635541, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.532951593399048, "step": 45000}
{"episode_reward": 1.9043464709289917, "episode": 46.0, "batch_reward": 0.010239003557362594, "critic_loss": 0.0012469417273059662, "actor_loss": -14.357661076933146, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.13722276687622, "step": 46000}
{"episode_reward": 2.2414579110306088, "episode": 47.0, "batch_reward": 0.01023889334674459, "critic_loss": 0.001377066503831884, "actor_loss": -15.070483297288417, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.37703275680542, "step": 47000}
{"episode_reward": 2.744121385790527, "episode": 48.0, "batch_reward": 0.01025371202698443, "critic_loss": 0.0011909114654481527, "actor_loss": -14.876876967966556, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.935950756072998, "step": 48000}
{"episode_reward": 3.051909288537618, "episode": 49.0, "batch_reward": 0.010242542908410542, "critic_loss": 0.0010122107055904053, "actor_loss": -16.179883994191886, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.757631540298462, "step": 49000}
{"episode_reward": 2.768897254674828, "episode": 50.0, "batch_reward": 0.009895776867982932, "critic_loss": 0.0017487838110973825, "actor_loss": -14.990630573332309, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.165030002593994, "step": 50000}
{"episode_reward": 2.423159461072144, "episode": 51.0, "batch_reward": 0.009758886359166354, "critic_loss": 0.0008872121035528835, "actor_loss": -14.150565735250712, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.004831314086914, "step": 51000}
{"episode_reward": 2.3685970821726907, "episode": 52.0, "batch_reward": 0.009732854980393313, "critic_loss": 0.0009193141386640491, "actor_loss": -14.447850261181593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.56753897666931, "step": 52000}
{"episode_reward": 2.393209483758558, "episode": 53.0, "batch_reward": 0.009468022993067279, "critic_loss": 0.0016949861350949505, "actor_loss": -15.907331432521342, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.033486366271973, "step": 53000}
{"episode_reward": 2.1591171469557757, "episode": 54.0, "batch_reward": 0.009371202424983493, "critic_loss": 0.0009661671814137663, "actor_loss": -16.14763297918439, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.225850582122803, "step": 54000}
{"episode_reward": 2.331074352957927, "episode": 55.0, "batch_reward": 0.0093673671890283, "critic_loss": 0.0011459166458589608, "actor_loss": -14.316950349658727, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.896244525909424, "step": 55000}
{"episode_reward": 2.366084056250994, "episode": 56.0, "batch_reward": 0.009060735992621631, "critic_loss": 0.0013437119014233759, "actor_loss": -14.711037151277065, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.453460693359375, "step": 56000}
{"episode_reward": 2.7770556297044253, "episode": 57.0, "batch_reward": 0.009003882004413754, "critic_loss": 0.0012814922586112515, "actor_loss": -14.171901203930378, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.62535047531128, "step": 57000}
{"episode_reward": 2.604315260047617, "episode": 58.0, "batch_reward": 0.008786253000609577, "critic_loss": 0.0009874085057526827, "actor_loss": -15.53427959367633, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.92392063140869, "step": 58000}
{"episode_reward": 3.474714199077333, "episode": 59.0, "batch_reward": 0.008781826182384975, "critic_loss": 0.0013394738893111936, "actor_loss": -14.764283222973347, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.96729803085327, "step": 59000}
{"episode_reward": 2.309201067688921, "episode": 60.0, "batch_reward": 0.008773108399822377, "critic_loss": 0.0009767690002991003, "actor_loss": -15.518804965376853, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.06012725830078, "step": 60000}
{"episode_reward": 1.851156979127773, "episode": 61.0, "batch_reward": 0.008713886058074423, "critic_loss": 0.0008829539063408447, "actor_loss": -14.216537998825311, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.530126094818115, "step": 61000}
{"episode_reward": 1.7733222190229285, "episode": 62.0, "batch_reward": 0.008761499282321892, "critic_loss": 0.0006093006573901221, "actor_loss": -16.10284788070619, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.642980337142944, "step": 62000}
{"episode_reward": 3.366815278312659, "episode": 63.0, "batch_reward": 0.008334866003366187, "critic_loss": 0.0010248375504997966, "actor_loss": -13.91000391127169, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.42857003211975, "step": 63000}
{"episode_reward": 2.3067405640038268, "episode": 64.0, "batch_reward": 0.008215826397528872, "critic_loss": 0.0010653518422295747, "actor_loss": -15.384196443274616, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.896504402160645, "step": 64000}
{"episode_reward": 2.1776249571192565, "episode": 65.0, "batch_reward": 0.008191222758498044, "critic_loss": 0.0010727168330231507, "actor_loss": -14.792118988335133, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.87755846977234, "step": 65000}
{"episode_reward": 3.299774188700872, "episode": 66.0, "batch_reward": 0.00798386276117526, "critic_loss": 0.0010204424081639446, "actor_loss": -15.195774801671504, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.230522632598877, "step": 66000}
{"episode_reward": 2.353248089360434, "episode": 67.0, "batch_reward": 0.008061119849211536, "critic_loss": 0.0014864675077405991, "actor_loss": -16.137890558734536, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.24826669692993, "step": 67000}
{"episode_reward": 2.39027516695815, "episode": 68.0, "batch_reward": 0.00795720046677161, "critic_loss": 0.0008899805958099024, "actor_loss": -16.24863143400848, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.98562979698181, "step": 68000}
{"episode_reward": 2.53089172317328, "episode": 69.0, "batch_reward": 0.007940599478431978, "critic_loss": 0.0008205095201119548, "actor_loss": -14.705481271490454, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.67525577545166, "step": 69000}
{"episode_reward": 1.819882155314453, "episode": 70.0, "batch_reward": 0.0077724751874338836, "critic_loss": 0.0007851353030710015, "actor_loss": -14.886111803025008, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.941823482513428, "step": 70000}
{"episode_reward": 2.6379561995437504, "episode": 71.0, "batch_reward": 0.007816607250715606, "critic_loss": 0.0009336307553458028, "actor_loss": -14.4077183778584, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.56973886489868, "step": 71000}
{"episode_reward": 2.4731551501236093, "episode": 72.0, "batch_reward": 0.007743106383713894, "critic_loss": 0.0009145614423978259, "actor_loss": -14.47014228834212, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.40828537940979, "step": 72000}
{"episode_reward": 2.4323857842440937, "episode": 73.0, "batch_reward": 0.007715580753050745, "critic_loss": 0.0008302020372630067, "actor_loss": -14.64792132177949, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.795217752456665, "step": 73000}
{"episode_reward": 2.792605779494502, "episode": 74.0, "batch_reward": 0.007405062806908972, "critic_loss": 0.0008983502239025257, "actor_loss": -14.87456743811071, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.31968855857849, "step": 74000}
{"episode_reward": 2.034913942555338, "episode": 75.0, "batch_reward": 0.007246002183645032, "critic_loss": 0.000947634949854546, "actor_loss": -14.802434047430754, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.2515389919281, "step": 75000}
{"episode_reward": 2.1317297152525487, "episode": 76.0, "batch_reward": 0.007492434233077802, "critic_loss": 0.0006500074014693382, "actor_loss": -14.39181240722537, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.44038438796997, "step": 76000}
{"episode_reward": 2.538719556057837, "episode": 77.0, "batch_reward": 0.007455858346424066, "critic_loss": 0.0008679834584581841, "actor_loss": -15.050857826888562, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.11957359313965, "step": 77000}
{"episode_reward": 2.499612539052745, "episode": 78.0, "batch_reward": 0.007538459260133095, "critic_loss": 0.0007493149102447205, "actor_loss": -14.279394486919045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.20403504371643, "step": 78000}
{"episode_reward": 2.6104199564100767, "episode": 79.0, "batch_reward": 0.007140859744395129, "critic_loss": 0.0009352942224650178, "actor_loss": -15.93013185544312, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.853590965270996, "step": 79000}
{"episode_reward": 2.4938961280054346, "episode": 80.0, "batch_reward": 0.007205094840028323, "critic_loss": 0.0008366200730233687, "actor_loss": -15.69388318347931, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.22938585281372, "step": 80000}
{"episode_reward": 2.0659708924111806, "episode": 81.0, "batch_reward": 0.006949560215231031, "critic_loss": 0.0006334318195004016, "actor_loss": -15.857197827294469, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.76620030403137, "step": 81000}
{"episode_reward": 2.520863416827245, "episode": 82.0, "batch_reward": 0.006862697970704176, "critic_loss": 0.0006650109701222391, "actor_loss": -14.560063283041119, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.44561219215393, "step": 82000}
{"episode_reward": 1.8953384666178574, "episode": 83.0, "batch_reward": 0.006901887776795775, "critic_loss": 0.0007995534984984261, "actor_loss": -15.863190458014607, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.64265537261963, "step": 83000}
{"episode_reward": 2.317841652127407, "episode": 84.0, "batch_reward": 0.0068678976683877405, "critic_loss": 0.0008083656606868316, "actor_loss": -16.032835349589586, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.429579734802246, "step": 84000}
{"episode_reward": 3.0484192584010765, "episode": 85.0, "batch_reward": 0.006746381562785246, "critic_loss": 0.0006737539698369801, "actor_loss": -14.743524153381586, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.166057109832764, "step": 85000}
{"episode_reward": 3.3659194149604517, "episode": 86.0, "batch_reward": 0.0069316773897735405, "critic_loss": 0.0006971866857384157, "actor_loss": -14.806313234820962, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.494121551513672, "step": 86000}
{"episode_reward": 2.3082955418295965, "episode": 87.0, "batch_reward": 0.006756767414859496, "critic_loss": 0.0007872373548343603, "actor_loss": -15.56539916524291, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.40614414215088, "step": 87000}
{"episode_reward": 2.0803333061597393, "episode": 88.0, "batch_reward": 0.006428475331398659, "critic_loss": 0.0006075898624221736, "actor_loss": -15.248860501423477, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.79396367073059, "step": 88000}
{"episode_reward": 2.3673585930994605, "episode": 89.0, "batch_reward": 0.006659885623841546, "critic_loss": 0.0006822505725067458, "actor_loss": -14.019489746794104, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.770463466644287, "step": 89000}
{"episode_reward": 2.160132195666146, "episode": 90.0, "batch_reward": 0.006716054797754623, "critic_loss": 0.0010157589868831564, "actor_loss": -14.141180253647267, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.06977081298828, "step": 90000}
{"episode_reward": 2.999300011013728, "episode": 91.0, "batch_reward": 0.006553564028814435, "critic_loss": 0.0007912819929370016, "actor_loss": -13.545788886748253, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.08234906196594, "step": 91000}
{"episode_reward": 2.335453569363094, "episode": 92.0, "batch_reward": 0.0064876576437382025, "critic_loss": 0.0010048897280466916, "actor_loss": -15.206304554410279, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.850345134735107, "step": 92000}
{"episode_reward": 2.9880149667270923, "episode": 93.0, "batch_reward": 0.0064197014073142785, "critic_loss": 0.0009530859491605952, "actor_loss": -13.601255025677382, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.207730531692505, "step": 93000}
{"episode_reward": 2.2061675461952834, "episode": 94.0, "batch_reward": 0.006316851448151283, "critic_loss": 0.0007761162895330927, "actor_loss": -15.135692361116408, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.89236330986023, "step": 94000}
{"episode_reward": 2.6865549537685425, "episode": 95.0, "batch_reward": 0.006269907529698685, "critic_loss": 0.0010113845151718125, "actor_loss": -16.196731755636634, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.10032033920288, "step": 95000}
{"episode_reward": 2.5384488990566325, "episode": 96.0, "batch_reward": 0.0065024845686275515, "critic_loss": 0.0013034320924489294, "actor_loss": -14.314296512044967, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.69791603088379, "step": 96000}
{"episode_reward": 1.8539370717827577, "episode": 97.0, "batch_reward": 0.0064028332772431895, "critic_loss": 0.0010785963751077361, "actor_loss": -13.600169825583697, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.306005239486694, "step": 97000}
{"episode_reward": 2.3012633328096888, "episode": 98.0, "batch_reward": 0.006140122034586966, "critic_loss": 0.0009280102343618637, "actor_loss": -15.78388425526768, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.93260431289673, "step": 98000}
{"episode_reward": 2.326862020050739, "episode": 99.0, "batch_reward": 0.006187883068574592, "critic_loss": 0.0011885089474999405, "actor_loss": -15.304214058175683, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.40753197669983, "step": 99000}
{"episode_reward": 2.3907644855368626, "episode": 100.0, "batch_reward": 0.006172270051785745, "critic_loss": 0.0007092873872370547, "actor_loss": -14.88630791170895, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.94542646408081, "step": 100000}
{"episode_reward": 2.1990721475186543, "episode": 101.0, "batch_reward": 0.006146345772314816, "critic_loss": 0.0010753133530160995, "actor_loss": -14.242933343291282, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.84256052970886, "step": 101000}
{"episode_reward": 2.0231516297906675, "episode": 102.0, "batch_reward": 0.006264707807567902, "critic_loss": 0.0012559385568092693, "actor_loss": -15.434231018245221, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.094603061676025, "step": 102000}
{"episode_reward": 2.47413025333378, "episode": 103.0, "batch_reward": 0.006228092103498057, "critic_loss": 0.0009253144719914417, "actor_loss": -15.370097305297852, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.083430528640747, "step": 103000}
{"episode_reward": 2.5048610349358382, "episode": 104.0, "batch_reward": 0.006195573117118329, "critic_loss": 0.0010454618385265348, "actor_loss": -13.87908489794284, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.964857578277588, "step": 104000}
{"episode_reward": 2.194353896211335, "episode": 105.0, "batch_reward": 0.006077561812940985, "critic_loss": 0.0007684355710334785, "actor_loss": -14.653574393674731, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.085569858551025, "step": 105000}
{"episode_reward": 2.303616241178471, "episode": 106.0, "batch_reward": 0.0057583140169736, "critic_loss": 0.0007415121746089426, "actor_loss": -13.820916259221733, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.37158179283142, "step": 106000}
{"episode_reward": 2.020624704513712, "episode": 107.0, "batch_reward": 0.005913347043097019, "critic_loss": 0.0011038485494100315, "actor_loss": -14.062188192926348, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.763795852661133, "step": 107000}
{"episode_reward": 2.410231618764313, "episode": 108.0, "batch_reward": 0.006054311732528731, "critic_loss": 0.0010279531936794228, "actor_loss": -16.13704220326245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.035576105117798, "step": 108000}
{"episode_reward": 1.9792008912612848, "episode": 109.0, "batch_reward": 0.00573648204060737, "critic_loss": 0.0007941000179016555, "actor_loss": -14.700013400301337, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.136030435562134, "step": 109000}
{"episode_reward": 2.7951599195890315, "episode": 110.0, "batch_reward": 0.006052489742636681, "critic_loss": 0.001209870463120751, "actor_loss": -15.41911362786591, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.44627809524536, "step": 110000}
{"episode_reward": 2.2306684360413684, "episode": 111.0, "batch_reward": 0.005735712240333669, "critic_loss": 0.0007898255471118318, "actor_loss": -14.47071768372506, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.30502510070801, "step": 111000}
{"episode_reward": 2.7895228186398167, "episode": 112.0, "batch_reward": 0.005865467895404436, "critic_loss": 0.0006885687048998079, "actor_loss": -16.06815568065643, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.06653094291687, "step": 112000}
{"episode_reward": 2.3328055621746167, "episode": 113.0, "batch_reward": 0.005734059121110476, "critic_loss": 0.0006459035134939768, "actor_loss": -13.876094208590686, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.77828311920166, "step": 113000}
{"episode_reward": 3.1123931202883526, "episode": 114.0, "batch_reward": 0.005800012413528748, "critic_loss": 0.0009036485110336798, "actor_loss": -15.356990008845925, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.846187114715576, "step": 114000}
{"episode_reward": 2.6278342239344687, "episode": 115.0, "batch_reward": 0.005502333985059522, "critic_loss": 0.0006186215685320349, "actor_loss": -14.991586617030203, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.68157935142517, "step": 115000}
{"episode_reward": 2.5751473210717504, "episode": 116.0, "batch_reward": 0.005721831121947616, "critic_loss": 0.0008314963168741087, "actor_loss": -14.879729693338275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.95325779914856, "step": 116000}
{"episode_reward": 1.7523629075542533, "episode": 117.0, "batch_reward": 0.005611799519509077, "critic_loss": 0.0005465400254652195, "actor_loss": -13.8777367997095, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.114553928375244, "step": 117000}
{"episode_reward": 2.8077985015418907, "episode": 118.0, "batch_reward": 0.005573763997643254, "critic_loss": 0.0004902575596624956, "actor_loss": -14.048602404005825, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.62165927886963, "step": 118000}
{"episode_reward": 2.3062039292777303, "episode": 119.0, "batch_reward": 0.005690152359660715, "critic_loss": 0.0008161526662115648, "actor_loss": -14.165204472750425, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.019898653030396, "step": 119000}
{"episode_reward": 2.3345417775234987, "episode": 120.0, "batch_reward": 0.005652108924929053, "critic_loss": 0.0007073820004952722, "actor_loss": -14.10872326464206, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.78753423690796, "step": 120000}
{"episode_reward": 2.6662078850317084, "episode": 121.0, "batch_reward": 0.005457251791725867, "critic_loss": 0.0008562125416538038, "actor_loss": -13.68754049193114, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.040868282318115, "step": 121000}
{"episode_reward": 2.717593858068362, "episode": 122.0, "batch_reward": 0.005488703805836849, "critic_loss": 0.0007820189835128986, "actor_loss": -15.1141183886379, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.531654357910156, "step": 122000}
{"episode_reward": 2.041531710252014, "episode": 123.0, "batch_reward": 0.005505024218582549, "critic_loss": 0.000676758283021627, "actor_loss": -14.818968248143792, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.869970083236694, "step": 123000}
{"episode_reward": 2.549511600167305, "episode": 124.0, "batch_reward": 0.00555466117837932, "critic_loss": 0.0008607524867547909, "actor_loss": -15.532284451410174, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.5761296749115, "step": 124000}
{"episode_reward": 3.017694912496985, "episode": 125.0, "batch_reward": 0.005345583524787798, "critic_loss": 0.0006203395925113, "actor_loss": -14.686562417432667, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.21391487121582, "step": 125000}
{"episode_reward": 2.072128367423832, "episode": 126.0, "batch_reward": 0.005526079335832037, "critic_loss": 0.0008202436820720322, "actor_loss": -15.347805173955857, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.599799871444702, "step": 126000}
{"episode_reward": 3.1173259296122695, "episode": 127.0, "batch_reward": 0.005405314066330902, "critic_loss": 0.0008501837071125919, "actor_loss": -15.999172279752791, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.58979892730713, "step": 127000}
{"episode_reward": 2.829876846234053, "episode": 128.0, "batch_reward": 0.005378361199749633, "critic_loss": 0.0005837330881313392, "actor_loss": -15.524147773236036, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.674091339111328, "step": 128000}
{"episode_reward": 2.669493713724499, "episode": 129.0, "batch_reward": 0.005416271413909272, "critic_loss": 0.0007190005261381885, "actor_loss": -15.229442108921706, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.608250617980957, "step": 129000}
{"episode_reward": 3.0096433270977476, "episode": 130.0, "batch_reward": 0.005246531645883806, "critic_loss": 0.000620479420731499, "actor_loss": -14.812510374121368, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.064931392669678, "step": 130000}
{"episode_reward": 1.8744724207460963, "episode": 131.0, "batch_reward": 0.005275441055884585, "critic_loss": 0.0005039454270790885, "actor_loss": -14.45251082894206, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.30158805847168, "step": 131000}
{"episode_reward": 2.284352303708065, "episode": 132.0, "batch_reward": 0.005239280623849481, "critic_loss": 0.0005861514452317351, "actor_loss": -14.748236051812768, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.59219264984131, "step": 132000}
{"episode_reward": 2.747433438465469, "episode": 133.0, "batch_reward": 0.005196017690235749, "critic_loss": 0.0005565243339187873, "actor_loss": -13.62474020459503, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.43243908882141, "step": 133000}
{"episode_reward": 2.3254756636846245, "episode": 134.0, "batch_reward": 0.00537274362903554, "critic_loss": 0.0004916181650714862, "actor_loss": -14.564192284286023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.916353940963745, "step": 134000}
{"episode_reward": 2.8014890567369597, "episode": 135.0, "batch_reward": 0.005066038273042068, "critic_loss": 0.0003449025317968335, "actor_loss": -15.452324639737606, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.607747316360474, "step": 135000}
{"episode_reward": 2.562258110365214, "episode": 136.0, "batch_reward": 0.0052810179727384825, "critic_loss": 0.0009073322756321431, "actor_loss": -15.34149200168252, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.135592699050903, "step": 136000}
{"episode_reward": 2.3266866374895905, "episode": 137.0, "batch_reward": 0.005136389803607017, "critic_loss": 0.0006443674554775498, "actor_loss": -14.926292114026845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.945685148239136, "step": 137000}
{"episode_reward": 2.4369959225003726, "episode": 138.0, "batch_reward": 0.0051977284769527616, "critic_loss": 0.000715658506615, "actor_loss": -15.04720255894959, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.019495487213135, "step": 138000}
{"episode_reward": 2.3771710484565753, "episode": 139.0, "batch_reward": 0.005063640555832535, "critic_loss": 0.0008729728143971442, "actor_loss": -14.217481870889664, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.94246006011963, "step": 139000}
{"episode_reward": 2.4910675779657225, "episode": 140.0, "batch_reward": 0.005297295594471506, "critic_loss": 0.0005736443750538456, "actor_loss": -14.634473848156631, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.85009741783142, "step": 140000}
{"episode_reward": 2.666822390862336, "episode": 141.0, "batch_reward": 0.005075978078995832, "critic_loss": 0.0006680934108608199, "actor_loss": -14.803687073871494, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.57975673675537, "step": 141000}
{"episode_reward": 2.545933336106898, "episode": 142.0, "batch_reward": 0.005059316232101992, "critic_loss": 0.0007480708174862229, "actor_loss": -14.69182334536314, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.368250608444214, "step": 142000}
{"episode_reward": 2.307628659246869, "episode": 143.0, "batch_reward": 0.005169867500895634, "critic_loss": 0.0007251920343660459, "actor_loss": -14.810452716305852, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.309754371643066, "step": 143000}
{"episode_reward": 2.9734045701259157, "episode": 144.0, "batch_reward": 0.005017629599315114, "critic_loss": 0.0005607672825153713, "actor_loss": -14.98056093722582, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.0339834690094, "step": 144000}
{"episode_reward": 2.3955625299904653, "episode": 145.0, "batch_reward": 0.0049224997691344466, "critic_loss": 0.0005779134643180442, "actor_loss": -15.924483562693, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.421249866485596, "step": 145000}
{"episode_reward": 3.6183711215200374, "episode": 146.0, "batch_reward": 0.004958803530083969, "critic_loss": 0.0006144523693692463, "actor_loss": -14.098089324131609, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.839491605758667, "step": 146000}
{"episode_reward": 2.461315473925545, "episode": 147.0, "batch_reward": 0.005027088121627457, "critic_loss": 0.0008823884659577743, "actor_loss": -14.682799429476262, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.354677438735962, "step": 147000}
{"episode_reward": 2.355287579526562, "episode": 148.0, "batch_reward": 0.004890241862391122, "critic_loss": 0.0005648973571824172, "actor_loss": -14.917135135538876, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.274404525756836, "step": 148000}
{"episode_reward": 2.844107688646543, "episode": 149.0, "batch_reward": 0.004978500924422405, "critic_loss": 0.0008675288224676478, "actor_loss": -14.857581378512084, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.611867904663086, "step": 149000}
{"episode_reward": 2.770471075678706, "episode": 150.0, "batch_reward": 0.004778551553725265, "critic_loss": 0.0005373915420532285, "actor_loss": -15.567021995455027, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
