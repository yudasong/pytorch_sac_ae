{"episode_reward": 0.0, "episode": 1.0, "duration": 19.141939640045166, "step": 1000}
{"episode_reward": 7.051849146674556, "episode": 2.0, "duration": 1.5804932117462158, "step": 2000}
{"episode_reward": 365.4711839680396, "episode": 3.0, "batch_reward": 0.17479153764013372, "critic_loss": 0.018495706452315137, "actor_loss": -23.44880476675066, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 67.50295352935791, "step": 3000}
{"episode_reward": 2.4688016088350535, "episode": 4.0, "batch_reward": 0.10852946455776691, "critic_loss": 0.008649119958863593, "actor_loss": -21.455048862457275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.252461910247803, "step": 4000}
{"episode_reward": 1.817528128506601, "episode": 5.0, "batch_reward": 0.08437088496983051, "critic_loss": 0.006527487289975397, "actor_loss": -20.092695764541627, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.801061153411865, "step": 5000}
{"episode_reward": 1.7289444076818867, "episode": 6.0, "batch_reward": 0.0685224218852818, "critic_loss": 0.007626324518350884, "actor_loss": -20.71818937110901, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.530633687973022, "step": 6000}
{"episode_reward": 1.8330429436278948, "episode": 7.0, "batch_reward": 0.05861303728632629, "critic_loss": 0.006899908256600611, "actor_loss": -18.85745324373245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.059903621673584, "step": 7000}
{"episode_reward": 2.4920231838364426, "episode": 8.0, "batch_reward": 0.051501962473616006, "critic_loss": 0.006962732774089091, "actor_loss": -19.80647056555748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.733697414398193, "step": 8000}
{"episode_reward": 3.219378154580518, "episode": 9.0, "batch_reward": 0.04554062878713012, "critic_loss": 0.004946818878233898, "actor_loss": -19.823566874742507, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.4611234664917, "step": 9000}
{"episode_reward": 2.4397796304822705, "episode": 10.0, "batch_reward": 0.041203706696629525, "critic_loss": 0.006046833315805997, "actor_loss": -19.393342661380768, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.58857536315918, "step": 10000}
{"episode_reward": 2.2580882500872903, "episode": 11.0, "batch_reward": 0.03783415233157575, "critic_loss": 0.005257558024430182, "actor_loss": -19.230021592140197, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.3842716217041, "step": 11000}
{"episode_reward": 2.50784457694108, "episode": 12.0, "batch_reward": 0.033987972523085774, "critic_loss": 0.004571666342089884, "actor_loss": -19.400637776613234, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.240153074264526, "step": 12000}
{"episode_reward": 2.8546978733596213, "episode": 13.0, "batch_reward": 0.03155674435663968, "critic_loss": 0.0039473354076908435, "actor_loss": -18.64074415719509, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.236425161361694, "step": 13000}
{"episode_reward": 2.7566489302380037, "episode": 14.0, "batch_reward": 0.029767399762291463, "critic_loss": 0.0035162484413012863, "actor_loss": -18.278936012506485, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.555559873580933, "step": 14000}
{"episode_reward": 2.8001850202095273, "episode": 15.0, "batch_reward": 0.027509957182221115, "critic_loss": 0.0038742305301711893, "actor_loss": -19.48893897151947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.72770595550537, "step": 15000}
{"episode_reward": 2.030007516261023, "episode": 16.0, "batch_reward": 0.026078206640668213, "critic_loss": 0.0031359881709940966, "actor_loss": -18.62756254041195, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.285783767700195, "step": 16000}
{"episode_reward": 2.7427729121346394, "episode": 17.0, "batch_reward": 0.024561474608955906, "critic_loss": 0.0036347186568309554, "actor_loss": -18.341840646624565, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.456968307495117, "step": 17000}
{"episode_reward": 1.7547701864749528, "episode": 18.0, "batch_reward": 0.023874041538219898, "critic_loss": 0.003544380866049323, "actor_loss": -18.282061069130897, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.231966018676758, "step": 18000}
{"episode_reward": 3.48802522214035, "episode": 19.0, "batch_reward": 0.02250797979766503, "critic_loss": 0.002286494371597655, "actor_loss": -18.4297094322443, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.268946886062622, "step": 19000}
{"episode_reward": 2.859164130765626, "episode": 20.0, "batch_reward": 0.021350076479138808, "critic_loss": 0.0027003050046769204, "actor_loss": -19.436672263860704, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.730801343917847, "step": 20000}
{"episode_reward": 2.1989934601359282, "episode": 21.0, "batch_reward": 0.01996053262008354, "critic_loss": 0.0027343051871139322, "actor_loss": -17.1149770860672, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.3065619468689, "step": 21000}
{"episode_reward": 2.4289431225982323, "episode": 22.0, "batch_reward": 0.01959804026549682, "critic_loss": 0.003009132109473285, "actor_loss": -18.915776388525963, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.962185621261597, "step": 22000}
{"episode_reward": 3.5978645903234088, "episode": 23.0, "batch_reward": 0.01889559427695349, "critic_loss": 0.0017706308230262947, "actor_loss": -17.76047999548912, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.69906234741211, "step": 23000}
{"episode_reward": 2.716422029460028, "episode": 24.0, "batch_reward": 0.018033444877481087, "critic_loss": 0.002757682189141633, "actor_loss": -18.40064566528797, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.93455743789673, "step": 24000}
{"episode_reward": 2.640200190991172, "episode": 25.0, "batch_reward": 0.017719438857631758, "critic_loss": 0.0017925340068031801, "actor_loss": -18.968027844190598, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.94565725326538, "step": 25000}
{"episode_reward": 2.792624895327828, "episode": 26.0, "batch_reward": 0.017002930217422545, "critic_loss": 0.002095252451232227, "actor_loss": -18.46769497990608, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.18525719642639, "step": 26000}
{"episode_reward": 2.5710646706560736, "episode": 27.0, "batch_reward": 0.016772543388651685, "critic_loss": 0.0023140026114997453, "actor_loss": -17.678688735067844, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.139464616775513, "step": 27000}
{"episode_reward": 2.663092037996538, "episode": 28.0, "batch_reward": 0.016122662793844937, "critic_loss": 0.0016505426299481768, "actor_loss": -18.774744235515595, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.75684666633606, "step": 28000}
{"episode_reward": 2.6053872628071826, "episode": 29.0, "batch_reward": 0.01529375828220509, "critic_loss": 0.0024797354367692604, "actor_loss": -17.68538426887989, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.30276870727539, "step": 29000}
{"episode_reward": 2.2945832028588002, "episode": 30.0, "batch_reward": 0.015046447799541056, "critic_loss": 0.0024364981002290734, "actor_loss": -17.216360495328903, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.44415044784546, "step": 30000}
{"episode_reward": 2.421921343534554, "episode": 31.0, "batch_reward": 0.014494262676686048, "critic_loss": 0.001135890346900851, "actor_loss": -17.54560757166147, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.19951272010803, "step": 31000}
{"episode_reward": 2.1435049804331126, "episode": 32.0, "batch_reward": 0.01425230362941511, "critic_loss": 0.0015941844045883045, "actor_loss": -17.952991252660752, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.922857522964478, "step": 32000}
{"episode_reward": 2.5680668124232184, "episode": 33.0, "batch_reward": 0.013777825829572975, "critic_loss": 0.0016763052738679107, "actor_loss": -17.86541998261213, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.89978790283203, "step": 33000}
{"episode_reward": 2.2903258442223535, "episode": 34.0, "batch_reward": 0.013714305839268491, "critic_loss": 0.0017262551286185044, "actor_loss": -17.56655849337578, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.14406704902649, "step": 34000}
{"episode_reward": 3.1085421242473568, "episode": 35.0, "batch_reward": 0.012689300001598895, "critic_loss": 0.0015541163751040586, "actor_loss": -18.006793994545937, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.562507390975952, "step": 35000}
{"episode_reward": 2.6485595722026756, "episode": 36.0, "batch_reward": 0.012550402983673848, "critic_loss": 0.00120093240593269, "actor_loss": -19.000662032663822, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.941688537597656, "step": 36000}
{"episode_reward": 2.3665330481232028, "episode": 37.0, "batch_reward": 0.012635105376713909, "critic_loss": 0.0014426345912070247, "actor_loss": -17.59398273563385, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.917189836502075, "step": 37000}
{"episode_reward": 2.482254518810039, "episode": 38.0, "batch_reward": 0.012297299344674684, "critic_loss": 0.001638603049213998, "actor_loss": -17.574037329673768, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.20668911933899, "step": 38000}
{"episode_reward": 2.198824856157678, "episode": 39.0, "batch_reward": 0.012131480920827015, "critic_loss": 0.0017490900587072246, "actor_loss": -17.51338204598427, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.313642024993896, "step": 39000}
{"episode_reward": 2.5284578938947844, "episode": 40.0, "batch_reward": 0.011834980634739622, "critic_loss": 0.0013224763016260112, "actor_loss": -17.90028276515007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.873872995376587, "step": 40000}
{"episode_reward": 2.5441308010680364, "episode": 41.0, "batch_reward": 0.011506882317364216, "critic_loss": 0.0009817865900040488, "actor_loss": -16.54573708355427, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.82706952095032, "step": 41000}
{"episode_reward": 2.605564311247226, "episode": 42.0, "batch_reward": 0.011337624161504209, "critic_loss": 0.0017435066244543122, "actor_loss": -17.41655253636837, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.4798481464386, "step": 42000}
{"episode_reward": 2.787803393290467, "episode": 43.0, "batch_reward": 0.0111053649057867, "critic_loss": 0.0012902565751865042, "actor_loss": -18.12226718866825, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.03313374519348, "step": 43000}
{"episode_reward": 2.2862976625570584, "episode": 44.0, "batch_reward": 0.01111162137100473, "critic_loss": 0.0012164772547766916, "actor_loss": -19.51954669213295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.24175786972046, "step": 44000}
{"episode_reward": 2.9146047414823455, "episode": 45.0, "batch_reward": 0.010715632085921243, "critic_loss": 0.0012264509106607875, "actor_loss": -18.39947773525119, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.227688312530518, "step": 45000}
{"episode_reward": 1.9043464709289917, "episode": 46.0, "batch_reward": 0.010239003584138118, "critic_loss": 0.0013545022296966636, "actor_loss": -16.961249182760717, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.09815216064453, "step": 46000}
{"episode_reward": 2.2414579110306088, "episode": 47.0, "batch_reward": 0.0102388933673501, "critic_loss": 0.0013770758600294356, "actor_loss": -17.78921428820491, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.90938115119934, "step": 47000}
{"episode_reward": 2.744121385790527, "episode": 48.0, "batch_reward": 0.010253712064470165, "critic_loss": 0.0010068055697083763, "actor_loss": -17.595304855674506, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.353124380111694, "step": 48000}
{"episode_reward": 3.051909288537618, "episode": 49.0, "batch_reward": 0.010242542938096448, "critic_loss": 0.0010758361940825126, "actor_loss": -19.077046204179524, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.785998106002808, "step": 49000}
{"episode_reward": 2.768897254356056, "episode": 50.0, "batch_reward": 0.009895776885910892, "critic_loss": 0.0013876511591734016, "actor_loss": -18.541997093707323, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.19434642791748, "step": 50000}
{"episode_reward": 2.423159461072144, "episode": 51.0, "batch_reward": 0.009758886375348084, "critic_loss": 0.0008545587051958137, "actor_loss": -18.230144646018744, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.155513048172, "step": 51000}
{"episode_reward": 2.3685970821726907, "episode": 52.0, "batch_reward": 0.009732855006121099, "critic_loss": 0.0010001970382509172, "actor_loss": -17.23327153366804, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.704794883728027, "step": 52000}
{"episode_reward": 2.393209483758558, "episode": 53.0, "batch_reward": 0.009468023023800924, "critic_loss": 0.0012905965287718572, "actor_loss": -18.396905574411154, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.61123752593994, "step": 53000}
{"episode_reward": 2.1591171469557757, "episode": 54.0, "batch_reward": 0.009371202451176942, "critic_loss": 0.0011311002109068795, "actor_loss": -19.341644537210463, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.184130430221558, "step": 54000}
{"episode_reward": 2.331074352957927, "episode": 55.0, "batch_reward": 0.009367367210099474, "critic_loss": 0.0011426724403245316, "actor_loss": -17.69598071321845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.180514335632324, "step": 55000}
{"episode_reward": 2.366084056250994, "episode": 56.0, "batch_reward": 0.009060736024170183, "critic_loss": 0.001042209580198687, "actor_loss": -18.028942518055437, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.331111192703247, "step": 56000}
{"episode_reward": 2.7770556297044253, "episode": 57.0, "batch_reward": 0.009003882035147398, "critic_loss": 0.0012784938602490002, "actor_loss": -17.847143812298775, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.204688549041748, "step": 57000}
{"episode_reward": 2.604315260047617, "episode": 58.0, "batch_reward": 0.008786253016674891, "critic_loss": 0.0008906181437741907, "actor_loss": -18.021449284493922, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.816880226135254, "step": 58000}
{"episode_reward": 3.4747142046406734, "episode": 59.0, "batch_reward": 0.008781826192862354, "critic_loss": 0.0014724726197200653, "actor_loss": -18.37337059351802, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.872546911239624, "step": 59000}
{"episode_reward": 2.309201067688921, "episode": 60.0, "batch_reward": 0.008773108419729397, "critic_loss": 0.0009926082558522466, "actor_loss": -18.658222923755645, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.240370750427246, "step": 60000}
{"episode_reward": 1.851156979127773, "episode": 61.0, "batch_reward": 0.008713886083569378, "critic_loss": 0.0010567535831432905, "actor_loss": -17.53706041625142, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.989315032958984, "step": 61000}
{"episode_reward": 1.7733222190229285, "episode": 62.0, "batch_reward": 0.008761499303975143, "critic_loss": 0.0006470166149265423, "actor_loss": -18.831228381931783, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.006497383117676, "step": 62000}
{"episode_reward": 3.366815278312659, "episode": 63.0, "batch_reward": 0.008334866026300005, "critic_loss": 0.0011736359767528484, "actor_loss": -17.644881938159465, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.029955863952637, "step": 63000}
{"episode_reward": 2.3067405640038268, "episode": 64.0, "batch_reward": 0.008215826409985312, "critic_loss": 0.0012360419570104568, "actor_loss": -17.990173610508442, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.548550605773926, "step": 64000}
{"episode_reward": 2.1776249571192565, "episode": 65.0, "batch_reward": 0.00819122278864961, "critic_loss": 0.0011678823176916923, "actor_loss": -18.170469620108605, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.22590708732605, "step": 65000}
{"episode_reward": 3.299774188700872, "episode": 66.0, "batch_reward": 0.00798386278154794, "critic_loss": 0.0011807475757414068, "actor_loss": -17.74600697129965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.423974752426147, "step": 66000}
{"episode_reward": 2.353248089360434, "episode": 67.0, "batch_reward": 0.00806111987971235, "critic_loss": 0.0014158209400047782, "actor_loss": -18.626861660808324, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.749242782592773, "step": 67000}
{"episode_reward": 2.39027516695815, "episode": 68.0, "batch_reward": 0.007957200483768247, "critic_loss": 0.0009783778909222746, "actor_loss": -19.509217971995472, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.85556721687317, "step": 68000}
{"episode_reward": 2.53089172317328, "episode": 69.0, "batch_reward": 0.007940599511843175, "critic_loss": 0.001241434699659294, "actor_loss": -17.37372890712321, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.282675981521606, "step": 69000}
{"episode_reward": 1.819882155314453, "episode": 70.0, "batch_reward": 0.007772475209203549, "critic_loss": 0.0008357786454580491, "actor_loss": -17.436771211639048, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.099167346954346, "step": 70000}
{"episode_reward": 2.6379561995437504, "episode": 71.0, "batch_reward": 0.007816607277025468, "critic_loss": 0.00123504784720717, "actor_loss": -17.078197788745165, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.38461494445801, "step": 71000}
{"episode_reward": 2.4731551501236093, "episode": 72.0, "batch_reward": 0.007743106403038837, "critic_loss": 0.0010496344447747106, "actor_loss": -17.550303983554244, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.514623165130615, "step": 72000}
{"episode_reward": 2.4323857842440937, "episode": 73.0, "batch_reward": 0.007715580770629458, "critic_loss": 0.0011784722438933385, "actor_loss": -18.08799967224896, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.884560585021973, "step": 73000}
{"episode_reward": 2.792605779494502, "episode": 74.0, "batch_reward": 0.007405062828329392, "critic_loss": 0.0010126555786882819, "actor_loss": -17.902442595049738, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.3279767036438, "step": 74000}
{"episode_reward": 2.034913942555338, "episode": 75.0, "batch_reward": 0.007246002214611508, "critic_loss": 0.001073261188586912, "actor_loss": -17.7654751496166, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.35983443260193, "step": 75000}
{"episode_reward": 2.1317297152525487, "episode": 76.0, "batch_reward": 0.007492434267536737, "critic_loss": 0.0008534815974962839, "actor_loss": -17.716547267228364, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.78014826774597, "step": 76000}
{"episode_reward": 2.538719556057837, "episode": 77.0, "batch_reward": 0.007455858367495239, "critic_loss": 0.0008920617135372595, "actor_loss": -17.954566437825562, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.895647048950195, "step": 77000}
{"episode_reward": 2.499612539052745, "episode": 78.0, "batch_reward": 0.0075384592816699295, "critic_loss": 0.0009331958374023088, "actor_loss": -17.59759458695352, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.572284936904907, "step": 78000}
{"episode_reward": 2.6104199564100767, "episode": 79.0, "batch_reward": 0.00714085976104252, "critic_loss": 0.0010917041486281959, "actor_loss": -18.303758788451553, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.799052715301514, "step": 79000}
{"episode_reward": 2.4938961280054346, "episode": 80.0, "batch_reward": 0.007205094857839867, "critic_loss": 0.00101393631648898, "actor_loss": -18.599922237232327, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.20637607574463, "step": 80000}
{"episode_reward": 2.0659708924111806, "episode": 81.0, "batch_reward": 0.006949560223612934, "critic_loss": 0.0007055133680150903, "actor_loss": -18.16642959022522, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.226125717163086, "step": 81000}
{"episode_reward": 2.520863416827245, "episode": 82.0, "batch_reward": 0.006862698000972159, "critic_loss": 0.0006980112203673343, "actor_loss": -16.992662428483367, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.342291116714478, "step": 82000}
{"episode_reward": 1.8953384666178574, "episode": 83.0, "batch_reward": 0.006901887797866948, "critic_loss": 0.0011422498520914815, "actor_loss": -18.47092317892611, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.836045265197754, "step": 83000}
{"episode_reward": 2.317841652127407, "episode": 84.0, "batch_reward": 0.006867897684685886, "critic_loss": 0.0010081522802174731, "actor_loss": -18.580070006027817, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.60151219367981, "step": 84000}
{"episode_reward": 3.0484192584010765, "episode": 85.0, "batch_reward": 0.006746381580946035, "critic_loss": 0.0006021854563623493, "actor_loss": -17.5885067037791, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.617380380630493, "step": 85000}
{"episode_reward": 3.3659194149604517, "episode": 86.0, "batch_reward": 0.006931677409913391, "critic_loss": 0.0008809101512088091, "actor_loss": -17.354738178774713, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.17343235015869, "step": 86000}
{"episode_reward": 2.3082955418295965, "episode": 87.0, "batch_reward": 0.006756767431157641, "critic_loss": 0.000708187289586931, "actor_loss": -19.00325672608614, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.74261164665222, "step": 87000}
{"episode_reward": 2.0803333061597393, "episode": 88.0, "batch_reward": 0.006428475346649066, "critic_loss": 0.000562024388311329, "actor_loss": -17.028799141243102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.625826835632324, "step": 88000}
{"episode_reward": 2.3673585930994605, "episode": 89.0, "batch_reward": 0.006659885632107034, "critic_loss": 0.0008087078193202615, "actor_loss": -16.80357259103656, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.64468002319336, "step": 89000}
{"episode_reward": 2.160132195666146, "episode": 90.0, "batch_reward": 0.006716054815915413, "critic_loss": 0.0009840339243201014, "actor_loss": -16.98426651223004, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.350581884384155, "step": 90000}
{"episode_reward": 2.999300011013728, "episode": 91.0, "batch_reward": 0.006553564047091641, "critic_loss": 0.0007269183269600035, "actor_loss": -16.801753456398846, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.794907331466675, "step": 91000}
{"episode_reward": 2.335453569363094, "episode": 92.0, "batch_reward": 0.006487657662830316, "critic_loss": 0.0008297602229067706, "actor_loss": -17.815584960088135, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.565549612045288, "step": 92000}
{"episode_reward": 2.9880149667270923, "episode": 93.0, "batch_reward": 0.006419701422564685, "critic_loss": 0.0007858340891525586, "actor_loss": -16.742054669484496, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.381646394729614, "step": 93000}
{"episode_reward": 2.2061675461952834, "episode": 94.0, "batch_reward": 0.0063168514679418876, "critic_loss": 0.0007625473036205221, "actor_loss": -18.096094578176736, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.562007665634155, "step": 94000}
{"episode_reward": 2.6865549537685425, "episode": 95.0, "batch_reward": 0.006269907538662665, "critic_loss": 0.0007728788254244137, "actor_loss": -19.212731250062586, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.61439561843872, "step": 95000}
{"episode_reward": 2.5384488990566325, "episode": 96.0, "batch_reward": 0.006502484594238922, "critic_loss": 0.0010680929603204276, "actor_loss": -17.686866111323237, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.779341220855713, "step": 96000}
{"episode_reward": 1.8539370717827577, "episode": 97.0, "batch_reward": 0.00640283329715021, "critic_loss": 0.0008912146370239497, "actor_loss": -16.441032185301186, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.67572855949402, "step": 97000}
{"episode_reward": 2.3012633328096888, "episode": 98.0, "batch_reward": 0.006140122044715099, "critic_loss": 0.0006532351857185859, "actor_loss": -19.457415361501276, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.888879537582397, "step": 98000}
{"episode_reward": 2.326862020050739, "episode": 99.0, "batch_reward": 0.006187883088481613, "critic_loss": 0.0008600768120813882, "actor_loss": -17.49679737596959, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.601605892181396, "step": 99000}
{"episode_reward": 2.3907644855368626, "episode": 100.0, "batch_reward": 0.006172270072624087, "critic_loss": 0.0008791429933517065, "actor_loss": -17.548292469605805, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.687477111816406, "step": 100000}
{"episode_reward": 2.1990721475186543, "episode": 101.0, "batch_reward": 0.006146345794433728, "critic_loss": 0.000823762453506788, "actor_loss": -17.14173465874046, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.176706314086914, "step": 101000}
{"episode_reward": 2.0231516297906675, "episode": 102.0, "batch_reward": 0.006264707827707753, "critic_loss": 0.0008143244174934807, "actor_loss": -18.216715638913215, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.64276432991028, "step": 102000}
{"episode_reward": 2.47413025333378, "episode": 103.0, "batch_reward": 0.0062280921267811205, "critic_loss": 0.0006184992078124196, "actor_loss": -18.271819854810833, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.165568351745605, "step": 103000}
{"episode_reward": 2.5048610349358382, "episode": 104.0, "batch_reward": 0.006195573122357018, "critic_loss": 0.0009235567383730085, "actor_loss": -16.903722348481416, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.5439670085907, "step": 104000}
{"episode_reward": 2.194353896211335, "episode": 105.0, "batch_reward": 0.006077561826910823, "critic_loss": 0.0007120584706426598, "actor_loss": -17.079205478437245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.53742790222168, "step": 105000}
{"episode_reward": 2.303616241178471, "episode": 106.0, "batch_reward": 0.005758314044564031, "critic_loss": 0.0007431098197830579, "actor_loss": -17.310693000368772, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.31336522102356, "step": 106000}
{"episode_reward": 2.020624704513712, "episode": 107.0, "batch_reward": 0.005913347054040059, "critic_loss": 0.0011008105947803415, "actor_loss": -16.255567744664848, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.531545400619507, "step": 107000}
{"episode_reward": 2.410231618764313, "episode": 108.0, "batch_reward": 0.006054311747197062, "critic_loss": 0.0008944926384319842, "actor_loss": -19.09799916511029, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.726730823516846, "step": 108000}
{"episode_reward": 1.9792008912612848, "episode": 109.0, "batch_reward": 0.005736482061212882, "critic_loss": 0.0007461731348503235, "actor_loss": -17.539494906552136, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.644819498062134, "step": 109000}
{"episode_reward": 2.7951599195890315, "episode": 110.0, "batch_reward": 0.006052489753696136, "critic_loss": 0.0010457705998960592, "actor_loss": -19.384445084497333, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.853707313537598, "step": 110000}
{"episode_reward": 2.2306684360413684, "episode": 111.0, "batch_reward": 0.00573571226245258, "critic_loss": 0.0005307193648441171, "actor_loss": -16.95593425156176, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.87026238441467, "step": 111000}
{"episode_reward": 2.7895228186398167, "episode": 112.0, "batch_reward": 0.005865467906347476, "critic_loss": 0.0004715774856358621, "actor_loss": -18.90777294062823, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.60919165611267, "step": 112000}
{"episode_reward": 2.3328055621746167, "episode": 113.0, "batch_reward": 0.005734059136477299, "critic_loss": 0.0005370819258660049, "actor_loss": -17.30849396353215, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.6552574634552, "step": 113000}
{"episode_reward": 3.1123931202883526, "episode": 114.0, "batch_reward": 0.0058000124273821715, "critic_loss": 0.0007983063405245048, "actor_loss": -18.140084927670657, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.955389976501465, "step": 114000}
{"episode_reward": 2.6278342239344687, "episode": 115.0, "batch_reward": 0.005502333995304071, "critic_loss": 0.0005482491200400545, "actor_loss": -18.067844583481552, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.645482063293457, "step": 115000}
{"episode_reward": 2.5751473210717504, "episode": 116.0, "batch_reward": 0.005721831137198024, "critic_loss": 0.0007497333893734322, "actor_loss": -18.07493026597798, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.683796405792236, "step": 116000}
{"episode_reward": 1.7523629075542533, "episode": 117.0, "batch_reward": 0.005611799541395158, "critic_loss": 0.000511564723245101, "actor_loss": -17.070258184328676, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.539562702178955, "step": 117000}
{"episode_reward": 2.8077985015418907, "episode": 118.0, "batch_reward": 0.005573764007654972, "critic_loss": 0.0006086497442029212, "actor_loss": -17.243923673413693, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.1393826007843, "step": 118000}
{"episode_reward": 2.3062039292777303, "episode": 119.0, "batch_reward": 0.005690152379102074, "critic_loss": 0.0007089742704974924, "actor_loss": -17.125960905849933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.654347896575928, "step": 119000}
{"episode_reward": 2.3345417775234987, "episode": 120.0, "batch_reward": 0.005652108939015307, "critic_loss": 0.0005787057198940602, "actor_loss": -17.129955736003815, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.542795181274414, "step": 120000}
{"episode_reward": 2.6662078850317084, "episode": 121.0, "batch_reward": 0.005457251804182306, "critic_loss": 0.0008400661900195701, "actor_loss": -17.355797333426775, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.930789947509766, "step": 121000}
{"episode_reward": 2.717593858068362, "episode": 122.0, "batch_reward": 0.005488703810609877, "critic_loss": 0.0006373323859243101, "actor_loss": -17.954709883019326, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.60561466217041, "step": 122000}
{"episode_reward": 2.041531710252014, "episode": 123.0, "batch_reward": 0.0055050242306897415, "critic_loss": 0.0006699960830865166, "actor_loss": -17.65726629780233, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.147406339645386, "step": 123000}
{"episode_reward": 2.549511600167305, "episode": 124.0, "batch_reward": 0.005554661187110468, "critic_loss": 0.0008368753880349687, "actor_loss": -18.95987199868262, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.51283574104309, "step": 124000}
{"episode_reward": 3.017694912496985, "episode": 125.0, "batch_reward": 0.005345583532005548, "critic_loss": 0.000471951153169357, "actor_loss": -17.644115441732108, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.593579053878784, "step": 125000}
{"episode_reward": 2.072128367423832, "episode": 126.0, "batch_reward": 0.005526079346775077, "critic_loss": 0.0007639723954453075, "actor_loss": -18.305255852356552, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.7646222114563, "step": 126000}
{"episode_reward": 3.1173259296122695, "episode": 127.0, "batch_reward": 0.005405314083909616, "critic_loss": 0.0007539205490993482, "actor_loss": -18.424449356243013, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.649261713027954, "step": 127000}
{"episode_reward": 2.829876846234053, "episode": 128.0, "batch_reward": 0.0053783612197730694, "critic_loss": 0.0006157274855522701, "actor_loss": -18.06253495518863, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.750675201416016, "step": 128000}
{"episode_reward": 2.669493713724499, "episode": 129.0, "batch_reward": 0.00541627142613288, "critic_loss": 0.0005804374246035877, "actor_loss": -17.777099914066493, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.887081384658813, "step": 129000}
{"episode_reward": 3.0096433270977476, "episode": 130.0, "batch_reward": 0.005246531660319306, "critic_loss": 0.0006200047042548249, "actor_loss": -17.94689664117247, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.974433183670044, "step": 130000}
{"episode_reward": 1.8744724207460963, "episode": 131.0, "batch_reward": 0.005275441070552916, "critic_loss": 0.000516838767711306, "actor_loss": -16.583232379712165, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.59387183189392, "step": 131000}
{"episode_reward": 2.284352303708065, "episode": 132.0, "batch_reward": 0.005239280636538751, "critic_loss": 0.0006302957596890337, "actor_loss": -18.23666267630458, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.620754718780518, "step": 132000}
{"episode_reward": 2.747433438465469, "episode": 133.0, "batch_reward": 0.005196017698966898, "critic_loss": 0.000523251432705365, "actor_loss": -16.759634530872106, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.357351541519165, "step": 133000}
{"episode_reward": 2.3254756636846245, "episode": 134.0, "batch_reward": 0.0053727436360204595, "critic_loss": 0.0005229604350060981, "actor_loss": -17.40225588607043, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.257062673568726, "step": 134000}
{"episode_reward": 2.8014890567369597, "episode": 135.0, "batch_reward": 0.0050660382856149225, "critic_loss": 0.0004152272262144834, "actor_loss": -18.349157852210105, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.624802112579346, "step": 135000}
{"episode_reward": 2.562258110365214, "episode": 136.0, "batch_reward": 0.005281017978908494, "critic_loss": 0.0009020451179749216, "actor_loss": -18.29939881116897, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.288228034973145, "step": 136000}
{"episode_reward": 2.3266866374895905, "episode": 137.0, "batch_reward": 0.005136389818973839, "critic_loss": 0.0005410983910023788, "actor_loss": -17.646598155371844, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.352911472320557, "step": 137000}
{"episode_reward": 2.4369959225003726, "episode": 138.0, "batch_reward": 0.005197728489409201, "critic_loss": 0.0006945336929238692, "actor_loss": -17.94336743743718, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.599679708480835, "step": 138000}
{"episode_reward": 2.3771710484565753, "episode": 139.0, "batch_reward": 0.005063640561886132, "critic_loss": 0.0008507717357315414, "actor_loss": -17.34925581867248, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.6584894657135, "step": 139000}
{"episode_reward": 2.4910675779657225, "episode": 140.0, "batch_reward": 0.005297295607393607, "critic_loss": 0.0006119489628654265, "actor_loss": -18.71345691009611, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.24488854408264, "step": 140000}
{"episode_reward": 2.666822390862336, "episode": 141.0, "batch_reward": 0.005075978090404533, "critic_loss": 0.0007505014726702938, "actor_loss": -17.52262913226336, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.937315702438354, "step": 141000}
{"episode_reward": 2.545933336106898, "episode": 142.0, "batch_reward": 0.00505931624409277, "critic_loss": 0.0008050001030624116, "actor_loss": -17.467975432924927, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.625102043151855, "step": 142000}
{"episode_reward": 2.307628659246869, "episode": 143.0, "batch_reward": 0.005169867513002828, "critic_loss": 0.0006123788174354558, "actor_loss": -16.997722146861257, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.78227186203003, "step": 143000}
{"episode_reward": 2.9734045701259157, "episode": 144.0, "batch_reward": 0.0050176296086283404, "critic_loss": 0.0005665350419458263, "actor_loss": -18.227576580688357, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.59097695350647, "step": 144000}
{"episode_reward": 2.3955625299904653, "episode": 145.0, "batch_reward": 0.004922499778564088, "critic_loss": 0.00046318776015868935, "actor_loss": -18.408199957348405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.60625720024109, "step": 145000}
{"episode_reward": 3.6183711215200374, "episode": 146.0, "batch_reward": 0.00495880354498513, "critic_loss": 0.0004779211421864602, "actor_loss": -16.81294412128627, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.896553993225098, "step": 146000}
{"episode_reward": 2.461315473925545, "episode": 147.0, "batch_reward": 0.0050270881358301265, "critic_loss": 0.0006847543736676016, "actor_loss": -17.75343945799023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.663251161575317, "step": 147000}
{"episode_reward": 2.355287579526562, "episode": 148.0, "batch_reward": 0.0048902418689103794, "critic_loss": 0.0005136831765830721, "actor_loss": -18.048795644938945, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.817918300628662, "step": 148000}
{"episode_reward": 2.844107688646543, "episode": 149.0, "batch_reward": 0.0049785009409533815, "critic_loss": 0.0006708579307796754, "actor_loss": -17.75188039672375, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.625919342041016, "step": 149000}
{"episode_reward": 2.770471075678706, "episode": 150.0, "batch_reward": 0.004778551565133966, "critic_loss": 0.00046718599849373274, "actor_loss": -17.74955407295376, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
