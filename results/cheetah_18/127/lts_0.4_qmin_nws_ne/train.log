{"episode_reward": 0.0, "episode": 1.0, "duration": 17.68507719039917, "step": 1000}
{"episode_reward": 7.051849146674556, "episode": 2.0, "duration": 1.6493220329284668, "step": 2000}
{"episode_reward": 365.4711839680396, "episode": 3.0, "batch_reward": 0.17479153755815, "critic_loss": 0.018546057754081635, "actor_loss": -16.339883864862102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 71.30178141593933, "step": 3000}
{"episode_reward": 2.468800524771921, "episode": 4.0, "batch_reward": 0.10852946406602859, "critic_loss": 0.009000053291325458, "actor_loss": -14.509143809795379, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.600615739822388, "step": 4000}
{"episode_reward": 1.8175274649345412, "episode": 5.0, "batch_reward": 0.08437088461220264, "critic_loss": 0.006847920664236881, "actor_loss": -14.17142344379425, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.894137859344482, "step": 5000}
{"episode_reward": 1.7289447148538186, "episode": 6.0, "batch_reward": 0.06852242175862193, "critic_loss": 0.007974192160298117, "actor_loss": -14.622446838855744, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.822627067565918, "step": 6000}
{"episode_reward": 1.8330429325679636, "episode": 7.0, "batch_reward": 0.05861303692124784, "critic_loss": 0.007404976352991071, "actor_loss": -13.042127668380738, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.55806040763855, "step": 7000}
{"episode_reward": 2.4920224572556493, "episode": 8.0, "batch_reward": 0.05150196225568652, "critic_loss": 0.00688831208035117, "actor_loss": -13.586284737586976, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.389612913131714, "step": 8000}
{"episode_reward": 3.219378683613999, "episode": 9.0, "batch_reward": 0.04554062854498625, "critic_loss": 0.0054537455136305655, "actor_loss": -13.879119429826737, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.26820659637451, "step": 9000}
{"episode_reward": 2.4397796537964616, "episode": 10.0, "batch_reward": 0.04120370654761791, "critic_loss": 0.005458953556953929, "actor_loss": -13.63879764020443, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.122631788253784, "step": 10000}
{"episode_reward": 2.2580881076423047, "episode": 11.0, "batch_reward": 0.037834152146242556, "critic_loss": 0.005726501079741865, "actor_loss": -13.833070569515229, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.09355640411377, "step": 11000}
{"episode_reward": 2.5078443379070294, "episode": 12.0, "batch_reward": 0.03398797242436558, "critic_loss": 0.004954299500503112, "actor_loss": -13.54486285173893, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.050472259521484, "step": 12000}
{"episode_reward": 2.8546982756427, "episode": 13.0, "batch_reward": 0.0315567442085594, "critic_loss": 0.0047725175441300964, "actor_loss": -12.426841278195381, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.529158115386963, "step": 13000}
{"episode_reward": 2.756649101568224, "episode": 14.0, "batch_reward": 0.02976739971432835, "critic_loss": 0.004000392172805732, "actor_loss": -13.321494269132614, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.33547806739807, "step": 14000}
{"episode_reward": 2.8001853115375503, "episode": 15.0, "batch_reward": 0.02750995711190626, "critic_loss": 0.0041940669165633155, "actor_loss": -13.708304138541221, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.611221313476562, "step": 15000}
{"episode_reward": 2.0300074940186748, "episode": 16.0, "batch_reward": 0.026078206503298132, "critic_loss": 0.003044503704382805, "actor_loss": -12.429952570438385, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.384467363357544, "step": 16000}
{"episode_reward": 2.7427729756154635, "episode": 17.0, "batch_reward": 0.024561474540736525, "critic_loss": 0.003734446118323831, "actor_loss": -12.915773505926133, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.354288816452026, "step": 17000}
{"episode_reward": 1.7547701838288376, "episode": 18.0, "batch_reward": 0.02387404145579785, "critic_loss": 0.0038139288393140305, "actor_loss": -12.807601154088974, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.367045402526855, "step": 18000}
{"episode_reward": 3.488025193278519, "episode": 19.0, "batch_reward": 0.0225079797655344, "critic_loss": 0.003080848126352066, "actor_loss": -12.716968674302102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.98010778427124, "step": 19000}
{"episode_reward": 2.8591640164329393, "episode": 20.0, "batch_reward": 0.021350076451897623, "critic_loss": 0.003112795167595323, "actor_loss": -13.021990604758262, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.810176372528076, "step": 20000}
{"episode_reward": 2.198993391897607, "episode": 21.0, "batch_reward": 0.019960532577708363, "critic_loss": 0.003326766585189034, "actor_loss": -11.69961613869667, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.11648869514465, "step": 21000}
{"episode_reward": 2.4289430897595743, "episode": 22.0, "batch_reward": 0.01959804022940807, "critic_loss": 0.0034165046276466455, "actor_loss": -14.14090821635723, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.227111339569092, "step": 22000}
{"episode_reward": 3.597864463388656, "episode": 23.0, "batch_reward": 0.018895594185916707, "critic_loss": 0.002374879267212236, "actor_loss": -12.867494277775288, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.369653701782227, "step": 23000}
{"episode_reward": 2.716421994627218, "episode": 24.0, "batch_reward": 0.018033444793662055, "critic_loss": 0.00313830343693553, "actor_loss": -12.801914014041424, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.393340349197388, "step": 24000}
{"episode_reward": 2.6402001905008428, "episode": 25.0, "batch_reward": 0.017719438798027114, "critic_loss": 0.0023237640332517914, "actor_loss": -12.722551884055138, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.319581270217896, "step": 25000}
{"episode_reward": 2.792624879052881, "episode": 26.0, "batch_reward": 0.017002930111484602, "critic_loss": 0.002392901624683873, "actor_loss": -11.918412374675274, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.215559720993042, "step": 26000}
{"episode_reward": 2.571064661899908, "episode": 27.0, "batch_reward": 0.016772543349536137, "critic_loss": 0.0027497827881452393, "actor_loss": -11.249846975386143, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.344545364379883, "step": 27000}
{"episode_reward": 2.663092037515448, "episode": 28.0, "batch_reward": 0.016122662716777994, "critic_loss": 0.0018643480869650374, "actor_loss": -13.051915349185467, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.89153003692627, "step": 28000}
{"episode_reward": 2.6053872638562217, "episode": 29.0, "batch_reward": 0.015293758248444646, "critic_loss": 0.0024885210209613434, "actor_loss": -12.382022344589233, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.856112241744995, "step": 29000}
{"episode_reward": 2.2945832007377427, "episode": 30.0, "batch_reward": 0.015046447752974928, "critic_loss": 0.0027350010223963182, "actor_loss": -11.023850029706955, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.0196213722229, "step": 30000}
{"episode_reward": 2.4219213438219405, "episode": 31.0, "batch_reward": 0.014494262613356113, "critic_loss": 0.0013633180791075574, "actor_loss": -12.347108783364297, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.39333248138428, "step": 31000}
{"episode_reward": 2.143504978176949, "episode": 32.0, "batch_reward": 0.014252303598565049, "critic_loss": 0.0017434444747123053, "actor_loss": -12.636692612469197, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.816959381103516, "step": 32000}
{"episode_reward": 2.5680668124232184, "episode": 33.0, "batch_reward": 0.013777825796511024, "critic_loss": 0.0019394723974401132, "actor_loss": -13.025560438811778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.584110021591187, "step": 33000}
{"episode_reward": 2.290325845004257, "episode": 34.0, "batch_reward": 0.013714305780013092, "critic_loss": 0.0022461111813463506, "actor_loss": -11.360240884363652, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.12907385826111, "step": 34000}
{"episode_reward": 3.1085421242473568, "episode": 35.0, "batch_reward": 0.012689299969235434, "critic_loss": 0.0015501936158398167, "actor_loss": -12.812718317627906, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.931649446487427, "step": 35000}
{"episode_reward": 2.6485595747651036, "episode": 36.0, "batch_reward": 0.01255040294060018, "critic_loss": 0.0015796326184572536, "actor_loss": -13.030329833507539, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.205405473709106, "step": 36000}
{"episode_reward": 2.3665330478612256, "episode": 37.0, "batch_reward": 0.012635105330962688, "critic_loss": 0.0020965848770429147, "actor_loss": -12.10152132165432, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.69061779975891, "step": 37000}
{"episode_reward": 2.482254518810039, "episode": 38.0, "batch_reward": 0.012297299313242548, "critic_loss": 0.001584514830596163, "actor_loss": -12.611477669477463, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.726890802383423, "step": 38000}
{"episode_reward": 2.1988248567991473, "episode": 39.0, "batch_reward": 0.012131480897543952, "critic_loss": 0.0018177211775982868, "actor_loss": -12.437942984044552, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.40321397781372, "step": 39000}
{"episode_reward": 2.5284578938947844, "episode": 40.0, "batch_reward": 0.011834980599000119, "critic_loss": 0.0018288715315866284, "actor_loss": -11.888670775413512, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.767889499664307, "step": 40000}
{"episode_reward": 2.5441308010680364, "episode": 41.0, "batch_reward": 0.011506882245186716, "critic_loss": 0.00126051611857838, "actor_loss": -9.820093585133552, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 44.07347512245178, "step": 41000}
{"episode_reward": 2.605564311247226, "episode": 42.0, "batch_reward": 0.011337624116102233, "critic_loss": 0.0019911789370162295, "actor_loss": -11.925251935064793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.352919816970825, "step": 42000}
{"episode_reward": 2.787803393290467, "episode": 43.0, "batch_reward": 0.01110536487621721, "critic_loss": 0.001877545672497945, "actor_loss": -12.340475866734982, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.601616621017456, "step": 43000}
{"episode_reward": 2.2862976625570584, "episode": 44.0, "batch_reward": 0.011111621325369924, "critic_loss": 0.0014953549100828242, "actor_loss": -13.212853626698255, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.872393369674683, "step": 44000}
{"episode_reward": 2.9146047414823455, "episode": 45.0, "batch_reward": 0.010715632054721936, "critic_loss": 0.0016059948031252134, "actor_loss": -13.079287304878235, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.96806812286377, "step": 45000}
{"episode_reward": 1.9043464709289917, "episode": 46.0, "batch_reward": 0.010239003536757081, "critic_loss": 0.0010665135276467482, "actor_loss": -11.114989810973405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.367001056671143, "step": 46000}
{"episode_reward": 2.2414579110306088, "episode": 47.0, "batch_reward": 0.01023889334523119, "critic_loss": 0.0011547409345548658, "actor_loss": -11.644677217364311, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.186867237091064, "step": 47000}
{"episode_reward": 2.744121385790527, "episode": 48.0, "batch_reward": 0.01025371204491239, "critic_loss": 0.0014099149827488872, "actor_loss": -12.872700558930635, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.93116807937622, "step": 48000}
{"episode_reward": 3.051909288537618, "episode": 49.0, "batch_reward": 0.010242542920750566, "critic_loss": 0.0010262143452928285, "actor_loss": -13.16958754476905, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.660716772079468, "step": 49000}
{"episode_reward": 2.76889725640298, "episode": 50.0, "batch_reward": 0.00989577686204575, "critic_loss": 0.0013404362377914367, "actor_loss": -11.687047653198242, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.305110692977905, "step": 50000}
{"episode_reward": 2.423159461072144, "episode": 51.0, "batch_reward": 0.009758886358118616, "critic_loss": 0.0008886405102057324, "actor_loss": -10.959941287845373, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.19855737686157, "step": 51000}
{"episode_reward": 2.3685970821726907, "episode": 52.0, "batch_reward": 0.009732854994013905, "critic_loss": 0.0011130910157407924, "actor_loss": -10.843926432400941, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.75561761856079, "step": 52000}
{"episode_reward": 2.393209483758558, "episode": 53.0, "batch_reward": 0.009468022984918207, "critic_loss": 0.0016270562249010253, "actor_loss": -12.841786544412374, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.053656101226807, "step": 53000}
{"episode_reward": 2.1591171469557757, "episode": 54.0, "batch_reward": 0.009371202414506116, "critic_loss": 0.0011774324481812074, "actor_loss": -11.831994095951318, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.845348119735718, "step": 54000}
{"episode_reward": 2.331074352957927, "episode": 55.0, "batch_reward": 0.009367367192870006, "critic_loss": 0.0013789757349113644, "actor_loss": -12.071426473349332, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.323877811431885, "step": 55000}
{"episode_reward": 2.366084056250994, "episode": 56.0, "batch_reward": 0.009060735991573893, "critic_loss": 0.001637479099947086, "actor_loss": -11.466079278677702, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.59429931640625, "step": 56000}
{"episode_reward": 2.7770556297044253, "episode": 57.0, "batch_reward": 0.009003881997312418, "critic_loss": 0.0013607863374163571, "actor_loss": -12.050804986327886, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.126601457595825, "step": 57000}
{"episode_reward": 2.604315260047617, "episode": 58.0, "batch_reward": 0.008786252995021641, "critic_loss": 0.0010487161415585434, "actor_loss": -12.525137287050486, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.188782691955566, "step": 58000}
{"episode_reward": 3.4747142017956545, "episode": 59.0, "batch_reward": 0.00878182617505081, "critic_loss": 0.00140815630297584, "actor_loss": -12.113378489732742, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.885867595672607, "step": 59000}
{"episode_reward": 2.309201067688921, "episode": 60.0, "batch_reward": 0.008773108396562748, "critic_loss": 0.0009072041556246404, "actor_loss": -12.334426860839129, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.21596384048462, "step": 60000}
{"episode_reward": 1.851156979127773, "episode": 61.0, "batch_reward": 0.008713886063313112, "critic_loss": 0.0013695030947747, "actor_loss": -11.333183501511812, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.600000619888306, "step": 61000}
{"episode_reward": 1.7733222190229285, "episode": 62.0, "batch_reward": 0.008761499288957565, "critic_loss": 0.0008570187831719523, "actor_loss": -12.627453691542149, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.06818985939026, "step": 62000}
{"episode_reward": 3.366815278312659, "episode": 63.0, "batch_reward": 0.008334866017568856, "critic_loss": 0.0012383884842056432, "actor_loss": -10.79151243211329, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.471168518066406, "step": 63000}
{"episode_reward": 2.3067405640038268, "episode": 64.0, "batch_reward": 0.008215826395316981, "critic_loss": 0.0013147024766876712, "actor_loss": -12.144636750116945, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.527690887451172, "step": 64000}
{"episode_reward": 2.1776249571192565, "episode": 65.0, "batch_reward": 0.008191222746274434, "critic_loss": 0.0013101005849639477, "actor_loss": -11.377387727886438, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.572975635528564, "step": 65000}
{"episode_reward": 3.299774188700872, "episode": 66.0, "batch_reward": 0.007983862756984308, "critic_loss": 0.0010899219426355558, "actor_loss": -11.8361106903404, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.741335153579712, "step": 66000}
{"episode_reward": 2.353248089360434, "episode": 67.0, "batch_reward": 0.008061119844089262, "critic_loss": 0.0015755664427961164, "actor_loss": -13.55009943047166, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.353087663650513, "step": 67000}
{"episode_reward": 2.39027516695815, "episode": 68.0, "batch_reward": 0.007957200464559718, "critic_loss": 0.000828927448939794, "actor_loss": -12.954698992818594, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.86332392692566, "step": 68000}
{"episode_reward": 2.53089172317328, "episode": 69.0, "batch_reward": 0.007940599477849902, "critic_loss": 0.0014041114810606813, "actor_loss": -11.236400505959988, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.29511523246765, "step": 69000}
{"episode_reward": 1.819882155314453, "episode": 70.0, "batch_reward": 0.007772475180914625, "critic_loss": 0.0009294490835891338, "actor_loss": -12.418377456068992, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.518747806549072, "step": 70000}
{"episode_reward": 2.6379561995437504, "episode": 71.0, "batch_reward": 0.00781660726305563, "critic_loss": 0.0011775482894954622, "actor_loss": -11.816343792662025, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.207578897476196, "step": 71000}
{"episode_reward": 2.4731551501236093, "episode": 72.0, "batch_reward": 0.0077431063742842525, "critic_loss": 0.0010186899022555736, "actor_loss": -11.408802461564541, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.56048059463501, "step": 72000}
{"episode_reward": 2.4323857842440937, "episode": 73.0, "batch_reward": 0.00771558075863868, "critic_loss": 0.0009368135499062191, "actor_loss": -11.764604628741742, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.749452590942383, "step": 73000}
{"episode_reward": 2.792605779494502, "episode": 74.0, "batch_reward": 0.007405062805744819, "critic_loss": 0.0010091623989537766, "actor_loss": -12.054726241409778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.213010787963867, "step": 74000}
{"episode_reward": 2.034913942555338, "episode": 75.0, "batch_reward": 0.0072460021843435245, "critic_loss": 0.0009250329576971126, "actor_loss": -11.392159453853965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.57094955444336, "step": 75000}
{"episode_reward": 2.1317297152525487, "episode": 76.0, "batch_reward": 0.0074924342288868505, "critic_loss": 0.00085382441217007, "actor_loss": -11.155733218714595, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.677256107330322, "step": 76000}
{"episode_reward": 2.538719556057837, "episode": 77.0, "batch_reward": 0.007455858365865424, "critic_loss": 0.0009579298168209789, "actor_loss": -11.987840647518635, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.373539447784424, "step": 77000}
{"episode_reward": 2.499612539052745, "episode": 78.0, "batch_reward": 0.007538459252915345, "critic_loss": 0.0012357794687668502, "actor_loss": -11.154580519080161, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.923649072647095, "step": 78000}
{"episode_reward": 2.6104199564100767, "episode": 79.0, "batch_reward": 0.007140859742998146, "critic_loss": 0.001112972273291234, "actor_loss": -12.218885801389813, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.315988302230835, "step": 79000}
{"episode_reward": 2.4938961280054346, "episode": 80.0, "batch_reward": 0.007205094833159819, "critic_loss": 0.000947450611005479, "actor_loss": -12.629100341826677, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.45321226119995, "step": 80000}
{"episode_reward": 2.0659708924111806, "episode": 81.0, "batch_reward": 0.006949560208362527, "critic_loss": 0.0008001194248972752, "actor_loss": -12.85472124569118, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.70408535003662, "step": 81000}
{"episode_reward": 2.520863416827245, "episode": 82.0, "batch_reward": 0.0068626979745458816, "critic_loss": 0.0006960503359041468, "actor_loss": -11.502127668946981, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.537893772125244, "step": 82000}
{"episode_reward": 1.8953384666178574, "episode": 83.0, "batch_reward": 0.006901887781452388, "critic_loss": 0.0011469687895369134, "actor_loss": -12.680483440890908, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.25926685333252, "step": 83000}
{"episode_reward": 2.317841652127407, "episode": 84.0, "batch_reward": 0.006867897671298124, "critic_loss": 0.0010462296077021166, "actor_loss": -12.558348886460065, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.42290449142456, "step": 84000}
{"episode_reward": 3.0484192584010765, "episode": 85.0, "batch_reward": 0.006746381568838842, "critic_loss": 0.0009718287950636295, "actor_loss": -11.853391867622733, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.053568601608276, "step": 85000}
{"episode_reward": 3.3659194149604517, "episode": 86.0, "batch_reward": 0.006931677384418435, "critic_loss": 0.0008242796876220382, "actor_loss": -12.270826787874103, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.89138102531433, "step": 86000}
{"episode_reward": 2.3082955418295965, "episode": 87.0, "batch_reward": 0.006756767424056307, "critic_loss": 0.000872114333214995, "actor_loss": -12.675440052598715, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.146934270858765, "step": 87000}
{"episode_reward": 2.0803333061597393, "episode": 88.0, "batch_reward": 0.00642847534304019, "critic_loss": 0.0006258103294785542, "actor_loss": -12.482471912994981, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.140148162841797, "step": 88000}
{"episode_reward": 2.3673585930994605, "episode": 89.0, "batch_reward": 0.006659885627566837, "critic_loss": 0.0009470206934056478, "actor_loss": -11.072601523533464, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.959068298339844, "step": 89000}
{"episode_reward": 2.160132195666146, "episode": 90.0, "batch_reward": 0.006716054810793139, "critic_loss": 0.000981233530503232, "actor_loss": -11.139595963791013, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.161070585250854, "step": 90000}
{"episode_reward": 2.999300011013728, "episode": 91.0, "batch_reward": 0.006553564021131024, "critic_loss": 0.0008658801659985329, "actor_loss": -10.897666082069279, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.91254663467407, "step": 91000}
{"episode_reward": 2.335453569363094, "episode": 92.0, "batch_reward": 0.006487657645251602, "critic_loss": 0.0009110101579408365, "actor_loss": -12.085336134299636, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.773333311080933, "step": 92000}
{"episode_reward": 2.9880149667270923, "episode": 93.0, "batch_reward": 0.006419701396371238, "critic_loss": 0.0010188318967266242, "actor_loss": -10.897656851738692, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.433237552642822, "step": 93000}
{"episode_reward": 2.2061675461952834, "episode": 94.0, "batch_reward": 0.0063168514450080695, "critic_loss": 0.0010615754561913492, "actor_loss": -11.779016844958067, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.364509105682373, "step": 94000}
{"episode_reward": 2.6865549537685425, "episode": 95.0, "batch_reward": 0.006269907531095669, "critic_loss": 0.0009179193612289964, "actor_loss": -12.896535481899976, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.38636016845703, "step": 95000}
{"episode_reward": 2.5384488990566325, "episode": 96.0, "batch_reward": 0.006502484578406438, "critic_loss": 0.001548558096961642, "actor_loss": -11.313037465915084, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.471212148666382, "step": 96000}
{"episode_reward": 1.8539370717827577, "episode": 97.0, "batch_reward": 0.006402833280502818, "critic_loss": 0.0008392785659925721, "actor_loss": -11.180406825080514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.730265855789185, "step": 97000}
{"episode_reward": 2.3012633328096888, "episode": 98.0, "batch_reward": 0.006140122021664866, "critic_loss": 0.0005525657055095507, "actor_loss": -13.309125984176994, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.66862964630127, "step": 98000}
{"episode_reward": 2.326862020050739, "episode": 99.0, "batch_reward": 0.006187883062870242, "critic_loss": 0.0011646128019383469, "actor_loss": -12.003461674168706, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.42184853553772, "step": 99000}
{"episode_reward": 2.3907644855368626, "episode": 100.0, "batch_reward": 0.006172270051087253, "critic_loss": 0.0007690993824071483, "actor_loss": -12.653912888944149, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.230647325515747, "step": 100000}
{"episode_reward": 2.1990721475186543, "episode": 101.0, "batch_reward": 0.006146345772896893, "critic_loss": 0.0009789169592950202, "actor_loss": -11.12271862001717, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.076521158218384, "step": 101000}
{"episode_reward": 2.0231516297906675, "episode": 102.0, "batch_reward": 0.00626470781234093, "critic_loss": 0.0012577246441214812, "actor_loss": -12.375400972351432, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.680208921432495, "step": 102000}
{"episode_reward": 2.47413025333378, "episode": 103.0, "batch_reward": 0.006228092105826363, "critic_loss": 0.0009544171122543049, "actor_loss": -12.307145860806108, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.807292461395264, "step": 103000}
{"episode_reward": 2.5048610349358382, "episode": 104.0, "batch_reward": 0.006195573118515312, "critic_loss": 0.0014120360965825967, "actor_loss": -11.294278562895954, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.97823143005371, "step": 104000}
{"episode_reward": 2.194353896211335, "episode": 105.0, "batch_reward": 0.006077561819809489, "critic_loss": 0.0012111023433681112, "actor_loss": -11.772614706203342, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.499085426330566, "step": 105000}
{"episode_reward": 2.303616241178471, "episode": 106.0, "batch_reward": 0.005758314024657011, "critic_loss": 0.0007006371224451869, "actor_loss": -11.70567968571186, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.878045797348022, "step": 106000}
{"episode_reward": 2.020624704513712, "episode": 107.0, "batch_reward": 0.005913347034482285, "critic_loss": 0.001118147250595939, "actor_loss": -10.94202482676506, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.05182671546936, "step": 107000}
{"episode_reward": 2.410231618764313, "episode": 108.0, "batch_reward": 0.006054311725427397, "critic_loss": 0.0012093860767599836, "actor_loss": -12.836313252002, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.44450545310974, "step": 108000}
{"episode_reward": 1.9792008912612848, "episode": 109.0, "batch_reward": 0.005736482042004354, "critic_loss": 0.0009600735038166021, "actor_loss": -11.164753785833716, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.299500226974487, "step": 109000}
{"episode_reward": 2.7951599195890315, "episode": 110.0, "batch_reward": 0.006052489734254777, "critic_loss": 0.0013442144381551771, "actor_loss": -12.826980388022958, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.430809497833252, "step": 110000}
{"episode_reward": 2.2306684360413684, "episode": 111.0, "batch_reward": 0.005735712246387265, "critic_loss": 0.0008021741220472905, "actor_loss": -11.701214829973877, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.01567625999451, "step": 111000}
{"episode_reward": 2.7895228186398167, "episode": 112.0, "batch_reward": 0.00586546790599823, "critic_loss": 0.0007297672771419457, "actor_loss": -12.883452421493828, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.78115439414978, "step": 112000}
{"episode_reward": 2.3328055621746167, "episode": 113.0, "batch_reward": 0.005734059119597078, "critic_loss": 0.0006666712326004927, "actor_loss": -11.104814934588969, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.418965339660645, "step": 113000}
{"episode_reward": 3.1123931202883526, "episode": 114.0, "batch_reward": 0.005800012417836115, "critic_loss": 0.0012443566112233385, "actor_loss": -12.766182944953442, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.879629135131836, "step": 114000}
{"episode_reward": 2.6278342239344687, "episode": 115.0, "batch_reward": 0.005502333989366889, "critic_loss": 0.0006926711699852603, "actor_loss": -12.454516149558128, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.9875385761261, "step": 115000}
{"episode_reward": 2.5751473210717504, "episode": 116.0, "batch_reward": 0.005721831126487814, "critic_loss": 0.0008361587600011262, "actor_loss": -11.933906766928732, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.564139366149902, "step": 116000}
{"episode_reward": 1.7523629075542533, "episode": 117.0, "batch_reward": 0.005611799522070214, "critic_loss": 0.0006570735706791311, "actor_loss": -11.047291864447295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.017265796661377, "step": 117000}
{"episode_reward": 2.8077985015418907, "episode": 118.0, "batch_reward": 0.005573764002067037, "critic_loss": 0.0006474913557140098, "actor_loss": -11.279665301688015, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.48187804222107, "step": 118000}
{"episode_reward": 2.3062039292777303, "episode": 119.0, "batch_reward": 0.005690152357332409, "critic_loss": 0.0006256548432356794, "actor_loss": -11.215681863829493, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.318739414215088, "step": 119000}
{"episode_reward": 2.3345417775234987, "episode": 120.0, "batch_reward": 0.005652108927839436, "critic_loss": 0.0008456741814370616, "actor_loss": -11.043944541364908, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.15786385536194, "step": 120000}
{"episode_reward": 2.6662078850317084, "episode": 121.0, "batch_reward": 0.00545725179312285, "critic_loss": 0.0008432712598150829, "actor_loss": -10.447297359362244, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.68868350982666, "step": 121000}
{"episode_reward": 2.717593858068362, "episode": 122.0, "batch_reward": 0.00548870380083099, "critic_loss": 0.0009029890413494286, "actor_loss": -12.641824910610914, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.3779878616333, "step": 122000}
{"episode_reward": 2.041531710252014, "episode": 123.0, "batch_reward": 0.005505024216836319, "critic_loss": 0.0008247830088694173, "actor_loss": -11.870425139725208, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.006176710128784, "step": 123000}
{"episode_reward": 2.549511600167305, "episode": 124.0, "batch_reward": 0.00555466118385084, "critic_loss": 0.000853515054583113, "actor_loss": -12.226689372681081, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.87090516090393, "step": 124000}
{"episode_reward": 3.017694912496985, "episode": 125.0, "batch_reward": 0.005345583523157984, "critic_loss": 0.0006038384336297895, "actor_loss": -12.02906160493195, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.674015045166016, "step": 125000}
{"episode_reward": 2.072128367423832, "episode": 126.0, "batch_reward": 0.005526079323375598, "critic_loss": 0.0009221921784555889, "actor_loss": -12.22440539394319, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.320449590682983, "step": 126000}
{"episode_reward": 3.1173259296122695, "episode": 127.0, "batch_reward": 0.005405314069474116, "critic_loss": 0.0008806455295125488, "actor_loss": -13.283457602076233, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.021069765090942, "step": 127000}
{"episode_reward": 2.829876846234053, "episode": 128.0, "batch_reward": 0.005378361203474924, "critic_loss": 0.0006956512806791579, "actor_loss": -12.217438030548394, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.659348726272583, "step": 128000}
{"episode_reward": 2.669493713724499, "episode": 129.0, "batch_reward": 0.005416271408903412, "critic_loss": 0.0006913373780771507, "actor_loss": -12.279203292407095, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.149226188659668, "step": 129000}
{"episode_reward": 3.0096433270977476, "episode": 130.0, "batch_reward": 0.0052465316475136204, "critic_loss": 0.0006841930797345413, "actor_loss": -11.98080856229365, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.284874200820923, "step": 130000}
{"episode_reward": 1.8744724207460963, "episode": 131.0, "batch_reward": 0.005275441061588936, "critic_loss": 0.0005096570557798259, "actor_loss": -11.860327113099396, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.63544201850891, "step": 131000}
{"episode_reward": 2.284352303708065, "episode": 132.0, "batch_reward": 0.005239280622801743, "critic_loss": 0.0008136292757444608, "actor_loss": -11.976705658219755, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.07046389579773, "step": 132000}
{"episode_reward": 2.747433438465469, "episode": 133.0, "batch_reward": 0.0051960176919819785, "critic_loss": 0.0005524243558684247, "actor_loss": -10.20507395863533, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.128803730010986, "step": 133000}
{"episode_reward": 2.3254756636846245, "episode": 134.0, "batch_reward": 0.0053727436383487655, "critic_loss": 0.0006651287476415746, "actor_loss": -11.854006724454463, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.34595274925232, "step": 134000}
{"episode_reward": 2.8014890567369597, "episode": 135.0, "batch_reward": 0.005066038271063007, "critic_loss": 0.0005057444183294138, "actor_loss": -12.682631720639765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.60389494895935, "step": 135000}
{"episode_reward": 2.562258110365214, "episode": 136.0, "batch_reward": 0.005281017974950373, "critic_loss": 0.0009532823413537699, "actor_loss": -12.27492844285816, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.270253896713257, "step": 136000}
{"episode_reward": 2.3266866374895905, "episode": 137.0, "batch_reward": 0.005136389806866645, "critic_loss": 0.0006935516520170494, "actor_loss": -12.389066743195057, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.69138240814209, "step": 137000}
{"episode_reward": 2.4369959225003726, "episode": 138.0, "batch_reward": 0.0051977284764871, "critic_loss": 0.0008571712578923325, "actor_loss": -11.683072757259012, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.29323959350586, "step": 138000}
{"episode_reward": 2.3771710484565753, "episode": 139.0, "batch_reward": 0.005063640556298196, "critic_loss": 0.001020262811336579, "actor_loss": -11.739846636988222, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.518060445785522, "step": 139000}
{"episode_reward": 2.4910675779657225, "episode": 140.0, "batch_reward": 0.005297295594005846, "critic_loss": 0.0005755374507771194, "actor_loss": -11.567568432360888, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.185760498046875, "step": 140000}
{"episode_reward": 2.666822390862336, "episode": 141.0, "batch_reward": 0.005075978072709404, "critic_loss": 0.0007669561643615453, "actor_loss": -12.85765218450874, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.400814056396484, "step": 141000}
{"episode_reward": 2.545933336106898, "episode": 142.0, "batch_reward": 0.0050593162289587785, "critic_loss": 0.0006476774810453207, "actor_loss": -12.038365404196083, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.63536500930786, "step": 142000}
{"episode_reward": 2.307628659246869, "episode": 143.0, "batch_reward": 0.005169867503223941, "critic_loss": 0.0006789745086225594, "actor_loss": -11.801435008317233, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.074015855789185, "step": 143000}
{"episode_reward": 2.9734045701259157, "episode": 144.0, "batch_reward": 0.005017629598500207, "critic_loss": 0.0006735602359731275, "actor_loss": -12.207737482354045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.649035453796387, "step": 144000}
{"episode_reward": 2.3955625299904653, "episode": 145.0, "batch_reward": 0.004922499778796919, "critic_loss": 0.0005993041617639392, "actor_loss": -12.50385494479537, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.48349618911743, "step": 145000}
{"episode_reward": 3.6183711215200374, "episode": 146.0, "batch_reward": 0.004958803538582288, "critic_loss": 0.0007080561103466607, "actor_loss": -11.266060461364686, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.26881194114685, "step": 146000}
{"episode_reward": 2.461315473925545, "episode": 147.0, "batch_reward": 0.00502708812430501, "critic_loss": 0.0008546392889766139, "actor_loss": -11.67482668402791, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.67680859565735, "step": 147000}
{"episode_reward": 2.355287579526562, "episode": 148.0, "batch_reward": 0.004890241867280565, "critic_loss": 0.0006468503191154013, "actor_loss": -11.61147094359994, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.39437747001648, "step": 148000}
{"episode_reward": 2.844107688646543, "episode": 149.0, "batch_reward": 0.004978500925353728, "critic_loss": 0.000690870173746589, "actor_loss": -12.143631007768214, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.585089445114136, "step": 149000}
{"episode_reward": 2.770471075678706, "episode": 150.0, "batch_reward": 0.004778551549417898, "critic_loss": 0.00035593415263065255, "actor_loss": -13.324930173315108, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
