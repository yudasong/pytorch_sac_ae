{"episode_reward": 0.0, "episode": 1.0, "duration": 17.208330154418945, "step": 1000}
{"episode_reward": 4.20996189079657, "episode": 2.0, "duration": 1.4914515018463135, "step": 2000}
{"episode_reward": 252.2457487418515, "episode": 3.0, "batch_reward": 0.12123337573123082, "critic_loss": 0.010252190152021381, "actor_loss": -12.078685090407914, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 61.218026876449585, "step": 3000}
{"episode_reward": 8.305873940117362, "episode": 4.0, "batch_reward": 0.07708281091228128, "critic_loss": 0.006536263461457565, "actor_loss": -12.248672452926636, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.185168981552124, "step": 4000}
{"episode_reward": 4.252875047028531, "episode": 5.0, "batch_reward": 0.06012438231147826, "critic_loss": 0.004796801311196759, "actor_loss": -11.335714675426484, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.201399326324463, "step": 5000}
{"episode_reward": 2.786422878744098, "episode": 6.0, "batch_reward": 0.050363905584439636, "critic_loss": 0.005629049988812767, "actor_loss": -11.330342228412627, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.2171573638916, "step": 6000}
{"episode_reward": 4.831593287155327, "episode": 7.0, "batch_reward": 0.04341216413583607, "critic_loss": 0.005354129993589595, "actor_loss": -10.009024737596512, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20769691467285, "step": 7000}
{"episode_reward": 3.181644315420528, "episode": 8.0, "batch_reward": 0.03800361240655184, "critic_loss": 0.005988162744732108, "actor_loss": -11.948118779182433, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18338632583618, "step": 8000}
{"episode_reward": 4.08242192130335, "episode": 9.0, "batch_reward": 0.03366996771935374, "critic_loss": 0.004726531651598634, "actor_loss": -10.618131790041923, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.189876556396484, "step": 9000}
{"episode_reward": 3.3092948527869455, "episode": 10.0, "batch_reward": 0.03032637624628842, "critic_loss": 0.0038297328208864202, "actor_loss": -11.204764037251472, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18850541114807, "step": 10000}
{"episode_reward": 2.611694208217812, "episode": 11.0, "batch_reward": 0.027003637508954854, "critic_loss": 0.004368240014882758, "actor_loss": -9.43573713696003, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.483272552490234, "step": 11000}
{"episode_reward": 2.617753478146181, "episode": 12.0, "batch_reward": 0.025727860706858337, "critic_loss": 0.003087442143994849, "actor_loss": -10.865108120560645, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.199010848999023, "step": 12000}
{"episode_reward": 3.157990896340241, "episode": 13.0, "batch_reward": 0.023761170449899508, "critic_loss": 0.004208741163864034, "actor_loss": -10.43396244096756, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.221685647964478, "step": 13000}
{"episode_reward": 3.9219312933363075, "episode": 14.0, "batch_reward": 0.022212600542232393, "critic_loss": 0.00263970519558643, "actor_loss": -11.366465012907982, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.163437843322754, "step": 14000}
{"episode_reward": 3.564632735074452, "episode": 15.0, "batch_reward": 0.021280186479911208, "critic_loss": 0.0037627293164550794, "actor_loss": -11.091516268849373, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.17888879776001, "step": 15000}
{"episode_reward": 3.194176450611982, "episode": 16.0, "batch_reward": 0.020033870455110445, "critic_loss": 0.003597866198280826, "actor_loss": -11.202971964240074, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.213862895965576, "step": 16000}
{"episode_reward": 3.5932217763133876, "episode": 17.0, "batch_reward": 0.01911519120144658, "critic_loss": 0.003108396808442194, "actor_loss": -10.797322810173034, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16798496246338, "step": 17000}
{"episode_reward": 2.881365978237919, "episode": 18.0, "batch_reward": 0.018181027300888674, "critic_loss": 0.0026542932544689393, "actor_loss": -11.277162434458733, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16481900215149, "step": 18000}
{"episode_reward": 4.263538403856293, "episode": 19.0, "batch_reward": 0.017100399663904683, "critic_loss": 0.0036540474014764186, "actor_loss": -10.667887950778008, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.201192378997803, "step": 19000}
{"episode_reward": 4.295717411295562, "episode": 20.0, "batch_reward": 0.016602331107016654, "critic_loss": 0.0027738818795187397, "actor_loss": -11.096802489161492, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.172608375549316, "step": 20000}
{"episode_reward": 4.134389341314078, "episode": 21.0, "batch_reward": 0.015622417553793639, "critic_loss": 0.0018374722307635239, "actor_loss": -10.544933050096034, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.50297737121582, "step": 21000}
{"episode_reward": 3.156737453289471, "episode": 22.0, "batch_reward": 0.015004139875527471, "critic_loss": 0.0033925229816231875, "actor_loss": -10.705556239783764, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.193178176879883, "step": 22000}
{"episode_reward": 2.790886039318132, "episode": 23.0, "batch_reward": 0.014864534783409909, "critic_loss": 0.0019221187352522974, "actor_loss": -9.733384146153927, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19828224182129, "step": 23000}
{"episode_reward": 2.844977964693822, "episode": 24.0, "batch_reward": 0.014061236831126735, "critic_loss": 0.002744764406539616, "actor_loss": -9.96166943103075, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.195014238357544, "step": 24000}
{"episode_reward": 2.782034915143541, "episode": 25.0, "batch_reward": 0.013263409161707386, "critic_loss": 0.002111122775284457, "actor_loss": -10.586218770444393, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.147823572158813, "step": 25000}
{"episode_reward": 2.9936803908823366, "episode": 26.0, "batch_reward": 0.013359784208470955, "critic_loss": 0.0026086396669925305, "actor_loss": -9.738983264386654, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.57172441482544, "step": 26000}
{"episode_reward": 2.986279350761027, "episode": 27.0, "batch_reward": 0.012861486845184117, "critic_loss": 0.003415737165509199, "actor_loss": -9.984119854390622, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.178109884262085, "step": 27000}
{"episode_reward": 3.4266184303168945, "episode": 28.0, "batch_reward": 0.012815959292696789, "critic_loss": 0.0019294239558221306, "actor_loss": -10.692050185978413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16149878501892, "step": 28000}
{"episode_reward": 3.230353254524161, "episode": 29.0, "batch_reward": 0.012301530725322665, "critic_loss": 0.002538904332788661, "actor_loss": -9.743229871451854, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.165079593658447, "step": 29000}
{"episode_reward": 3.4610987083530964, "episode": 30.0, "batch_reward": 0.01202931095706299, "critic_loss": 0.001962566108079045, "actor_loss": -10.128472426116467, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.206740379333496, "step": 30000}
{"episode_reward": 2.407113658263071, "episode": 31.0, "batch_reward": 0.011894979474600404, "critic_loss": 0.0015698375039282838, "actor_loss": -10.892191904485225, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.48076772689819, "step": 31000}
{"episode_reward": 2.857689261672946, "episode": 32.0, "batch_reward": 0.011571196112781764, "critic_loss": 0.002115941035270225, "actor_loss": -9.772306944310666, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.206562995910645, "step": 32000}
{"episode_reward": 3.425073566409853, "episode": 33.0, "batch_reward": 0.010935292301699519, "critic_loss": 0.0016951919183738936, "actor_loss": -10.330088532596827, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.184924125671387, "step": 33000}
{"episode_reward": 3.342584105283695, "episode": 34.0, "batch_reward": 0.010804521410726011, "critic_loss": 0.0020319458705271246, "actor_loss": -10.96209686550498, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19136905670166, "step": 34000}
{"episode_reward": 3.7391800369214683, "episode": 35.0, "batch_reward": 0.0108148136890959, "critic_loss": 0.001780332535308844, "actor_loss": -9.921636883348226, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20082402229309, "step": 35000}
{"episode_reward": 3.447046196619952, "episode": 36.0, "batch_reward": 0.010436680069891737, "critic_loss": 0.0017315169971916476, "actor_loss": -10.77158753478527, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18795156478882, "step": 36000}
{"episode_reward": 2.939428476702493, "episode": 37.0, "batch_reward": 0.010329850661568343, "critic_loss": 0.0016049730328086297, "actor_loss": -9.977880485206843, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19893789291382, "step": 37000}
{"episode_reward": 2.5966489478549972, "episode": 38.0, "batch_reward": 0.010419648922979832, "critic_loss": 0.001293780761945527, "actor_loss": -9.195596095919608, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.191203117370605, "step": 38000}
{"episode_reward": 3.1738438520273844, "episode": 39.0, "batch_reward": 0.009913623015163466, "critic_loss": 0.0017445379596902058, "actor_loss": -10.21443554148078, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19109010696411, "step": 39000}
{"episode_reward": 2.9843852135883617, "episode": 40.0, "batch_reward": 0.009795839028665796, "critic_loss": 0.0014483677815878763, "actor_loss": -11.179614912241698, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.1819167137146, "step": 40000}
{"episode_reward": 3.899632468547643, "episode": 41.0, "batch_reward": 0.00973946647089906, "critic_loss": 0.0012067723953659878, "actor_loss": -10.795629612088204, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.49388599395752, "step": 41000}
{"episode_reward": 3.884491592420959, "episode": 42.0, "batch_reward": 0.009492310559144243, "critic_loss": 0.0012857547905732645, "actor_loss": -10.593663500398398, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.194498300552368, "step": 42000}
{"episode_reward": 3.5220446463147903, "episode": 43.0, "batch_reward": 0.009503711131168529, "critic_loss": 0.001557296327635413, "actor_loss": -10.456192000359296, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.169506549835205, "step": 43000}
{"episode_reward": 4.518035600149896, "episode": 44.0, "batch_reward": 0.00921469402150251, "critic_loss": 0.0015252973655224195, "actor_loss": -10.381222408533096, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18725895881653, "step": 44000}
{"episode_reward": 3.2995199543321587, "episode": 45.0, "batch_reward": 0.009165083807893097, "critic_loss": 0.001079401703471376, "actor_loss": -9.280289733350276, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.215460300445557, "step": 45000}
{"episode_reward": 4.364781831075713, "episode": 46.0, "batch_reward": 0.009085074291797356, "critic_loss": 0.0016703488652710804, "actor_loss": -8.760282327830792, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.187324285507202, "step": 46000}
{"episode_reward": 4.495168629479908, "episode": 47.0, "batch_reward": 0.008813281905371695, "critic_loss": 0.0015986068798520138, "actor_loss": -10.173442884534598, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19240164756775, "step": 47000}
{"episode_reward": 3.804238378852073, "episode": 48.0, "batch_reward": 0.00878170348657295, "critic_loss": 0.0016862217342932127, "actor_loss": -9.787654691696167, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20537519454956, "step": 48000}
{"episode_reward": 4.044074101401185, "episode": 49.0, "batch_reward": 0.008708123540738598, "critic_loss": 0.000935442115413025, "actor_loss": -9.97213890364766, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18147850036621, "step": 49000}
{"episode_reward": 3.9900921119440094, "episode": 50.0, "batch_reward": 0.00850536664086394, "critic_loss": 0.0013541065668614466, "actor_loss": -10.039585334479808, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.17966055870056, "step": 50000}
{"episode_reward": 2.939450305530753, "episode": 51.0, "batch_reward": 0.008389398858649656, "critic_loss": 0.0016573636333851026, "actor_loss": -9.252747673988342, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.487008571624756, "step": 51000}
{"episode_reward": 4.2456066029907396, "episode": 52.0, "batch_reward": 0.008257635395508259, "critic_loss": 0.0013190989472859655, "actor_loss": -9.239541269123555, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.178907871246338, "step": 52000}
{"episode_reward": 2.8881023965824086, "episode": 53.0, "batch_reward": 0.008457242521690205, "critic_loss": 0.0013777090181974927, "actor_loss": -10.217270235538482, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.183767795562744, "step": 53000}
{"episode_reward": 2.918044752826295, "episode": 54.0, "batch_reward": 0.00788518384261988, "critic_loss": 0.0015275858239256195, "actor_loss": -9.866778547614812, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.173139333724976, "step": 54000}
{"episode_reward": 3.073323249240194, "episode": 55.0, "batch_reward": 0.007963874884182588, "critic_loss": 0.0012686634676574613, "actor_loss": -10.18823213492334, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.171688318252563, "step": 55000}
{"episode_reward": 3.9331719637669074, "episode": 56.0, "batch_reward": 0.007843488816171884, "critic_loss": 0.0010021608536444546, "actor_loss": -10.51992261429131, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.195082426071167, "step": 56000}
{"episode_reward": 3.3640953343290545, "episode": 57.0, "batch_reward": 0.007722117046825587, "critic_loss": 0.001508513501692505, "actor_loss": -10.045771064952016, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.168710231781006, "step": 57000}
{"episode_reward": 3.0310015394780963, "episode": 58.0, "batch_reward": 0.0077059189593419435, "critic_loss": 0.0010711174042262429, "actor_loss": -10.832516084432601, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.174328804016113, "step": 58000}
{"episode_reward": 4.307197265094693, "episode": 59.0, "batch_reward": 0.0077337207342498, "critic_loss": 0.0012182708734144398, "actor_loss": -10.631952917411924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.185254335403442, "step": 59000}
{"episode_reward": 3.231488168345494, "episode": 60.0, "batch_reward": 0.00776379291783087, "critic_loss": 0.001559427455853438, "actor_loss": -9.207204665347934, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.179553747177124, "step": 60000}
{"episode_reward": 3.2130442708109515, "episode": 61.0, "batch_reward": 0.007419002823298797, "critic_loss": 0.001062169301250833, "actor_loss": -9.8380204551965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.47922372817993, "step": 61000}
{"episode_reward": 4.7605153766572075, "episode": 62.0, "batch_reward": 0.007554982071975246, "critic_loss": 0.0011920769253119944, "actor_loss": -8.79755836673081, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.206401109695435, "step": 62000}
{"episode_reward": 3.6825684785479442, "episode": 63.0, "batch_reward": 0.007407931081252173, "critic_loss": 0.0010008429233494098, "actor_loss": -10.023168569415807, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.192582607269287, "step": 63000}
{"episode_reward": 3.246779928560807, "episode": 64.0, "batch_reward": 0.007471832202747464, "critic_loss": 0.0009871276454578037, "actor_loss": -11.004220080852509, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.166677951812744, "step": 64000}
{"episode_reward": 3.632018530685139, "episode": 65.0, "batch_reward": 0.007410644843475893, "critic_loss": 0.0017553052223011037, "actor_loss": -10.284816300928593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.217324018478394, "step": 65000}
{"episode_reward": 2.908123553656707, "episode": 66.0, "batch_reward": 0.0072555208605481316, "critic_loss": 0.000990807103458792, "actor_loss": -9.559745369970798, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21065640449524, "step": 66000}
{"episode_reward": 3.0249215525741158, "episode": 67.0, "batch_reward": 0.007349242450902239, "critic_loss": 0.0011123141632997432, "actor_loss": -9.820201616764068, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.185892343521118, "step": 67000}
{"episode_reward": 3.8077886643343892, "episode": 68.0, "batch_reward": 0.007215501518687233, "critic_loss": 0.0010419952641350391, "actor_loss": -10.087864949524402, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.234036684036255, "step": 68000}
{"episode_reward": 3.9382152049063572, "episode": 69.0, "batch_reward": 0.007121690643951297, "critic_loss": 0.0012058055644811247, "actor_loss": -9.746400847434998, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.189465045928955, "step": 69000}
{"episode_reward": 4.268618803850209, "episode": 70.0, "batch_reward": 0.007046337188454345, "critic_loss": 0.0011362088985042646, "actor_loss": -8.906827552318573, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.193392038345337, "step": 70000}
{"episode_reward": 3.4454878199393835, "episode": 71.0, "batch_reward": 0.007072029015049338, "critic_loss": 0.001058258270175429, "actor_loss": -9.427394319534303, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.4847514629364, "step": 71000}
{"episode_reward": 3.5452931972693102, "episode": 72.0, "batch_reward": 0.006986458027968183, "critic_loss": 0.0012493542734155199, "actor_loss": -8.638669058337808, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18761706352234, "step": 72000}
{"episode_reward": 4.512015966783293, "episode": 73.0, "batch_reward": 0.0068248727729078385, "critic_loss": 0.0010633316224666488, "actor_loss": -10.071129707306623, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.193159103393555, "step": 73000}
{"episode_reward": 3.7911675729289063, "episode": 74.0, "batch_reward": 0.00693131829239428, "critic_loss": 0.0011782146377445316, "actor_loss": -9.678992448970675, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.205665826797485, "step": 74000}
{"episode_reward": 3.442316335775349, "episode": 75.0, "batch_reward": 0.006838295216672122, "critic_loss": 0.0009241964307730086, "actor_loss": -10.261656701035797, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16885256767273, "step": 75000}
{"episode_reward": 3.2424489427894296, "episode": 76.0, "batch_reward": 0.006799004671513103, "critic_loss": 0.0010497115980251691, "actor_loss": -10.71979813542962, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.202248573303223, "step": 76000}
{"episode_reward": 3.9410474873030084, "episode": 77.0, "batch_reward": 0.006823744204710238, "critic_loss": 0.0010534910418427898, "actor_loss": -10.457043569222092, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.216716527938843, "step": 77000}
{"episode_reward": 4.005693525051707, "episode": 78.0, "batch_reward": 0.006626620538998395, "critic_loss": 0.0011086247818820993, "actor_loss": -9.469860039182008, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.197915077209473, "step": 78000}
{"episode_reward": 2.854436862018444, "episode": 79.0, "batch_reward": 0.00642970483796671, "critic_loss": 0.0012410524396218533, "actor_loss": -10.372772723697127, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.205729722976685, "step": 79000}
{"episode_reward": 4.008461272895712, "episode": 80.0, "batch_reward": 0.006719052559696138, "critic_loss": 0.0008769944813393522, "actor_loss": -10.191868322707712, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21950912475586, "step": 80000}
{"episode_reward": 3.516069848894724, "episode": 81.0, "batch_reward": 0.006632824715226889, "critic_loss": 0.0012924856338759128, "actor_loss": -9.92035006082803, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.4943265914917, "step": 81000}
{"episode_reward": 4.3154652837490115, "episode": 82.0, "batch_reward": 0.0066848917794413865, "critic_loss": 0.0010447477869856812, "actor_loss": -10.908454947695136, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21659803390503, "step": 82000}
{"episode_reward": 2.9506867267832186, "episode": 83.0, "batch_reward": 0.006633859039982781, "critic_loss": 0.0013037352947685577, "actor_loss": -8.75012337961793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.180082082748413, "step": 83000}
{"episode_reward": 4.226571722041664, "episode": 84.0, "batch_reward": 0.006532811289886012, "critic_loss": 0.001056075766988215, "actor_loss": -9.86130403933674, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.206141710281372, "step": 84000}
{"episode_reward": 4.21608964321351, "episode": 85.0, "batch_reward": 0.006519913157215342, "critic_loss": 0.001305207929082826, "actor_loss": -9.782286778822542, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.23386836051941, "step": 85000}
{"episode_reward": 4.3248495432041745, "episode": 86.0, "batch_reward": 0.006496232070727274, "critic_loss": 0.0012407906248809012, "actor_loss": -10.37871816458553, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19104552268982, "step": 86000}
{"episode_reward": 3.3162417009601173, "episode": 87.0, "batch_reward": 0.00661324232048355, "critic_loss": 0.001173034534429462, "actor_loss": -9.464502748839557, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.200074195861816, "step": 87000}
{"episode_reward": 3.276028807627063, "episode": 88.0, "batch_reward": 0.006604937305441126, "critic_loss": 0.0009293715165877074, "actor_loss": -9.403556949168443, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.23266053199768, "step": 88000}
{"episode_reward": 3.3005862274692057, "episode": 89.0, "batch_reward": 0.006394933567382396, "critic_loss": 0.0010188140398458927, "actor_loss": -9.529196342743933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.193068981170654, "step": 89000}
{"episode_reward": 3.119853167354711, "episode": 90.0, "batch_reward": 0.006377431203611195, "critic_loss": 0.000850284555926919, "actor_loss": -10.307383246988058, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19827699661255, "step": 90000}
{"episode_reward": 3.545836894955419, "episode": 91.0, "batch_reward": 0.006430395556846633, "critic_loss": 0.0008909979246091098, "actor_loss": -10.105374045006931, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.484559774398804, "step": 91000}
{"episode_reward": 3.0154347223804097, "episode": 92.0, "batch_reward": 0.0063678693850524725, "critic_loss": 0.001382577321081044, "actor_loss": -8.675684680223466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.219506978988647, "step": 92000}
{"episode_reward": 3.528248557325345, "episode": 93.0, "batch_reward": 0.006341966735897586, "critic_loss": 0.0009448142968140019, "actor_loss": -9.393247860096395, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.160912036895752, "step": 93000}
{"episode_reward": 3.5750908011266382, "episode": 94.0, "batch_reward": 0.006310299885459244, "critic_loss": 0.0010047907319676596, "actor_loss": -8.614523858174682, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20758056640625, "step": 94000}
{"episode_reward": 3.9431198789739685, "episode": 95.0, "batch_reward": 0.00629633972630836, "critic_loss": 0.0007593256679247133, "actor_loss": -9.907757217369973, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.210504055023193, "step": 95000}
{"episode_reward": 3.2788003465678, "episode": 96.0, "batch_reward": 0.006279495582450181, "critic_loss": 0.0011187083867152977, "actor_loss": -9.327635555543006, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.181408405303955, "step": 96000}
{"episode_reward": 3.812138281285505, "episode": 97.0, "batch_reward": 0.006179692136123777, "critic_loss": 0.000865734959996189, "actor_loss": -10.68964110504836, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.208969831466675, "step": 97000}
{"episode_reward": 2.7743603026097885, "episode": 98.0, "batch_reward": 0.006133145407424308, "critic_loss": 0.0009512222482626385, "actor_loss": -11.278273887448012, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18851637840271, "step": 98000}
{"episode_reward": 4.122393858108579, "episode": 99.0, "batch_reward": 0.005964980936143547, "critic_loss": 0.000973879212349857, "actor_loss": -9.830867162354291, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.192091941833496, "step": 99000}
{"episode_reward": 2.8434213694290236, "episode": 100.0, "batch_reward": 0.006164348961086944, "critic_loss": 0.0007771087205292133, "actor_loss": -9.256909913748503, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.22067165374756, "step": 100000}
{"episode_reward": 3.575475200750031, "episode": 101.0, "batch_reward": 0.0060466521154157815, "critic_loss": 0.0007720360228413483, "actor_loss": -9.50495543140173, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.41505575180054, "step": 101000}
{"episode_reward": 4.153208278459157, "episode": 102.0, "batch_reward": 0.005968604546971619, "critic_loss": 0.0009024798813079542, "actor_loss": -9.84073892813176, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.613219499588013, "step": 102000}
{"episode_reward": 2.856779076619111, "episode": 103.0, "batch_reward": 0.0060098702092655, "critic_loss": 0.0008084012642175367, "actor_loss": -8.98707354838401, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19483780860901, "step": 103000}
{"episode_reward": 3.3549337769391703, "episode": 104.0, "batch_reward": 0.0059576022073160855, "critic_loss": 0.0008717958758643362, "actor_loss": -10.362260340116919, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.12325930595398, "step": 104000}
{"episode_reward": 3.055422226200176, "episode": 105.0, "batch_reward": 0.00580185438785702, "critic_loss": 0.0007327507063491793, "actor_loss": -8.520384480684996, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20629119873047, "step": 105000}
{"episode_reward": 2.8192632238600392, "episode": 106.0, "batch_reward": 0.0058675877552013846, "critic_loss": 0.001175220753888425, "actor_loss": -9.374782601758838, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.223323106765747, "step": 106000}
{"episode_reward": 4.1394517180019, "episode": 107.0, "batch_reward": 0.005762526626000181, "critic_loss": 0.0008806157344843086, "actor_loss": -9.505153771273791, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.162124633789062, "step": 107000}
{"episode_reward": 2.3190408027447615, "episode": 108.0, "batch_reward": 0.005915525701828301, "critic_loss": 0.0013228418571234214, "actor_loss": -9.57269494605437, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19900393486023, "step": 108000}
{"episode_reward": 3.209021486353588, "episode": 109.0, "batch_reward": 0.005893396916799247, "critic_loss": 0.0008660912046689191, "actor_loss": -10.882356953270733, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.186974048614502, "step": 109000}
{"episode_reward": 3.865250794668465, "episode": 110.0, "batch_reward": 0.005810262502171099, "critic_loss": 0.0008065471657610032, "actor_loss": -10.494075435906648, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16840171813965, "step": 110000}
{"episode_reward": 2.999217477278801, "episode": 111.0, "batch_reward": 0.005744489792734384, "critic_loss": 0.0009632112379767932, "actor_loss": -9.309930044103414, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.52171564102173, "step": 111000}
{"episode_reward": 3.2381626481721844, "episode": 112.0, "batch_reward": 0.005832472981885076, "critic_loss": 0.0008633314615726703, "actor_loss": -8.980145913526416, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.199769496917725, "step": 112000}
{"episode_reward": 2.8996932310378627, "episode": 113.0, "batch_reward": 0.005651447796844877, "critic_loss": 0.0009500468932455988, "actor_loss": -8.20410561530292, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.216525077819824, "step": 113000}
{"episode_reward": 3.5117657974179526, "episode": 114.0, "batch_reward": 0.00567246747459285, "critic_loss": 0.0011182144305930705, "actor_loss": -10.164649551991372, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.217922687530518, "step": 114000}
{"episode_reward": 2.6561334188102457, "episode": 115.0, "batch_reward": 0.005644911523442715, "critic_loss": 0.0011977781804707775, "actor_loss": -9.893190171197057, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.15516757965088, "step": 115000}
{"episode_reward": 3.770298715091493, "episode": 116.0, "batch_reward": 0.00558878524764441, "critic_loss": 0.0007812489062416717, "actor_loss": -10.082184981159866, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20223379135132, "step": 116000}
{"episode_reward": 3.617635738509497, "episode": 117.0, "batch_reward": 0.0056234411050099875, "critic_loss": 0.0007037985094320902, "actor_loss": -10.084772657424212, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.225892305374146, "step": 117000}
{"episode_reward": 3.3964485556351156, "episode": 118.0, "batch_reward": 0.005661072254646569, "critic_loss": 0.0008455138119425101, "actor_loss": -8.5833273928985, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.17803144454956, "step": 118000}
{"episode_reward": 4.072254344792106, "episode": 119.0, "batch_reward": 0.005597170916618779, "critic_loss": 0.0009389793812188145, "actor_loss": -9.43247326106578, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19943618774414, "step": 119000}
{"episode_reward": 4.321877936085863, "episode": 120.0, "batch_reward": 0.005516948514152319, "critic_loss": 0.0007890450610830158, "actor_loss": -9.949194732125848, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.185714960098267, "step": 120000}
{"episode_reward": 3.9504231395722322, "episode": 121.0, "batch_reward": 0.005650627415860072, "critic_loss": 0.0009014838453877019, "actor_loss": -8.908419471554458, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.441933155059814, "step": 121000}
{"episode_reward": 4.047990938733387, "episode": 122.0, "batch_reward": 0.005606417932547629, "critic_loss": 0.0008097865853669646, "actor_loss": -10.931124563731252, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.180518865585327, "step": 122000}
{"episode_reward": 2.6066041040026446, "episode": 123.0, "batch_reward": 0.0055593439843505624, "critic_loss": 0.0009862002432200825, "actor_loss": -12.235762748643756, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19603157043457, "step": 123000}
{"episode_reward": 3.0964403135417182, "episode": 124.0, "batch_reward": 0.005602327668340876, "critic_loss": 0.0007545194999947853, "actor_loss": -11.444384038090705, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.193883180618286, "step": 124000}
{"episode_reward": 3.2750628263360673, "episode": 125.0, "batch_reward": 0.005418658918468281, "critic_loss": 0.0007659696108748903, "actor_loss": -9.099068020168692, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.167300939559937, "step": 125000}
{"episode_reward": 3.972178241011675, "episode": 126.0, "batch_reward": 0.005376359918853268, "critic_loss": 0.0005851765092593268, "actor_loss": -9.69287232416123, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.308402061462402, "step": 126000}
{"episode_reward": 3.3375232729376414, "episode": 127.0, "batch_reward": 0.005503435712307691, "critic_loss": 0.0006620988293434493, "actor_loss": -10.993770386561751, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.443775177001953, "step": 127000}
{"episode_reward": 4.010665202745311, "episode": 128.0, "batch_reward": 0.005463175018085167, "critic_loss": 0.0008585135611465375, "actor_loss": -9.227354213535786, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.149467945098877, "step": 128000}
{"episode_reward": 3.0766051144456315, "episode": 129.0, "batch_reward": 0.005248235921375454, "critic_loss": 0.0006376040073737386, "actor_loss": -10.326739507064223, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.1775324344635, "step": 129000}
{"episode_reward": 3.99701142603443, "episode": 130.0, "batch_reward": 0.005578737320378423, "critic_loss": 0.001094893785957538, "actor_loss": -10.923293850515037, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.157994031906128, "step": 130000}
{"episode_reward": 3.3788128370029127, "episode": 131.0, "batch_reward": 0.005354164487682283, "critic_loss": 0.0008062503124674549, "actor_loss": -8.767458552382886, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.44827914237976, "step": 131000}
{"episode_reward": 2.747037965641934, "episode": 132.0, "batch_reward": 0.005493981081061065, "critic_loss": 0.0006327601140292245, "actor_loss": -10.14811406315118, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.170098781585693, "step": 132000}
{"episode_reward": 3.803651176511157, "episode": 133.0, "batch_reward": 0.005445096555398777, "critic_loss": 0.000998143512482784, "actor_loss": -9.69219869609177, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.14640998840332, "step": 133000}
{"episode_reward": 3.3190664134415195, "episode": 134.0, "batch_reward": 0.005488320025033317, "critic_loss": 0.000752351517916395, "actor_loss": -10.21179909292236, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.180392742156982, "step": 134000}
{"episode_reward": 3.95735950450248, "episode": 135.0, "batch_reward": 0.005487993941176683, "critic_loss": 0.0007164781059036614, "actor_loss": -10.606485220920295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.197913885116577, "step": 135000}
{"episode_reward": 2.590145686987532, "episode": 136.0, "batch_reward": 0.005369206346571446, "critic_loss": 0.0006453435889488901, "actor_loss": -9.949947518073023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.182864904403687, "step": 136000}
{"episode_reward": 4.3043697443255855, "episode": 137.0, "batch_reward": 0.005399573172442615, "critic_loss": 0.0009518663054732315, "actor_loss": -10.665834488000721, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19468879699707, "step": 137000}
{"episode_reward": 3.1498487763068788, "episode": 138.0, "batch_reward": 0.005460226748138666, "critic_loss": 0.0006876211882117786, "actor_loss": -9.426630837548524, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.1654269695282, "step": 138000}
{"episode_reward": 2.83568416423411, "episode": 139.0, "batch_reward": 0.005270596196874976, "critic_loss": 0.000861956961962278, "actor_loss": -9.097318584635854, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.146093606948853, "step": 139000}
{"episode_reward": 4.29686562810538, "episode": 140.0, "batch_reward": 0.005482712404220365, "critic_loss": 0.0006490126924472861, "actor_loss": -9.688898555338383, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.190308094024658, "step": 140000}
{"episode_reward": 3.904526770069566, "episode": 141.0, "batch_reward": 0.005287524483865127, "critic_loss": 0.000717441407887236, "actor_loss": -10.331346598304808, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.46297335624695, "step": 141000}
{"episode_reward": 3.535925618508551, "episode": 142.0, "batch_reward": 0.005318540752865374, "critic_loss": 0.0006597819711569173, "actor_loss": -9.877318293448537, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.13695740699768, "step": 142000}
{"episode_reward": 3.2581960844988265, "episode": 143.0, "batch_reward": 0.005353237974457443, "critic_loss": 0.0005642158676746476, "actor_loss": -10.008226465608924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18805456161499, "step": 143000}
{"episode_reward": 2.988053581777458, "episode": 144.0, "batch_reward": 0.0052846313288901, "critic_loss": 0.0006436761060977005, "actor_loss": -9.676159848175942, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.154677391052246, "step": 144000}
{"episode_reward": 3.7705036809221326, "episode": 145.0, "batch_reward": 0.005278780440567061, "critic_loss": 0.0006846309355751145, "actor_loss": -10.39973162458837, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.188788890838623, "step": 145000}
{"episode_reward": 4.166132262557763, "episode": 146.0, "batch_reward": 0.005079624152043835, "critic_loss": 0.0006326884939626325, "actor_loss": -9.940190861344337, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.199244260787964, "step": 146000}
{"episode_reward": 3.2780706114055116, "episode": 147.0, "batch_reward": 0.0052176915130112324, "critic_loss": 0.0005688243475924537, "actor_loss": -9.878587587617337, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.156700134277344, "step": 147000}
{"episode_reward": 3.9867347500343966, "episode": 148.0, "batch_reward": 0.005191255499608815, "critic_loss": 0.0005341353705589426, "actor_loss": -9.616460613261909, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.194173336029053, "step": 148000}
{"episode_reward": 3.147298817728628, "episode": 149.0, "batch_reward": 0.005249968724325299, "critic_loss": 0.0007470182578435924, "actor_loss": -9.224454077377915, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.198559045791626, "step": 149000}
{"episode_reward": 3.788235558404496, "episode": 150.0, "batch_reward": 0.005193786385585554, "critic_loss": 0.0009034924176085042, "actor_loss": -8.634732484191655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
