{"episode_reward": 0.0, "episode": 1.0, "duration": 17.296014070510864, "step": 1000}
{"episode_reward": 4.20996189079657, "episode": 2.0, "duration": 1.4906260967254639, "step": 2000}
{"episode_reward": 252.2457487418515, "episode": 3.0, "batch_reward": 0.12208943703485434, "critic_loss": 0.012270607152870207, "actor_loss": -29.24891706617499, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 60.81751728057861, "step": 3000}
{"episode_reward": 16.800927515420124, "episode": 4.0, "batch_reward": 0.08025660714507103, "critic_loss": 0.010190418434794991, "actor_loss": -22.119934141159057, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.125303983688354, "step": 4000}
{"episode_reward": 7.10032439688334, "episode": 5.0, "batch_reward": 0.06351197591423988, "critic_loss": 0.011623516228515655, "actor_loss": -21.740671610832216, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.115009546279907, "step": 5000}
{"episode_reward": 9.087878211160449, "episode": 6.0, "batch_reward": 0.05427759642526507, "critic_loss": 0.014090704002417624, "actor_loss": -21.539095229148863, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.13033175468445, "step": 6000}
{"episode_reward": 12.406388207497365, "episode": 7.0, "batch_reward": 0.04980831358209253, "critic_loss": 0.015584086963441223, "actor_loss": -21.997003026008606, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.13219666481018, "step": 7000}
{"episode_reward": 34.283324770539096, "episode": 8.0, "batch_reward": 0.04644730274751782, "critic_loss": 0.014467214670963586, "actor_loss": -22.422324312210083, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.088764190673828, "step": 8000}
{"episode_reward": 16.096225271027688, "episode": 9.0, "batch_reward": 0.04239520164579153, "critic_loss": 0.012344387143384665, "actor_loss": -20.697498129844664, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.12742853164673, "step": 9000}
{"episode_reward": 9.886875571841964, "episode": 10.0, "batch_reward": 0.038315796543844045, "critic_loss": 0.011165681911166757, "actor_loss": -21.636189173460007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.135778665542603, "step": 10000}
{"episode_reward": 5.692776402789074, "episode": 11.0, "batch_reward": 0.03472205090336502, "critic_loss": 0.008842908771475777, "actor_loss": -20.465290938615798, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.53776717185974, "step": 11000}
{"episode_reward": 7.375100021295977, "episode": 12.0, "batch_reward": 0.03320276363287121, "critic_loss": 0.007279271380743012, "actor_loss": -20.783294945716857, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.110581159591675, "step": 12000}
{"episode_reward": 8.200814330322554, "episode": 13.0, "batch_reward": 0.031031522277742623, "critic_loss": 0.007236126254079864, "actor_loss": -20.90398606109619, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.142902135849, "step": 13000}
{"episode_reward": 7.492939596205892, "episode": 14.0, "batch_reward": 0.029323885194957256, "critic_loss": 0.006226224549696781, "actor_loss": -20.79673586368561, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.105360746383667, "step": 14000}
{"episode_reward": 8.126853098369045, "episode": 15.0, "batch_reward": 0.02819260006584227, "critic_loss": 0.007809561559581198, "actor_loss": -19.850986491441727, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.13857889175415, "step": 15000}
{"episode_reward": 7.805576852905221, "episode": 16.0, "batch_reward": 0.02675519796740264, "critic_loss": 0.007637000158545561, "actor_loss": -20.160256011009217, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.159486055374146, "step": 16000}
{"episode_reward": 7.351706864293033, "episode": 17.0, "batch_reward": 0.025628517907112836, "critic_loss": 0.005406631563906558, "actor_loss": -20.315363733530045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.106003761291504, "step": 17000}
{"episode_reward": 8.804492449990315, "episode": 18.0, "batch_reward": 0.024570300401188433, "critic_loss": 0.006554035178734921, "actor_loss": -20.235938653707503, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.118820190429688, "step": 18000}
{"episode_reward": 5.8981477294212565, "episode": 19.0, "batch_reward": 0.023272496468387544, "critic_loss": 0.005792914380086586, "actor_loss": -19.885631868481635, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.13477659225464, "step": 19000}
{"episode_reward": 6.795584324687634, "episode": 20.0, "batch_reward": 0.022666217361576854, "critic_loss": 0.004926480299327522, "actor_loss": -20.243079511642456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.115217685699463, "step": 20000}
{"episode_reward": 8.176667435708252, "episode": 21.0, "batch_reward": 0.0214668439514935, "critic_loss": 0.004997128342627548, "actor_loss": -18.691555602908135, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.50451350212097, "step": 21000}
{"episode_reward": 6.728459531942717, "episode": 22.0, "batch_reward": 0.020783272329252214, "critic_loss": 0.004560564802086446, "actor_loss": -20.019584164381026, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.123212099075317, "step": 22000}
{"episode_reward": 7.983727999567889, "episode": 23.0, "batch_reward": 0.020720566449221222, "critic_loss": 0.005356988537590951, "actor_loss": -19.846537313103674, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.109686851501465, "step": 23000}
{"episode_reward": 8.254735876651282, "episode": 24.0, "batch_reward": 0.019765212613157928, "critic_loss": 0.0049899227060377594, "actor_loss": -19.01288932263851, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.146425008773804, "step": 24000}
{"episode_reward": 6.453340074239857, "episode": 25.0, "batch_reward": 0.0190057175620459, "critic_loss": 0.003892140897922218, "actor_loss": -20.061480954527855, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.106221675872803, "step": 25000}
{"episode_reward": 6.996650936364665, "episode": 26.0, "batch_reward": 0.019058898143004627, "critic_loss": 0.0037070924050640313, "actor_loss": -19.354604709148408, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.127984285354614, "step": 26000}
{"episode_reward": 8.901526883001537, "episode": 27.0, "batch_reward": 0.018458322989288716, "critic_loss": 0.004655205354851205, "actor_loss": -19.20819792628288, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.34847593307495, "step": 27000}
{"episode_reward": 6.408418747978232, "episode": 28.0, "batch_reward": 0.018436648866161705, "critic_loss": 0.004651255977049004, "actor_loss": -18.995450563192367, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.299264430999756, "step": 28000}
{"episode_reward": 8.365250398946273, "episode": 29.0, "batch_reward": 0.017889012383297084, "critic_loss": 0.0034035494052805006, "actor_loss": -19.48983742785454, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.097421884536743, "step": 29000}
{"episode_reward": 8.650430026193368, "episode": 30.0, "batch_reward": 0.017699910740368068, "critic_loss": 0.003420325055893045, "actor_loss": -19.300549931168558, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.109253883361816, "step": 30000}
{"episode_reward": 8.982291651974169, "episode": 31.0, "batch_reward": 0.017346343842800708, "critic_loss": 0.003338548249885207, "actor_loss": -19.117844848632814, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.50587582588196, "step": 31000}
{"episode_reward": 5.578667775928442, "episode": 32.0, "batch_reward": 0.01713238911051303, "critic_loss": 0.003849590586440172, "actor_loss": -19.044250246167184, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.08080554008484, "step": 32000}
{"episode_reward": 6.754245639629441, "episode": 33.0, "batch_reward": 0.016506541674491018, "critic_loss": 0.0033846475543105044, "actor_loss": -19.201120142221452, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.12678074836731, "step": 33000}
{"episode_reward": 9.14770462585286, "episode": 34.0, "batch_reward": 0.016253429491538553, "critic_loss": 0.0037321795916650446, "actor_loss": -18.95811163663864, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.130084991455078, "step": 34000}
{"episode_reward": 9.900624972407648, "episode": 35.0, "batch_reward": 0.016107039398979395, "critic_loss": 0.003888353011163417, "actor_loss": -18.760295967400076, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.11449885368347, "step": 35000}
{"episode_reward": 6.197045427346418, "episode": 36.0, "batch_reward": 0.015900225976482035, "critic_loss": 0.0036959152439958415, "actor_loss": -19.179280680418014, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.114964485168457, "step": 36000}
{"episode_reward": 8.738612479859349, "episode": 37.0, "batch_reward": 0.015669168211985378, "critic_loss": 0.0036870853287982756, "actor_loss": -18.968063614070417, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.126670598983765, "step": 37000}
{"episode_reward": 7.9246315049964915, "episode": 38.0, "batch_reward": 0.0158868249040097, "critic_loss": 0.003022494801261928, "actor_loss": -19.658361276745797, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.10638999938965, "step": 38000}
{"episode_reward": 6.635264493932806, "episode": 39.0, "batch_reward": 0.015261600639205427, "critic_loss": 0.003270539802266285, "actor_loss": -19.814576751589776, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.11101484298706, "step": 39000}
{"episode_reward": 7.216974398310959, "episode": 40.0, "batch_reward": 0.015097567866556347, "critic_loss": 0.0030240489808493294, "actor_loss": -20.15829605668783, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.114427089691162, "step": 40000}
{"episode_reward": 7.222334269807413, "episode": 41.0, "batch_reward": 0.014914153722580523, "critic_loss": 0.002785343434661627, "actor_loss": -19.091030873954296, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.51988697052002, "step": 41000}
{"episode_reward": 7.357604671735941, "episode": 42.0, "batch_reward": 0.014711960113607347, "critic_loss": 0.0030652557094581426, "actor_loss": -19.237915935993193, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.164886474609375, "step": 42000}
{"episode_reward": 8.107835129266201, "episode": 43.0, "batch_reward": 0.01460013751219958, "critic_loss": 0.002518963794631418, "actor_loss": -19.576282801747322, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.11253786087036, "step": 43000}
{"episode_reward": 8.57926350964474, "episode": 44.0, "batch_reward": 0.014529070420656353, "critic_loss": 0.003180039499362465, "actor_loss": -18.874151042997838, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.121655702590942, "step": 44000}
{"episode_reward": 8.007249140946508, "episode": 45.0, "batch_reward": 0.01428457258315757, "critic_loss": 0.0024801013414398766, "actor_loss": -18.853685420811175, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.124647855758667, "step": 45000}
{"episode_reward": 8.367840466959214, "episode": 46.0, "batch_reward": 0.014260947508271784, "critic_loss": 0.0026786667263368146, "actor_loss": -19.008656709492207, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.08738112449646, "step": 46000}
{"episode_reward": 8.078904443053043, "episode": 47.0, "batch_reward": 0.014029807487968355, "critic_loss": 0.002495122142863693, "actor_loss": -18.21635053551197, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.129151582717896, "step": 47000}
{"episode_reward": 7.9293221005910475, "episode": 48.0, "batch_reward": 0.013772053273394704, "critic_loss": 0.002178639201592887, "actor_loss": -18.872919522821903, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.156883239746094, "step": 48000}
{"episode_reward": 6.911552133173772, "episode": 49.0, "batch_reward": 0.013631372224539518, "critic_loss": 0.002383711469825357, "actor_loss": -18.924442674160005, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.113106727600098, "step": 49000}
{"episode_reward": 8.539968435796883, "episode": 50.0, "batch_reward": 0.01355935175390914, "critic_loss": 0.0022218047300993932, "actor_loss": -18.173020427644254, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.129487991333008, "step": 50000}
{"episode_reward": 8.467508852443686, "episode": 51.0, "batch_reward": 0.013489005160052329, "critic_loss": 0.0022770745673042258, "actor_loss": -19.097953624069692, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.50467038154602, "step": 51000}
{"episode_reward": 7.128135004587594, "episode": 52.0, "batch_reward": 0.013171828916296363, "critic_loss": 0.002493278940062737, "actor_loss": -18.343138812065124, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.112146139144897, "step": 52000}
{"episode_reward": 6.752032428967913, "episode": 53.0, "batch_reward": 0.013425601909402759, "critic_loss": 0.0020113431421050335, "actor_loss": -18.619555583506823, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.122292280197144, "step": 53000}
{"episode_reward": 8.228969301686917, "episode": 54.0, "batch_reward": 0.012708564165513962, "critic_loss": 0.0019644067672488744, "actor_loss": -18.98930579328537, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.11199450492859, "step": 54000}
{"episode_reward": 6.0107522730912795, "episode": 55.0, "batch_reward": 0.012896808156277985, "critic_loss": 0.002507627440878423, "actor_loss": -19.254836051374674, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.112043857574463, "step": 55000}
{"episode_reward": 8.093170923986557, "episode": 56.0, "batch_reward": 0.012734180849511176, "critic_loss": 0.0024295945732737893, "actor_loss": -19.0545892482996, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.10448718070984, "step": 56000}
{"episode_reward": 9.27198901120389, "episode": 57.0, "batch_reward": 0.012704448684118689, "critic_loss": 0.002526974627340678, "actor_loss": -18.845196387529374, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.13532328605652, "step": 57000}
{"episode_reward": 8.37527025442541, "episode": 58.0, "batch_reward": 0.012620226036990062, "critic_loss": 0.0019251691164681689, "actor_loss": -19.076701194375755, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.113540410995483, "step": 58000}
{"episode_reward": 7.134262256799242, "episode": 59.0, "batch_reward": 0.012596429494209587, "critic_loss": 0.0018415730833658017, "actor_loss": -19.80537713164091, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.117942810058594, "step": 59000}
{"episode_reward": 6.621643993035773, "episode": 60.0, "batch_reward": 0.012617995178792626, "critic_loss": 0.0023040859202155843, "actor_loss": -18.957605276286603, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.1155903339386, "step": 60000}
{"episode_reward": 9.040802335315547, "episode": 61.0, "batch_reward": 0.012309664046857505, "critic_loss": 0.001583962123055244, "actor_loss": -18.5172135463655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.461044788360596, "step": 61000}
{"episode_reward": 8.960867032331427, "episode": 62.0, "batch_reward": 0.012448386495001615, "critic_loss": 0.0021507899632852057, "actor_loss": -18.590091971248388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.089412212371826, "step": 62000}
{"episode_reward": 7.618768600069851, "episode": 63.0, "batch_reward": 0.012277758522424846, "critic_loss": 0.0020973605209728702, "actor_loss": -18.702282451450824, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.133769512176514, "step": 63000}
{"episode_reward": 7.17420489944805, "episode": 64.0, "batch_reward": 0.01228279439266771, "critic_loss": 0.001524723022332182, "actor_loss": -18.839752622842788, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.07305073738098, "step": 64000}
{"episode_reward": 6.471897850751742, "episode": 65.0, "batch_reward": 0.012230515016708523, "critic_loss": 0.002336793322436279, "actor_loss": -18.92887270718813, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.104339122772217, "step": 65000}
{"episode_reward": 5.768680736351441, "episode": 66.0, "batch_reward": 0.012079555246047676, "critic_loss": 0.0017575891344458797, "actor_loss": -19.101565837323665, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.13574981689453, "step": 66000}
{"episode_reward": 6.55961098173259, "episode": 67.0, "batch_reward": 0.012136828547809272, "critic_loss": 0.001751532841299195, "actor_loss": -18.85798506605625, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.09707760810852, "step": 67000}
{"episode_reward": 6.015044505835857, "episode": 68.0, "batch_reward": 0.011960174411768094, "critic_loss": 0.0018964707780396565, "actor_loss": -19.04807822595537, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.11443781852722, "step": 68000}
{"episode_reward": 6.753773542128947, "episode": 69.0, "batch_reward": 0.011769884889712557, "critic_loss": 0.0017411461418669206, "actor_loss": -18.425120101302863, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.15980625152588, "step": 69000}
{"episode_reward": 8.776079823218954, "episode": 70.0, "batch_reward": 0.011753989049699157, "critic_loss": 0.0019490527000743897, "actor_loss": -18.787324792653322, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.094706058502197, "step": 70000}
{"episode_reward": 9.21987883854012, "episode": 71.0, "batch_reward": 0.01172385794762522, "critic_loss": 0.0027489374433644115, "actor_loss": -18.623402659863235, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.510090589523315, "step": 71000}
{"episode_reward": 8.155609250652297, "episode": 72.0, "batch_reward": 0.011742846156470478, "critic_loss": 0.002301381069351919, "actor_loss": -18.938400367945434, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.092532873153687, "step": 72000}
{"episode_reward": 8.87584387352163, "episode": 73.0, "batch_reward": 0.011593263280112296, "critic_loss": 0.002311854779254645, "actor_loss": -18.291561601519586, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.101936101913452, "step": 73000}
{"episode_reward": 8.402209139865231, "episode": 74.0, "batch_reward": 0.011696820226032287, "critic_loss": 0.002090275763650425, "actor_loss": -18.067335266590117, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.104711771011353, "step": 74000}
{"episode_reward": 8.338564541369443, "episode": 75.0, "batch_reward": 0.011533414307748898, "critic_loss": 0.0019386887546861543, "actor_loss": -19.527602158263324, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.098479509353638, "step": 75000}
{"episode_reward": 8.90864140292771, "episode": 76.0, "batch_reward": 0.0116026256759651, "critic_loss": 0.0017009732506994624, "actor_loss": -18.885176751479506, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.120760202407837, "step": 76000}
{"episode_reward": 6.75615833701382, "episode": 77.0, "batch_reward": 0.011522540611913427, "critic_loss": 0.0016695283195294905, "actor_loss": -18.609833166182042, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.095696687698364, "step": 77000}
{"episode_reward": 7.530101185585962, "episode": 78.0, "batch_reward": 0.011333666241029277, "critic_loss": 0.0014836128987080882, "actor_loss": -18.340988523349164, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.092729568481445, "step": 78000}
{"episode_reward": 7.511929653482775, "episode": 79.0, "batch_reward": 0.011002392542082816, "critic_loss": 0.0016221163461741525, "actor_loss": -18.785337593987585, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.087867259979248, "step": 79000}
{"episode_reward": 6.593502621305432, "episode": 80.0, "batch_reward": 0.011439142838586122, "critic_loss": 0.0014576167096965946, "actor_loss": -19.055613614872097, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.10891604423523, "step": 80000}
{"episode_reward": 5.663276784310322, "episode": 81.0, "batch_reward": 0.011271454175002873, "critic_loss": 0.0017549709076993168, "actor_loss": -18.86034703619778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.4859561920166, "step": 81000}
{"episode_reward": 7.822612767506198, "episode": 82.0, "batch_reward": 0.011296758660115301, "critic_loss": 0.0016338240090408363, "actor_loss": -18.396161367326975, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.10145139694214, "step": 82000}
{"episode_reward": 6.3077612735246325, "episode": 83.0, "batch_reward": 0.011204149072058498, "critic_loss": 0.0016281145897519308, "actor_loss": -18.981071232572198, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.12142825126648, "step": 83000}
{"episode_reward": 7.999105996310961, "episode": 84.0, "batch_reward": 0.011111175459809601, "critic_loss": 0.0024791365964338184, "actor_loss": -18.51624832096696, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.1028790473938, "step": 84000}
{"episode_reward": 8.26773783518296, "episode": 85.0, "batch_reward": 0.011160345605807379, "critic_loss": 0.0016193143559503369, "actor_loss": -19.254777449190616, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.09521460533142, "step": 85000}
{"episode_reward": 6.736834598167283, "episode": 86.0, "batch_reward": 0.010930165556259454, "critic_loss": 0.0017863958481757437, "actor_loss": -18.916356509417295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.109821796417236, "step": 86000}
{"episode_reward": 8.813036506858326, "episode": 87.0, "batch_reward": 0.011161572367418557, "critic_loss": 0.0019201873302517923, "actor_loss": -18.850515349186956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.12296986579895, "step": 87000}
{"episode_reward": 8.610640297941888, "episode": 88.0, "batch_reward": 0.011241376507561653, "critic_loss": 0.001839547932991991, "actor_loss": -18.798769645154476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.074168920516968, "step": 88000}
{"episode_reward": 8.229553167728822, "episode": 89.0, "batch_reward": 0.010889221009099856, "critic_loss": 0.0018661760884860995, "actor_loss": -19.01460347170383, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.08187699317932, "step": 89000}
{"episode_reward": 6.56316497376515, "episode": 90.0, "batch_reward": 0.010971750282682479, "critic_loss": 0.0014695243262976873, "actor_loss": -17.942655367352067, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.855961084365845, "step": 90000}
{"episode_reward": 5.946323836233078, "episode": 91.0, "batch_reward": 0.011085419996408746, "critic_loss": 0.0012652266362856608, "actor_loss": -18.865322550412266, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.028002977371216, "step": 91000}
{"episode_reward": 7.694321866529758, "episode": 92.0, "batch_reward": 0.01084253100096248, "critic_loss": 0.001282362638070481, "actor_loss": -18.345974068854005, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.827107906341553, "step": 92000}
{"episode_reward": 7.0759065456112635, "episode": 93.0, "batch_reward": 0.010767155193025246, "critic_loss": 0.0013466937447083182, "actor_loss": -18.866817417765038, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.810361862182617, "step": 93000}
{"episode_reward": 6.878381534940094, "episode": 94.0, "batch_reward": 0.010778475947910919, "critic_loss": 0.0013705516355403233, "actor_loss": -18.103790370058267, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.81500244140625, "step": 94000}
{"episode_reward": 7.562012242548371, "episode": 95.0, "batch_reward": 0.010793216256424785, "critic_loss": 0.0014419898039341205, "actor_loss": -18.645560346022247, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.819777727127075, "step": 95000}
{"episode_reward": 7.5161423291266125, "episode": 96.0, "batch_reward": 0.010751186006702483, "critic_loss": 0.0016845842172915583, "actor_loss": -18.66809679313004, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.816466569900513, "step": 96000}
{"episode_reward": 11.03891200834973, "episode": 97.0, "batch_reward": 0.010819486366584898, "critic_loss": 0.0016596009171335026, "actor_loss": -18.543237329773604, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.821767807006836, "step": 97000}
{"episode_reward": 7.874625702559542, "episode": 98.0, "batch_reward": 0.010812966543249786, "critic_loss": 0.0015852903006307315, "actor_loss": -18.44230861108005, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.843342065811157, "step": 98000}
{"episode_reward": 9.429514308681, "episode": 99.0, "batch_reward": 0.010510205772239715, "critic_loss": 0.0018019945389823988, "actor_loss": -18.45976366391033, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.828771114349365, "step": 99000}
{"episode_reward": 8.210523559972737, "episode": 100.0, "batch_reward": 0.010690054222475737, "critic_loss": 0.0017145810018701013, "actor_loss": -18.56958932363987, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.809815168380737, "step": 100000}
{"episode_reward": 6.077327757081545, "episode": 101.0, "batch_reward": 0.010572036707075313, "critic_loss": 0.0019593006596842315, "actor_loss": -17.912749858118595, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.023422718048096, "step": 101000}
{"episode_reward": 10.061231780903515, "episode": 102.0, "batch_reward": 0.0105363820574712, "critic_loss": 0.0021310736094892493, "actor_loss": -18.74985257525742, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.80867838859558, "step": 102000}
{"episode_reward": 7.0560597181836595, "episode": 103.0, "batch_reward": 0.010489957680460066, "critic_loss": 0.0014993017672968562, "actor_loss": -18.085524525418876, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.81641435623169, "step": 103000}
{"episode_reward": 7.736320212775242, "episode": 104.0, "batch_reward": 0.010478036913555115, "critic_loss": 0.0016953913518809713, "actor_loss": -18.738286359339952, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.818243265151978, "step": 104000}
{"episode_reward": 7.727511977706171, "episode": 105.0, "batch_reward": 0.010360815159510822, "critic_loss": 0.0015245684552646708, "actor_loss": -18.334211287019773, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.8457248210907, "step": 105000}
{"episode_reward": 4.946819199013892, "episode": 106.0, "batch_reward": 0.010486466089263558, "critic_loss": 0.001436133589799283, "actor_loss": -18.95012717902288, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.80730104446411, "step": 106000}
{"episode_reward": 6.581220228028923, "episode": 107.0, "batch_reward": 0.01033039819309488, "critic_loss": 0.0012650534797576257, "actor_loss": -18.64961154556647, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.810972929000854, "step": 107000}
{"episode_reward": 6.298589050525944, "episode": 108.0, "batch_reward": 0.010313729368848727, "critic_loss": 0.001537974741513608, "actor_loss": -18.538673879342152, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.821849822998047, "step": 108000}
{"episode_reward": 7.28123689974913, "episode": 109.0, "batch_reward": 0.010379766626516356, "critic_loss": 0.0014013052079535555, "actor_loss": -18.74185156709328, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.783958673477173, "step": 109000}
{"episode_reward": 8.822090463299004, "episode": 110.0, "batch_reward": 0.010315527455881239, "critic_loss": 0.0013876701647823212, "actor_loss": -18.530982116766275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.793768644332886, "step": 110000}
{"episode_reward": 9.873250896645807, "episode": 111.0, "batch_reward": 0.010294051055330784, "critic_loss": 0.0018026594430557452, "actor_loss": -18.422449581887573, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.057860374450684, "step": 111000}
{"episode_reward": 7.934700595750935, "episode": 112.0, "batch_reward": 0.01038036811258644, "critic_loss": 0.0018465613314619987, "actor_loss": -18.892970806859434, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.80586552619934, "step": 112000}
{"episode_reward": 8.932744187179168, "episode": 113.0, "batch_reward": 0.010308778014266863, "critic_loss": 0.0015217810679750983, "actor_loss": -18.065978989996015, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.803478717803955, "step": 113000}
{"episode_reward": 7.040879792905134, "episode": 114.0, "batch_reward": 0.010320279211737216, "critic_loss": 0.0012920285222644452, "actor_loss": -18.936104058474303, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.824345588684082, "step": 114000}
{"episode_reward": 7.066273004321671, "episode": 115.0, "batch_reward": 0.010200381044764072, "critic_loss": 0.0016582468260894529, "actor_loss": -18.98136924251914, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.807793617248535, "step": 115000}
{"episode_reward": 5.459516650052276, "episode": 116.0, "batch_reward": 0.010029485565377399, "critic_loss": 0.0014578510560531869, "actor_loss": -18.884216522643342, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.808856964111328, "step": 116000}
{"episode_reward": 4.255091449011251, "episode": 117.0, "batch_reward": 0.010122069330420345, "critic_loss": 0.00161862103533349, "actor_loss": -18.69953888427373, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.765837907791138, "step": 117000}
{"episode_reward": 5.855023726989104, "episode": 118.0, "batch_reward": 0.0101803432055749, "critic_loss": 0.0015145687017065939, "actor_loss": -18.61417785237357, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.796218156814575, "step": 118000}
{"episode_reward": 8.613806797689364, "episode": 119.0, "batch_reward": 0.010066942306002602, "critic_loss": 0.0018811908528441562, "actor_loss": -18.475482982371002, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.791451930999756, "step": 119000}
{"episode_reward": 10.167108492666202, "episode": 120.0, "batch_reward": 0.01000604318943806, "critic_loss": 0.0021774510943796486, "actor_loss": -19.340894587228075, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.696532011032104, "step": 120000}
{"episode_reward": 7.998379868063385, "episode": 121.0, "batch_reward": 0.010062797663034872, "critic_loss": 0.0018153024486382491, "actor_loss": -18.749251016512513, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 35.74342465400696, "step": 121000}
{"episode_reward": 8.022099337498153, "episode": 122.0, "batch_reward": 0.010036274583078921, "critic_loss": 0.0021098589726607315, "actor_loss": -18.983802873700856, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.783901929855347, "step": 122000}
{"episode_reward": 7.52006458830969, "episode": 123.0, "batch_reward": 0.010070737830363214, "critic_loss": 0.0020705416585551574, "actor_loss": -19.10801193075627, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.757708311080933, "step": 123000}
{"episode_reward": 8.6882728056149, "episode": 124.0, "batch_reward": 0.010118968284223229, "critic_loss": 0.002524829883186612, "actor_loss": -18.681061553603037, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.80724048614502, "step": 124000}
{"episode_reward": 7.1107738776022105, "episode": 125.0, "batch_reward": 0.009749795015668496, "critic_loss": 0.0019480760645819827, "actor_loss": -19.49185334500298, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.734973192214966, "step": 125000}
{"episode_reward": 8.061443273691935, "episode": 126.0, "batch_reward": 0.00986031317920424, "critic_loss": 0.001661031828582054, "actor_loss": -19.098352782230823, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.76640486717224, "step": 126000}
{"episode_reward": 6.2401393411745545, "episode": 127.0, "batch_reward": 0.009869526509894058, "critic_loss": 0.0016309876565646845, "actor_loss": -18.725091781747526, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.721238136291504, "step": 127000}
{"episode_reward": 7.217648268449253, "episode": 128.0, "batch_reward": 0.009867944879457354, "critic_loss": 0.0014956347206025384, "actor_loss": -18.827452521538362, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.719860553741455, "step": 128000}
{"episode_reward": 5.804494703555827, "episode": 129.0, "batch_reward": 0.009642278505023569, "critic_loss": 0.001832607069023652, "actor_loss": -19.000313870439307, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.714084148406982, "step": 129000}
{"episode_reward": 7.310984301538764, "episode": 130.0, "batch_reward": 0.00997654404817149, "critic_loss": 0.0021681238914316053, "actor_loss": -18.504512611184268, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.997435331344604, "step": 130000}
{"episode_reward": 9.18087597655653, "episode": 131.0, "batch_reward": 0.00983133044023998, "critic_loss": 0.001687641471158713, "actor_loss": -17.579093508262186, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.11957025527954, "step": 131000}
{"episode_reward": 8.080103589431918, "episode": 132.0, "batch_reward": 0.010005182387772947, "critic_loss": 0.0018452707137330436, "actor_loss": -17.954063180388882, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.62620735168457, "step": 132000}
{"episode_reward": 6.3964076800522545, "episode": 133.0, "batch_reward": 0.009904097790829838, "critic_loss": 0.0019745997710560914, "actor_loss": -19.110895333349706, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.576570510864258, "step": 133000}
{"episode_reward": 7.409584637104034, "episode": 134.0, "batch_reward": 0.009970653829164802, "critic_loss": 0.0019774622381082737, "actor_loss": -18.951986422669144, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.31426215171814, "step": 134000}
{"episode_reward": 8.767810040028579, "episode": 135.0, "batch_reward": 0.009904579853406176, "critic_loss": 0.0016037864059908315, "actor_loss": -19.1277928711921, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.103965759277344, "step": 135000}
{"episode_reward": 6.829077534707913, "episode": 136.0, "batch_reward": 0.009759648648556322, "critic_loss": 0.0016544535232533234, "actor_loss": -19.0948818010157, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.76364803314209, "step": 136000}
{"episode_reward": 8.856437479952186, "episode": 137.0, "batch_reward": 0.009880049974657594, "critic_loss": 0.0019323503560735845, "actor_loss": -18.850945991484448, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.550076246261597, "step": 137000}
{"episode_reward": 9.663219533423876, "episode": 138.0, "batch_reward": 0.009972493341658265, "critic_loss": 0.0015828559547953773, "actor_loss": -17.713925467699767, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.567511320114136, "step": 138000}
{"episode_reward": 5.990747257598087, "episode": 139.0, "batch_reward": 0.009686957927653566, "critic_loss": 0.0019939172505401073, "actor_loss": -17.543134894732386, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.807164669036865, "step": 139000}
{"episode_reward": 9.268729732168456, "episode": 140.0, "batch_reward": 0.009888794714584946, "critic_loss": 0.0014536021286330652, "actor_loss": -18.037283538054673, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.559465408325195, "step": 140000}
{"episode_reward": 8.429637958977397, "episode": 141.0, "batch_reward": 0.009612154268659651, "critic_loss": 0.0016255352738080546, "actor_loss": -18.11135772695765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.013128995895386, "step": 141000}
{"episode_reward": 7.565738355887186, "episode": 142.0, "batch_reward": 0.009717000238830223, "critic_loss": 0.0013636753086611862, "actor_loss": -18.258063402818514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.770840167999268, "step": 142000}
{"episode_reward": 6.363896305602234, "episode": 143.0, "batch_reward": 0.009773940664948896, "critic_loss": 0.0017682412011490668, "actor_loss": -19.217128892494365, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.650657653808594, "step": 143000}
{"episode_reward": 7.313240151579008, "episode": 144.0, "batch_reward": 0.009747776254545898, "critic_loss": 0.0012338439068989829, "actor_loss": -18.852773913010957, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.624269723892212, "step": 144000}
{"episode_reward": 7.195859203768285, "episode": 145.0, "batch_reward": 0.009647961594397202, "critic_loss": 0.0015849226204445585, "actor_loss": -19.867907608974726, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.03387141227722, "step": 145000}
{"episode_reward": 7.464739516455447, "episode": 146.0, "batch_reward": 0.009575786055065691, "critic_loss": 0.0012446487658598925, "actor_loss": -18.429317847577856, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.123010873794556, "step": 146000}
{"episode_reward": 6.2873301148427565, "episode": 147.0, "batch_reward": 0.009664424743503331, "critic_loss": 0.0014758906290517188, "actor_loss": -18.388786182223818, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.8411762714386, "step": 147000}
{"episode_reward": 7.416015584177749, "episode": 148.0, "batch_reward": 0.00952649488998577, "critic_loss": 0.001544070891890442, "actor_loss": -18.654402582166718, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.26474905014038, "step": 148000}
{"episode_reward": 5.499851167222417, "episode": 149.0, "batch_reward": 0.009694705077679827, "critic_loss": 0.001466672841386753, "actor_loss": -18.5465097657335, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.60859441757202, "step": 149000}
{"episode_reward": 5.647048299564049, "episode": 150.0, "batch_reward": 0.009593230054713785, "critic_loss": 0.0014368608387594576, "actor_loss": -18.603793745582923, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
