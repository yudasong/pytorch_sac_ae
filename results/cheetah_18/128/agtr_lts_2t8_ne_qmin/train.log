{"episode_reward": 0.0, "episode": 1.0, "duration": 14.749180793762207, "step": 1000}
{"episode_reward": 4.20996189079657, "episode": 2.0, "duration": 1.5354175567626953, "step": 2000}
{"episode_reward": 252.2457487418515, "episode": 3.0, "batch_reward": 0.13953697242441476, "critic_loss": 0.30517103996037165, "actor_loss": -36.58381413325829, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 72.88360333442688, "step": 3000}
{"episode_reward": 360.2796459363865, "episode": 4.0, "batch_reward": 0.20587564386427404, "critic_loss": 0.49028287369012835, "actor_loss": -39.0168653717041, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.04632544517517, "step": 4000}
{"episode_reward": 132.62957867700771, "episode": 5.0, "batch_reward": 0.1976880908459425, "critic_loss": 0.5573917740285397, "actor_loss": -35.39018530273437, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.996870040893555, "step": 5000}
{"episode_reward": 182.92916680465135, "episode": 6.0, "batch_reward": 0.19685730008780955, "critic_loss": 0.6163269967138767, "actor_loss": -33.33516383743286, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.641457557678223, "step": 6000}
{"episode_reward": 189.09608683692625, "episode": 7.0, "batch_reward": 0.20295539949834346, "critic_loss": 0.7664977277815342, "actor_loss": -33.05150200271606, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.05728554725647, "step": 7000}
{"episode_reward": 403.4645704469717, "episode": 8.0, "batch_reward": 0.21097153986990452, "critic_loss": 0.873197051435709, "actor_loss": -33.044611602783206, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.916588068008423, "step": 8000}
{"episode_reward": 61.052872933572374, "episode": 9.0, "batch_reward": 0.2120354436635971, "critic_loss": 1.0855600691139697, "actor_loss": -32.64802139663696, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.292866945266724, "step": 9000}
{"episode_reward": 457.13666897110033, "episode": 10.0, "batch_reward": 0.2322276212722063, "critic_loss": 1.0685562831163407, "actor_loss": -34.71835692214966, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.19626474380493, "step": 10000}
{"episode_reward": 404.2900203891859, "episode": 11.0, "batch_reward": 0.2473974815607071, "critic_loss": 1.2131700629293918, "actor_loss": -36.43835931777954, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.54220247268677, "step": 11000}
{"episode_reward": 307.27034333527223, "episode": 12.0, "batch_reward": 0.2538066434413195, "critic_loss": 1.4291444770097732, "actor_loss": -38.09734325790405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.442706823349, "step": 12000}
{"episode_reward": 297.79779870135235, "episode": 13.0, "batch_reward": 0.25595475123822686, "critic_loss": 1.5889977402091027, "actor_loss": -39.31679294967651, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.735469341278076, "step": 13000}
{"episode_reward": 192.39547728258955, "episode": 14.0, "batch_reward": 0.255646155372262, "critic_loss": 1.6318663327097893, "actor_loss": -40.49686171340942, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.12532329559326, "step": 14000}
{"episode_reward": 311.1717777292145, "episode": 15.0, "batch_reward": 0.2617308254390955, "critic_loss": 1.641114843904972, "actor_loss": -40.8311298828125, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.772563934326172, "step": 15000}
{"episode_reward": 414.19853149594024, "episode": 16.0, "batch_reward": 0.26597060483694074, "critic_loss": 1.9659867717027664, "actor_loss": -41.3587197189331, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.25529146194458, "step": 16000}
{"episode_reward": 221.73441400460553, "episode": 17.0, "batch_reward": 0.2644572590738535, "critic_loss": 2.39759905064106, "actor_loss": -41.619796855926516, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.206186532974243, "step": 17000}
{"episode_reward": 313.1016371476587, "episode": 18.0, "batch_reward": 0.2602195459008217, "critic_loss": 2.8408543614149093, "actor_loss": -41.925412994384764, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.283292531967163, "step": 18000}
{"episode_reward": 49.89752143496613, "episode": 19.0, "batch_reward": 0.24920768892765044, "critic_loss": 3.830286598801613, "actor_loss": -41.17203999710083, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.966485500335693, "step": 19000}
{"episode_reward": 65.920727961107, "episode": 20.0, "batch_reward": 0.23840683986246586, "critic_loss": 4.197506138682366, "actor_loss": -41.39066427612305, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.05799889564514, "step": 20000}
{"episode_reward": 36.58837608606286, "episode": 21.0, "batch_reward": 0.22984748131036759, "critic_loss": 5.295175296902657, "actor_loss": -41.41889518737793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.77615404129028, "step": 21000}
{"episode_reward": 76.72817203209542, "episode": 22.0, "batch_reward": 0.22119068263471126, "critic_loss": 6.482110816717148, "actor_loss": -42.55995811843872, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.917593240737915, "step": 22000}
{"episode_reward": 27.017713713252732, "episode": 23.0, "batch_reward": 0.21267806531488895, "critic_loss": 8.050287155151366, "actor_loss": -43.42070497894287, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.68011999130249, "step": 23000}
{"episode_reward": 22.111855760658656, "episode": 24.0, "batch_reward": 0.20269590133428575, "critic_loss": 7.734849318027496, "actor_loss": -44.85581813430786, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.732846975326538, "step": 24000}
{"episode_reward": 17.31861206004598, "episode": 25.0, "batch_reward": 0.19605616161227227, "critic_loss": 6.892545442819595, "actor_loss": -49.27075929260254, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.531057357788086, "step": 25000}
{"episode_reward": 7.021466997449295, "episode": 26.0, "batch_reward": 0.18819211371243, "critic_loss": 5.877700950145721, "actor_loss": -50.58148740005493, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.320072889328003, "step": 26000}
{"episode_reward": 8.223281304501523, "episode": 27.0, "batch_reward": 0.18237227100133896, "critic_loss": 5.857916394233704, "actor_loss": -53.650378089904784, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.5118625164032, "step": 27000}
{"episode_reward": 49.84805717224523, "episode": 28.0, "batch_reward": 0.17935351403802632, "critic_loss": 8.320878957271576, "actor_loss": -58.07286304473877, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.421314239501953, "step": 28000}
{"episode_reward": 159.2832356490995, "episode": 29.0, "batch_reward": 0.1792880314141512, "critic_loss": 11.203801408767701, "actor_loss": -60.03154399871826, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.785449504852295, "step": 29000}
{"episode_reward": 153.04496747718977, "episode": 30.0, "batch_reward": 0.17896199773997068, "critic_loss": 13.969177574634552, "actor_loss": -60.399668933868405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.217592239379883, "step": 30000}
{"episode_reward": 236.64276376259804, "episode": 31.0, "batch_reward": 0.1801497118398547, "critic_loss": 14.605481154441833, "actor_loss": -65.02833010101318, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.32267951965332, "step": 31000}
{"episode_reward": 141.19520141788334, "episode": 32.0, "batch_reward": 0.18272864750027656, "critic_loss": 14.14602071428299, "actor_loss": -65.37330438232422, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.80664563179016, "step": 32000}
{"episode_reward": 291.1120979546422, "episode": 33.0, "batch_reward": 0.18487657461315393, "critic_loss": 11.84430507493019, "actor_loss": -67.8231979675293, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.902229070663452, "step": 33000}
{"episode_reward": 406.7784456702791, "episode": 34.0, "batch_reward": 0.19289780533313752, "critic_loss": 8.154544937610627, "actor_loss": -69.03337768554688, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.04390597343445, "step": 34000}
{"episode_reward": 403.530308563659, "episode": 35.0, "batch_reward": 0.19953217421472072, "critic_loss": 7.03438882946968, "actor_loss": -65.92614897155762, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.877140045166016, "step": 35000}
