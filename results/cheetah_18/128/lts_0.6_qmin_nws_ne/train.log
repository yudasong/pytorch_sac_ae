{"episode_reward": 0.0, "episode": 1.0, "duration": 17.288148880004883, "step": 1000}
{"episode_reward": 4.20996189079657, "episode": 2.0, "duration": 1.5100014209747314, "step": 2000}
{"episode_reward": 252.2457487418515, "episode": 3.0, "batch_reward": 0.12122327892129005, "critic_loss": 0.010345209872394505, "actor_loss": -22.690929868902895, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 60.5053825378418, "step": 3000}
{"episode_reward": 8.946544522232335, "episode": 4.0, "batch_reward": 0.07766542284935712, "critic_loss": 0.009503040345851333, "actor_loss": -20.765425070285797, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.000840425491333, "step": 4000}
{"episode_reward": 6.449861565125179, "episode": 5.0, "batch_reward": 0.06090135676600039, "critic_loss": 0.009903857360361144, "actor_loss": -20.653799299240113, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.024513483047485, "step": 5000}
{"episode_reward": 3.1896717278636135, "episode": 6.0, "batch_reward": 0.050862552801147105, "critic_loss": 0.010129997581709176, "actor_loss": -20.599977286577225, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.983566999435425, "step": 6000}
{"episode_reward": 3.881394614386439, "episode": 7.0, "batch_reward": 0.04386900417879224, "critic_loss": 0.010534074177034199, "actor_loss": -20.644608228445055, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00724172592163, "step": 7000}
{"episode_reward": 3.751872863411042, "episode": 8.0, "batch_reward": 0.038338938100263474, "critic_loss": 0.007309893185622059, "actor_loss": -19.912141301870346, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02298617362976, "step": 8000}
{"episode_reward": 3.1592762470774884, "episode": 9.0, "batch_reward": 0.033920086205005646, "critic_loss": 0.007386267565656453, "actor_loss": -19.332760045051575, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.997741222381592, "step": 9000}
{"episode_reward": 3.349179888240437, "episode": 10.0, "batch_reward": 0.03056987304892391, "critic_loss": 0.007173320536268875, "actor_loss": -20.286421374559403, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.983346939086914, "step": 10000}
{"episode_reward": 3.067278385918371, "episode": 11.0, "batch_reward": 0.027334653561003504, "critic_loss": 0.005725569586153142, "actor_loss": -18.83628428554535, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.348177433013916, "step": 11000}
{"episode_reward": 3.5946072355998337, "episode": 12.0, "batch_reward": 0.026069254445843397, "critic_loss": 0.0066982247095438655, "actor_loss": -20.195829429388045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.001947164535522, "step": 12000}
{"episode_reward": 3.122047084056506, "episode": 13.0, "batch_reward": 0.02397337329061702, "critic_loss": 0.005283263029763475, "actor_loss": -20.420592506408692, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.99468970298767, "step": 13000}
{"episode_reward": 2.7972948255148946, "episode": 14.0, "batch_reward": 0.022378785443492234, "critic_loss": 0.004850582424085587, "actor_loss": -19.71605532693863, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01330304145813, "step": 14000}
{"episode_reward": 3.3869946622543488, "episode": 15.0, "batch_reward": 0.021438397634774446, "critic_loss": 0.00513248328489135, "actor_loss": -18.78855616414547, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.003204107284546, "step": 15000}
{"episode_reward": 3.185774659087671, "episode": 16.0, "batch_reward": 0.020176301691215485, "critic_loss": 0.0060963320993469095, "actor_loss": -19.420564198613167, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01311206817627, "step": 16000}
{"episode_reward": 3.4607606701522067, "episode": 17.0, "batch_reward": 0.019185299814213068, "critic_loss": 0.003956731664628023, "actor_loss": -20.19700750184059, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.972450256347656, "step": 17000}
{"episode_reward": 2.5214374566655744, "episode": 18.0, "batch_reward": 0.018344070680439473, "critic_loss": 0.005527944717294304, "actor_loss": -19.43467838418484, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.97313666343689, "step": 18000}
{"episode_reward": 3.594247447213877, "episode": 19.0, "batch_reward": 0.017120187781751155, "critic_loss": 0.0045243996673962105, "actor_loss": -19.59259650337696, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.014525890350342, "step": 19000}
{"episode_reward": 2.2346467853630125, "episode": 20.0, "batch_reward": 0.016507715748623013, "critic_loss": 0.0038946414657111744, "actor_loss": -20.099871490597724, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.949535369873047, "step": 20000}
{"episode_reward": 2.695751600579537, "episode": 21.0, "batch_reward": 0.01551984575833194, "critic_loss": 0.0038922205263224896, "actor_loss": -18.523342892646788, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.33663582801819, "step": 21000}
{"episode_reward": 3.45158506537741, "episode": 22.0, "batch_reward": 0.014934000266017392, "critic_loss": 0.0045490070032828955, "actor_loss": -20.440441221952437, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.979450225830078, "step": 22000}
{"episode_reward": 2.731488120004241, "episode": 23.0, "batch_reward": 0.014769640389597043, "critic_loss": 0.003639128844210063, "actor_loss": -19.85944273376465, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02229070663452, "step": 23000}
{"episode_reward": 2.8101166603771612, "episode": 24.0, "batch_reward": 0.013962849526898935, "critic_loss": 0.003988436225976329, "actor_loss": -18.418011437773703, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21517300605774, "step": 24000}
{"episode_reward": 3.5129409284650865, "episode": 25.0, "batch_reward": 0.013205201829783618, "critic_loss": 0.003297142628347501, "actor_loss": -19.674390584230423, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.26142144203186, "step": 25000}
{"episode_reward": 3.185923576725708, "episode": 26.0, "batch_reward": 0.013286450543440879, "critic_loss": 0.0036553604622313286, "actor_loss": -18.8821233574152, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01030683517456, "step": 26000}
{"episode_reward": 2.998288085389559, "episode": 27.0, "batch_reward": 0.012803076579933986, "critic_loss": 0.0034294968412286837, "actor_loss": -18.988921136379243, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.973485469818115, "step": 27000}
{"episode_reward": 2.8296202368644683, "episode": 28.0, "batch_reward": 0.012809646401321515, "critic_loss": 0.0035171657241589854, "actor_loss": -19.59735061764717, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00062370300293, "step": 28000}
{"episode_reward": 3.315730544508635, "episode": 29.0, "batch_reward": 0.01219090453814715, "critic_loss": 0.003230179667734774, "actor_loss": -19.18508856254816, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01944351196289, "step": 29000}
{"episode_reward": 2.972260344417658, "episode": 30.0, "batch_reward": 0.011955168870510534, "critic_loss": 0.0027992140582064166, "actor_loss": -18.313815188407897, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.004992246627808, "step": 30000}
{"episode_reward": 2.9245791766026477, "episode": 31.0, "batch_reward": 0.011836366332368925, "critic_loss": 0.0024310085582983447, "actor_loss": -19.41479766488075, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.38376426696777, "step": 31000}
{"episode_reward": 2.4753222536383377, "episode": 32.0, "batch_reward": 0.011528374938527122, "critic_loss": 0.0028493724192521766, "actor_loss": -18.846328641593455, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.971998929977417, "step": 32000}
{"episode_reward": 3.57050052190423, "episode": 33.0, "batch_reward": 0.010837826875853352, "critic_loss": 0.0023250605876673942, "actor_loss": -19.75324859046936, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.99020028114319, "step": 33000}
{"episode_reward": 3.668922364694697, "episode": 34.0, "batch_reward": 0.010789603446377442, "critic_loss": 0.002075641380637535, "actor_loss": -19.600809213995934, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01768970489502, "step": 34000}
{"episode_reward": 4.31851898209169, "episode": 35.0, "batch_reward": 0.010791510018520058, "critic_loss": 0.0022453509307088096, "actor_loss": -17.500562699615955, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.017613410949707, "step": 35000}
{"episode_reward": 3.6786926832390083, "episode": 36.0, "batch_reward": 0.010389064211398362, "critic_loss": 0.0025601934139704097, "actor_loss": -19.792582667052745, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.009509801864624, "step": 36000}
{"episode_reward": 2.7511264665866095, "episode": 37.0, "batch_reward": 0.010273240495007486, "critic_loss": 0.001956907131432672, "actor_loss": -18.717585245966912, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01332998275757, "step": 37000}
{"episode_reward": 2.4476342116707945, "episode": 38.0, "batch_reward": 0.010387871282873676, "critic_loss": 0.002232325511955423, "actor_loss": -18.161006476759912, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.007214069366455, "step": 38000}
{"episode_reward": 3.339574020967138, "episode": 39.0, "batch_reward": 0.009895967469085007, "critic_loss": 0.0022356139638723105, "actor_loss": -19.805470478475094, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.990583181381226, "step": 39000}
{"episode_reward": 3.211621173912692, "episode": 40.0, "batch_reward": 0.009756021276349202, "critic_loss": 0.0017804148651921422, "actor_loss": -20.84159229403734, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.990997552871704, "step": 40000}
{"episode_reward": 3.3738625683476995, "episode": 41.0, "batch_reward": 0.009702160021988674, "critic_loss": 0.002082032351914677, "actor_loss": -20.218786588072778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.36182999610901, "step": 41000}
{"episode_reward": 3.882756476998224, "episode": 42.0, "batch_reward": 0.009437594842398539, "critic_loss": 0.002469756677353871, "actor_loss": -19.715411761045456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.98021650314331, "step": 42000}
{"episode_reward": 3.335743365389574, "episode": 43.0, "batch_reward": 0.009481522354995832, "critic_loss": 0.0019237332725242596, "actor_loss": -20.212679898798466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.9969162940979, "step": 43000}
{"episode_reward": 3.0682051631359757, "episode": 44.0, "batch_reward": 0.009195508411386982, "critic_loss": 0.002212622453516815, "actor_loss": -18.274018168747425, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.004552125930786, "step": 44000}
{"episode_reward": 2.9312667213483574, "episode": 45.0, "batch_reward": 0.009089565253583714, "critic_loss": 0.0021360473968379664, "actor_loss": -18.092337091088297, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.997836589813232, "step": 45000}
{"episode_reward": 4.407426328145082, "episode": 46.0, "batch_reward": 0.009028548407368362, "critic_loss": 0.0022180708853702527, "actor_loss": -18.96303626990318, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.008142232894897, "step": 46000}
{"episode_reward": 3.6790050435840613, "episode": 47.0, "batch_reward": 0.008702320482116192, "critic_loss": 0.002725375288253417, "actor_loss": -18.687845887482165, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.984800100326538, "step": 47000}
{"episode_reward": 3.545632664335222, "episode": 48.0, "batch_reward": 0.00864844710752368, "critic_loss": 0.0021150367266236573, "actor_loss": -18.44488865429163, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.975343465805054, "step": 48000}
{"episode_reward": 2.8587845248942947, "episode": 49.0, "batch_reward": 0.008625814567436465, "critic_loss": 0.0019346494850033195, "actor_loss": -18.81019691902399, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.98693323135376, "step": 49000}
{"episode_reward": 3.9115001004948784, "episode": 50.0, "batch_reward": 0.00845961911440827, "critic_loss": 0.0019202235258708243, "actor_loss": -18.672588565975428, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.029459476470947, "step": 50000}
{"episode_reward": 2.7349135402977502, "episode": 51.0, "batch_reward": 0.008266417054692284, "critic_loss": 0.0017640119129791855, "actor_loss": -18.91424652558565, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.373059034347534, "step": 51000}
{"episode_reward": 3.9556303180423886, "episode": 52.0, "batch_reward": 0.008100813563796692, "critic_loss": 0.002789077920380805, "actor_loss": -18.20880289852619, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.994605541229248, "step": 52000}
{"episode_reward": 2.0028368606825486, "episode": 53.0, "batch_reward": 0.008302613224601373, "critic_loss": 0.0016098244789318414, "actor_loss": -18.59844435837865, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.994683265686035, "step": 53000}
{"episode_reward": 2.774374152071804, "episode": 54.0, "batch_reward": 0.007739365454297513, "critic_loss": 0.0016683581297475029, "actor_loss": -18.694784897327423, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.019869804382324, "step": 54000}
{"episode_reward": 3.619117882296142, "episode": 55.0, "batch_reward": 0.007841085031162947, "critic_loss": 0.00176048080943292, "actor_loss": -18.87958814704418, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.045576095581055, "step": 55000}
{"episode_reward": 2.464952781383567, "episode": 56.0, "batch_reward": 0.007729975749272853, "critic_loss": 0.0016052000053605297, "actor_loss": -19.755509593099355, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.995664834976196, "step": 56000}
{"episode_reward": 3.4319961117915776, "episode": 57.0, "batch_reward": 0.007518143595661968, "critic_loss": 0.0021106501541871695, "actor_loss": -19.17726331833005, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.99222493171692, "step": 57000}
{"episode_reward": 2.3445369998246846, "episode": 58.0, "batch_reward": 0.007485637391451746, "critic_loss": 0.0016345981882768684, "actor_loss": -19.492751644581556, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.976917505264282, "step": 58000}
{"episode_reward": 3.7805370479165874, "episode": 59.0, "batch_reward": 0.007605141053907573, "critic_loss": 0.0013181060647548292, "actor_loss": -19.79994405734539, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.9906747341156, "step": 59000}
{"episode_reward": 3.2715558484865346, "episode": 60.0, "batch_reward": 0.007596921855700202, "critic_loss": 0.0022560150535209686, "actor_loss": -19.063495360255242, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.011456727981567, "step": 60000}
{"episode_reward": 3.746270784918119, "episode": 61.0, "batch_reward": 0.007246900897473097, "critic_loss": 0.0013205780480057, "actor_loss": -18.053316814392804, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.3645133972168, "step": 61000}
{"episode_reward": 3.9404181003848398, "episode": 62.0, "batch_reward": 0.007442431788891554, "critic_loss": 0.001913410118628235, "actor_loss": -18.236763705939055, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.017096519470215, "step": 62000}
{"episode_reward": 3.2624699927733665, "episode": 63.0, "batch_reward": 0.007208401420153677, "critic_loss": 0.0018246396039176033, "actor_loss": -18.539515695780516, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02233076095581, "step": 63000}
{"episode_reward": 3.0800937615898327, "episode": 64.0, "batch_reward": 0.0073077914474997665, "critic_loss": 0.0015060190122530913, "actor_loss": -19.840417905539276, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00584387779236, "step": 64000}
{"episode_reward": 3.0701189455017937, "episode": 65.0, "batch_reward": 0.007224994238698855, "critic_loss": 0.0027396932075062067, "actor_loss": -19.723903364568947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.033801794052124, "step": 65000}
{"episode_reward": 2.762393565582842, "episode": 66.0, "batch_reward": 0.007059946611057967, "critic_loss": 0.00181736772012664, "actor_loss": -19.51967044016719, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.005081176757812, "step": 66000}
{"episode_reward": 3.046138268299635, "episode": 67.0, "batch_reward": 0.007212606540415436, "critic_loss": 0.0017119915888324613, "actor_loss": -17.91334187710285, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.998403072357178, "step": 67000}
{"episode_reward": 3.594269981391493, "episode": 68.0, "batch_reward": 0.0070517092638183385, "critic_loss": 0.0014714005435089348, "actor_loss": -19.34313548964262, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02360701560974, "step": 68000}
{"episode_reward": 3.1637958077477935, "episode": 69.0, "batch_reward": 0.006933695085113868, "critic_loss": 0.001694633189450542, "actor_loss": -18.526726885437967, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.996239185333252, "step": 69000}
{"episode_reward": 4.3904127747184285, "episode": 70.0, "batch_reward": 0.006865510290837847, "critic_loss": 0.0014606187983517884, "actor_loss": -18.155603118121626, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.996577501296997, "step": 70000}
{"episode_reward": 3.9228687606568715, "episode": 71.0, "batch_reward": 0.006901613803580404, "critic_loss": 0.0013908529827313032, "actor_loss": -18.274355391323567, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.372185707092285, "step": 71000}
{"episode_reward": 3.149014585136766, "episode": 72.0, "batch_reward": 0.006814628926571459, "critic_loss": 0.0016685819614649519, "actor_loss": -18.64673440286517, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01704740524292, "step": 72000}
{"episode_reward": 4.153305586175408, "episode": 73.0, "batch_reward": 0.006680975967086852, "critic_loss": 0.001335973451132304, "actor_loss": -18.76182262760401, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.013525247573853, "step": 73000}
{"episode_reward": 3.231042314674425, "episode": 74.0, "batch_reward": 0.006745466019492596, "critic_loss": 0.001557943542022258, "actor_loss": -18.82846320861578, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.003489017486572, "step": 74000}
{"episode_reward": 3.400949530434662, "episode": 75.0, "batch_reward": 0.006692260369891301, "critic_loss": 0.0018279720818827627, "actor_loss": -20.117991216003894, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02218222618103, "step": 75000}
{"episode_reward": 2.9389935290157307, "episode": 76.0, "batch_reward": 0.006601237560040317, "critic_loss": 0.00136040477406641, "actor_loss": -19.004333441376687, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.03374743461609, "step": 76000}
{"episode_reward": 2.444315184625464, "episode": 77.0, "batch_reward": 0.006656964068301022, "critic_loss": 0.001198149529271177, "actor_loss": -19.138727976709603, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.999566555023193, "step": 77000}
{"episode_reward": 3.204736608157765, "episode": 78.0, "batch_reward": 0.006473708442412317, "critic_loss": 0.0013209639530032292, "actor_loss": -18.874383184343575, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.001564741134644, "step": 78000}
{"episode_reward": 2.640636388677149, "episode": 79.0, "batch_reward": 0.0062388249123469, "critic_loss": 0.0019304224554871326, "actor_loss": -19.790490970656275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.997334241867065, "step": 79000}
{"episode_reward": 3.8854104635179967, "episode": 80.0, "batch_reward": 0.0065301589951850475, "critic_loss": 0.001203090032809996, "actor_loss": -19.501027509778737, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.004358768463135, "step": 80000}
{"episode_reward": 2.497239228198506, "episode": 81.0, "batch_reward": 0.006422008082037791, "critic_loss": 0.001435111386308563, "actor_loss": -18.929997544243932, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.3352530002594, "step": 81000}
{"episode_reward": 3.327206953531211, "episode": 82.0, "batch_reward": 0.006516358288004995, "critic_loss": 0.0012455127405919483, "actor_loss": -19.29880598500371, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01103448867798, "step": 82000}
{"episode_reward": 3.7260962216024116, "episode": 83.0, "batch_reward": 0.006410461065592244, "critic_loss": 0.0013912146164329897, "actor_loss": -18.741831032186745, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.022088050842285, "step": 83000}
{"episode_reward": 2.668685624881789, "episode": 84.0, "batch_reward": 0.006294729043263942, "critic_loss": 0.001352975269590388, "actor_loss": -18.990294237077237, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.993205070495605, "step": 84000}
{"episode_reward": 3.542638477376125, "episode": 85.0, "batch_reward": 0.006264850642415695, "critic_loss": 0.0009852083036239491, "actor_loss": -19.409681993722916, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.006441116333008, "step": 85000}
{"episode_reward": 3.0902265688729775, "episode": 86.0, "batch_reward": 0.006264426876790821, "critic_loss": 0.0015776612588233546, "actor_loss": -18.731440264850853, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02661418914795, "step": 86000}
{"episode_reward": 2.793074600234184, "episode": 87.0, "batch_reward": 0.006355954068945721, "critic_loss": 0.001437032600333623, "actor_loss": -18.243282768368722, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.969507932662964, "step": 87000}
{"episode_reward": 2.7536585476760056, "episode": 88.0, "batch_reward": 0.006370170306880027, "critic_loss": 0.0012404352903104155, "actor_loss": -18.61770261785388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00977325439453, "step": 88000}
{"episode_reward": 3.086655899102599, "episode": 89.0, "batch_reward": 0.006159406943246722, "critic_loss": 0.00121599206850442, "actor_loss": -19.16883190257847, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.019514083862305, "step": 89000}
{"episode_reward": 2.51508586719897, "episode": 90.0, "batch_reward": 0.006085213726619259, "critic_loss": 0.0012955731031179312, "actor_loss": -18.603982411116363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.980735301971436, "step": 90000}
{"episode_reward": 2.5611216495834457, "episode": 91.0, "batch_reward": 0.006182394028175622, "critic_loss": 0.0010877006265800446, "actor_loss": -19.350251580297947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.33019018173218, "step": 91000}
{"episode_reward": 3.5310424496830906, "episode": 92.0, "batch_reward": 0.00614986840239726, "critic_loss": 0.001356338682853675, "actor_loss": -17.30638888606429, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01710534095764, "step": 92000}
{"episode_reward": 2.5982367971593545, "episode": 93.0, "batch_reward": 0.006097769538173452, "critic_loss": 0.0016974485311075113, "actor_loss": -18.603163779512048, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01206374168396, "step": 93000}
{"episode_reward": 2.866691444022743, "episode": 94.0, "batch_reward": 0.006081804943270981, "critic_loss": 0.0010277823163305584, "actor_loss": -17.55026930706203, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.988630056381226, "step": 94000}
{"episode_reward": 4.040437000231471, "episode": 95.0, "batch_reward": 0.00605002348870039, "critic_loss": 0.0012038955327807344, "actor_loss": -18.656100946635007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.020482778549194, "step": 95000}
{"episode_reward": 2.7764926872367326, "episode": 96.0, "batch_reward": 0.006026215776801109, "critic_loss": 0.0009032736121989728, "actor_loss": -17.796685043141245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.34122633934021, "step": 96000}
{"episode_reward": 3.572741854842876, "episode": 97.0, "batch_reward": 0.0059035676551284265, "critic_loss": 0.0013041974888365075, "actor_loss": -18.532002371028067, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.021074533462524, "step": 97000}
{"episode_reward": 2.5677311461280006, "episode": 98.0, "batch_reward": 0.005886392447166145, "critic_loss": 0.0007735071306342433, "actor_loss": -19.512741699576377, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.984534740447998, "step": 98000}
{"episode_reward": 2.6532759083236215, "episode": 99.0, "batch_reward": 0.005714235006133095, "critic_loss": 0.0012732256356612196, "actor_loss": -18.456529044792056, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.9944908618927, "step": 99000}
{"episode_reward": 3.4666660722668903, "episode": 100.0, "batch_reward": 0.005939178050961345, "critic_loss": 0.0008804828605861985, "actor_loss": -18.84793076366186, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.004291772842407, "step": 100000}
{"episode_reward": 3.0580734316520113, "episode": 101.0, "batch_reward": 0.0057779324100119996, "critic_loss": 0.0008982913192558044, "actor_loss": -17.59530090099573, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.38174223899841, "step": 101000}
{"episode_reward": 3.8853475816917045, "episode": 102.0, "batch_reward": 0.005706054411479272, "critic_loss": 0.0012777552874176762, "actor_loss": -18.907724402844906, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00666880607605, "step": 102000}
{"episode_reward": 2.8040028580435097, "episode": 103.0, "batch_reward": 0.0057711922258604316, "critic_loss": 0.0012214320492093976, "actor_loss": -17.849257287710905, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01990556716919, "step": 103000}
{"episode_reward": 3.1852086661693764, "episode": 104.0, "batch_reward": 0.005667060560081154, "critic_loss": 0.0010931534294286393, "actor_loss": -19.642660703748465, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.03135633468628, "step": 104000}
{"episode_reward": 2.857823241465004, "episode": 105.0, "batch_reward": 0.005543460611370392, "critic_loss": 0.0008946434551762649, "actor_loss": -17.640522211432458, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.002506494522095, "step": 105000}
{"episode_reward": 2.2490077992160717, "episode": 106.0, "batch_reward": 0.005589027536567301, "critic_loss": 0.001502501372819097, "actor_loss": -18.263570779174565, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.014845609664917, "step": 106000}
{"episode_reward": 3.7493641703124707, "episode": 107.0, "batch_reward": 0.005490473457844928, "critic_loss": 0.0010074409553708392, "actor_loss": -18.702306483998896, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.05166745185852, "step": 107000}
{"episode_reward": 2.4568317404221935, "episode": 108.0, "batch_reward": 0.005593687686487101, "critic_loss": 0.0010111035139307204, "actor_loss": -18.453065647289158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.998950958251953, "step": 108000}
{"episode_reward": 2.782393040996289, "episode": 109.0, "batch_reward": 0.005628076990135014, "critic_loss": 0.0008339486718541594, "actor_loss": -18.585402884438633, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.009676933288574, "step": 109000}
{"episode_reward": 2.8085078762076137, "episode": 110.0, "batch_reward": 0.005572351988172158, "critic_loss": 0.0009615575240095495, "actor_loss": -19.085280439838765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.030858993530273, "step": 110000}
{"episode_reward": 3.5105117471973744, "episode": 111.0, "batch_reward": 0.005501323102391325, "critic_loss": 0.0012976787401130423, "actor_loss": -17.84002345068753, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.38783049583435, "step": 111000}
{"episode_reward": 2.796525708839444, "episode": 112.0, "batch_reward": 0.005507869423599914, "critic_loss": 0.0010305172223015688, "actor_loss": -18.514185477644205, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.009934902191162, "step": 112000}
{"episode_reward": 2.6707668319061346, "episode": 113.0, "batch_reward": 0.005383350219344721, "critic_loss": 0.0009646788824829855, "actor_loss": -17.024839440107346, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00839853286743, "step": 113000}
{"episode_reward": 3.4982331752467664, "episode": 114.0, "batch_reward": 0.005430789567180909, "critic_loss": 0.0008492101819865638, "actor_loss": -18.270509166046978, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01515483856201, "step": 114000}
{"episode_reward": 2.7794648222675113, "episode": 115.0, "batch_reward": 0.005373151262756437, "critic_loss": 0.0012268258793155836, "actor_loss": -19.62192017441988, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.965935230255127, "step": 115000}
{"episode_reward": 3.484725634586674, "episode": 116.0, "batch_reward": 0.005311542621231638, "critic_loss": 0.0007484849923857837, "actor_loss": -18.312372561499476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00052762031555, "step": 116000}
{"episode_reward": 2.948855943828673, "episode": 117.0, "batch_reward": 0.005347048685187474, "critic_loss": 0.0010688435116535402, "actor_loss": -18.69626135021448, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.011412858963013, "step": 117000}
{"episode_reward": 2.9435590546937154, "episode": 118.0, "batch_reward": 0.005351295635220595, "critic_loss": 0.0007769112526257231, "actor_loss": -18.31939632794261, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.98581838607788, "step": 118000}
{"episode_reward": 3.7461963920340002, "episode": 119.0, "batch_reward": 0.005343741948017851, "critic_loss": 0.001266143205059052, "actor_loss": -18.812928103342653, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.000571250915527, "step": 119000}
{"episode_reward": 3.16103897234298, "episode": 120.0, "batch_reward": 0.0052277565486729145, "critic_loss": 0.0009406307918725361, "actor_loss": -19.061154019787907, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.014805793762207, "step": 120000}
{"episode_reward": 3.1892060233750206, "episode": 121.0, "batch_reward": 0.005393020725459791, "critic_loss": 0.0007000784956326243, "actor_loss": -19.063121172606944, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.33013367652893, "step": 121000}
{"episode_reward": 2.8048389881728353, "episode": 122.0, "batch_reward": 0.005336994138429873, "critic_loss": 0.0010598999387511868, "actor_loss": -19.74924120463431, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02833890914917, "step": 122000}
{"episode_reward": 3.00827809013796, "episode": 123.0, "batch_reward": 0.00526162766572088, "critic_loss": 0.001095305382314109, "actor_loss": -20.05629885806143, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.273801803588867, "step": 123000}
{"episode_reward": 2.313729964066078, "episode": 124.0, "batch_reward": 0.005319822847726755, "critic_loss": 0.0012989594638165727, "actor_loss": -19.428565584018827, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.046043395996094, "step": 124000}
{"episode_reward": 2.7991188525848765, "episode": 125.0, "batch_reward": 0.005139175391988829, "critic_loss": 0.0009462966374521784, "actor_loss": -19.61061839622259, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.99346685409546, "step": 125000}
{"episode_reward": 2.792249755723474, "episode": 126.0, "batch_reward": 0.005073974843369797, "critic_loss": 0.0008236020283766266, "actor_loss": -20.059889638200403, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.9582622051239, "step": 126000}
{"episode_reward": 3.0925671135631734, "episode": 127.0, "batch_reward": 0.005198817793047056, "critic_loss": 0.0008036300581916293, "actor_loss": -19.12228508362174, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.020515203475952, "step": 127000}
{"episode_reward": 3.144722498993705, "episode": 128.0, "batch_reward": 0.005169269041740336, "critic_loss": 0.000695695056932891, "actor_loss": -18.56379076898098, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.01421093940735, "step": 128000}
{"episode_reward": 2.5794485946462564, "episode": 129.0, "batch_reward": 0.0049513707734877245, "critic_loss": 0.0006723256203913479, "actor_loss": -19.232520966961978, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.969393730163574, "step": 129000}
{"episode_reward": 3.058091348929583, "episode": 130.0, "batch_reward": 0.0052529238013084975, "critic_loss": 0.0009294473318623204, "actor_loss": -18.811425628855826, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.962238311767578, "step": 130000}
{"episode_reward": 3.4536483633830013, "episode": 131.0, "batch_reward": 0.005069017761969007, "critic_loss": 0.0007529822917094861, "actor_loss": -17.44511831830442, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.353684425354004, "step": 131000}
{"episode_reward": 3.8010127639544162, "episode": 132.0, "batch_reward": 0.005205730355926789, "critic_loss": 0.0006772722157729731, "actor_loss": -18.753521730527282, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.974213123321533, "step": 132000}
{"episode_reward": 2.536137122833016, "episode": 133.0, "batch_reward": 0.005166378671652637, "critic_loss": 0.0008679641605631331, "actor_loss": -19.130012628674507, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.966758728027344, "step": 133000}
{"episode_reward": 3.784446041677738, "episode": 134.0, "batch_reward": 0.005209688027855009, "critic_loss": 0.0006869295859432895, "actor_loss": -19.24336621440947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00489616394043, "step": 134000}
{"episode_reward": 3.2681831873263225, "episode": 135.0, "batch_reward": 0.005206524808658287, "critic_loss": 0.0007626876376489235, "actor_loss": -19.745429321736097, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00711464881897, "step": 135000}
{"episode_reward": 2.695026674611741, "episode": 136.0, "batch_reward": 0.0050624931154306975, "critic_loss": 0.0005668197410996072, "actor_loss": -20.112601239040494, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 19.991251230239868, "step": 136000}
{"episode_reward": 2.831759071436228, "episode": 137.0, "batch_reward": 0.005129318365827203, "critic_loss": 0.0005424475923136924, "actor_loss": -19.74105255147815, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.009475469589233, "step": 137000}
{"episode_reward": 2.907103027376272, "episode": 138.0, "batch_reward": 0.005114614566555247, "critic_loss": 0.0008174180202804564, "actor_loss": -17.69198890711367, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.035157203674316, "step": 138000}
{"episode_reward": 2.9430135950043583, "episode": 139.0, "batch_reward": 0.004960351431975141, "critic_loss": 0.0009013357641924813, "actor_loss": -17.257329220712187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.011476278305054, "step": 139000}
{"episode_reward": 3.559961639214858, "episode": 140.0, "batch_reward": 0.0051178815125022085, "critic_loss": 0.0005897180955180374, "actor_loss": -18.19195890545845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.017126083374023, "step": 140000}
{"episode_reward": 4.234844468530447, "episode": 141.0, "batch_reward": 0.004983530503348447, "critic_loss": 0.0008445929015761067, "actor_loss": -19.61515760448575, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.361661434173584, "step": 141000}
{"episode_reward": 4.32282646239281, "episode": 142.0, "batch_reward": 0.005025241854949855, "critic_loss": 0.0009153303335951933, "actor_loss": -18.501502793848516, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00890326499939, "step": 142000}
{"episode_reward": 3.564978897986141, "episode": 143.0, "batch_reward": 0.005027074254117906, "critic_loss": 0.0008567450733753503, "actor_loss": -19.79994534429908, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.02765965461731, "step": 143000}
{"episode_reward": 3.118535345701762, "episode": 144.0, "batch_reward": 0.004998873800272122, "critic_loss": 0.0006272882577250129, "actor_loss": -18.734159023433925, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.003649711608887, "step": 144000}
{"episode_reward": 3.429438980475299, "episode": 145.0, "batch_reward": 0.004945437470450997, "critic_loss": 0.0008441514916376036, "actor_loss": -20.7884979108125, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.000430583953857, "step": 145000}
{"episode_reward": 2.859672057917149, "episode": 146.0, "batch_reward": 0.0047913320818915965, "critic_loss": 0.0006241710917529417, "actor_loss": -18.116985323727132, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.0429470539093, "step": 146000}
{"episode_reward": 2.315955114893918, "episode": 147.0, "batch_reward": 0.004908428669208661, "critic_loss": 0.0007394965114363004, "actor_loss": -19.170593926385045, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00041699409485, "step": 147000}
{"episode_reward": 3.528062046830584, "episode": 148.0, "batch_reward": 0.0049255748656578365, "critic_loss": 0.0007716290681892133, "actor_loss": -19.49124306766689, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.00589942932129, "step": 148000}
{"episode_reward": 3.0151937188257736, "episode": 149.0, "batch_reward": 0.004984460307750851, "critic_loss": 0.0007423807363738888, "actor_loss": -18.87149300326407, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.002737045288086, "step": 149000}
{"episode_reward": 2.3786596683351577, "episode": 150.0, "batch_reward": 0.004906488313339651, "critic_loss": 0.0006596641165997426, "actor_loss": -18.80181192302704, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
