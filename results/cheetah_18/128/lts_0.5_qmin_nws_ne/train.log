{"episode_reward": 0.0, "episode": 1.0, "duration": 17.280439138412476, "step": 1000}
{"episode_reward": 4.20996189079657, "episode": 2.0, "duration": 1.50276780128479, "step": 2000}
{"episode_reward": 252.2457487418515, "episode": 3.0, "batch_reward": 0.12117906032045032, "critic_loss": 0.009735442460435416, "actor_loss": -18.95683355011693, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 61.95399737358093, "step": 3000}
{"episode_reward": 7.599613748924245, "episode": 4.0, "batch_reward": 0.07689585677161813, "critic_loss": 0.004214648684835993, "actor_loss": -16.593231008768083, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.471182823181152, "step": 4000}
{"episode_reward": 4.254933663908769, "episode": 5.0, "batch_reward": 0.060817024569958446, "critic_loss": 0.00839963251852896, "actor_loss": -16.441407245635986, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.466373443603516, "step": 5000}
{"episode_reward": 8.938774139769059, "episode": 6.0, "batch_reward": 0.05159779234416783, "critic_loss": 0.009348234589910135, "actor_loss": -17.09998836183548, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.452883005142212, "step": 6000}
{"episode_reward": 7.556859804998065, "episode": 7.0, "batch_reward": 0.0449252029042691, "critic_loss": 0.0046980608849553395, "actor_loss": -17.515567900180816, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46066927909851, "step": 7000}
{"episode_reward": 6.059603669255788, "episode": 8.0, "batch_reward": 0.03962576714344323, "critic_loss": 0.007176343842758797, "actor_loss": -17.38487330031395, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.448590517044067, "step": 8000}
{"episode_reward": 6.566789119275338, "episode": 9.0, "batch_reward": 0.03546533104870468, "critic_loss": 0.005719204522087238, "actor_loss": -16.424145582437514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.4726881980896, "step": 9000}
{"episode_reward": 6.084969876926492, "episode": 10.0, "batch_reward": 0.03218028705660254, "critic_loss": 0.007028942596516572, "actor_loss": -17.58017690014839, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.499764680862427, "step": 10000}
{"episode_reward": 5.564948989614976, "episode": 11.0, "batch_reward": 0.02896733904723078, "critic_loss": 0.004701392063114327, "actor_loss": -16.300196345329283, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.793264627456665, "step": 11000}
{"episode_reward": 5.446642108037592, "episode": 12.0, "batch_reward": 0.027794846150558442, "critic_loss": 0.004487471493484918, "actor_loss": -17.890708216905594, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.449325799942017, "step": 12000}
{"episode_reward": 6.193470044792271, "episode": 13.0, "batch_reward": 0.02583420688379556, "critic_loss": 0.005587724052777048, "actor_loss": -16.848368055105208, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.440383434295654, "step": 13000}
{"episode_reward": 6.070723470393135, "episode": 14.0, "batch_reward": 0.02436082526529208, "critic_loss": 0.0037195227990450804, "actor_loss": -16.901428168535233, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.444230556488037, "step": 14000}
{"episode_reward": 6.947097451744457, "episode": 15.0, "batch_reward": 0.023494477735832332, "critic_loss": 0.005089256761129946, "actor_loss": -16.485654267311098, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46838355064392, "step": 15000}
{"episode_reward": 5.710381651558892, "episode": 16.0, "batch_reward": 0.022261373070068657, "critic_loss": 0.005256221912219189, "actor_loss": -17.674230051755906, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.447572708129883, "step": 16000}
{"episode_reward": 6.620926817589798, "episode": 17.0, "batch_reward": 0.021370601901318877, "critic_loss": 0.0031807936242548747, "actor_loss": -17.04685166501999, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.45443034172058, "step": 17000}
{"episode_reward": 5.5731784387623575, "episode": 18.0, "batch_reward": 0.020498106613755226, "critic_loss": 0.004624945463554468, "actor_loss": -17.92976324212551, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.450321435928345, "step": 18000}
{"episode_reward": 4.916202774228202, "episode": 19.0, "batch_reward": 0.019340367570985107, "critic_loss": 0.004398806267810869, "actor_loss": -16.656022487282755, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.476088523864746, "step": 19000}
{"episode_reward": 6.855292031903401, "episode": 20.0, "batch_reward": 0.01889398450125009, "critic_loss": 0.00410141969306278, "actor_loss": -17.603177719831468, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.441397428512573, "step": 20000}
{"episode_reward": 7.515618592997856, "episode": 21.0, "batch_reward": 0.017988600990734994, "critic_loss": 0.003346796428173548, "actor_loss": -16.570541143894197, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.80434250831604, "step": 21000}
{"episode_reward": 7.591430936699041, "episode": 22.0, "batch_reward": 0.01741548227518797, "critic_loss": 0.00392954791340162, "actor_loss": -17.812083961963655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.439703702926636, "step": 22000}
{"episode_reward": 5.834654786323738, "episode": 23.0, "batch_reward": 0.01721845453605056, "critic_loss": 0.003564728353769169, "actor_loss": -16.415316395759582, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.45168685913086, "step": 23000}
{"episode_reward": 5.423887921579432, "episode": 24.0, "batch_reward": 0.01650094313034788, "critic_loss": 0.003972294706254615, "actor_loss": -15.750472978830338, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.458422660827637, "step": 24000}
{"episode_reward": 5.295662979369916, "episode": 25.0, "batch_reward": 0.01570931818615645, "critic_loss": 0.0031954246239038185, "actor_loss": -17.460356279730796, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.541204690933228, "step": 25000}
{"episode_reward": 6.869140290071481, "episode": 26.0, "batch_reward": 0.01583999331574887, "critic_loss": 0.0031184425728279165, "actor_loss": -16.004770764470102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.776877403259277, "step": 26000}
{"episode_reward": 5.494626457196965, "episode": 27.0, "batch_reward": 0.015330650786869227, "critic_loss": 0.003512529829778941, "actor_loss": -16.126737021684647, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.478609323501587, "step": 27000}
{"episode_reward": 6.022101807176864, "episode": 28.0, "batch_reward": 0.015386025980580599, "critic_loss": 0.0032701203587348574, "actor_loss": -16.897854504704476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.414851427078247, "step": 28000}
{"episode_reward": 6.956746846553895, "episode": 29.0, "batch_reward": 0.014810220283456146, "critic_loss": 0.003138808500589221, "actor_loss": -16.543433992147445, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46408224105835, "step": 29000}
{"episode_reward": 4.727918058759567, "episode": 30.0, "batch_reward": 0.014552785399369895, "critic_loss": 0.0031843366356915795, "actor_loss": -15.90239754629135, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.476728439331055, "step": 30000}
{"episode_reward": 5.044318422553102, "episode": 31.0, "batch_reward": 0.014403536293189972, "critic_loss": 0.0022059480847674423, "actor_loss": -18.013782601237295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.772217988967896, "step": 31000}
{"episode_reward": 4.705037629061296, "episode": 32.0, "batch_reward": 0.014027842239011079, "critic_loss": 0.0029734842746111097, "actor_loss": -16.725391268134118, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.459566354751587, "step": 32000}
{"episode_reward": 5.819486027881393, "episode": 33.0, "batch_reward": 0.013380075810011476, "critic_loss": 0.0025334139397746184, "actor_loss": -17.139027721643448, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.487533807754517, "step": 33000}
{"episode_reward": 5.58866673928533, "episode": 34.0, "batch_reward": 0.013328174621332437, "critic_loss": 0.0023271290945849612, "actor_loss": -17.256743705630303, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.455993175506592, "step": 34000}
{"episode_reward": 7.576312104879189, "episode": 35.0, "batch_reward": 0.013297401417512447, "critic_loss": 0.0028133346012764377, "actor_loss": -16.180537983179093, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.474687576293945, "step": 35000}
{"episode_reward": 5.856064938640176, "episode": 36.0, "batch_reward": 0.012988881573081017, "critic_loss": 0.0022944289867009504, "actor_loss": -17.46952096879482, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.45912742614746, "step": 36000}
{"episode_reward": 7.34572452633843, "episode": 37.0, "batch_reward": 0.012831265506334602, "critic_loss": 0.0027346843307022935, "actor_loss": -16.74138261514902, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.467110633850098, "step": 37000}
{"episode_reward": 4.732657951203743, "episode": 38.0, "batch_reward": 0.012971297321841121, "critic_loss": 0.0018099797027098248, "actor_loss": -16.222709179222583, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.487528085708618, "step": 38000}
{"episode_reward": 5.315302430247915, "episode": 39.0, "batch_reward": 0.01247065719193779, "critic_loss": 0.002366924442096206, "actor_loss": -17.105743710696697, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.438728094100952, "step": 39000}
{"episode_reward": 6.841289289350881, "episode": 40.0, "batch_reward": 0.012415263662580401, "critic_loss": 0.0018062044500547928, "actor_loss": -18.40402835983038, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.42789125442505, "step": 40000}
{"episode_reward": 6.221029581856524, "episode": 41.0, "batch_reward": 0.012290474113542586, "critic_loss": 0.0021739719083416274, "actor_loss": -17.76696544212103, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.77858304977417, "step": 41000}
{"episode_reward": 5.400767423087219, "episode": 42.0, "batch_reward": 0.01199461137270555, "critic_loss": 0.0025616663277251064, "actor_loss": -17.180847146213054, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.470829010009766, "step": 42000}
{"episode_reward": 5.050667329972776, "episode": 43.0, "batch_reward": 0.01201234141900204, "critic_loss": 0.0017678392451343824, "actor_loss": -17.68840937054157, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.473758697509766, "step": 43000}
{"episode_reward": 7.236926125887832, "episode": 44.0, "batch_reward": 0.011738864434650168, "critic_loss": 0.0023338579003611814, "actor_loss": -16.699733010947703, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.449928760528564, "step": 44000}
{"episode_reward": 6.392540952357938, "episode": 45.0, "batch_reward": 0.011688701855484396, "critic_loss": 0.0020493013901359517, "actor_loss": -16.184860948204996, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.479592084884644, "step": 45000}
{"episode_reward": 4.879013244397206, "episode": 46.0, "batch_reward": 0.0115725518129766, "critic_loss": 0.0021602989881648682, "actor_loss": -16.386962138235567, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.479032516479492, "step": 46000}
{"episode_reward": 6.864373942202568, "episode": 47.0, "batch_reward": 0.011246966891922056, "critic_loss": 0.0024405133664695314, "actor_loss": -16.546736382961274, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46812415122986, "step": 47000}
{"episode_reward": 5.319130096940916, "episode": 48.0, "batch_reward": 0.011212107536150142, "critic_loss": 0.0017520663286850321, "actor_loss": -15.772220923483372, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.44158625602722, "step": 48000}
{"episode_reward": 6.197693211928851, "episode": 49.0, "batch_reward": 0.011219355506356806, "critic_loss": 0.001767810554767493, "actor_loss": -15.844629617929458, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.429367780685425, "step": 49000}
{"episode_reward": 6.518821472059168, "episode": 50.0, "batch_reward": 0.010987195774912834, "critic_loss": 0.001885703934727644, "actor_loss": -15.905591480314731, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.446313858032227, "step": 50000}
{"episode_reward": 5.406287413937402, "episode": 51.0, "batch_reward": 0.010820771816652268, "critic_loss": 0.0018025668186382974, "actor_loss": -16.411546977579594, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.793418407440186, "step": 51000}
{"episode_reward": 6.411876480568138, "episode": 52.0, "batch_reward": 0.01071339767612517, "critic_loss": 0.002174682939759805, "actor_loss": -16.014401844739915, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.440847635269165, "step": 52000}
{"episode_reward": 6.132700998631164, "episode": 53.0, "batch_reward": 0.010975026715081186, "critic_loss": 0.0012517238367945537, "actor_loss": -16.869744711101056, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.444624185562134, "step": 53000}
{"episode_reward": 6.1215125676824025, "episode": 54.0, "batch_reward": 0.01043415620131418, "critic_loss": 0.0018469368572550592, "actor_loss": -16.516813722550868, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.45901894569397, "step": 54000}
{"episode_reward": 6.1349389896710225, "episode": 55.0, "batch_reward": 0.010451184126315639, "critic_loss": 0.0018526859866033192, "actor_loss": -16.84688637381792, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.41632318496704, "step": 55000}
{"episode_reward": 5.516440613647352, "episode": 56.0, "batch_reward": 0.010364144681021571, "critic_loss": 0.0016258050127362367, "actor_loss": -17.68952001363039, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.459758520126343, "step": 56000}
{"episode_reward": 6.844481056307389, "episode": 57.0, "batch_reward": 0.010168003738159313, "critic_loss": 0.002281159064878011, "actor_loss": -17.02900577199459, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.45648956298828, "step": 57000}
{"episode_reward": 6.842238694453611, "episode": 58.0, "batch_reward": 0.01017707642680034, "critic_loss": 0.0017515843756991671, "actor_loss": -17.35781910979748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.435696840286255, "step": 58000}
{"episode_reward": 5.105584232036263, "episode": 59.0, "batch_reward": 0.010233381768688559, "critic_loss": 0.0018347554901920375, "actor_loss": -17.426639672636984, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.44356679916382, "step": 59000}
{"episode_reward": 5.751416596886918, "episode": 60.0, "batch_reward": 0.01022924737096764, "critic_loss": 0.002857582231219567, "actor_loss": -17.040263138711452, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.43694043159485, "step": 60000}
{"episode_reward": 5.733430042149204, "episode": 61.0, "batch_reward": 0.009908494969364256, "critic_loss": 0.00164616608027427, "actor_loss": -16.50932869106531, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.79734683036804, "step": 61000}
{"episode_reward": 6.778477500836501, "episode": 62.0, "batch_reward": 0.010073628095909953, "critic_loss": 0.0017949431132437894, "actor_loss": -15.331883242368699, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.497060537338257, "step": 62000}
{"episode_reward": 5.709738262755454, "episode": 63.0, "batch_reward": 0.00981353655247949, "critic_loss": 0.0018702372639017994, "actor_loss": -16.622437578469516, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.445823431015015, "step": 63000}
{"episode_reward": 7.042031910335044, "episode": 64.0, "batch_reward": 0.009926930412882938, "critic_loss": 0.001385259278344165, "actor_loss": -17.53193647533655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.481863021850586, "step": 64000}
{"episode_reward": 5.583374959133435, "episode": 65.0, "batch_reward": 0.009902181681944058, "critic_loss": 0.0030325296968730982, "actor_loss": -16.627390876322984, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.490169525146484, "step": 65000}
{"episode_reward": 5.644852523737964, "episode": 66.0, "batch_reward": 0.009741455625742674, "critic_loss": 0.0014264556706548319, "actor_loss": -16.881621779978275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.441720247268677, "step": 66000}
{"episode_reward": 5.789792241007475, "episode": 67.0, "batch_reward": 0.009841546828160063, "critic_loss": 0.0015649728419593885, "actor_loss": -15.96086692583561, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.458659172058105, "step": 67000}
{"episode_reward": 5.378462889584305, "episode": 68.0, "batch_reward": 0.009670003748498857, "critic_loss": 0.0015021102639766468, "actor_loss": -17.463628073394297, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.459267377853394, "step": 68000}
{"episode_reward": 5.4635845834177115, "episode": 69.0, "batch_reward": 0.009571065446361899, "critic_loss": 0.0016651962544419804, "actor_loss": -16.20944382750988, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.453755617141724, "step": 69000}
{"episode_reward": 6.139653735375486, "episode": 70.0, "batch_reward": 0.009488688718294724, "critic_loss": 0.001259915414993884, "actor_loss": -15.944085436999798, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.48227334022522, "step": 70000}
{"episode_reward": 6.690690118903533, "episode": 71.0, "batch_reward": 0.009543741397559643, "critic_loss": 0.001460676697668532, "actor_loss": -16.079530658513306, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.81292653083801, "step": 71000}
{"episode_reward": 4.871810439129225, "episode": 72.0, "batch_reward": 0.009421403508400544, "critic_loss": 0.0010352216534447508, "actor_loss": -16.665479825109244, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.440736293792725, "step": 72000}
{"episode_reward": 5.693092173464392, "episode": 73.0, "batch_reward": 0.009271670002257452, "critic_loss": 0.0009943779484237894, "actor_loss": -16.00954847496748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.49198293685913, "step": 73000}
{"episode_reward": 6.315865549662325, "episode": 74.0, "batch_reward": 0.00932436893414706, "critic_loss": 0.0014507928220336907, "actor_loss": -16.793265233159065, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.44518518447876, "step": 74000}
{"episode_reward": 6.146370828903768, "episode": 75.0, "batch_reward": 0.009279651476535946, "critic_loss": 0.0013395004870399134, "actor_loss": -17.435667394787075, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.453415870666504, "step": 75000}
{"episode_reward": 4.907879552640161, "episode": 76.0, "batch_reward": 0.009215711981058122, "critic_loss": 0.0013655235663463828, "actor_loss": -17.30353515601158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.498697996139526, "step": 76000}
{"episode_reward": 5.240265400730982, "episode": 77.0, "batch_reward": 0.009263579379301518, "critic_loss": 0.0010943875091979861, "actor_loss": -16.451046290040015, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.428648710250854, "step": 77000}
{"episode_reward": 6.080366860567237, "episode": 78.0, "batch_reward": 0.009039255624869838, "critic_loss": 0.0008652335938313626, "actor_loss": -16.448241087406874, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.47091031074524, "step": 78000}
{"episode_reward": 5.232888071239036, "episode": 79.0, "batch_reward": 0.008829948197118938, "critic_loss": 0.0015030908055487088, "actor_loss": -17.551042233794927, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46285057067871, "step": 79000}
{"episode_reward": 5.714840383622054, "episode": 80.0, "batch_reward": 0.00911875508679077, "critic_loss": 0.0010553697524192103, "actor_loss": -17.367412027448417, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.460503101348877, "step": 80000}
{"episode_reward": 4.81443373223463, "episode": 81.0, "batch_reward": 0.009037760274950415, "critic_loss": 0.0013555860345732071, "actor_loss": -17.29358328962326, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.836639642715454, "step": 81000}
{"episode_reward": 6.770602029812347, "episode": 82.0, "batch_reward": 0.00909277344821021, "critic_loss": 0.001065766711588367, "actor_loss": -17.42302350601554, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.439391374588013, "step": 82000}
{"episode_reward": 5.659924464721405, "episode": 83.0, "batch_reward": 0.008975326699437574, "critic_loss": 0.001155329621193232, "actor_loss": -16.57474936428666, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.469170093536377, "step": 83000}
{"episode_reward": 4.968812908903894, "episode": 84.0, "batch_reward": 0.008924945172853768, "critic_loss": 0.0010445690188171284, "actor_loss": -17.039837495177984, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.4760262966156, "step": 84000}
{"episode_reward": 6.653527255026282, "episode": 85.0, "batch_reward": 0.008860094530507923, "critic_loss": 0.0008405333993250679, "actor_loss": -17.095166264086963, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.44048833847046, "step": 85000}
{"episode_reward": 7.229327278778226, "episode": 86.0, "batch_reward": 0.008877090875990689, "critic_loss": 0.0009772218593971046, "actor_loss": -16.12348577979207, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.457084894180298, "step": 86000}
{"episode_reward": 5.170655634739533, "episode": 87.0, "batch_reward": 0.008949471837608143, "critic_loss": 0.0010674258465478488, "actor_loss": -16.449875691890718, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.48616862297058, "step": 87000}
{"episode_reward": 5.79268804390272, "episode": 88.0, "batch_reward": 0.008991998821729795, "critic_loss": 0.0009694640977249946, "actor_loss": -15.93429709070921, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.444303274154663, "step": 88000}
{"episode_reward": 5.64941776677913, "episode": 89.0, "batch_reward": 0.008788088319590315, "critic_loss": 0.0012212047471475672, "actor_loss": -16.514187047600746, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.48105764389038, "step": 89000}
{"episode_reward": 5.950275744850157, "episode": 90.0, "batch_reward": 0.008766114492667839, "critic_loss": 0.0010654926903480373, "actor_loss": -16.701883241295814, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.439598321914673, "step": 90000}
{"episode_reward": 5.60056152062741, "episode": 91.0, "batch_reward": 0.008833055988885462, "critic_loss": 0.000991205169211753, "actor_loss": -16.58000303861499, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.838051080703735, "step": 91000}
{"episode_reward": 6.535403602318944, "episode": 92.0, "batch_reward": 0.008764568164478988, "critic_loss": 0.0011475291355782246, "actor_loss": -15.010348066776992, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46944499015808, "step": 92000}
{"episode_reward": 5.196252243907598, "episode": 93.0, "batch_reward": 0.008776451331796125, "critic_loss": 0.0013793369003542466, "actor_loss": -16.38678003743291, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.427037239074707, "step": 93000}
{"episode_reward": 5.079234579349794, "episode": 94.0, "batch_reward": 0.008723295482806861, "critic_loss": 0.0009051950019202195, "actor_loss": -15.340467509955168, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.459195137023926, "step": 94000}
{"episode_reward": 7.223570328391744, "episode": 95.0, "batch_reward": 0.008643804855644702, "critic_loss": 0.0012645216414748574, "actor_loss": -17.162789778351783, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46431612968445, "step": 95000}
{"episode_reward": 6.595888749214059, "episode": 96.0, "batch_reward": 0.008681277290917933, "critic_loss": 0.0010237524915864924, "actor_loss": -15.011306241959334, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.695857286453247, "step": 96000}
{"episode_reward": 6.516387514786275, "episode": 97.0, "batch_reward": 0.008568290042690933, "critic_loss": 0.0013420337633069722, "actor_loss": -16.70418748897314, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.531803131103516, "step": 97000}
{"episode_reward": 5.463122010447618, "episode": 98.0, "batch_reward": 0.00853330210945569, "critic_loss": 0.0006121855265737395, "actor_loss": -17.428047625720502, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.430028676986694, "step": 98000}
{"episode_reward": 5.448239853018917, "episode": 99.0, "batch_reward": 0.00837798918550834, "critic_loss": 0.0013911937290031348, "actor_loss": -16.561823446378114, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.402228593826294, "step": 99000}
{"episode_reward": 6.418991413937243, "episode": 100.0, "batch_reward": 0.008591049395734445, "critic_loss": 0.0010390208876342513, "actor_loss": -16.325608502686023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.47525453567505, "step": 100000}
{"episode_reward": 5.273162576415595, "episode": 101.0, "batch_reward": 0.00842543762549758, "critic_loss": 0.0011093015190235746, "actor_loss": -15.199512627393007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.80254244804382, "step": 101000}
{"episode_reward": 7.861433670056358, "episode": 102.0, "batch_reward": 0.008344935138477012, "critic_loss": 0.0014884051818371518, "actor_loss": -16.38435883408785, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.467870950698853, "step": 102000}
{"episode_reward": 4.526823199472708, "episode": 103.0, "batch_reward": 0.008389216390205546, "critic_loss": 0.001253018388088094, "actor_loss": -15.144678184181451, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.50518226623535, "step": 103000}
{"episode_reward": 4.668870595177846, "episode": 104.0, "batch_reward": 0.008315386430826037, "critic_loss": 0.0010028111314713897, "actor_loss": -16.70904674693942, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.415882110595703, "step": 104000}
{"episode_reward": 5.111986115758413, "episode": 105.0, "batch_reward": 0.008222360720625148, "critic_loss": 0.0010001947483251569, "actor_loss": -15.191396109968425, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.449146509170532, "step": 105000}
{"episode_reward": 4.622714987418071, "episode": 106.0, "batch_reward": 0.008199811513535679, "critic_loss": 0.0011569262229313608, "actor_loss": -16.371964644789696, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46936273574829, "step": 106000}
{"episode_reward": 6.531408793991265, "episode": 107.0, "batch_reward": 0.008144676430616529, "critic_loss": 0.001147224661461223, "actor_loss": -17.225869970992207, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.401064157485962, "step": 107000}
{"episode_reward": 5.886685088833801, "episode": 108.0, "batch_reward": 0.008256800742587075, "critic_loss": 0.001155494511433062, "actor_loss": -16.24520038563013, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.455085039138794, "step": 108000}
{"episode_reward": 5.597194712233203, "episode": 109.0, "batch_reward": 0.008305893371114507, "critic_loss": 0.001114682698091201, "actor_loss": -16.579121183887125, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.466235160827637, "step": 109000}
{"episode_reward": 6.320606315515011, "episode": 110.0, "batch_reward": 0.008219495329773053, "critic_loss": 0.0010026978989262716, "actor_loss": -16.709972188159824, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.425020456314087, "step": 110000}
{"episode_reward": 6.248539790389066, "episode": 111.0, "batch_reward": 0.008150808587437495, "critic_loss": 0.0010742437502049141, "actor_loss": -15.661115160197019, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.859551191329956, "step": 111000}
{"episode_reward": 5.374333026168357, "episode": 112.0, "batch_reward": 0.008247448123758659, "critic_loss": 0.0009210781565743673, "actor_loss": -16.04103697834909, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46860122680664, "step": 112000}
{"episode_reward": 5.28068809062813, "episode": 113.0, "batch_reward": 0.008011180229950697, "critic_loss": 0.0011822908281246783, "actor_loss": -15.137513514444231, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.47443413734436, "step": 113000}
{"episode_reward": 5.837540237465771, "episode": 114.0, "batch_reward": 0.008072927067987621, "critic_loss": 0.0010964478907008015, "actor_loss": -15.990552702173591, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.467177391052246, "step": 114000}
{"episode_reward": 5.304608569198674, "episode": 115.0, "batch_reward": 0.00802017697528936, "critic_loss": 0.0013798752421680547, "actor_loss": -17.61715894946456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.424654722213745, "step": 115000}
{"episode_reward": 6.596205914052126, "episode": 116.0, "batch_reward": 0.007991373050259426, "critic_loss": 0.0010675907028344226, "actor_loss": -16.698460346341133, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.478235960006714, "step": 116000}
{"episode_reward": 5.602385575192146, "episode": 117.0, "batch_reward": 0.007989415721502155, "critic_loss": 0.0009551198803856096, "actor_loss": -16.768329597473144, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.463449954986572, "step": 117000}
{"episode_reward": 5.664002508680008, "episode": 118.0, "batch_reward": 0.008007252846146002, "critic_loss": 0.0008744582530271145, "actor_loss": -16.104106342092155, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.451427221298218, "step": 118000}
{"episode_reward": 6.690677742314119, "episode": 119.0, "batch_reward": 0.007977418026188388, "critic_loss": 0.0010760748980974313, "actor_loss": -16.437486952900887, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.464510202407837, "step": 119000}
{"episode_reward": 7.201626559574419, "episode": 120.0, "batch_reward": 0.007902709421236068, "critic_loss": 0.0010545822335698177, "actor_loss": -16.887680595904587, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.436527729034424, "step": 120000}
{"episode_reward": 5.608587574776617, "episode": 121.0, "batch_reward": 0.008022711785277352, "critic_loss": 0.0008234126723873487, "actor_loss": -16.565265161573887, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.75601148605347, "step": 121000}
{"episode_reward": 5.388206272902378, "episode": 122.0, "batch_reward": 0.007968075481709093, "critic_loss": 0.0011937706500539207, "actor_loss": -17.944024235129355, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.458872318267822, "step": 122000}
{"episode_reward": 4.151254330229265, "episode": 123.0, "batch_reward": 0.00793918076925911, "critic_loss": 0.00138026475060542, "actor_loss": -17.94978830394149, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46664524078369, "step": 123000}
{"episode_reward": 5.856356423204036, "episode": 124.0, "batch_reward": 0.00797599552711472, "critic_loss": 0.0012803075620686287, "actor_loss": -17.21876515673101, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.75672698020935, "step": 124000}
{"episode_reward": 5.66148257428182, "episode": 125.0, "batch_reward": 0.007839711517794059, "critic_loss": 0.0010792723593804113, "actor_loss": -17.218586587131025, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.424707412719727, "step": 125000}
{"episode_reward": 5.250755128897976, "episode": 126.0, "batch_reward": 0.007734516522148624, "critic_loss": 0.0007372589996775786, "actor_loss": -17.362161526218056, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.369436264038086, "step": 126000}
{"episode_reward": 5.851331087690592, "episode": 127.0, "batch_reward": 0.007869401338510214, "critic_loss": 0.0007093818836292485, "actor_loss": -17.225818742886187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.4040207862854, "step": 127000}
{"episode_reward": 6.120201531685378, "episode": 128.0, "batch_reward": 0.007770918512251228, "critic_loss": 0.0008520531078502245, "actor_loss": -16.227420438855887, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.44357991218567, "step": 128000}
{"episode_reward": 4.368722234597337, "episode": 129.0, "batch_reward": 0.007630581634817645, "critic_loss": 0.0009492796677805017, "actor_loss": -16.48807899206877, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.435293436050415, "step": 129000}
{"episode_reward": 6.570167911374966, "episode": 130.0, "batch_reward": 0.007931976730003953, "critic_loss": 0.0011545128526158806, "actor_loss": -17.21501669049263, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.462275505065918, "step": 130000}
{"episode_reward": 5.579539489108213, "episode": 131.0, "batch_reward": 0.007711530927335844, "critic_loss": 0.0012433284758226364, "actor_loss": -15.25798655743897, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.76576018333435, "step": 131000}
{"episode_reward": 5.6210456901668255, "episode": 132.0, "batch_reward": 0.007891655393177644, "critic_loss": 0.000992933365097997, "actor_loss": -16.312212438181042, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.450327157974243, "step": 132000}
{"episode_reward": 5.981014453086591, "episode": 133.0, "batch_reward": 0.007824526007520035, "critic_loss": 0.0009947934494412038, "actor_loss": -16.638839315339922, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.436912536621094, "step": 133000}
{"episode_reward": 5.952845761858185, "episode": 134.0, "batch_reward": 0.007837496462278068, "critic_loss": 0.0008443693232511578, "actor_loss": -16.692706626221536, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.414772272109985, "step": 134000}
{"episode_reward": 5.82104458820143, "episode": 135.0, "batch_reward": 0.007835235056467355, "critic_loss": 0.0010402853108898854, "actor_loss": -17.617141493216156, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.417591333389282, "step": 135000}
{"episode_reward": 5.131417159994014, "episode": 136.0, "batch_reward": 0.007766632949234918, "critic_loss": 0.0008714847584342351, "actor_loss": -17.887511406883597, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.468467950820923, "step": 136000}
{"episode_reward": 7.354028526149576, "episode": 137.0, "batch_reward": 0.007749891455750913, "critic_loss": 0.000988629733579728, "actor_loss": -16.30946460980177, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.44083595275879, "step": 137000}
{"episode_reward": 7.058147565441726, "episode": 138.0, "batch_reward": 0.007830129366833716, "critic_loss": 0.0010858913181546086, "actor_loss": -16.039824857458473, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46863627433777, "step": 138000}
{"episode_reward": 3.783853602661078, "episode": 139.0, "batch_reward": 0.007611059297341853, "critic_loss": 0.0016340302023600088, "actor_loss": -15.516963846728206, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.450299978256226, "step": 139000}
{"episode_reward": 6.907065693365003, "episode": 140.0, "batch_reward": 0.007799477118998766, "critic_loss": 0.0009287020477058832, "actor_loss": -15.911590451866388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.417367458343506, "step": 140000}
{"episode_reward": 6.1882312313463, "episode": 141.0, "batch_reward": 0.007634601240744814, "critic_loss": 0.0013719325976635445, "actor_loss": -17.669251040548087, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.79503560066223, "step": 141000}
{"episode_reward": 5.759610025888434, "episode": 142.0, "batch_reward": 0.007658415895421058, "critic_loss": 0.0014894531735990314, "actor_loss": -16.040357596024872, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.455565690994263, "step": 142000}
{"episode_reward": 5.6900095356563085, "episode": 143.0, "batch_reward": 0.007673953636549413, "critic_loss": 0.0011387253068824066, "actor_loss": -17.870647916033864, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.465919017791748, "step": 143000}
{"episode_reward": 5.811941181265294, "episode": 144.0, "batch_reward": 0.00764475825522095, "critic_loss": 0.0008721867149179162, "actor_loss": -16.750291018173098, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.490304946899414, "step": 144000}
{"episode_reward": 6.229021689304845, "episode": 145.0, "batch_reward": 0.007607295564608648, "critic_loss": 0.0009842187439389818, "actor_loss": -17.869327787473797, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.455089330673218, "step": 145000}
{"episode_reward": 5.817308191457033, "episode": 146.0, "batch_reward": 0.007462258144747466, "critic_loss": 0.0008142337493300147, "actor_loss": -16.42411649610102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.46355390548706, "step": 146000}
{"episode_reward": 5.802157719812456, "episode": 147.0, "batch_reward": 0.00758027600706555, "critic_loss": 0.0011525458132200584, "actor_loss": -17.279723563104866, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.49040126800537, "step": 147000}
{"episode_reward": 5.865527760461456, "episode": 148.0, "batch_reward": 0.007591870352393016, "critic_loss": 0.0009935886090170242, "actor_loss": -16.828258351117373, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.44764757156372, "step": 148000}
{"episode_reward": 5.740106780470533, "episode": 149.0, "batch_reward": 0.007619281265186146, "critic_loss": 0.0008952105028984078, "actor_loss": -15.98022336731851, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.482693433761597, "step": 149000}
{"episode_reward": 4.400751422569147, "episode": 150.0, "batch_reward": 0.007576880441047251, "critic_loss": 0.000972862532336876, "actor_loss": -16.235234185352923, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
