{"episode_reward": 0.0, "episode": 1.0, "duration": 17.33188223838806, "step": 1000}
{"episode_reward": 4.20996189079657, "episode": 2.0, "duration": 1.4918866157531738, "step": 2000}
{"episode_reward": 252.2457487418515, "episode": 3.0, "batch_reward": 0.12149277918858105, "critic_loss": 0.012215687171043658, "actor_loss": -26.23642532774905, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 61.07218337059021, "step": 3000}
{"episode_reward": 12.98753825536511, "episode": 4.0, "batch_reward": 0.07915901340916753, "critic_loss": 0.010344028562307359, "actor_loss": -22.649949056625367, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.158847332000732, "step": 4000}
{"episode_reward": 6.524136874053631, "episode": 5.0, "batch_reward": 0.06237719196639955, "critic_loss": 0.009866003010887652, "actor_loss": -21.618321779012682, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.208540439605713, "step": 5000}
{"episode_reward": 6.624484830985843, "episode": 6.0, "batch_reward": 0.052701815493404865, "critic_loss": 0.01113016234501265, "actor_loss": -20.54412230682373, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.182745933532715, "step": 6000}
{"episode_reward": 5.478924085019522, "episode": 7.0, "batch_reward": 0.04539946752972901, "critic_loss": 0.008374174310825764, "actor_loss": -20.26515542960167, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19891095161438, "step": 7000}
{"episode_reward": 4.507539213942347, "episode": 8.0, "batch_reward": 0.040347905859351156, "critic_loss": 0.007581132939667441, "actor_loss": -18.77826084756851, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.184593200683594, "step": 8000}
{"episode_reward": 9.21889792182879, "episode": 9.0, "batch_reward": 0.03598937963135541, "critic_loss": 0.0070816215052036564, "actor_loss": -18.022460369348526, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.187759399414062, "step": 9000}
{"episode_reward": 5.9091227854558745, "episode": 10.0, "batch_reward": 0.032596586815081534, "critic_loss": 0.005970368827402126, "actor_loss": -18.92822264623642, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20094108581543, "step": 10000}
{"episode_reward": 5.622100261741637, "episode": 11.0, "batch_reward": 0.029445720467716454, "critic_loss": 0.005741331829223782, "actor_loss": -17.76102940917015, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.613290786743164, "step": 11000}
{"episode_reward": 5.912073316409426, "episode": 12.0, "batch_reward": 0.028220596378669142, "critic_loss": 0.004161977736686822, "actor_loss": -18.107481689929962, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.196813821792603, "step": 12000}
{"episode_reward": 6.135080962467202, "episode": 13.0, "batch_reward": 0.02640943435812369, "critic_loss": 0.004621863965934608, "actor_loss": -18.297474728107453, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19431447982788, "step": 13000}
{"episode_reward": 6.694178769373077, "episode": 14.0, "batch_reward": 0.024941796884872018, "critic_loss": 0.004095358032558579, "actor_loss": -18.02155489063263, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.180629014968872, "step": 14000}
{"episode_reward": 7.8228131721332606, "episode": 15.0, "batch_reward": 0.023978822993580253, "critic_loss": 0.00418332134699449, "actor_loss": -17.92406212759018, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.187314987182617, "step": 15000}
{"episode_reward": 6.271066342344922, "episode": 16.0, "batch_reward": 0.022730852915905417, "critic_loss": 0.0037748955287388525, "actor_loss": -17.282726337075232, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.221763610839844, "step": 16000}
{"episode_reward": 5.596349365999239, "episode": 17.0, "batch_reward": 0.021844678229652345, "critic_loss": 0.0032775063251610846, "actor_loss": -17.63231342101097, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16912007331848, "step": 17000}
{"episode_reward": 8.077332097593574, "episode": 18.0, "batch_reward": 0.02096713189315051, "critic_loss": 0.0036087504660827106, "actor_loss": -17.271118976473808, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.17219877243042, "step": 18000}
{"episode_reward": 7.38358243081414, "episode": 19.0, "batch_reward": 0.019878401991911234, "critic_loss": 0.0029340202638413757, "actor_loss": -17.61989508384466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.219122886657715, "step": 19000}
{"episode_reward": 6.644856243554412, "episode": 20.0, "batch_reward": 0.019385844385717065, "critic_loss": 0.002668776974489447, "actor_loss": -17.569857106268405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.167028665542603, "step": 20000}
{"episode_reward": 6.060619091334582, "episode": 21.0, "batch_reward": 0.018378091195598245, "critic_loss": 0.0027560716435255017, "actor_loss": -16.445006648659707, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.65341067314148, "step": 21000}
{"episode_reward": 6.7627561438416794, "episode": 22.0, "batch_reward": 0.017757174809230492, "critic_loss": 0.0030505764624103905, "actor_loss": -17.75402980828285, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16394805908203, "step": 22000}
{"episode_reward": 5.656168871131416, "episode": 23.0, "batch_reward": 0.017801468275953084, "critic_loss": 0.003515951572393533, "actor_loss": -17.745031207740308, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.173619508743286, "step": 23000}
{"episode_reward": 9.057499898407972, "episode": 24.0, "batch_reward": 0.01699318550294265, "critic_loss": 0.003107749747054186, "actor_loss": -16.209687885046005, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.176610469818115, "step": 24000}
{"episode_reward": 6.570177568215162, "episode": 25.0, "batch_reward": 0.016325677365530283, "critic_loss": 0.0023218968610162847, "actor_loss": -17.50048188775778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.200995445251465, "step": 25000}
{"episode_reward": 8.43081058970952, "episode": 26.0, "batch_reward": 0.016542006819043307, "critic_loss": 0.003084835031651892, "actor_loss": -16.536130772829054, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.35527777671814, "step": 26000}
{"episode_reward": 9.370106908377387, "episode": 27.0, "batch_reward": 0.016105193027528004, "critic_loss": 0.002865221765358001, "actor_loss": -17.036882647693158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.470690965652466, "step": 27000}
{"episode_reward": 8.607284536363277, "episode": 28.0, "batch_reward": 0.01622119238274172, "critic_loss": 0.0023490254406351597, "actor_loss": -16.927768671393395, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.15103793144226, "step": 28000}
{"episode_reward": 8.832927311955995, "episode": 29.0, "batch_reward": 0.015610039989929646, "critic_loss": 0.0028118847565492613, "actor_loss": -16.956771271467208, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.166635036468506, "step": 29000}
{"episode_reward": 6.999859179431442, "episode": 30.0, "batch_reward": 0.015488103479146958, "critic_loss": 0.002155293202085886, "actor_loss": -16.405417057156562, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.223093032836914, "step": 30000}
{"episode_reward": 7.431747430832488, "episode": 31.0, "batch_reward": 0.015318970887921751, "critic_loss": 0.00173396098788362, "actor_loss": -16.765045055031777, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.64003872871399, "step": 31000}
{"episode_reward": 5.382187414460158, "episode": 32.0, "batch_reward": 0.01505914696585387, "critic_loss": 0.002006729175336659, "actor_loss": -16.50644392052293, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.212307929992676, "step": 32000}
{"episode_reward": 6.49704448667395, "episode": 33.0, "batch_reward": 0.014493583971867337, "critic_loss": 0.0016124435750534758, "actor_loss": -16.848884842723606, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19644045829773, "step": 33000}
{"episode_reward": 7.744097752097354, "episode": 34.0, "batch_reward": 0.014200891565298662, "critic_loss": 0.0015377623245294671, "actor_loss": -17.181131109625102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19504952430725, "step": 34000}
{"episode_reward": 5.496263105986571, "episode": 35.0, "batch_reward": 0.014082976524950936, "critic_loss": 0.0018001386044779793, "actor_loss": -16.427962002247572, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.212341785430908, "step": 35000}
{"episode_reward": 4.378051411828624, "episode": 36.0, "batch_reward": 0.013861868197564036, "critic_loss": 0.0023253803615807555, "actor_loss": -16.693555050253867, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19173264503479, "step": 36000}
{"episode_reward": 10.529313387406054, "episode": 37.0, "batch_reward": 0.013742196972481906, "critic_loss": 0.0019929671652789693, "actor_loss": -16.25926984798908, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20754313468933, "step": 37000}
{"episode_reward": 7.5345561075981555, "episode": 38.0, "batch_reward": 0.013939830512506888, "critic_loss": 0.0017544016242027282, "actor_loss": -16.71807203951478, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21143889427185, "step": 38000}
{"episode_reward": 6.7455272182309916, "episode": 39.0, "batch_reward": 0.013388754826970398, "critic_loss": 0.0018693960774398874, "actor_loss": -17.04436663082242, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.196216344833374, "step": 39000}
{"episode_reward": 5.850490189535295, "episode": 40.0, "batch_reward": 0.013239482373697683, "critic_loss": 0.001889943742120522, "actor_loss": -17.69654718288779, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20376443862915, "step": 40000}
{"episode_reward": 7.367621078230272, "episode": 41.0, "batch_reward": 0.013201824321877212, "critic_loss": 0.0018381920664396602, "actor_loss": -17.111200776845216, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.60855555534363, "step": 41000}
{"episode_reward": 8.192272824144014, "episode": 42.0, "batch_reward": 0.013050094886450097, "critic_loss": 0.002104192395549035, "actor_loss": -16.974880341947078, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.213563442230225, "step": 42000}
{"episode_reward": 6.828713571037349, "episode": 43.0, "batch_reward": 0.01290700819529593, "critic_loss": 0.001887109967850847, "actor_loss": -17.60540499767661, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.17957305908203, "step": 43000}
{"episode_reward": 8.043643258074887, "episode": 44.0, "batch_reward": 0.012799460067413747, "critic_loss": 0.0018136668037914206, "actor_loss": -16.612367107391357, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.220003128051758, "step": 44000}
{"episode_reward": 7.225168995488154, "episode": 45.0, "batch_reward": 0.012580924894660712, "critic_loss": 0.001577539557038108, "actor_loss": -15.937550170600414, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.213848114013672, "step": 45000}
{"episode_reward": 6.860091563365596, "episode": 46.0, "batch_reward": 0.012608991814777254, "critic_loss": 0.001600001954822801, "actor_loss": -16.524420112669468, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18214511871338, "step": 46000}
{"episode_reward": 8.547547331059004, "episode": 47.0, "batch_reward": 0.012422645127167925, "critic_loss": 0.001747337439272087, "actor_loss": -16.329505249276757, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18911623954773, "step": 47000}
{"episode_reward": 6.468441056058779, "episode": 48.0, "batch_reward": 0.012220626576105132, "critic_loss": 0.0015193860107392538, "actor_loss": -16.061910798877477, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.221004247665405, "step": 48000}
{"episode_reward": 7.76670044977561, "episode": 49.0, "batch_reward": 0.012084944781148806, "critic_loss": 0.0014986710762022995, "actor_loss": -16.63543858471513, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.171698331832886, "step": 49000}
{"episode_reward": 7.887064684167567, "episode": 50.0, "batch_reward": 0.012046464962419122, "critic_loss": 0.0012496741290233331, "actor_loss": -16.102199225872756, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.207485914230347, "step": 50000}
{"episode_reward": 5.774213820456237, "episode": 51.0, "batch_reward": 0.011936386090470479, "critic_loss": 0.0011306974124308908, "actor_loss": -17.46558665563166, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.64829754829407, "step": 51000}
{"episode_reward": 5.79129118429212, "episode": 52.0, "batch_reward": 0.01167421834054403, "critic_loss": 0.0013572672701266128, "actor_loss": -16.04257264649868, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.190001010894775, "step": 52000}
{"episode_reward": 5.493459735549102, "episode": 53.0, "batch_reward": 0.011846151983365416, "critic_loss": 0.0011394544172944733, "actor_loss": -16.562656788021325, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.1986563205719, "step": 53000}
{"episode_reward": 5.440529856496866, "episode": 54.0, "batch_reward": 0.01119719632063061, "critic_loss": 0.0012002003250381676, "actor_loss": -16.96448646056652, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.181427478790283, "step": 54000}
{"episode_reward": 6.210283389940844, "episode": 55.0, "batch_reward": 0.011342910105828196, "critic_loss": 0.001627257243206259, "actor_loss": -16.825253790333868, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.211613655090332, "step": 55000}
{"episode_reward": 6.922683276285877, "episode": 56.0, "batch_reward": 0.011228990149800666, "critic_loss": 0.0016384410297614522, "actor_loss": -16.914586135104297, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18418836593628, "step": 56000}
{"episode_reward": 7.650444424061986, "episode": 57.0, "batch_reward": 0.011214633095078171, "critic_loss": 0.0020438985759101344, "actor_loss": -16.395472087472676, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.16542649269104, "step": 57000}
{"episode_reward": 10.283970900481302, "episode": 58.0, "batch_reward": 0.011198696410516277, "critic_loss": 0.001921304916293593, "actor_loss": -16.725335744068026, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.205886840820312, "step": 58000}
{"episode_reward": 8.001704639489123, "episode": 59.0, "batch_reward": 0.011195153850363567, "critic_loss": 0.0018239703584113159, "actor_loss": -17.488536962166428, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.215532779693604, "step": 59000}
{"episode_reward": 7.843034944800334, "episode": 60.0, "batch_reward": 0.011231035774108023, "critic_loss": 0.0021709464743034914, "actor_loss": -16.7546705814898, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.186320304870605, "step": 60000}
{"episode_reward": 9.46178096542678, "episode": 61.0, "batch_reward": 0.010948145751142875, "critic_loss": 0.0015199983115890064, "actor_loss": -15.954052298009396, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.65595626831055, "step": 61000}
{"episode_reward": 9.399313243998712, "episode": 62.0, "batch_reward": 0.01112014686735347, "critic_loss": 0.0018771836535597685, "actor_loss": -16.118245235562323, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.187593698501587, "step": 62000}
{"episode_reward": 8.38228370980125, "episode": 63.0, "batch_reward": 0.011019716843031347, "critic_loss": 0.001792222668940667, "actor_loss": -16.103258601143956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.177065134048462, "step": 63000}
{"episode_reward": 10.022324064538356, "episode": 64.0, "batch_reward": 0.011067720809951425, "critic_loss": 0.0016938884141563903, "actor_loss": -16.92921578553319, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.198021411895752, "step": 64000}
{"episode_reward": 8.441065156969653, "episode": 65.0, "batch_reward": 0.011046481145080179, "critic_loss": 0.0019242725300136954, "actor_loss": -16.44209639006853, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20636820793152, "step": 65000}
{"episode_reward": 7.188775261987848, "episode": 66.0, "batch_reward": 0.010876736219041049, "critic_loss": 0.0017230968295771163, "actor_loss": -16.630021275579928, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.190068006515503, "step": 66000}
{"episode_reward": 8.102234082682667, "episode": 67.0, "batch_reward": 0.011016272518550977, "critic_loss": 0.0013858411752444226, "actor_loss": -16.183955038413405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20260715484619, "step": 67000}
{"episode_reward": 7.429023331040543, "episode": 68.0, "batch_reward": 0.010904481484787539, "critic_loss": 0.0015652325842238498, "actor_loss": -16.74783159983903, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.168886184692383, "step": 68000}
{"episode_reward": 6.0768611740792675, "episode": 69.0, "batch_reward": 0.010779581299750134, "critic_loss": 0.0011305903548782226, "actor_loss": -15.88128560809046, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19571328163147, "step": 69000}
{"episode_reward": 8.634570434679324, "episode": 70.0, "batch_reward": 0.010725064571946859, "critic_loss": 0.0012509711598977446, "actor_loss": -16.677365523442624, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.213917016983032, "step": 70000}
{"episode_reward": 10.149777640758202, "episode": 71.0, "batch_reward": 0.01076533137424849, "critic_loss": 0.0012512969213130417, "actor_loss": -16.482383975803852, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.643179178237915, "step": 71000}
{"episode_reward": 8.032359327976241, "episode": 72.0, "batch_reward": 0.010731991478707641, "critic_loss": 0.0012219117571657988, "actor_loss": -16.200216704934835, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20201349258423, "step": 72000}
{"episode_reward": 8.415738041518336, "episode": 73.0, "batch_reward": 0.010599331667879597, "critic_loss": 0.0012375544739043106, "actor_loss": -16.479094034001232, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.225098371505737, "step": 73000}
{"episode_reward": 8.124212082381954, "episode": 74.0, "batch_reward": 0.010706461224937812, "critic_loss": 0.0012307955617143307, "actor_loss": -15.562918007180095, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.227404356002808, "step": 74000}
{"episode_reward": 8.162571886259569, "episode": 75.0, "batch_reward": 0.010549354543210938, "critic_loss": 0.001347306694908184, "actor_loss": -17.53972976627946, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.183680057525635, "step": 75000}
{"episode_reward": 8.513396282642637, "episode": 76.0, "batch_reward": 0.010645621853182092, "critic_loss": 0.001142279246923863, "actor_loss": -16.767448329985143, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.229503870010376, "step": 76000}
{"episode_reward": 6.735375420940065, "episode": 77.0, "batch_reward": 0.010589991533663124, "critic_loss": 0.001378540490011801, "actor_loss": -16.712500637926162, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.198436975479126, "step": 77000}
{"episode_reward": 7.463904211550576, "episode": 78.0, "batch_reward": 0.010462803489994258, "critic_loss": 0.0014404850586433894, "actor_loss": -16.23349767765403, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19540786743164, "step": 78000}
{"episode_reward": 9.651818574131832, "episode": 79.0, "batch_reward": 0.01018284228281118, "critic_loss": 0.0011710293873329646, "actor_loss": -16.960986380890013, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21313214302063, "step": 79000}
{"episode_reward": 7.841525444930248, "episode": 80.0, "batch_reward": 0.010538121969439089, "critic_loss": 0.001234332702064421, "actor_loss": -16.965009148038924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.193952322006226, "step": 80000}
{"episode_reward": 6.883479046395881, "episode": 81.0, "batch_reward": 0.010443258977262303, "critic_loss": 0.0014662750770512503, "actor_loss": -16.556378469467163, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.61792325973511, "step": 81000}
{"episode_reward": 9.868466423846996, "episode": 82.0, "batch_reward": 0.010544594967970625, "critic_loss": 0.0014435607180930674, "actor_loss": -16.268097861990334, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.227853298187256, "step": 82000}
{"episode_reward": 7.733792938258599, "episode": 83.0, "batch_reward": 0.010444362781010567, "critic_loss": 0.0013476297423476353, "actor_loss": -16.681644663646818, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21268367767334, "step": 83000}
{"episode_reward": 9.356009499624168, "episode": 84.0, "batch_reward": 0.010311868626857176, "critic_loss": 0.0015410141493630363, "actor_loss": -16.640687898479403, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.200360536575317, "step": 84000}
{"episode_reward": 9.775030554791542, "episode": 85.0, "batch_reward": 0.01043262743181549, "critic_loss": 0.0012989389799186028, "actor_loss": -17.030701037362217, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.22033452987671, "step": 85000}
{"episode_reward": 8.852357086458975, "episode": 86.0, "batch_reward": 0.010259686789941042, "critic_loss": 0.001441327643900877, "actor_loss": -16.447465583220126, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19913077354431, "step": 86000}
{"episode_reward": 9.718290698319745, "episode": 87.0, "batch_reward": 0.010496935366187244, "critic_loss": 0.0016026411253551487, "actor_loss": -16.11746758778393, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.202333688735962, "step": 87000}
{"episode_reward": 9.628660166377212, "episode": 88.0, "batch_reward": 0.010554195473436266, "critic_loss": 0.0013995572204876226, "actor_loss": -16.115615609645843, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.22810959815979, "step": 88000}
{"episode_reward": 6.838649957022248, "episode": 89.0, "batch_reward": 0.010213119922671467, "critic_loss": 0.0014755975781590679, "actor_loss": -16.52294785299897, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.18405556678772, "step": 89000}
{"episode_reward": 7.140441742066417, "episode": 90.0, "batch_reward": 0.010302041093586014, "critic_loss": 0.001280977248417912, "actor_loss": -15.850756538841873, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.2164044380188, "step": 90000}
{"episode_reward": 8.071364729685445, "episode": 91.0, "batch_reward": 0.01048930392949842, "critic_loss": 0.001231329007059685, "actor_loss": -17.157631582837553, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.67156744003296, "step": 91000}
{"episode_reward": 8.17549991485308, "episode": 92.0, "batch_reward": 0.010222888150950894, "critic_loss": 0.0012791956048604333, "actor_loss": -15.889944009393453, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.219414949417114, "step": 92000}
{"episode_reward": 8.307873746252762, "episode": 93.0, "batch_reward": 0.010222816649824381, "critic_loss": 0.0015593559151311639, "actor_loss": -16.367789481967687, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.291171073913574, "step": 93000}
{"episode_reward": 8.019539175544603, "episode": 94.0, "batch_reward": 0.010172134398017079, "critic_loss": 0.001304759037171607, "actor_loss": -15.617095794640482, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.475116729736328, "step": 94000}
{"episode_reward": 7.701751134912195, "episode": 95.0, "batch_reward": 0.01023768130526878, "critic_loss": 0.00160336141064181, "actor_loss": -16.39548873246461, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.203475952148438, "step": 95000}
{"episode_reward": 11.252269074049467, "episode": 96.0, "batch_reward": 0.010236502220854163, "critic_loss": 0.0024027708730427547, "actor_loss": -16.17735631287098, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.202411890029907, "step": 96000}
{"episode_reward": 12.259622795046344, "episode": 97.0, "batch_reward": 0.010318117349175737, "critic_loss": 0.00202961697865976, "actor_loss": -16.309389778345825, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.183510303497314, "step": 97000}
{"episode_reward": 7.994590681160525, "episode": 98.0, "batch_reward": 0.010309256851905957, "critic_loss": 0.001999390914803371, "actor_loss": -16.204504425525666, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19137692451477, "step": 98000}
{"episode_reward": 10.08110493884131, "episode": 99.0, "batch_reward": 0.009992698024027049, "critic_loss": 0.0020121649845386857, "actor_loss": -16.193731229938567, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20522928237915, "step": 99000}
{"episode_reward": 11.49271703899869, "episode": 100.0, "batch_reward": 0.010259302442427724, "critic_loss": 0.0018594207796559204, "actor_loss": -16.482514171272516, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.195743322372437, "step": 100000}
{"episode_reward": 8.359061632590741, "episode": 101.0, "batch_reward": 0.010097584596369415, "critic_loss": 0.001802446061425144, "actor_loss": -15.684886077187956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.6772255897522, "step": 101000}
{"episode_reward": 10.593936256528602, "episode": 102.0, "batch_reward": 0.010095848361961543, "critic_loss": 0.0016709216763847509, "actor_loss": -16.971284345753492, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.233145713806152, "step": 102000}
{"episode_reward": 7.293345359571213, "episode": 103.0, "batch_reward": 0.010080137117067352, "critic_loss": 0.0012826524584670551, "actor_loss": -15.90096386501193, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.215863466262817, "step": 103000}
{"episode_reward": 8.83870560292128, "episode": 104.0, "batch_reward": 0.010074058126891033, "critic_loss": 0.0012921996354998556, "actor_loss": -16.971776062954216, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.188145399093628, "step": 104000}
{"episode_reward": 8.255853445276536, "episode": 105.0, "batch_reward": 0.009977367327315733, "critic_loss": 0.0010606271521101007, "actor_loss": -15.60014447831735, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.239643335342407, "step": 105000}
{"episode_reward": 5.72820240924222, "episode": 106.0, "batch_reward": 0.010074658527970314, "critic_loss": 0.0013149317454517586, "actor_loss": -16.405772341419013, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.214342832565308, "step": 106000}
{"episode_reward": 7.980481912290409, "episode": 107.0, "batch_reward": 0.009937167277559639, "critic_loss": 0.0012377017436374445, "actor_loss": -16.384029690600933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.201894998550415, "step": 107000}
{"episode_reward": 7.932244076579814, "episode": 108.0, "batch_reward": 0.010050966197857633, "critic_loss": 0.00140718229408958, "actor_loss": -16.36565643517673, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.23714542388916, "step": 108000}
{"episode_reward": 8.962210255615782, "episode": 109.0, "batch_reward": 0.010060525678563864, "critic_loss": 0.0014076717912976165, "actor_loss": -16.244558634355663, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.222050428390503, "step": 109000}
{"episode_reward": 8.753706342478496, "episode": 110.0, "batch_reward": 0.009953223006101325, "critic_loss": 0.0012778931336069946, "actor_loss": -16.340993202365937, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.205379247665405, "step": 110000}
{"episode_reward": 7.119417214649182, "episode": 111.0, "batch_reward": 0.00996015707962215, "critic_loss": 0.0011412630344566424, "actor_loss": -15.653162607565521, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.69743609428406, "step": 111000}
{"episode_reward": 6.562698750507746, "episode": 112.0, "batch_reward": 0.00999791182205081, "critic_loss": 0.0014179945020296146, "actor_loss": -16.48423751106113, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.203567028045654, "step": 112000}
{"episode_reward": 8.82467608801753, "episode": 113.0, "batch_reward": 0.009971138963941485, "critic_loss": 0.0014268278488889336, "actor_loss": -15.144921111539006, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.215617656707764, "step": 113000}
{"episode_reward": 8.243972207727143, "episode": 114.0, "batch_reward": 0.009961977115133778, "critic_loss": 0.0014897137563384603, "actor_loss": -16.02583639755845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20040249824524, "step": 114000}
{"episode_reward": 8.196509790199299, "episode": 115.0, "batch_reward": 0.009935927562415599, "critic_loss": 0.0015717281107499729, "actor_loss": -16.714650503650308, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.179917335510254, "step": 115000}
{"episode_reward": 8.54274768463378, "episode": 116.0, "batch_reward": 0.009754469576524571, "critic_loss": 0.0019699599996674806, "actor_loss": -16.288704834543168, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20293378829956, "step": 116000}
{"episode_reward": 8.11137703952109, "episode": 117.0, "batch_reward": 0.00992884476785548, "critic_loss": 0.0016782526405004319, "actor_loss": -16.021624520868063, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.248429775238037, "step": 117000}
{"episode_reward": 7.672026682945151, "episode": 118.0, "batch_reward": 0.009866977981757372, "critic_loss": 0.001534525660914369, "actor_loss": -15.917132524222136, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.193538665771484, "step": 118000}
{"episode_reward": 9.642715486612953, "episode": 119.0, "batch_reward": 0.009901976449880749, "critic_loss": 0.0015145317197602709, "actor_loss": -16.189250596106053, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.229132175445557, "step": 119000}
{"episode_reward": 9.34719074789614, "episode": 120.0, "batch_reward": 0.00973306554206647, "critic_loss": 0.001215684529801365, "actor_loss": -16.536186583917587, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.2318115234375, "step": 120000}
{"episode_reward": 5.144561101374094, "episode": 121.0, "batch_reward": 0.009852644729660824, "critic_loss": 0.0011016261846234557, "actor_loss": -16.221358356155456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.95904183387756, "step": 121000}
{"episode_reward": 6.629960471556142, "episode": 122.0, "batch_reward": 0.009738565451698378, "critic_loss": 0.0010501048502046615, "actor_loss": -16.96973524165526, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.1999728679657, "step": 122000}
{"episode_reward": 5.3960462586460745, "episode": 123.0, "batch_reward": 0.009781278133392335, "critic_loss": 0.0012072762110328767, "actor_loss": -16.955376812878995, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.180471420288086, "step": 123000}
{"episode_reward": 7.749910474605436, "episode": 124.0, "batch_reward": 0.00987194880307652, "critic_loss": 0.0012055164326739032, "actor_loss": -16.639678563594817, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.221332550048828, "step": 124000}
{"episode_reward": 7.467847686941934, "episode": 125.0, "batch_reward": 0.009521877646213397, "critic_loss": 0.001257681899907766, "actor_loss": -16.932224798120558, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.190565586090088, "step": 125000}
{"episode_reward": 8.956081038853622, "episode": 126.0, "batch_reward": 0.00956787339784205, "critic_loss": 0.0011178247844654834, "actor_loss": -17.284929440908133, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19903540611267, "step": 126000}
{"episode_reward": 8.1974971357111, "episode": 127.0, "batch_reward": 0.009613854707684368, "critic_loss": 0.0013289197972335388, "actor_loss": -16.17168161676824, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.210977792739868, "step": 127000}
{"episode_reward": 8.196232555018748, "episode": 128.0, "batch_reward": 0.009641953504411504, "critic_loss": 0.0012958810352138243, "actor_loss": -16.420128198496997, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.217973709106445, "step": 128000}
{"episode_reward": 7.39193586415667, "episode": 129.0, "batch_reward": 0.009422556575387717, "critic_loss": 0.0014680381555226632, "actor_loss": -16.589678822278977, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.170442581176758, "step": 129000}
{"episode_reward": 8.291556486118694, "episode": 130.0, "batch_reward": 0.009808514316333459, "critic_loss": 0.001951051262090914, "actor_loss": -16.42648838607222, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21887731552124, "step": 130000}
{"episode_reward": 10.417928117359272, "episode": 131.0, "batch_reward": 0.009660618778318166, "critic_loss": 0.0014897666057513562, "actor_loss": -15.184239423587918, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.69128489494324, "step": 131000}
{"episode_reward": 10.54696486382191, "episode": 132.0, "batch_reward": 0.00984855229058303, "critic_loss": 0.0013349112418363802, "actor_loss": -15.921574511364103, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.237778186798096, "step": 132000}
{"episode_reward": 7.526285558850644, "episode": 133.0, "batch_reward": 0.009651792442193255, "critic_loss": 0.0010110272496531253, "actor_loss": -16.75207662869245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.24704933166504, "step": 133000}
{"episode_reward": 6.966771911417209, "episode": 134.0, "batch_reward": 0.009757159680593758, "critic_loss": 0.0009618157859513304, "actor_loss": -16.58689129738882, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.224573373794556, "step": 134000}
{"episode_reward": 8.714834875792288, "episode": 135.0, "batch_reward": 0.009726083834189922, "critic_loss": 0.0009518638936715433, "actor_loss": -16.71595233661309, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.215790033340454, "step": 135000}
{"episode_reward": 5.145229812996297, "episode": 136.0, "batch_reward": 0.00955722595565021, "critic_loss": 0.0008029094157100189, "actor_loss": -16.95476776637137, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.190369606018066, "step": 136000}
{"episode_reward": 6.105799968942622, "episode": 137.0, "batch_reward": 0.009660046591656283, "critic_loss": 0.0008001001083539449, "actor_loss": -16.855286845095456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19274377822876, "step": 137000}
{"episode_reward": 6.155358785336612, "episode": 138.0, "batch_reward": 0.009724166297353804, "critic_loss": 0.0008979399711679434, "actor_loss": -15.572542873062194, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.19170069694519, "step": 138000}
{"episode_reward": 4.110867722180301, "episode": 139.0, "batch_reward": 0.009416272718226537, "critic_loss": 0.0009404408765840344, "actor_loss": -14.949043566081674, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.214934825897217, "step": 139000}
{"episode_reward": 8.640475092697534, "episode": 140.0, "batch_reward": 0.009646478193346412, "critic_loss": 0.0009180304526962572, "actor_loss": -15.340304612077773, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.20113754272461, "step": 140000}
{"episode_reward": 6.864298809953355, "episode": 141.0, "batch_reward": 0.009432294474449009, "critic_loss": 0.0010164251618116397, "actor_loss": -16.169825158867987, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.618308782577515, "step": 141000}
{"episode_reward": 8.118527770776272, "episode": 142.0, "batch_reward": 0.009418609255924821, "critic_loss": 0.0010374789776542456, "actor_loss": -16.044214623115955, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.217785120010376, "step": 142000}
{"episode_reward": 6.361283600636212, "episode": 143.0, "batch_reward": 0.009562383369542658, "critic_loss": 0.000987492546068097, "actor_loss": -16.953892896309494, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.17580485343933, "step": 143000}
{"episode_reward": 6.855247435489374, "episode": 144.0, "batch_reward": 0.009495590354315936, "critic_loss": 0.000940875919724931, "actor_loss": -16.168044576309622, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.196382761001587, "step": 144000}
{"episode_reward": 7.034296904934992, "episode": 145.0, "batch_reward": 0.00943046447332017, "critic_loss": 0.001030598935627495, "actor_loss": -17.600140254465863, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.223178148269653, "step": 145000}
{"episode_reward": 6.616876332859753, "episode": 146.0, "batch_reward": 0.009345610688906164, "critic_loss": 0.001462961529759923, "actor_loss": -15.744525169122964, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.21570324897766, "step": 146000}
{"episode_reward": 7.7105861273101866, "episode": 147.0, "batch_reward": 0.009389004735974595, "critic_loss": 0.0015737017261853907, "actor_loss": -16.023475415371358, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.200735569000244, "step": 147000}
{"episode_reward": 8.713187893307843, "episode": 148.0, "batch_reward": 0.009320810524746776, "critic_loss": 0.0017609928346355446, "actor_loss": -16.308693663455546, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.235517024993896, "step": 148000}
{"episode_reward": 7.794391157787264, "episode": 149.0, "batch_reward": 0.009525770008331165, "critic_loss": 0.0014503074442909565, "actor_loss": -16.551579492099584, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.221104860305786, "step": 149000}
{"episode_reward": 9.078056097068876, "episode": 150.0, "batch_reward": 0.009456603809492663, "critic_loss": 0.0013871443036769051, "actor_loss": -16.33848845165968, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
