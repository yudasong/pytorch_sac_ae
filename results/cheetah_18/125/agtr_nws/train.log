{"episode": 1.0, "duration": 18.798896074295044, "episode_reward": 5.1931688606333894, "step": 1000}
{"episode": 2.0, "duration": 1.6677844524383545, "episode_reward": 454.7514538707513, "step": 2000}
{"episode": 3.0, "batch_reward": 0.22146671936884846, "actor_loss": -46.10038691761495, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 62.62071633338928, "episode_reward": 59.65271625925752, "step": 3000}
{"episode": 4.0, "batch_reward": 0.15415650083124638, "actor_loss": -39.95254554748535, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.404200792312622, "episode_reward": 29.791851813304888, "step": 4000}
{"episode": 5.0, "batch_reward": 0.12502626284211873, "actor_loss": -38.436425590515135, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.37864065170288, "episode_reward": 31.383086402982862, "step": 5000}
{"episode": 6.0, "batch_reward": 0.10869538684934378, "actor_loss": -37.35139753723144, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.70652174949646, "episode_reward": 44.17623820828264, "step": 6000}
{"episode": 7.0, "batch_reward": 0.10019665122032166, "actor_loss": -36.79321377563477, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.895742177963257, "episode_reward": 57.450846434220054, "step": 7000}
{"episode": 8.0, "batch_reward": 0.09339971436187625, "actor_loss": -36.33461212921143, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.981050968170166, "episode_reward": 39.054128377220664, "step": 8000}
{"episode": 9.0, "batch_reward": 0.08816150989383459, "actor_loss": -35.98709992980957, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.86624526977539, "episode_reward": 55.96638423272477, "step": 9000}
{"episode": 10.0, "batch_reward": 0.08591872857138515, "actor_loss": -30.5887713470459, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 3778.4002718925476, "episode_reward": 95.38178311625221, "step": 10000}
{"episode": 11.0, "batch_reward": 0.08779751648008824, "actor_loss": -30.731414207458496, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.23347020149231, "episode_reward": 119.40991441533335, "step": 11000}
{"episode": 12.0, "batch_reward": 0.09185258033126592, "actor_loss": -27.841919227600098, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.8467345237732, "episode_reward": 119.95358622157337, "step": 12000}
{"episode": 13.0, "batch_reward": 0.09426353054121137, "actor_loss": -28.05541510772705, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.337716579437256, "episode_reward": 180.3775834935196, "step": 13000}
{"episode": 14.0, "batch_reward": 0.1025700085312128, "actor_loss": -26.57663158035278, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.07123827934265, "episode_reward": 212.57788382823972, "step": 14000}
{"episode": 15.0, "batch_reward": 0.11111428663134575, "actor_loss": -27.196652935028077, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.59871530532837, "episode_reward": 207.5546483786868, "step": 15000}
{"episode": 16.0, "batch_reward": 0.11549578879773617, "actor_loss": -26.12446496963501, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.3230345249176, "episode_reward": 166.41899606198396, "step": 16000}
{"episode": 17.0, "batch_reward": 0.11947142727673053, "actor_loss": -26.374491931915284, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.55060863494873, "episode_reward": 172.25856448706463, "step": 17000}
{"episode": 18.0, "batch_reward": 0.1217333676442504, "actor_loss": -25.99251912689209, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.62188363075256, "episode_reward": 130.35458702355953, "step": 18000}
{"episode": 19.0, "batch_reward": 0.12280473352968693, "actor_loss": -25.762033546447753, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.995367765426636, "episode_reward": 173.87064229614472, "step": 19000}
{"episode": 20.0, "batch_reward": 0.12492595270276069, "actor_loss": -25.114706092834474, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.3750021457672, "episode_reward": 111.33112150272237, "step": 20000}
{"episode": 21.0, "batch_reward": 0.125579999409616, "actor_loss": -24.942512420654296, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.916935205459595, "episode_reward": 219.43324668981109, "step": 21000}
{"episode": 22.0, "batch_reward": 0.12901131592690945, "actor_loss": -24.919052349090578, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 403.26130771636963, "episode_reward": 186.86786734072953, "step": 22000}
{"episode": 23.0, "batch_reward": 0.13070882081985474, "actor_loss": -25.174865867614745, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.757549047470093, "episode_reward": 149.27515709616827, "step": 23000}
{"episode": 24.0, "batch_reward": 0.12984385291486977, "actor_loss": -24.587502460479737, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.2370524406433, "episode_reward": 38.378825695003854, "step": 24000}
{"episode": 25.0, "batch_reward": 0.1285903003513813, "actor_loss": -24.152942333221436, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.329911708831787, "episode_reward": 187.50858560239098, "step": 25000}
{"episode": 26.0, "batch_reward": 0.13042583323270082, "actor_loss": -23.81340224456787, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.93511605262756, "episode_reward": 105.78118076354536, "step": 26000}
{"episode": 27.0, "batch_reward": 0.1277801078632474, "actor_loss": -23.08130408859253, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.061954021453857, "episode_reward": 37.75580250284435, "step": 27000}
{"episode": 28.0, "batch_reward": 0.1265178354680538, "actor_loss": -22.283161651611326, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.5114529132843, "episode_reward": 111.6158325360206, "step": 28000}
{"episode": 29.0, "batch_reward": 0.12755055522173644, "actor_loss": -22.089493144989014, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.768458366394043, "episode_reward": 190.29295107757093, "step": 29000}
{"episode": 30.0, "batch_reward": 0.1287229667827487, "actor_loss": -21.7988796043396, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.66699385643005, "episode_reward": 187.69936931022178, "step": 30000}
{"episode": 31.0, "batch_reward": 0.13120357639342548, "actor_loss": -21.886481758117675, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.70665431022644, "episode_reward": 194.44634741573415, "step": 31000}
{"episode": 32.0, "batch_reward": 0.13105752914398908, "actor_loss": -21.363240421295167, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.692391872406, "episode_reward": 79.29104274239509, "step": 32000}
{"episode": 33.0, "batch_reward": 0.13176444364339113, "actor_loss": -21.193078838348388, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.46498394012451, "episode_reward": 248.56702133908937, "step": 33000}
{"episode": 34.0, "batch_reward": 0.13523432978242636, "actor_loss": -21.75813157272339, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.04571747779846, "episode_reward": 277.6254417821641, "step": 34000}
{"episode": 35.0, "batch_reward": 0.13809080983698369, "actor_loss": -22.03853776550293, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.669616222381592, "episode_reward": 123.16766179172987, "step": 35000}
{"episode": 36.0, "batch_reward": 0.13585586054623128, "actor_loss": -21.009175853729246, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.60036301612854, "episode_reward": 58.768486436928455, "step": 36000}
{"episode": 37.0, "batch_reward": 0.13664276452362537, "actor_loss": -20.928404258728026, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.424282789230347, "episode_reward": 251.33248087184285, "step": 37000}
{"episode": 38.0, "batch_reward": 0.13825591823458672, "actor_loss": -20.8382706489563, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 419.1167252063751, "episode_reward": 102.75693139604039, "step": 38000}
{"episode": 39.0, "batch_reward": 0.13909832373261452, "actor_loss": -20.74659965133667, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.456247329711914, "episode_reward": 271.7660279595893, "step": 39000}
{"episode": 40.0, "batch_reward": 0.1419052767381072, "actor_loss": -20.50374917602539, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.3357789516449, "episode_reward": 190.6888793522188, "step": 40000}
{"episode": 41.0, "batch_reward": 0.1434905727878213, "actor_loss": -20.561730840682984, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 32.05552434921265, "episode_reward": 264.67806378363105, "step": 41000}
{"episode": 42.0, "batch_reward": 0.14692165802419185, "actor_loss": -20.88402795791626, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.7003593444824, "episode_reward": 325.2784742249117, "step": 42000}
{"episode": 43.0, "batch_reward": 0.1496025534644723, "actor_loss": -21.10904023361206, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.35048770904541, "episode_reward": 96.8745569698125, "step": 43000}
{"episode": 44.0, "batch_reward": 0.14814368676394224, "actor_loss": -20.314444429397582, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.3720293045044, "episode_reward": 103.27189019342724, "step": 44000}
{"episode": 45.0, "batch_reward": 0.14674429804086686, "actor_loss": -19.941197439193726, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.709765911102295, "episode_reward": 61.55883333881282, "step": 45000}
{"episode": 46.0, "batch_reward": 0.14641365744173526, "actor_loss": -19.487964962005616, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 419.1022238731384, "episode_reward": 290.98675434127614, "step": 46000}
{"episode": 47.0, "batch_reward": 0.15007456534355879, "actor_loss": -19.750670497894287, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.194287300109863, "episode_reward": 200.96171004735004, "step": 47000}
{"episode": 48.0, "batch_reward": 0.14971647656708956, "actor_loss": -19.520096832275392, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 405.3640594482422, "episode_reward": 97.35612876010556, "step": 48000}
{"episode": 49.0, "batch_reward": 0.150167093783617, "actor_loss": -19.547983892440797, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.38364863395691, "episode_reward": 225.08016406469838, "step": 49000}
{"episode": 50.0, "batch_reward": 0.1511437651142478, "actor_loss": -19.58143565559387, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.9075481891632, "episode_reward": 258.28891994583967, "step": 50000}
{"episode": 51.0, "batch_reward": 0.15406142619997262, "actor_loss": -19.958864040374756, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.199310302734375, "episode_reward": 320.7330895041809, "step": 51000}
{"episode": 52.0, "batch_reward": 0.15696989250183105, "actor_loss": -20.096455638885498, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.4783651828766, "episode_reward": 338.2233685574913, "step": 52000}
{"episode": 53.0, "batch_reward": 0.16075142645835877, "actor_loss": -20.435576740264892, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.732645988464355, "episode_reward": 328.9825555292235, "step": 53000}
{"episode": 54.0, "batch_reward": 0.16279739977419377, "actor_loss": -20.663789882659913, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 408.04470562934875, "episode_reward": 231.0178322924703, "step": 54000}
{"episode": 55.0, "batch_reward": 0.16300672444701195, "actor_loss": -20.741115228652955, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.348646640777588, "episode_reward": 96.73481333386923, "step": 55000}
{"episode": 56.0, "batch_reward": 0.1634320025295019, "actor_loss": -20.291820392608642, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.3591425418854, "episode_reward": 258.4247743889872, "step": 56000}
{"episode": 57.0, "batch_reward": 0.16452476219832898, "actor_loss": -20.372997190475463, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.470543146133423, "episode_reward": 116.20621090423089, "step": 57000}
{"episode": 58.0, "batch_reward": 0.16511607459932565, "actor_loss": -20.0013804397583, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 409.88517904281616, "episode_reward": 338.8257956204036, "step": 58000}
{"episode": 59.0, "batch_reward": 0.16889574250578882, "actor_loss": -20.416569999694826, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.57644248008728, "episode_reward": 343.2331907282935, "step": 59000}
{"episode": 60.0, "batch_reward": 0.17088729966431856, "actor_loss": -20.316816324234008, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.24980545043945, "episode_reward": 238.78309339976263, "step": 60000}
{"episode": 61.0, "batch_reward": 0.1715490606725216, "actor_loss": -20.534403732299804, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 32.95830488204956, "episode_reward": 407.4824680322594, "step": 61000}
{"episode": 62.0, "batch_reward": 0.17527418260276317, "actor_loss": -20.54962306213379, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.9991488456726, "episode_reward": 231.33227710811798, "step": 62000}
{"episode": 63.0, "batch_reward": 0.1743999712318182, "actor_loss": -20.482880838394166, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.944795846939087, "episode_reward": 152.79676702066763, "step": 63000}
{"episode": 64.0, "batch_reward": 0.17486640835553408, "actor_loss": -19.761704456329344, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.76143407821655, "episode_reward": 103.90045708458555, "step": 64000}
{"episode": 65.0, "batch_reward": 0.17488451355695725, "actor_loss": -19.72057603645325, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.932792901992798, "episode_reward": 392.1620418674009, "step": 65000}
{"episode": 66.0, "batch_reward": 0.17859982454776763, "actor_loss": -20.25858058166504, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.2369804382324, "episode_reward": 406.08855516175254, "step": 66000}
{"episode": 67.0, "batch_reward": 0.18076407074928283, "actor_loss": -20.375494859695433, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.801930904388428, "episode_reward": 220.93672345692593, "step": 67000}
{"episode": 68.0, "batch_reward": 0.18226981268823148, "actor_loss": -20.404912521362306, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.57415199279785, "episode_reward": 219.74404333678913, "step": 68000}
{"episode": 69.0, "batch_reward": 0.182104124635458, "actor_loss": -20.272255504608154, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.47202205657959, "episode_reward": 427.849721033025, "step": 69000}
{"episode": 70.0, "batch_reward": 0.1872657526731491, "actor_loss": -20.84472269439697, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.1108410358429, "episode_reward": 421.6366324056434, "step": 70000}
{"episode": 71.0, "batch_reward": 0.18803417178988457, "actor_loss": -20.865746646881103, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.296133041381836, "episode_reward": 43.021810213740174, "step": 71000}
{"episode": 72.0, "batch_reward": 0.18836071041226388, "actor_loss": -20.659190925598146, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.1193001270294, "episode_reward": 377.8129396293744, "step": 72000}
{"episode": 73.0, "batch_reward": 0.19150954321026803, "actor_loss": -20.96035647392273, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.60854744911194, "episode_reward": 363.69805180105783, "step": 73000}
{"episode": 74.0, "batch_reward": 0.19193604776263237, "actor_loss": -21.052002851486208, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.2077753543854, "episode_reward": 392.67257167661205, "step": 74000}
{"episode": 75.0, "batch_reward": 0.19537636823952198, "actor_loss": -21.33580954360962, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.17103362083435, "episode_reward": 333.60383211645217, "step": 75000}
{"episode": 76.0, "batch_reward": 0.1964336851090193, "actor_loss": -21.635778469085693, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.8796091079712, "episode_reward": 383.88182493416684, "step": 76000}
{"episode": 77.0, "batch_reward": 0.20024359093606472, "actor_loss": -21.925130046844483, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.489784240722656, "episode_reward": 312.98352017875413, "step": 77000}
{"episode": 78.0, "batch_reward": 0.20034353551268577, "actor_loss": -21.929672492980956, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.011757850647, "episode_reward": 334.33037848141794, "step": 78000}
{"episode": 79.0, "batch_reward": 0.20166142413020133, "actor_loss": -22.064438842773438, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.064743280410767, "episode_reward": 249.24222866530474, "step": 79000}
{"episode": 80.0, "batch_reward": 0.2031298219114542, "actor_loss": -22.173761116027833, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.1220154762268, "episode_reward": 240.67748097251643, "step": 80000}
{"episode": 81.0, "batch_reward": 0.20324304251372813, "actor_loss": -21.991597873687745, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.5591082572937, "episode_reward": 108.35748550295206, "step": 81000}
{"episode": 82.0, "batch_reward": 0.20322270248830318, "actor_loss": -21.7687780380249, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.7724380493164, "episode_reward": 411.5927638650066, "step": 82000}
{"episode": 83.0, "batch_reward": 0.20599523355066776, "actor_loss": -22.01733707809448, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.51506233215332, "episode_reward": 325.8762412865524, "step": 83000}
{"episode": 84.0, "batch_reward": 0.20647964000701904, "actor_loss": -22.102838779449463, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.70122385025024, "episode_reward": 183.66720643624882, "step": 84000}
{"episode": 85.0, "batch_reward": 0.20585784661769868, "actor_loss": -21.966589282989503, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.385356426239014, "episode_reward": 181.14760260305388, "step": 85000}
{"episode": 86.0, "batch_reward": 0.2065614826232195, "actor_loss": -21.57301517868042, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.36308193206787, "episode_reward": 401.9764219084027, "step": 86000}
{"episode": 87.0, "batch_reward": 0.208142585799098, "actor_loss": -21.870779262542726, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.07573914527893, "episode_reward": 288.82118760330803, "step": 87000}
{"episode": 88.0, "batch_reward": 0.2103877584785223, "actor_loss": -22.044416385650635, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.9520423412323, "episode_reward": 281.2272702216739, "step": 88000}
{"episode": 89.0, "batch_reward": 0.2093420635908842, "actor_loss": -22.02429883956909, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.37335515022278, "episode_reward": 352.70620877246176, "step": 89000}
{"episode": 90.0, "batch_reward": 0.21266554418206216, "actor_loss": -22.21382885169983, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.0987641811371, "episode_reward": 404.01759303651806, "step": 90000}
{"episode": 91.0, "batch_reward": 0.21325337831676006, "actor_loss": -22.122714206695555, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.121482610702515, "episode_reward": 130.16242740586725, "step": 91000}
{"episode": 92.0, "batch_reward": 0.2132364948987961, "actor_loss": -22.088945011138915, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 404.16652727127075, "episode_reward": 375.821310006373, "step": 92000}
{"episode": 93.0, "batch_reward": 0.2145162584334612, "actor_loss": -22.318007396697997, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.722364902496338, "episode_reward": 425.1297148867801, "step": 93000}
{"episode": 94.0, "batch_reward": 0.2168722808212042, "actor_loss": -22.608184757232667, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.541139125824, "episode_reward": 380.12441456791413, "step": 94000}
{"episode": 95.0, "batch_reward": 0.2187472491711378, "actor_loss": -22.817042072296143, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.832788944244385, "episode_reward": 396.1232453915164, "step": 95000}
{"episode": 96.0, "batch_reward": 0.22128230388462544, "actor_loss": -23.285320034027098, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.8195071220398, "episode_reward": 401.1527542851723, "step": 96000}
{"episode": 97.0, "batch_reward": 0.2221629930883646, "actor_loss": -23.43868018722534, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.30407476425171, "episode_reward": 370.59469728837786, "step": 97000}
{"episode": 98.0, "batch_reward": 0.22480255225300788, "actor_loss": -23.506262088775635, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 409.3857705593109, "episode_reward": 350.4020730163567, "step": 98000}
{"episode": 99.0, "batch_reward": 0.22410308492183686, "actor_loss": -23.42603489303589, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.566831588745117, "episode_reward": 256.8973609145778, "step": 99000}
{"episode": 100.0, "batch_reward": 0.22434630694985389, "actor_loss": -23.356178382873534, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.3287832736969, "episode_reward": 373.6855328550047, "step": 100000}
{"episode": 101.0, "batch_reward": 0.22644512256979943, "actor_loss": -23.76445756149292, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.26345872879028, "episode_reward": 394.46439511096696, "step": 101000}
{"episode": 102.0, "batch_reward": 0.22941109128296375, "actor_loss": -23.89051596069336, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.5965316295624, "episode_reward": 364.8790176158451, "step": 102000}
{"episode": 103.0, "batch_reward": 0.23041058260202407, "actor_loss": -23.835290176391602, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.744744300842285, "episode_reward": 392.22043684618563, "step": 103000}
{"episode": 104.0, "batch_reward": 0.2310568518489599, "actor_loss": -23.97095248031616, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.39800333976746, "episode_reward": 379.0020121413903, "step": 104000}
{"episode": 105.0, "batch_reward": 0.23236817499995233, "actor_loss": -24.216104995727537, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.981677293777466, "episode_reward": 413.4566694287094, "step": 105000}
{"episode": 106.0, "batch_reward": 0.23442592319846153, "actor_loss": -24.446484699249268, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 409.5901165008545, "episode_reward": 429.9277045203293, "step": 106000}
{"episode": 107.0, "batch_reward": 0.23639108948409557, "actor_loss": -24.627973316192627, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.959342956542969, "episode_reward": 242.76119480503854, "step": 107000}
{"episode": 108.0, "batch_reward": 0.23817876587808132, "actor_loss": -24.72470240020752, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.46517062187195, "episode_reward": 421.5694253276458, "step": 108000}
{"episode": 109.0, "batch_reward": 0.23911679592728616, "actor_loss": -24.724048526763916, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.298079013824463, "episode_reward": 409.0351488253126, "step": 109000}
{"episode": 110.0, "batch_reward": 0.24077876223623754, "actor_loss": -24.856162521362304, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.2675139904022, "episode_reward": 432.27142604219756, "step": 110000}
{"episode": 111.0, "batch_reward": 0.24210294984281064, "actor_loss": -25.096214561462403, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 32.21400833129883, "episode_reward": 430.89876007803105, "step": 111000}
{"episode": 112.0, "batch_reward": 0.24299491207301616, "actor_loss": -25.15926946258545, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.5413248538971, "episode_reward": 408.1097596434762, "step": 112000}
{"episode": 113.0, "batch_reward": 0.2449922303855419, "actor_loss": -25.3461856842041, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.344965934753418, "episode_reward": 353.6874386966829, "step": 113000}
{"episode": 114.0, "batch_reward": 0.24608867035806178, "actor_loss": -25.728492584228515, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.040176153183, "episode_reward": 404.94822168107953, "step": 114000}
{"episode": 115.0, "batch_reward": 0.24738552182912826, "actor_loss": -25.859970569610596, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.456000566482544, "episode_reward": 440.9348792688018, "step": 115000}
{"episode": 116.0, "batch_reward": 0.248660380423069, "actor_loss": -26.049515106201174, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.52070474624634, "episode_reward": 419.8061989612365, "step": 116000}
{"episode": 117.0, "batch_reward": 0.25019417764246465, "actor_loss": -26.27594673538208, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.044735431671143, "episode_reward": 366.7250110002994, "step": 117000}
{"episode": 118.0, "batch_reward": 0.251581662684679, "actor_loss": -26.41050336074829, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.9357933998108, "episode_reward": 358.4237775063583, "step": 118000}
{"episode": 119.0, "batch_reward": 0.25295526322722434, "actor_loss": -26.53987656021118, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.02836275100708, "episode_reward": 376.65489070376157, "step": 119000}
{"episode": 120.0, "batch_reward": 0.253279398471117, "actor_loss": -26.834178260803224, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 408.7456018924713, "episode_reward": 407.33501052409446, "step": 120000}
{"episode": 121.0, "batch_reward": 0.25491848129034045, "actor_loss": -27.036907928466796, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.695858001708984, "episode_reward": 334.61053537831424, "step": 121000}
{"episode": 122.0, "batch_reward": 0.25465974448621276, "actor_loss": -26.70735153579712, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.855033159256, "episode_reward": 419.0368058093227, "step": 122000}
{"episode": 123.0, "batch_reward": 0.25629423202574253, "actor_loss": -26.982270572662355, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.06745958328247, "episode_reward": 359.38621419661473, "step": 123000}
{"episode": 124.0, "batch_reward": 0.25730688504874705, "actor_loss": -27.071583278656007, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.45106172561646, "episode_reward": 103.60499782313799, "step": 124000}
{"episode": 125.0, "batch_reward": 0.25653672820329665, "actor_loss": -27.127693084716796, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.394177198410034, "episode_reward": 449.2893172669092, "step": 125000}
{"episode": 126.0, "batch_reward": 0.2566217630952597, "actor_loss": -27.010941410064696, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.37454867362976, "episode_reward": 92.49594482217186, "step": 126000}
{"episode": 127.0, "batch_reward": 0.25703792133927345, "actor_loss": -27.051912494659422, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.361215591430664, "episode_reward": 401.74478073320114, "step": 127000}
{"episode": 128.0, "batch_reward": 0.2572871757745743, "actor_loss": -26.804377956390383, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.3274230957031, "episode_reward": 382.84142195127254, "step": 128000}
{"episode": 129.0, "batch_reward": 0.2587830476462841, "actor_loss": -27.037088481903076, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.593674659729004, "episode_reward": 418.9116457016854, "step": 129000}
{"episode": 130.0, "batch_reward": 0.26003751385211943, "actor_loss": -27.545553070068358, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.7192623615265, "episode_reward": 439.28151854265866, "step": 130000}
{"episode": 131.0, "batch_reward": 0.26086396522819993, "actor_loss": -27.54698712158203, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.63299202919006, "episode_reward": 471.0030504353927, "step": 131000}
{"episode": 132.0, "batch_reward": 0.26323525214195254, "actor_loss": -27.866094673156738, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.1272506713867, "episode_reward": 425.48101071493187, "step": 132000}
{"episode": 133.0, "batch_reward": 0.2638390913903713, "actor_loss": -27.946645950317382, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.696324348449707, "episode_reward": 439.7264928383754, "step": 133000}
{"episode": 134.0, "batch_reward": 0.265494276791811, "actor_loss": -27.9609602432251, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.2295904159546, "episode_reward": 336.5840929685957, "step": 134000}
{"episode": 135.0, "batch_reward": 0.2660987707674503, "actor_loss": -28.011402500152588, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.246203660964966, "episode_reward": 384.3651933115453, "step": 135000}
{"episode": 136.0, "batch_reward": 0.26675632928311827, "actor_loss": -28.222298858642578, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.65292382240295, "episode_reward": 408.41630901414857, "step": 136000}
{"episode": 137.0, "batch_reward": 0.26736362008750436, "actor_loss": -28.27704793167114, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.71144413948059, "episode_reward": 372.4935143605654, "step": 137000}
{"episode": 138.0, "batch_reward": 0.2683150790035725, "actor_loss": -28.53245358657837, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.7501344680786, "episode_reward": 397.61257984808833, "step": 138000}
{"episode": 139.0, "batch_reward": 0.2689880874305964, "actor_loss": -28.61010308456421, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.3458468914032, "episode_reward": 373.60704011483335, "step": 139000}
{"episode": 140.0, "batch_reward": 0.2697530196458101, "actor_loss": -28.570729782104493, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.3320562839508, "episode_reward": 399.8169406926611, "step": 140000}
{"episode": 141.0, "batch_reward": 0.27088688233494757, "actor_loss": -28.746397117614745, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.44792103767395, "episode_reward": 440.9699767320307, "step": 141000}
{"episode": 142.0, "batch_reward": 0.27097750240564344, "actor_loss": -29.143620635986327, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 406.3969898223877, "episode_reward": 429.82954967630974, "step": 142000}
{"episode": 143.0, "batch_reward": 0.27330885601043703, "actor_loss": -29.284668350219725, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.231407165527344, "episode_reward": 374.90637639537874, "step": 143000}
{"episode": 144.0, "batch_reward": 0.27305722181499004, "actor_loss": -29.16834955596924, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 405.5156252384186, "episode_reward": 270.109094888968, "step": 144000}
{"episode": 145.0, "batch_reward": 0.2748354014158249, "actor_loss": -29.519847930908202, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.119496822357178, "episode_reward": 417.42007773644923, "step": 145000}
{"episode": 146.0, "batch_reward": 0.27511532881855966, "actor_loss": -29.592007984161377, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 416.2931752204895, "episode_reward": 417.4983736432747, "step": 146000}
{"episode": 147.0, "batch_reward": 0.27663705784082415, "actor_loss": -29.648682861328126, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.56039047241211, "episode_reward": 419.2781961943781, "step": 147000}
{"episode": 148.0, "batch_reward": 0.2769756834059954, "actor_loss": -29.169777717590332, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 406.8288209438324, "episode_reward": 365.24523980270783, "step": 148000}
{"episode": 149.0, "batch_reward": 0.2764961949288845, "actor_loss": -29.080065258026124, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.917006492614746, "episode_reward": 418.2130222355107, "step": 149000}
{"episode": 150.0, "batch_reward": 0.2785019576251507, "actor_loss": -29.467158222198485, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "step": 150000}
