{"episode_reward": 0.0, "episode": 1.0, "duration": 20.22325372695923, "step": 1000}
{"episode_reward": 5.1931688606333894, "episode": 2.0, "duration": 1.5170624256134033, "step": 2000}
{"episode_reward": 454.7514538707513, "episode": 3.0, "batch_reward": 0.21850332719121388, "critic_loss": 0.07672039333764474, "actor_loss": -30.872048428609354, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 66.90546321868896, "step": 3000}
{"episode_reward": 28.516356934550014, "episode": 4.0, "batch_reward": 0.1521762242615223, "critic_loss": 0.08480240377411247, "actor_loss": -17.841085018634796, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.59660792350769, "step": 4000}
{"episode_reward": 90.67205640998023, "episode": 5.0, "batch_reward": 0.13486959893256426, "critic_loss": 0.10202291762828827, "actor_loss": -15.940642190933227, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.88451361656189, "step": 5000}
{"episode_reward": 40.7295605767502, "episode": 6.0, "batch_reward": 0.11316828902810812, "critic_loss": 0.06614927231706678, "actor_loss": -16.843784834861754, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.26235556602478, "step": 6000}
{"episode_reward": 9.259602306788599, "episode": 7.0, "batch_reward": 0.09717747201025485, "critic_loss": 0.06564389791153372, "actor_loss": -20.356983496189116, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.207777738571167, "step": 7000}
{"episode_reward": 12.906226625525365, "episode": 8.0, "batch_reward": 0.08776748309656977, "critic_loss": 0.06767550764605403, "actor_loss": -19.358021193027497, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.757280826568604, "step": 8000}
{"episode_reward": 29.855080789699084, "episode": 9.0, "batch_reward": 0.08116123592108489, "critic_loss": 0.0819834492392838, "actor_loss": -19.42067554855347, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.165748834609985, "step": 9000}
{"episode_reward": 53.29429157376025, "episode": 10.0, "batch_reward": 0.08167174408957362, "critic_loss": 0.10531301069632172, "actor_loss": -19.86195070362091, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.098337411880493, "step": 10000}
{"episode_reward": 91.49314044489392, "episode": 11.0, "batch_reward": 0.08259817046672106, "critic_loss": 0.1363819557763636, "actor_loss": -20.661059687137605, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.88807773590088, "step": 11000}
{"episode_reward": 93.17865527212142, "episode": 12.0, "batch_reward": 0.08501744980737568, "critic_loss": 0.18724816703796388, "actor_loss": -20.228394164085387, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.982261180877686, "step": 12000}
{"episode_reward": 153.89022711458958, "episode": 13.0, "batch_reward": 0.08708202904090286, "critic_loss": 0.19816273667663337, "actor_loss": -20.676633494734766, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.455982446670532, "step": 13000}
{"episode_reward": 30.42774939639868, "episode": 14.0, "batch_reward": 0.08705259686335921, "critic_loss": 0.18650032457709312, "actor_loss": -19.456393846035002, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.211912393569946, "step": 14000}
{"episode_reward": 191.4604598303673, "episode": 15.0, "batch_reward": 0.09455004857853055, "critic_loss": 0.21163173304498195, "actor_loss": -23.194869520515205, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.21691608428955, "step": 15000}
{"episode_reward": 194.49081216186585, "episode": 16.0, "batch_reward": 0.1019771076887846, "critic_loss": 0.22634646640717984, "actor_loss": -22.69858783058822, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.055370807647705, "step": 16000}
{"episode_reward": 215.33938633012792, "episode": 17.0, "batch_reward": 0.10917654484510422, "critic_loss": 0.22041952781379223, "actor_loss": -22.425484167546035, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.399495840072632, "step": 17000}
{"episode_reward": 163.3280700220721, "episode": 18.0, "batch_reward": 0.11114298027008772, "critic_loss": 0.21254261261224747, "actor_loss": -22.67359961748123, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.067095041275024, "step": 18000}
{"episode_reward": 109.70099666949712, "episode": 19.0, "batch_reward": 0.11244431667029858, "critic_loss": 0.23039627350866795, "actor_loss": -22.55806435096264, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.45151972770691, "step": 19000}
{"episode_reward": 203.96924003471736, "episode": 20.0, "batch_reward": 0.11344555231183767, "critic_loss": 0.25004780101031066, "actor_loss": -23.55645811367035, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.566311359405518, "step": 20000}
{"episode_reward": 60.680884718263755, "episode": 21.0, "batch_reward": 0.11033352240920066, "critic_loss": 0.2102465335726738, "actor_loss": -23.171819896221162, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 37.383400440216064, "step": 21000}
{"episode_reward": 29.784440510068436, "episode": 22.0, "batch_reward": 0.11026624879986048, "critic_loss": 0.19788999381661415, "actor_loss": -22.56940300798416, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.85013747215271, "step": 22000}
{"episode_reward": 190.48022776060952, "episode": 23.0, "batch_reward": 0.1150685232579708, "critic_loss": 0.21292864578962326, "actor_loss": -23.7202528090477, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.99967122077942, "step": 23000}
{"episode_reward": 201.81956651920592, "episode": 24.0, "batch_reward": 0.11508980172127485, "critic_loss": 0.2257139524295926, "actor_loss": -23.703250174045564, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.374632835388184, "step": 24000}
{"episode_reward": 67.91896337783986, "episode": 25.0, "batch_reward": 0.11221469697356225, "critic_loss": 0.24467611453682184, "actor_loss": -22.3143916516304, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.72738814353943, "step": 25000}
{"episode_reward": 36.06524052458451, "episode": 26.0, "batch_reward": 0.11088106393069029, "critic_loss": 0.24294511757045983, "actor_loss": -22.477830682754515, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.448972940444946, "step": 26000}
{"episode_reward": 102.08483960370853, "episode": 27.0, "batch_reward": 0.11349610321968794, "critic_loss": 0.2461079310849309, "actor_loss": -22.766330795764922, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.723620891571045, "step": 27000}
{"episode_reward": 232.4830227420548, "episode": 28.0, "batch_reward": 0.11635602042078971, "critic_loss": 0.3274686519354582, "actor_loss": -22.44488230609894, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.915051698684692, "step": 28000}
{"episode_reward": 142.14532647318842, "episode": 29.0, "batch_reward": 0.11793094661831856, "critic_loss": 0.2841390233933926, "actor_loss": -23.255731169223786, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.457396984100342, "step": 29000}
{"episode_reward": 247.06536499580525, "episode": 30.0, "batch_reward": 0.12378565783053637, "critic_loss": 0.31632021832466123, "actor_loss": -23.145452034950257, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.10795521736145, "step": 30000}
{"episode_reward": 272.4340576906763, "episode": 31.0, "batch_reward": 0.12531518265604974, "critic_loss": 0.2983634765446186, "actor_loss": -23.988547070503234, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.15731906890869, "step": 31000}
{"episode_reward": 85.12535828693423, "episode": 32.0, "batch_reward": 0.12242676306515932, "critic_loss": 0.34473898488283156, "actor_loss": -23.025766122817995, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.092390537261963, "step": 32000}
{"episode_reward": 40.85030631925382, "episode": 33.0, "batch_reward": 0.12201740744709969, "critic_loss": 0.3013569985926151, "actor_loss": -23.664651659965514, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.826109409332275, "step": 33000}
{"episode_reward": 110.41532703116249, "episode": 34.0, "batch_reward": 0.1208893751502037, "critic_loss": 0.29648474560678006, "actor_loss": -22.588500507354738, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.66317582130432, "step": 34000}
{"episode_reward": 58.886333784264046, "episode": 35.0, "batch_reward": 0.12126371734589338, "critic_loss": 0.3510459774583578, "actor_loss": -23.307388072013854, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.08885884284973, "step": 35000}
{"episode_reward": 297.33268636428477, "episode": 36.0, "batch_reward": 0.1222115728110075, "critic_loss": 0.2884299190118909, "actor_loss": -22.29708874988556, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.972129583358765, "step": 36000}
{"episode_reward": 43.34568469107276, "episode": 37.0, "batch_reward": 0.12200636425614357, "critic_loss": 0.28811240980774167, "actor_loss": -22.777445591926575, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.36809492111206, "step": 37000}
{"episode_reward": 82.62861998984796, "episode": 38.0, "batch_reward": 0.12184785129129887, "critic_loss": 0.2476481559202075, "actor_loss": -23.136809007644654, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.304007291793823, "step": 38000}
{"episode_reward": 97.68819599297339, "episode": 39.0, "batch_reward": 0.12147058752179146, "critic_loss": 0.24689788664877416, "actor_loss": -23.180975620269777, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.621431589126587, "step": 39000}
{"episode_reward": 176.92310379331624, "episode": 40.0, "batch_reward": 0.12325563696026802, "critic_loss": 0.23872794807702302, "actor_loss": -23.40676186466217, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.78064727783203, "step": 40000}
{"episode_reward": 274.1450393830401, "episode": 41.0, "batch_reward": 0.12761274573206902, "critic_loss": 0.2601420175433159, "actor_loss": -23.574453845977782, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.50490117073059, "step": 41000}
{"episode_reward": 228.93436164564446, "episode": 42.0, "batch_reward": 0.13064953030645848, "critic_loss": 0.2541382605731487, "actor_loss": -23.12081206703186, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.473310947418213, "step": 42000}
{"episode_reward": 276.89832500213305, "episode": 43.0, "batch_reward": 0.13325189960002898, "critic_loss": 0.22630135854333638, "actor_loss": -23.528446179389952, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.31628704071045, "step": 43000}
{"episode_reward": 284.0368363373394, "episode": 44.0, "batch_reward": 0.13711091336607933, "critic_loss": 0.2142720284089446, "actor_loss": -23.56912295150757, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.009655237197876, "step": 44000}
{"episode_reward": 286.54059218233516, "episode": 45.0, "batch_reward": 0.14042975085228682, "critic_loss": 0.2220378214940429, "actor_loss": -24.56831963920593, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.21531057357788, "step": 45000}
{"episode_reward": 306.45697283179794, "episode": 46.0, "batch_reward": 0.1439977118447423, "critic_loss": 0.23383377765119076, "actor_loss": -24.474539192199707, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.289196014404297, "step": 46000}
{"episode_reward": 326.9169087093614, "episode": 47.0, "batch_reward": 0.1473667929843068, "critic_loss": 0.253033823043108, "actor_loss": -25.248364665031435, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.56781768798828, "step": 47000}
{"episode_reward": 245.93246709193014, "episode": 48.0, "batch_reward": 0.14931258068978787, "critic_loss": 0.26867528872191904, "actor_loss": -24.30366370677948, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.090879917144775, "step": 48000}
{"episode_reward": 74.15630449514993, "episode": 49.0, "batch_reward": 0.14887182705104351, "critic_loss": 0.24796224215626717, "actor_loss": -25.466054149627684, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.288100242614746, "step": 49000}
{"episode_reward": 303.9109679746857, "episode": 50.0, "batch_reward": 0.1513896344974637, "critic_loss": 0.2641065703332424, "actor_loss": -25.78891234588623, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.161861896514893, "step": 50000}
{"episode_reward": 284.75918167457473, "episode": 51.0, "batch_reward": 0.1541955260038376, "critic_loss": 0.2535853074416518, "actor_loss": -25.971620435714723, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.17440748214722, "step": 51000}
{"episode_reward": 255.07151897152417, "episode": 52.0, "batch_reward": 0.15583811395615338, "critic_loss": 0.2816685757488012, "actor_loss": -26.195448150634764, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.64401412010193, "step": 52000}
{"episode_reward": 203.19762793543333, "episode": 53.0, "batch_reward": 0.15719521886855364, "critic_loss": 0.3013126597851515, "actor_loss": -25.9913944644928, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.708669900894165, "step": 53000}
{"episode_reward": 276.3938696173897, "episode": 54.0, "batch_reward": 0.15769610419869423, "critic_loss": 0.29763392877578737, "actor_loss": -26.36309137916565, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.98847270011902, "step": 54000}
{"episode_reward": 71.52222488929631, "episode": 55.0, "batch_reward": 0.1583451666459441, "critic_loss": 0.2891370929479599, "actor_loss": -26.47461665534973, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.159656286239624, "step": 55000}
{"episode_reward": 323.8454773729793, "episode": 56.0, "batch_reward": 0.15915851321816443, "critic_loss": 0.29528819350898267, "actor_loss": -26.553220108032228, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.739511489868164, "step": 56000}
{"episode_reward": 58.851910166126835, "episode": 57.0, "batch_reward": 0.15923184916377067, "critic_loss": 0.3065217025205493, "actor_loss": -26.34919683265686, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.502676963806152, "step": 57000}
{"episode_reward": 331.38068743295815, "episode": 58.0, "batch_reward": 0.16221436950564386, "critic_loss": 0.34149759748578074, "actor_loss": -26.329950910568236, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.005064964294434, "step": 58000}
{"episode_reward": 183.6400505480061, "episode": 59.0, "batch_reward": 0.16029207982867957, "critic_loss": 0.3434002894535661, "actor_loss": -25.852883422851562, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.129780530929565, "step": 59000}
{"episode_reward": 67.19183558418436, "episode": 60.0, "batch_reward": 0.16116427842527628, "critic_loss": 0.3278344988822937, "actor_loss": -26.489202850341798, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.265993118286133, "step": 60000}
{"episode_reward": 227.04916237556697, "episode": 61.0, "batch_reward": 0.16037533527612685, "critic_loss": 0.3273310843259096, "actor_loss": -26.11865077209473, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.14417552947998, "step": 61000}
{"episode_reward": 119.42728101133231, "episode": 62.0, "batch_reward": 0.16012448715418579, "critic_loss": 0.33560485258698464, "actor_loss": -26.456405933380125, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.424602031707764, "step": 62000}
{"episode_reward": 148.36048968990258, "episode": 63.0, "batch_reward": 0.159218009583652, "critic_loss": 0.36783862751722335, "actor_loss": -25.800254705429076, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.219913721084595, "step": 63000}
{"episode_reward": 60.753180930040784, "episode": 64.0, "batch_reward": 0.15959305576235056, "critic_loss": 0.3522169995158911, "actor_loss": -26.162284172058104, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.150450229644775, "step": 64000}
{"episode_reward": 406.9183293108432, "episode": 65.0, "batch_reward": 0.1620640207082033, "critic_loss": 0.4009375064820051, "actor_loss": -26.246653268814086, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.708128690719604, "step": 65000}
{"episode_reward": 109.32452590184411, "episode": 66.0, "batch_reward": 0.1625614982843399, "critic_loss": 0.4126268882900476, "actor_loss": -26.277914432525634, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.956124305725098, "step": 66000}
{"episode_reward": 186.02383933373608, "episode": 67.0, "batch_reward": 0.1627463176622987, "critic_loss": 0.3835824626237154, "actor_loss": -26.5097741355896, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.764930248260498, "step": 67000}
{"episode_reward": 310.5159927852344, "episode": 68.0, "batch_reward": 0.16433238039910794, "critic_loss": 0.42866987980902194, "actor_loss": -25.900778213500978, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.907233238220215, "step": 68000}
{"episode_reward": 343.8491497653756, "episode": 69.0, "batch_reward": 0.16737167635560035, "critic_loss": 0.39502040691673757, "actor_loss": -26.20903640937805, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.28565239906311, "step": 69000}
{"episode_reward": 350.00239095720207, "episode": 70.0, "batch_reward": 0.17025174681842328, "critic_loss": 0.3681279289871454, "actor_loss": -27.349446031570434, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.349347829818726, "step": 70000}
{"episode_reward": 377.2249422478949, "episode": 71.0, "batch_reward": 0.17267399045079945, "critic_loss": 0.3740595789551735, "actor_loss": -27.20647999382019, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.380109786987305, "step": 71000}
{"episode_reward": 167.53763545767566, "episode": 72.0, "batch_reward": 0.17330719849467277, "critic_loss": 0.4204265539050102, "actor_loss": -27.08106674194336, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.87234854698181, "step": 72000}
{"episode_reward": 340.79270890903126, "episode": 73.0, "batch_reward": 0.1759495730251074, "critic_loss": 0.4801338307410479, "actor_loss": -27.39627906036377, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.439942598342896, "step": 73000}
{"episode_reward": 346.24830620628006, "episode": 74.0, "batch_reward": 0.17824743320047856, "critic_loss": 0.4408339060097933, "actor_loss": -28.087677667617797, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.719769954681396, "step": 74000}
{"episode_reward": 387.85122373723505, "episode": 75.0, "batch_reward": 0.18026381473243236, "critic_loss": 0.42904108424484727, "actor_loss": -28.208197200775146, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.551552057266235, "step": 75000}
{"episode_reward": 375.5021715878162, "episode": 76.0, "batch_reward": 0.1838288332670927, "critic_loss": 0.4524441335499287, "actor_loss": -28.812076642990114, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.82087016105652, "step": 76000}
{"episode_reward": 394.64754153302215, "episode": 77.0, "batch_reward": 0.18601218824088572, "critic_loss": 0.45776117262244226, "actor_loss": -28.41639053916931, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.02416229248047, "step": 77000}
{"episode_reward": 389.7247655160415, "episode": 78.0, "batch_reward": 0.18866736651957036, "critic_loss": 0.46251646254956724, "actor_loss": -28.854016410827636, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.13939595222473, "step": 78000}
{"episode_reward": 384.966548827411, "episode": 79.0, "batch_reward": 0.19077515564858913, "critic_loss": 0.4717170839458704, "actor_loss": -28.28168347167969, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.558061122894287, "step": 79000}
{"episode_reward": 389.42883598844696, "episode": 80.0, "batch_reward": 0.1938543056845665, "critic_loss": 0.4311272747963667, "actor_loss": -29.08070711517334, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.383591175079346, "step": 80000}
{"episode_reward": 406.272787693637, "episode": 81.0, "batch_reward": 0.1968500832170248, "critic_loss": 0.45126532150805, "actor_loss": -29.486958715438842, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.36330795288086, "step": 81000}
{"episode_reward": 428.08006952220325, "episode": 82.0, "batch_reward": 0.19973307852447034, "critic_loss": 0.43580304926633834, "actor_loss": -30.432792673110963, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.32535672187805, "step": 82000}
{"episode_reward": 398.22765638546105, "episode": 83.0, "batch_reward": 0.20159612123668194, "critic_loss": 0.45400164191424847, "actor_loss": -29.438645595550536, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.85657000541687, "step": 83000}
{"episode_reward": 193.28783011909286, "episode": 84.0, "batch_reward": 0.20183022357523442, "critic_loss": 0.4350419575124979, "actor_loss": -30.273133543014527, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.293182373046875, "step": 84000}
{"episode_reward": 376.4032758624734, "episode": 85.0, "batch_reward": 0.20372474032640459, "critic_loss": 0.43608486324548723, "actor_loss": -30.287676712036134, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.267171621322632, "step": 85000}
{"episode_reward": 396.86656064746757, "episode": 86.0, "batch_reward": 0.20581594929099084, "critic_loss": 0.4720321255773306, "actor_loss": -30.156035642623902, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.30666971206665, "step": 86000}
{"episode_reward": 180.95816535666367, "episode": 87.0, "batch_reward": 0.20483378300070762, "critic_loss": 0.4840080780684948, "actor_loss": -29.89126781082153, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.411277532577515, "step": 87000}
{"episode_reward": 100.9362276364044, "episode": 88.0, "batch_reward": 0.2045092003941536, "critic_loss": 0.445768934071064, "actor_loss": -29.447179723739623, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.025237321853638, "step": 88000}
{"episode_reward": 374.10737143029843, "episode": 89.0, "batch_reward": 0.2055546453744173, "critic_loss": 0.46991464836895463, "actor_loss": -30.34954739570618, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.562493562698364, "step": 89000}
{"episode_reward": 99.42181718150417, "episode": 90.0, "batch_reward": 0.20434502567350865, "critic_loss": 0.4646476454138756, "actor_loss": -30.568318572998045, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.94732165336609, "step": 90000}
{"episode_reward": 136.18551356920645, "episode": 91.0, "batch_reward": 0.20532866518199444, "critic_loss": 0.46427625282108786, "actor_loss": -29.95870433616638, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.04586362838745, "step": 91000}
{"episode_reward": 433.75731648895663, "episode": 92.0, "batch_reward": 0.2073766596019268, "critic_loss": 0.5155248973071576, "actor_loss": -29.871232551574707, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.018924713134766, "step": 92000}
{"episode_reward": 405.6993472026714, "episode": 93.0, "batch_reward": 0.20763508804142475, "critic_loss": 0.4951918246597052, "actor_loss": -29.828787546157837, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.063377141952515, "step": 93000}
{"episode_reward": 144.13470559052985, "episode": 94.0, "batch_reward": 0.2078544863909483, "critic_loss": 0.46468190899491313, "actor_loss": -29.9694582195282, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.462900161743164, "step": 94000}
{"episode_reward": 354.7174834301704, "episode": 95.0, "batch_reward": 0.2097901017218828, "critic_loss": 0.4673066051155329, "actor_loss": -30.97397059440613, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.006328582763672, "step": 95000}
{"episode_reward": 378.32530714736333, "episode": 96.0, "batch_reward": 0.21106180088222026, "critic_loss": 0.5479561284482479, "actor_loss": -30.965892974853517, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.254240036010742, "step": 96000}
{"episode_reward": 397.34929248729503, "episode": 97.0, "batch_reward": 0.21381915241479874, "critic_loss": 0.5650923560857772, "actor_loss": -31.454868577957154, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.79819130897522, "step": 97000}
{"episode_reward": 397.7226842208235, "episode": 98.0, "batch_reward": 0.2157226896584034, "critic_loss": 0.5065403168797493, "actor_loss": -30.662257707595824, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.92842674255371, "step": 98000}
{"episode_reward": 409.08996383726594, "episode": 99.0, "batch_reward": 0.21719021989405154, "critic_loss": 0.495080186098814, "actor_loss": -30.81963540840149, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.39952278137207, "step": 99000}
{"episode_reward": 417.4983583780387, "episode": 100.0, "batch_reward": 0.21980218029022217, "critic_loss": 0.487617600440979, "actor_loss": -31.146881942749022, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.278181076049805, "step": 100000}
{"episode_reward": 389.0626378181602, "episode": 101.0, "batch_reward": 0.2207946289628744, "critic_loss": 0.4666597876995802, "actor_loss": -31.523784471511842, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 37.74727463722229, "step": 101000}
{"episode_reward": 422.10183996958267, "episode": 102.0, "batch_reward": 0.22312213371694087, "critic_loss": 0.4514321097433567, "actor_loss": -31.559216276168822, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.123795747756958, "step": 102000}
{"episode_reward": 337.3157580081822, "episode": 103.0, "batch_reward": 0.2239807503670454, "critic_loss": 0.4381388976871967, "actor_loss": -31.513672872543335, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.98043155670166, "step": 103000}
{"episode_reward": 244.77385447464988, "episode": 104.0, "batch_reward": 0.2247787657380104, "critic_loss": 0.4676471786946058, "actor_loss": -31.453067901611327, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.26545000076294, "step": 104000}
{"episode_reward": 419.9922195106338, "episode": 105.0, "batch_reward": 0.22650429697334767, "critic_loss": 0.51865523609519, "actor_loss": -31.899424877166748, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.706029176712036, "step": 105000}
{"episode_reward": 398.98151772687964, "episode": 106.0, "batch_reward": 0.22839535921812057, "critic_loss": 0.4695657323747873, "actor_loss": -31.577508563995362, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.825657606124878, "step": 106000}
{"episode_reward": 435.0333750629576, "episode": 107.0, "batch_reward": 0.23033516703546048, "critic_loss": 0.4361162191927433, "actor_loss": -32.122850944519044, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.38511347770691, "step": 107000}
{"episode_reward": 424.39041837085296, "episode": 108.0, "batch_reward": 0.2315352371633053, "critic_loss": 0.4205571113526821, "actor_loss": -32.138795188903806, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.891438245773315, "step": 108000}
{"episode_reward": 436.92964247195977, "episode": 109.0, "batch_reward": 0.23412458324432373, "critic_loss": 0.4459496789723635, "actor_loss": -32.7126565208435, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.516072988510132, "step": 109000}
{"episode_reward": 416.20377882469415, "episode": 110.0, "batch_reward": 0.23490976272523403, "critic_loss": 0.42033924512565135, "actor_loss": -32.3397837638855, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.087404012680054, "step": 110000}
{"episode_reward": 370.45327168751936, "episode": 111.0, "batch_reward": 0.23658678440749645, "critic_loss": 0.44542111271619794, "actor_loss": -33.268459468841556, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.676231384277344, "step": 111000}
{"episode_reward": 378.24219941795684, "episode": 112.0, "batch_reward": 0.23878967188298703, "critic_loss": 0.4379413347989321, "actor_loss": -32.79747068405151, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.723273992538452, "step": 112000}
{"episode_reward": 411.25128859201277, "episode": 113.0, "batch_reward": 0.2393006440550089, "critic_loss": 0.43724886792898177, "actor_loss": -32.75311542129516, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.48074173927307, "step": 113000}
{"episode_reward": 383.75263485860023, "episode": 114.0, "batch_reward": 0.24087224225699902, "critic_loss": 0.4552415664792061, "actor_loss": -33.33389744186401, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.22951054573059, "step": 114000}
{"episode_reward": 391.2627761179496, "episode": 115.0, "batch_reward": 0.2423689471632242, "critic_loss": 0.41462367470562456, "actor_loss": -33.15277952957153, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.40323233604431, "step": 115000}
{"episode_reward": 392.49465661375154, "episode": 116.0, "batch_reward": 0.2433110100775957, "critic_loss": 0.4118117324113846, "actor_loss": -33.28555279159546, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.67909550666809, "step": 116000}
{"episode_reward": 80.55076520239534, "episode": 117.0, "batch_reward": 0.24191488252580165, "critic_loss": 0.38933534228801725, "actor_loss": -33.121138767242435, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.417800664901733, "step": 117000}
{"episode_reward": 411.4059725310737, "episode": 118.0, "batch_reward": 0.24317810595035552, "critic_loss": 0.44244915939867496, "actor_loss": -33.35579978179932, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.783538818359375, "step": 118000}
{"episode_reward": 427.6400124896635, "episode": 119.0, "batch_reward": 0.24524301406741142, "critic_loss": 0.3896978621631861, "actor_loss": -33.68031131362915, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.451300382614136, "step": 119000}
{"episode_reward": 438.8413665150191, "episode": 120.0, "batch_reward": 0.2448594155907631, "critic_loss": 0.37900010342895984, "actor_loss": -33.15198514938354, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.992462873458862, "step": 120000}
{"episode_reward": 69.68702761473571, "episode": 121.0, "batch_reward": 0.24484602020680904, "critic_loss": 0.397610561683774, "actor_loss": -33.18995967483521, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.1566276550293, "step": 121000}
{"episode_reward": 429.699620521074, "episode": 122.0, "batch_reward": 0.24703418868780136, "critic_loss": 0.4130049403905868, "actor_loss": -33.23338479995728, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.56472086906433, "step": 122000}
{"episode_reward": 391.6890592392369, "episode": 123.0, "batch_reward": 0.24851939633488654, "critic_loss": 0.40537812167406084, "actor_loss": -32.60046514129639, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.152930736541748, "step": 123000}
{"episode_reward": 438.8702624443147, "episode": 124.0, "batch_reward": 0.24955413508415222, "critic_loss": 0.3875710790455341, "actor_loss": -33.56321610641479, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.11532759666443, "step": 124000}
{"episode_reward": 417.34162755515706, "episode": 125.0, "batch_reward": 0.25108090381324294, "critic_loss": 0.4088943811058998, "actor_loss": -33.703317207336426, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.821582555770874, "step": 125000}
{"episode_reward": 391.9933342494166, "episode": 126.0, "batch_reward": 0.25103079314529897, "critic_loss": 0.386220699891448, "actor_loss": -33.84146001434326, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.20038866996765, "step": 126000}
{"episode_reward": 414.3245867111153, "episode": 127.0, "batch_reward": 0.2528356865644455, "critic_loss": 0.412430947586894, "actor_loss": -33.83761587142944, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.651032209396362, "step": 127000}
{"episode_reward": 418.28378456122414, "episode": 128.0, "batch_reward": 0.2541791318804026, "critic_loss": 0.3808399281948805, "actor_loss": -34.60192539978027, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.29174494743347, "step": 128000}
{"episode_reward": 357.40323732384445, "episode": 129.0, "batch_reward": 0.25491381284594533, "critic_loss": 0.4223958800062537, "actor_loss": -34.708405422210696, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.674018144607544, "step": 129000}
{"episode_reward": 147.16976627614497, "episode": 130.0, "batch_reward": 0.2549716126769781, "critic_loss": 0.4780608946084976, "actor_loss": -34.26478546142578, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.604376316070557, "step": 130000}
{"episode_reward": 406.26468000821313, "episode": 131.0, "batch_reward": 0.25619113306701186, "critic_loss": 0.46852040936052797, "actor_loss": -35.09158962249756, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.85504341125488, "step": 131000}
{"episode_reward": 412.0713941867464, "episode": 132.0, "batch_reward": 0.2574807956218719, "critic_loss": 0.4245028603374958, "actor_loss": -34.53207626342773, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.996684789657593, "step": 132000}
{"episode_reward": 424.90180289283114, "episode": 133.0, "batch_reward": 0.25781488154828547, "critic_loss": 0.40578155751526357, "actor_loss": -34.753255321502685, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.818392992019653, "step": 133000}
{"episode_reward": 449.3783079570053, "episode": 134.0, "batch_reward": 0.25912369875609875, "critic_loss": 0.44621580028533936, "actor_loss": -34.86682007598877, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.087623357772827, "step": 134000}
{"episode_reward": 401.65433203935936, "episode": 135.0, "batch_reward": 0.2601755607873201, "critic_loss": 0.4518984595239162, "actor_loss": -35.12624151611328, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.80805015563965, "step": 135000}
{"episode_reward": 387.80350611025983, "episode": 136.0, "batch_reward": 0.26169065484404563, "critic_loss": 0.42800471231341364, "actor_loss": -35.76790575790405, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.04676103591919, "step": 136000}
{"episode_reward": 382.5880831925552, "episode": 137.0, "batch_reward": 0.26214101845026017, "critic_loss": 0.45109670870006086, "actor_loss": -34.896869606018065, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.577672004699707, "step": 137000}
{"episode_reward": 389.86234460740746, "episode": 138.0, "batch_reward": 0.2634248243421316, "critic_loss": 0.45260189433395864, "actor_loss": -34.783568481445315, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.833904266357422, "step": 138000}
{"episode_reward": 385.50580141596606, "episode": 139.0, "batch_reward": 0.2647019213587046, "critic_loss": 0.4694079973101616, "actor_loss": -35.02550644302368, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.657217979431152, "step": 139000}
{"episode_reward": 447.2444208932738, "episode": 140.0, "batch_reward": 0.26510250245034694, "critic_loss": 0.4576177056133747, "actor_loss": -34.927197303771976, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.670286655426025, "step": 140000}
{"episode_reward": 409.1397520070931, "episode": 141.0, "batch_reward": 0.26677438922226426, "critic_loss": 0.4354466360658407, "actor_loss": -34.95481009674072, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.45156908035278, "step": 141000}
{"episode_reward": 461.8038254447407, "episode": 142.0, "batch_reward": 0.26728533004224303, "critic_loss": 0.41018947792053223, "actor_loss": -35.37117378616333, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.298208951950073, "step": 142000}
{"episode_reward": 448.4655192300895, "episode": 143.0, "batch_reward": 0.2701796645671129, "critic_loss": 0.4200612004995346, "actor_loss": -35.67995683670044, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.099089860916138, "step": 143000}
{"episode_reward": 438.92314265927257, "episode": 144.0, "batch_reward": 0.27017563666403294, "critic_loss": 0.40697750487923623, "actor_loss": -35.88317325592041, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.296376705169678, "step": 144000}
{"episode_reward": 406.0805144506357, "episode": 145.0, "batch_reward": 0.27211402989923955, "critic_loss": 0.3993363331705332, "actor_loss": -35.702301048278805, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.58679986000061, "step": 145000}
{"episode_reward": 452.45401782654585, "episode": 146.0, "batch_reward": 0.2722667751163244, "critic_loss": 0.41576271361112593, "actor_loss": -35.750733436584476, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.789270639419556, "step": 146000}
{"episode_reward": 203.62094098060038, "episode": 147.0, "batch_reward": 0.2725202170461416, "critic_loss": 0.4023860492259264, "actor_loss": -35.98558255386352, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.19459295272827, "step": 147000}
{"episode_reward": 425.7808980655456, "episode": 148.0, "batch_reward": 0.27405058571696284, "critic_loss": 0.3886893697679043, "actor_loss": -35.474166774749754, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.108785390853882, "step": 148000}
{"episode_reward": 436.3140286408136, "episode": 149.0, "batch_reward": 0.2738373516499996, "critic_loss": 0.42317204278707504, "actor_loss": -35.836840099334715, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.473693132400513, "step": 149000}
{"episode_reward": 462.2770540439498, "episode": 150.0, "batch_reward": 0.2758705318570137, "critic_loss": 0.4151658702790737, "actor_loss": -36.13280085372925, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
