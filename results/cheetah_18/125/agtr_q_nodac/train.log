{"episode": 1.0, "duration": 12.902437925338745, "episode_reward": 5.1931688606333894, "step": 1000}
{"episode": 2.0, "duration": 1.1357519626617432, "episode_reward": 454.7514538707513, "step": 2000}
{"episode": 3.0, "batch_reward": 0.21784059000277606, "actor_loss": -45.28418455089555, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 54.33354139328003, "episode_reward": 22.70302677605289, "step": 3000}
{"episode": 4.0, "batch_reward": 0.14441002774983644, "actor_loss": -40.38963818359375, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.206138134002686, "episode_reward": 40.17038579158602, "step": 4000}
{"episode": 5.0, "batch_reward": 0.12465707440674305, "actor_loss": -39.0738729019165, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.307454109191895, "episode_reward": 80.28584656388766, "step": 5000}
{"episode": 6.0, "batch_reward": 0.12134972578287125, "actor_loss": -38.71874771881104, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.149787425994873, "episode_reward": 138.31657539189897, "step": 6000}
{"episode": 7.0, "batch_reward": 0.12703978287428616, "actor_loss": -38.86550085449219, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.031805276870728, "episode_reward": 173.29482671805565, "step": 7000}
{"episode": 8.0, "batch_reward": 0.1365428542867303, "actor_loss": -39.18895163726807, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.256408214569092, "episode_reward": 217.658225784189, "step": 8000}
{"episode": 9.0, "batch_reward": 0.14351256959140302, "actor_loss": -39.49084168243408, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.916177988052368, "episode_reward": 196.4451596077976, "step": 9000}
{"episode": 10.0, "batch_reward": 0.15133768564462663, "actor_loss": -39.797049125671386, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.462871074676514, "episode_reward": 224.81111883613647, "step": 10000}
{"episode": 11.0, "batch_reward": 0.15791205144673587, "actor_loss": -40.07715732574463, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.643423318862915, "episode_reward": 226.12412643732057, "step": 11000}
{"episode": 12.0, "batch_reward": 0.16464284494519232, "actor_loss": -40.3657340927124, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.70028328895569, "episode_reward": 221.84325064253545, "step": 12000}
{"episode": 13.0, "batch_reward": 0.1661959157139063, "actor_loss": -40.31032718658447, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.108036518096924, "episode_reward": 168.92257615725507, "step": 13000}
{"episode": 14.0, "batch_reward": 0.16776572851836682, "actor_loss": -40.3340758972168, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.935218334197998, "episode_reward": 217.6270638207361, "step": 14000}
{"episode": 15.0, "batch_reward": 0.17222149531543254, "actor_loss": -40.549594131469725, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.07504415512085, "episode_reward": 205.09848452701806, "step": 15000}
{"episode": 16.0, "batch_reward": 0.17437723411619663, "actor_loss": -40.64828078460693, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.050179719924927, "episode_reward": 231.8178305298301, "step": 16000}
{"episode": 17.0, "batch_reward": 0.17712158079445361, "actor_loss": -40.75470410919189, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.828690767288208, "episode_reward": 193.92794722302685, "step": 17000}
{"episode": 18.0, "batch_reward": 0.17962992879748343, "actor_loss": -40.85503694152832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.73104977607727, "episode_reward": 240.40804925699666, "step": 18000}
{"episode": 19.0, "batch_reward": 0.1804231576025486, "actor_loss": -40.78620804595947, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.860849380493164, "episode_reward": 166.43856995261632, "step": 19000}
{"episode": 20.0, "batch_reward": 0.18116235484182835, "actor_loss": -40.782069999694826, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.199567317962646, "episode_reward": 224.13059661843363, "step": 20000}
{"episode": 21.0, "batch_reward": 0.18309052164852618, "actor_loss": -40.88990261077881, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 40.8656120300293, "episode_reward": 218.57930451980909, "step": 21000}
{"episode": 22.0, "batch_reward": 0.1853922982662916, "actor_loss": -40.98847551727295, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.869098663330078, "episode_reward": 215.26711446049353, "step": 22000}
{"episode": 23.0, "batch_reward": 0.18662919664382935, "actor_loss": -41.02269386291504, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.242629766464233, "episode_reward": 214.6923681437015, "step": 23000}
{"episode": 24.0, "batch_reward": 0.18701382943987846, "actor_loss": -41.04771591186523, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.86945414543152, "episode_reward": 217.4326050442457, "step": 24000}
{"episode": 25.0, "batch_reward": 0.18770975841581822, "actor_loss": -41.09975652313232, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.776970863342285, "episode_reward": 201.91620384073602, "step": 25000}
{"episode": 26.0, "batch_reward": 0.18845072707533836, "actor_loss": -41.13144058227539, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.937891244888306, "episode_reward": 162.4572772576559, "step": 26000}
{"episode": 27.0, "batch_reward": 0.18761329306662083, "actor_loss": -41.1197732925415, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.15091586112976, "episode_reward": 171.27757600649397, "step": 27000}
{"episode": 28.0, "batch_reward": 0.18759499889612197, "actor_loss": -41.09199598693848, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.679676055908203, "episode_reward": 178.3340587245797, "step": 28000}
{"episode": 29.0, "batch_reward": 0.18795253957808017, "actor_loss": -41.08366594696045, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.28961420059204, "episode_reward": 202.01660966949893, "step": 29000}
{"episode": 30.0, "batch_reward": 0.18765184807777405, "actor_loss": -41.06552807617187, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.279029369354248, "episode_reward": 192.40637005900328, "step": 30000}
{"episode": 31.0, "batch_reward": 0.18787403371930123, "actor_loss": -41.12795199584961, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 40.9598753452301, "episode_reward": 189.7124030890177, "step": 31000}
{"episode": 32.0, "batch_reward": 0.1875591359883547, "actor_loss": -41.078736862182616, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.625781297683716, "episode_reward": 197.76879748274854, "step": 32000}
{"episode": 33.0, "batch_reward": 0.18829085099697113, "actor_loss": -41.08174966430664, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.25210165977478, "episode_reward": 201.25626324249626, "step": 33000}
{"episode": 34.0, "batch_reward": 0.18837201580405236, "actor_loss": -41.14864631652832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.785471200942993, "episode_reward": 196.01478071547987, "step": 34000}
{"episode": 35.0, "batch_reward": 0.188675743624568, "actor_loss": -41.127374809265135, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.39407706260681, "episode_reward": 204.10586906360197, "step": 35000}
{"episode": 36.0, "batch_reward": 0.1890756585597992, "actor_loss": -41.13205143737793, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.68626308441162, "episode_reward": 207.74677862916215, "step": 36000}
{"episode": 37.0, "batch_reward": 0.18954598781466483, "actor_loss": -41.16214057922363, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.60372757911682, "episode_reward": 213.4172329858158, "step": 37000}
{"episode": 38.0, "batch_reward": 0.1899016832411289, "actor_loss": -41.216711738586426, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.651333570480347, "episode_reward": 194.97812353799026, "step": 38000}
{"episode": 39.0, "batch_reward": 0.19076860615611077, "actor_loss": -41.244319648742675, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.822267532348633, "episode_reward": 217.50519986478986, "step": 39000}
{"episode": 40.0, "batch_reward": 0.19089227922260762, "actor_loss": -41.22576587677002, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.57449436187744, "episode_reward": 221.68075808417098, "step": 40000}
{"episode": 41.0, "batch_reward": 0.19231310179829597, "actor_loss": -41.293928970336914, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 39.981865882873535, "episode_reward": 215.07246201163065, "step": 41000}
{"episode": 42.0, "batch_reward": 0.19247972038388253, "actor_loss": -41.28788226318359, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.023601770401, "episode_reward": 235.53817317784316, "step": 42000}
{"episode": 43.0, "batch_reward": 0.19174836893379688, "actor_loss": -41.08037225341797, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.860830068588257, "episode_reward": 50.81122233534008, "step": 43000}
{"episode": 44.0, "batch_reward": 0.1899453120827675, "actor_loss": -40.752776069641115, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.346039295196533, "episode_reward": 192.17020716545093, "step": 44000}
{"episode": 45.0, "batch_reward": 0.1903270406872034, "actor_loss": -40.756271034240726, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.212363243103027, "episode_reward": 224.54472766408975, "step": 45000}
{"episode": 46.0, "batch_reward": 0.19132210044562817, "actor_loss": -40.80171215057373, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.92350673675537, "episode_reward": 217.1007412383181, "step": 46000}
{"episode": 47.0, "batch_reward": 0.1919390051215887, "actor_loss": -40.85366581726074, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.346328496932983, "episode_reward": 230.53245222863305, "step": 47000}
{"episode": 48.0, "batch_reward": 0.1922488904595375, "actor_loss": -40.891542366027835, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.33237314224243, "episode_reward": 218.12736676127562, "step": 48000}
{"episode": 49.0, "batch_reward": 0.19360974115133286, "actor_loss": -40.91562831115723, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.25675892829895, "episode_reward": 201.07169993435264, "step": 49000}
{"episode": 50.0, "batch_reward": 0.19263781628012658, "actor_loss": -40.886139572143556, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.44723629951477, "episode_reward": 186.48107028853326, "step": 50000}
{"episode": 51.0, "batch_reward": 0.1928022589236498, "actor_loss": -40.89369829559326, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.19026446342468, "episode_reward": 198.79946981537404, "step": 51000}
{"episode": 52.0, "batch_reward": 0.19313734889030457, "actor_loss": -40.9363073425293, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.118778705596924, "episode_reward": 196.77743454319824, "step": 52000}
{"episode": 53.0, "batch_reward": 0.1932323077917099, "actor_loss": -40.90061929321289, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.67138171195984, "episode_reward": 222.69176458296587, "step": 53000}
{"episode": 54.0, "batch_reward": 0.1934628827124834, "actor_loss": -40.928325462341306, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.9590163230896, "episode_reward": 201.66911043672292, "step": 54000}
{"episode": 55.0, "batch_reward": 0.1940110764950514, "actor_loss": -41.02274047851562, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.200039863586426, "episode_reward": 220.45137239544826, "step": 55000}
{"episode": 56.0, "batch_reward": 0.19459750138223172, "actor_loss": -41.027503494262696, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.93586015701294, "episode_reward": 217.55163355343677, "step": 56000}
{"episode": 57.0, "batch_reward": 0.19420974437892438, "actor_loss": -40.98322886657715, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.66144847869873, "episode_reward": 197.4227735235257, "step": 57000}
{"episode": 58.0, "batch_reward": 0.19481580452620983, "actor_loss": -41.054175964355466, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.56200075149536, "episode_reward": 215.1367593750225, "step": 58000}
{"episode": 59.0, "batch_reward": 0.195191777959466, "actor_loss": -41.04496385192871, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.922106981277466, "episode_reward": 205.2082193563771, "step": 59000}
{"episode": 60.0, "batch_reward": 0.19541730308532715, "actor_loss": -41.0884108581543, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.434513330459595, "episode_reward": 199.58513811041382, "step": 60000}
{"episode": 61.0, "batch_reward": 0.19530935344099998, "actor_loss": -41.04824118041992, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 40.9908287525177, "episode_reward": 128.32800655809882, "step": 61000}
{"episode": 62.0, "batch_reward": 0.19428838561475276, "actor_loss": -40.9490365524292, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.858092546463013, "episode_reward": 218.1554362265049, "step": 62000}
{"episode": 63.0, "batch_reward": 0.1945003300458193, "actor_loss": -40.87588903045654, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.912484407424927, "episode_reward": 220.74935804586946, "step": 63000}
{"episode": 64.0, "batch_reward": 0.19508885623514652, "actor_loss": -40.956685272216795, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.363791465759277, "episode_reward": 224.5824170169369, "step": 64000}
{"episode": 65.0, "batch_reward": 0.1953385721296072, "actor_loss": -40.952677825927736, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.595123291015625, "episode_reward": 219.62292185482187, "step": 65000}
{"episode": 66.0, "batch_reward": 0.1955668884962797, "actor_loss": -40.937685333251956, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.122933387756348, "episode_reward": 117.44692321148557, "step": 66000}
{"episode": 67.0, "batch_reward": 0.19474170638620852, "actor_loss": -40.751967628479, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.163156986236572, "episode_reward": 203.51314753650445, "step": 67000}
{"episode": 68.0, "batch_reward": 0.19468220803141595, "actor_loss": -40.76655821228027, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.84529137611389, "episode_reward": 226.955106161355, "step": 68000}
{"episode": 69.0, "batch_reward": 0.19506518088281155, "actor_loss": -40.81428704071045, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.19273328781128, "episode_reward": 181.84805235874623, "step": 69000}
{"episode": 70.0, "batch_reward": 0.19501673850417137, "actor_loss": -40.789375213623046, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.151760578155518, "episode_reward": 200.37468679974742, "step": 70000}
{"episode": 71.0, "batch_reward": 0.19505248296260833, "actor_loss": -40.82244976043701, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.426828145980835, "episode_reward": 219.38994380140883, "step": 71000}
{"episode": 72.0, "batch_reward": 0.19599507482349873, "actor_loss": -40.86027972412109, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.015547275543213, "episode_reward": 231.99507813007685, "step": 72000}
{"episode": 73.0, "batch_reward": 0.19606296043097973, "actor_loss": -40.86408399963379, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.780109882354736, "episode_reward": 246.07315879862054, "step": 73000}
{"episode": 74.0, "batch_reward": 0.1960854817032814, "actor_loss": -40.73789407348633, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.329737186431885, "episode_reward": 23.987874644642496, "step": 74000}
{"episode": 75.0, "batch_reward": 0.19433473005890847, "actor_loss": -40.50362548828125, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.27544116973877, "episode_reward": 249.48990924084873, "step": 75000}
{"episode": 76.0, "batch_reward": 0.1956871366649866, "actor_loss": -40.5827967376709, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.09930419921875, "episode_reward": 244.1263358377816, "step": 76000}
{"episode": 77.0, "batch_reward": 0.1963003317117691, "actor_loss": -40.69357590484619, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.91579842567444, "episode_reward": 230.045648471922, "step": 77000}
{"episode": 78.0, "batch_reward": 0.19641416887938976, "actor_loss": -40.66774208831787, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.576616525650024, "episode_reward": 244.47964623045397, "step": 78000}
{"episode": 79.0, "batch_reward": 0.19691239927709103, "actor_loss": -40.68397238159179, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.73109793663025, "episode_reward": 203.4394093273204, "step": 79000}
{"episode": 80.0, "batch_reward": 0.19590312585234643, "actor_loss": -40.55793157196045, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.354857921600342, "episode_reward": 46.575490629327916, "step": 80000}
{"episode": 81.0, "batch_reward": 0.19512125191092491, "actor_loss": -40.378316932678224, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.0309624671936, "episode_reward": 238.79482770923255, "step": 81000}
{"episode": 82.0, "batch_reward": 0.19535723181068898, "actor_loss": -40.404528953552244, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.972375869750977, "episode_reward": 222.8495307161317, "step": 82000}
{"episode": 83.0, "batch_reward": 0.1959052292406559, "actor_loss": -40.49739823913574, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.794578552246094, "episode_reward": 220.5420728256702, "step": 83000}
{"episode": 84.0, "batch_reward": 0.1960904433131218, "actor_loss": -40.475165534973144, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.286824941635132, "episode_reward": 236.7658280541045, "step": 84000}
{"episode": 85.0, "batch_reward": 0.19690087874233722, "actor_loss": -40.50954530334473, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.78283953666687, "episode_reward": 229.9067486758007, "step": 85000}
{"episode": 86.0, "batch_reward": 0.1969393054395914, "actor_loss": -40.52470383453369, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.87226629257202, "episode_reward": 255.2911168219832, "step": 86000}
{"episode": 87.0, "batch_reward": 0.19789558862149714, "actor_loss": -40.57901532745361, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.1604585647583, "episode_reward": 257.3504298674332, "step": 87000}
{"episode": 88.0, "batch_reward": 0.19858427168428897, "actor_loss": -40.68430362701416, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.217307567596436, "episode_reward": 251.35007898086775, "step": 88000}
{"episode": 89.0, "batch_reward": 0.19922412465512754, "actor_loss": -40.643509132385255, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.389682054519653, "episode_reward": 230.68531233325604, "step": 89000}
{"episode": 90.0, "batch_reward": 0.19933859106898308, "actor_loss": -40.66976542663574, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.226025581359863, "episode_reward": 238.24432610398094, "step": 90000}
{"episode": 91.0, "batch_reward": 0.2000039496421814, "actor_loss": -40.682956497192386, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.1597261428833, "episode_reward": 237.71433511779483, "step": 91000}
{"episode": 92.0, "batch_reward": 0.20035841542482377, "actor_loss": -40.7410394744873, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.74053382873535, "episode_reward": 230.87209723485975, "step": 92000}
{"episode": 93.0, "batch_reward": 0.20065871775150299, "actor_loss": -40.770384437561034, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.292691469192505, "episode_reward": 243.65548775868353, "step": 93000}
{"episode": 94.0, "batch_reward": 0.20097869223356246, "actor_loss": -40.813836074829105, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.04875922203064, "episode_reward": 230.47862712540888, "step": 94000}
{"episode": 95.0, "batch_reward": 0.20126467454433442, "actor_loss": -40.82777569580078, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.96725106239319, "episode_reward": 249.58599518839097, "step": 95000}
{"episode": 96.0, "batch_reward": 0.20163841362297535, "actor_loss": -40.85647261047363, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.215848445892334, "episode_reward": 249.20065139748036, "step": 96000}
{"episode": 97.0, "batch_reward": 0.20257954475283624, "actor_loss": -40.8746284866333, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.108267784118652, "episode_reward": 224.8066004019824, "step": 97000}
{"episode": 98.0, "batch_reward": 0.2030806593000889, "actor_loss": -40.91918876647949, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.095882654190063, "episode_reward": 243.04356969792076, "step": 98000}
{"episode": 99.0, "batch_reward": 0.20303483544290066, "actor_loss": -40.93952709960938, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.987295866012573, "episode_reward": 228.87896758355896, "step": 99000}
{"episode": 100.0, "batch_reward": 0.2035688814818859, "actor_loss": -41.014198677062986, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.52802538871765, "episode_reward": 241.17468279342216, "step": 100000}
{"episode": 101.0, "batch_reward": 0.20351761998236179, "actor_loss": -41.00768782043457, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.34988045692444, "episode_reward": 222.61841809443936, "step": 101000}
{"episode": 102.0, "batch_reward": 0.20415265661478044, "actor_loss": -41.02665891265869, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.737950563430786, "episode_reward": 221.07850467292596, "step": 102000}
{"episode": 103.0, "batch_reward": 0.20411956845223903, "actor_loss": -41.02933869934082, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.261594533920288, "episode_reward": 242.5052452212508, "step": 103000}
{"episode": 104.0, "batch_reward": 0.20414702945947646, "actor_loss": -41.00107042694092, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.05666756629944, "episode_reward": 214.2249191676163, "step": 104000}
{"episode": 105.0, "batch_reward": 0.20412577161192894, "actor_loss": -40.96151285552978, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.12162971496582, "episode_reward": 46.73064766051769, "step": 105000}
{"episode": 106.0, "batch_reward": 0.20313944613933563, "actor_loss": -40.7620026473999, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.128119707107544, "episode_reward": 223.6547652205035, "step": 106000}
{"episode": 107.0, "batch_reward": 0.20328512382507324, "actor_loss": -40.854276504516605, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.92617106437683, "episode_reward": 210.12426217528977, "step": 107000}
{"episode": 108.0, "batch_reward": 0.20267046090960503, "actor_loss": -40.8037123413086, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.679614782333374, "episode_reward": 215.3899782222961, "step": 108000}
{"episode": 109.0, "batch_reward": 0.20333826243877412, "actor_loss": -40.81394751739502, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.304591178894043, "episode_reward": 245.10290732278136, "step": 109000}
{"episode": 110.0, "batch_reward": 0.20334626744687556, "actor_loss": -40.845330505371095, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.019557237625122, "episode_reward": 224.14239300161668, "step": 110000}
{"episode": 111.0, "batch_reward": 0.20302271431684493, "actor_loss": -40.766969261169436, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.39332818984985, "episode_reward": 98.44983425096041, "step": 111000}
{"episode": 112.0, "batch_reward": 0.2029265104830265, "actor_loss": -40.72627081298828, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.099637508392334, "episode_reward": 237.0624585370464, "step": 112000}
{"episode": 113.0, "batch_reward": 0.20360438583791257, "actor_loss": -40.737460426330564, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.14406943321228, "episode_reward": 239.34088855585964, "step": 113000}
{"episode": 114.0, "batch_reward": 0.20346428126096724, "actor_loss": -40.760809608459475, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.27116894721985, "episode_reward": 249.83818792839864, "step": 114000}
{"episode": 115.0, "batch_reward": 0.2040938171595335, "actor_loss": -40.809631134033204, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.93566107749939, "episode_reward": 240.8762311701637, "step": 115000}
{"episode": 116.0, "batch_reward": 0.20420174586772918, "actor_loss": -40.7989239730835, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.302363395690918, "episode_reward": 233.86599355631944, "step": 116000}
{"episode": 117.0, "batch_reward": 0.2039921471774578, "actor_loss": -40.78881719970703, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.33971619606018, "episode_reward": 62.79236954964244, "step": 117000}
{"episode": 118.0, "batch_reward": 0.20374926269054414, "actor_loss": -40.68218957519531, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.96619486808777, "episode_reward": 227.808793587984, "step": 118000}
{"episode": 119.0, "batch_reward": 0.20389834266901016, "actor_loss": -40.69705201721192, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.276158571243286, "episode_reward": 240.09753733413032, "step": 119000}
{"episode": 120.0, "batch_reward": 0.20385984475910662, "actor_loss": -40.6865291595459, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.991764783859253, "episode_reward": 249.11614208596563, "step": 120000}
{"episode": 121.0, "batch_reward": 0.20418761390447618, "actor_loss": -40.726042320251466, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.22502565383911, "episode_reward": 239.25546386621457, "step": 121000}
{"episode": 122.0, "batch_reward": 0.2045523685961962, "actor_loss": -40.72843130493164, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.986976146697998, "episode_reward": 245.53457663543824, "step": 122000}
{"episode": 123.0, "batch_reward": 0.20519052283465863, "actor_loss": -40.81389862823487, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.85615110397339, "episode_reward": 198.9439099043291, "step": 123000}
{"episode": 124.0, "batch_reward": 0.20473797236382962, "actor_loss": -40.688245124816895, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.805471420288086, "episode_reward": 241.4061324286883, "step": 124000}
{"episode": 125.0, "batch_reward": 0.20493700355291367, "actor_loss": -40.73128713989258, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.917589902877808, "episode_reward": 99.23903504786742, "step": 125000}
{"episode": 126.0, "batch_reward": 0.20396719969809055, "actor_loss": -40.547041313171384, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.373727083206177, "episode_reward": 54.746652829836776, "step": 126000}
{"episode": 127.0, "batch_reward": 0.20234226827323437, "actor_loss": -40.3565682220459, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.16267704963684, "episode_reward": 195.72478014769183, "step": 127000}
{"episode": 128.0, "batch_reward": 0.20265071864426137, "actor_loss": -40.35691696929932, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.840112447738647, "episode_reward": 135.1524294472192, "step": 128000}
{"episode": 129.0, "batch_reward": 0.20226612660288812, "actor_loss": -40.23563843536377, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.392712354660034, "episode_reward": 219.36424020939205, "step": 129000}
{"episode": 130.0, "batch_reward": 0.20228845243155957, "actor_loss": -40.267208282470705, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.191964149475098, "episode_reward": 233.84227436129643, "step": 130000}
{"episode": 131.0, "batch_reward": 0.20287549434602262, "actor_loss": -40.34422595977783, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.04762601852417, "episode_reward": 246.279537079692, "step": 131000}
{"episode": 132.0, "batch_reward": 0.20317996829748153, "actor_loss": -40.31186892700195, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.926042079925537, "episode_reward": 229.67035995478804, "step": 132000}
{"episode": 133.0, "batch_reward": 0.2035253775715828, "actor_loss": -40.377914894104, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.96705937385559, "episode_reward": 251.80860325363145, "step": 133000}
{"episode": 134.0, "batch_reward": 0.20354570898413657, "actor_loss": -40.34006601715088, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.044851303100586, "episode_reward": 225.6473331305954, "step": 134000}
{"episode": 135.0, "batch_reward": 0.20414451496303082, "actor_loss": -40.42520958709717, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.068334817886353, "episode_reward": 238.38515646687935, "step": 135000}
{"episode": 136.0, "batch_reward": 0.20464698587358, "actor_loss": -40.48291209411621, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.458882331848145, "episode_reward": 236.71896661655785, "step": 136000}
{"episode": 137.0, "batch_reward": 0.204207327991724, "actor_loss": -40.464934455871585, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.617381811141968, "episode_reward": 226.79084620658423, "step": 137000}
{"episode": 138.0, "batch_reward": 0.20502387170493602, "actor_loss": -40.51240076446533, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.048954486846924, "episode_reward": 248.19784210984483, "step": 138000}
{"episode": 139.0, "batch_reward": 0.2049206618219614, "actor_loss": -40.50745446014405, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.98431134223938, "episode_reward": 236.74190622613804, "step": 139000}
{"episode": 140.0, "batch_reward": 0.2047256937623024, "actor_loss": -40.44884214782715, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.304179906845093, "episode_reward": 92.00827282794717, "step": 140000}
{"episode": 141.0, "batch_reward": 0.20450262811779976, "actor_loss": -40.4367592086792, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.31582427024841, "episode_reward": 234.98103218274753, "step": 141000}
{"episode": 142.0, "batch_reward": 0.20448971135914326, "actor_loss": -40.43310944366455, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.470640659332275, "episode_reward": 214.84497743975697, "step": 142000}
{"episode": 143.0, "batch_reward": 0.204688937112689, "actor_loss": -40.433425132751466, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.090543746948242, "episode_reward": 235.79781229085344, "step": 143000}
{"episode": 144.0, "batch_reward": 0.20510618126392363, "actor_loss": -40.47097343444824, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.892168521881104, "episode_reward": 230.78712784666365, "step": 144000}
{"episode": 145.0, "batch_reward": 0.2047278865277767, "actor_loss": -40.4844499130249, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.899269580841064, "episode_reward": 240.68435503248259, "step": 145000}
{"episode": 146.0, "batch_reward": 0.20504697129130364, "actor_loss": -40.46849236297607, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.41688108444214, "episode_reward": 240.25098867043778, "step": 146000}
{"episode": 147.0, "batch_reward": 0.20530290149152278, "actor_loss": -40.4701859588623, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.06078553199768, "episode_reward": 175.20852090575082, "step": 147000}
{"episode": 148.0, "batch_reward": 0.2053452305048704, "actor_loss": -40.44329433441162, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.001054525375366, "episode_reward": 215.14956979887708, "step": 148000}
{"episode": 149.0, "batch_reward": 0.20487042915821074, "actor_loss": -40.39907496643066, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.460323810577393, "episode_reward": 31.280337053794792, "step": 149000}
{"episode": 150.0, "batch_reward": 0.20420691464841365, "actor_loss": -40.30293016815185, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "step": 150000}
