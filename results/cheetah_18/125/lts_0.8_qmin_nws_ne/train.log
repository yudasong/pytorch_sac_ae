{"episode_reward": 0.0, "episode": 1.0, "duration": 19.56860876083374, "step": 1000}
{"episode_reward": 5.1931688606333894, "episode": 2.0, "duration": 1.677459716796875, "step": 2000}
{"episode_reward": 454.7514538707513, "episode": 3.0, "batch_reward": 0.21589715242206992, "critic_loss": 0.023564145856677602, "actor_loss": -36.72160551832074, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 69.86491751670837, "step": 3000}
{"episode_reward": 3.3074686913662563, "episode": 4.0, "batch_reward": 0.1343202358931303, "critic_loss": 0.01827297632698901, "actor_loss": -34.592507351875305, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.63546895980835, "step": 4000}
{"episode_reward": 3.1337295122070947, "episode": 5.0, "batch_reward": 0.103647412981838, "critic_loss": 0.012150201860815286, "actor_loss": -31.727395815372468, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.20538306236267, "step": 5000}
{"episode_reward": 2.790344531270488, "episode": 6.0, "batch_reward": 0.08508741698414088, "critic_loss": 0.013080629605334251, "actor_loss": -31.851817583560944, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.056630611419678, "step": 6000}
{"episode_reward": 3.385485650256606, "episode": 7.0, "batch_reward": 0.07245122818648815, "critic_loss": 0.011727359747514128, "actor_loss": -31.77927644300461, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.545912265777588, "step": 7000}
{"episode_reward": 2.2627618391924984, "episode": 8.0, "batch_reward": 0.06354708743654192, "critic_loss": 0.012141938878921793, "actor_loss": -30.886001942157744, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.219534158706665, "step": 8000}
{"episode_reward": 2.7910979868010504, "episode": 9.0, "batch_reward": 0.05607601465657353, "critic_loss": 0.010162891397252679, "actor_loss": -30.63213361120224, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.43612504005432, "step": 9000}
{"episode_reward": 2.5988662388053596, "episode": 10.0, "batch_reward": 0.05092118277586997, "critic_loss": 0.009694872073829174, "actor_loss": -30.703135189056397, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.165630340576172, "step": 10000}
{"episode_reward": 2.157090545879746, "episode": 11.0, "batch_reward": 0.046316691508516666, "critic_loss": 0.009919353228469844, "actor_loss": -30.35925093269348, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.405324935913086, "step": 11000}
{"episode_reward": 2.3075445894167634, "episode": 12.0, "batch_reward": 0.042728663632646206, "critic_loss": 0.008716746561345644, "actor_loss": -29.52904514312744, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.261630535125732, "step": 12000}
{"episode_reward": 2.570709640178338, "episode": 13.0, "batch_reward": 0.03911663343757391, "critic_loss": 0.007850017985445447, "actor_loss": -29.77958886575699, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.3065128326416, "step": 13000}
{"episode_reward": 2.331253747092887, "episode": 14.0, "batch_reward": 0.035936038112733514, "critic_loss": 0.008016589325969107, "actor_loss": -28.617408953666686, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.905526876449585, "step": 14000}
{"episode_reward": 1.9940689518428072, "episode": 15.0, "batch_reward": 0.0341286703501828, "critic_loss": 0.0070975121320225295, "actor_loss": -31.5477303853035, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.445597887039185, "step": 15000}
{"episode_reward": 2.7307654033564797, "episode": 16.0, "batch_reward": 0.0316874604029581, "critic_loss": 0.006688115903467406, "actor_loss": -29.5895114300251, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.453413248062134, "step": 16000}
{"episode_reward": 2.8932669533582676, "episode": 17.0, "batch_reward": 0.030090198371559383, "critic_loss": 0.006299794524093159, "actor_loss": -30.27687340712547, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.770259380340576, "step": 17000}
{"episode_reward": 2.783896883826463, "episode": 18.0, "batch_reward": 0.028699099391233177, "critic_loss": 0.005334120503568556, "actor_loss": -29.535737399339677, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.046258211135864, "step": 18000}
{"episode_reward": 2.7462566792900636, "episode": 19.0, "batch_reward": 0.027094260988757016, "critic_loss": 0.006673659800348105, "actor_loss": -28.48792835879326, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.522566080093384, "step": 19000}
{"episode_reward": 3.30952951281666, "episode": 20.0, "batch_reward": 0.025670250174589456, "critic_loss": 0.006263869686226826, "actor_loss": -29.742080246686935, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.41609263420105, "step": 20000}
{"episode_reward": 3.8003366852935274, "episode": 21.0, "batch_reward": 0.025189163654576987, "critic_loss": 0.008363142657850403, "actor_loss": -29.644450118541716, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.98388314247131, "step": 21000}
{"episode_reward": 2.8369655190227774, "episode": 22.0, "batch_reward": 0.023823983900481835, "critic_loss": 0.004960737992325448, "actor_loss": -29.451172675848007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.342053651809692, "step": 22000}
{"episode_reward": 2.677515669638101, "episode": 23.0, "batch_reward": 0.02329874508548528, "critic_loss": 0.0051336131805146575, "actor_loss": -30.449842346668245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.810352087020874, "step": 23000}
{"episode_reward": 3.3387372795803056, "episode": 24.0, "batch_reward": 0.021957542936084793, "critic_loss": 0.006133907712501241, "actor_loss": -29.565114810228348, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.635710954666138, "step": 24000}
{"episode_reward": 3.0042608242543243, "episode": 25.0, "batch_reward": 0.021201846179086714, "critic_loss": 0.0053540562872221925, "actor_loss": -28.962066340684892, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.181427001953125, "step": 25000}
{"episode_reward": 2.3094518838766858, "episode": 26.0, "batch_reward": 0.020556431285338475, "critic_loss": 0.005963691441225819, "actor_loss": -30.032453730821608, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.533385038375854, "step": 26000}
{"episode_reward": 2.602813133957084, "episode": 27.0, "batch_reward": 0.020102976933121682, "critic_loss": 0.00430803174508037, "actor_loss": -29.044486867785455, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.844597339630127, "step": 27000}
{"episode_reward": 3.3584055617392194, "episode": 28.0, "batch_reward": 0.01928456021635793, "critic_loss": 0.004583436021799571, "actor_loss": -28.608825877189638, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.803619384765625, "step": 28000}
{"episode_reward": 2.2743214760242028, "episode": 29.0, "batch_reward": 0.019374588862759994, "critic_loss": 0.003650418032615562, "actor_loss": -29.268420967578887, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.339928150177002, "step": 29000}
{"episode_reward": 3.1047254770192256, "episode": 30.0, "batch_reward": 0.01807829174445942, "critic_loss": 0.0052102620644436685, "actor_loss": -28.58158452296257, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.812707662582397, "step": 30000}
{"episode_reward": 2.4669471402803977, "episode": 31.0, "batch_reward": 0.017643597665941344, "critic_loss": 0.005064323305807193, "actor_loss": -28.647653834342957, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.90516781806946, "step": 31000}
{"episode_reward": 2.728969600876092, "episode": 32.0, "batch_reward": 0.016949924437096343, "critic_loss": 0.003990887997744721, "actor_loss": -27.944208074092867, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.550940990447998, "step": 32000}
{"episode_reward": 2.588425766268033, "episode": 33.0, "batch_reward": 0.016715275811729952, "critic_loss": 0.004750286332811811, "actor_loss": -29.074548144578934, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.156620025634766, "step": 33000}
{"episode_reward": 2.59501566837978, "episode": 34.0, "batch_reward": 0.016279802756849676, "critic_loss": 0.0033047219858563038, "actor_loss": -27.890157658815383, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.098621368408203, "step": 34000}
{"episode_reward": 2.4425625717493755, "episode": 35.0, "batch_reward": 0.015753875109017827, "critic_loss": 0.003585430033184821, "actor_loss": -29.523742022275925, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.031986474990845, "step": 35000}
{"episode_reward": 2.9456377039151107, "episode": 36.0, "batch_reward": 0.015372033863095567, "critic_loss": 0.0032525951909337892, "actor_loss": -27.62261126983166, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.87153911590576, "step": 36000}
{"episode_reward": 2.4865769031641123, "episode": 37.0, "batch_reward": 0.015163127526640893, "critic_loss": 0.00383265160630981, "actor_loss": -29.272644202947617, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.74638319015503, "step": 37000}
{"episode_reward": 2.74230216218913, "episode": 38.0, "batch_reward": 0.014896403336664662, "critic_loss": 0.003657802571877255, "actor_loss": -29.115402441978453, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.696162939071655, "step": 38000}
{"episode_reward": 2.6209439245104247, "episode": 39.0, "batch_reward": 0.014390549775678664, "critic_loss": 0.0037631651130868703, "actor_loss": -29.121043302297593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.133193969726562, "step": 39000}
{"episode_reward": 3.308924124508676, "episode": 40.0, "batch_reward": 0.014235313650453463, "critic_loss": 0.003339082795500872, "actor_loss": -29.369968552947043, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.168302297592163, "step": 40000}
{"episode_reward": 2.5695286090569263, "episode": 41.0, "batch_reward": 0.014068705644225701, "critic_loss": 0.0031068235006387114, "actor_loss": -29.492101032674313, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.18584966659546, "step": 41000}
{"episode_reward": 3.0670868929662163, "episode": 42.0, "batch_reward": 0.013404451222857461, "critic_loss": 0.0030109354893356794, "actor_loss": -28.21404375177622, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.7168550491333, "step": 42000}
{"episode_reward": 2.670005537381984, "episode": 43.0, "batch_reward": 0.013406407619360834, "critic_loss": 0.004623732481108164, "actor_loss": -28.571371969759465, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.628862380981445, "step": 43000}
{"episode_reward": 3.1568789841769807, "episode": 44.0, "batch_reward": 0.013134231940610334, "critic_loss": 0.003734086017386289, "actor_loss": -28.399407533466817, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.252740621566772, "step": 44000}
{"episode_reward": 3.492129160607199, "episode": 45.0, "batch_reward": 0.013137577696004883, "critic_loss": 0.003189688061524066, "actor_loss": -28.53239646446705, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.703776359558105, "step": 45000}
{"episode_reward": 3.2172908077775872, "episode": 46.0, "batch_reward": 0.012658023266121745, "critic_loss": 0.003731059665195062, "actor_loss": -28.382805050253868, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.803473711013794, "step": 46000}
{"episode_reward": 2.789065463520434, "episode": 47.0, "batch_reward": 0.012892497238586657, "critic_loss": 0.0030087711982341716, "actor_loss": -29.077014926671982, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.756185054779053, "step": 47000}
{"episode_reward": 2.7928835189333845, "episode": 48.0, "batch_reward": 0.012240636921022088, "critic_loss": 0.0026018764690088573, "actor_loss": -28.56781418222189, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.926342010498047, "step": 48000}
{"episode_reward": 2.6550069330241044, "episode": 49.0, "batch_reward": 0.01236367458337918, "critic_loss": 0.0034079897117553627, "actor_loss": -29.553753025770188, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.708425998687744, "step": 49000}
{"episode_reward": 3.047374723826269, "episode": 50.0, "batch_reward": 0.011748558141407557, "critic_loss": 0.0028294697357705446, "actor_loss": -28.46010954833031, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.83567714691162, "step": 50000}
{"episode_reward": 2.5980884641695186, "episode": 51.0, "batch_reward": 0.011643796816351824, "critic_loss": 0.0028274662925468876, "actor_loss": -28.681434573709964, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.24340200424194, "step": 51000}
{"episode_reward": 3.6451148573709062, "episode": 52.0, "batch_reward": 0.011633667121408507, "critic_loss": 0.003056582677112601, "actor_loss": -28.456402696192264, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.72871232032776, "step": 52000}
{"episode_reward": 2.8375055049016034, "episode": 53.0, "batch_reward": 0.011664350894279777, "critic_loss": 0.0023388183724164266, "actor_loss": -28.804726601302622, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.494189500808716, "step": 53000}
{"episode_reward": 3.1705651927304963, "episode": 54.0, "batch_reward": 0.011017039348022081, "critic_loss": 0.0023142877722129923, "actor_loss": -28.366163373827934, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.803303956985474, "step": 54000}
{"episode_reward": 3.652411531055063, "episode": 55.0, "batch_reward": 0.011258502008393408, "critic_loss": 0.003165839626453817, "actor_loss": -29.615926589608193, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.05253553390503, "step": 55000}
{"episode_reward": 3.0206273011673686, "episode": 56.0, "batch_reward": 0.011051272131386214, "critic_loss": 0.0019116069646333926, "actor_loss": -29.19333503395319, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.340116500854492, "step": 56000}
{"episode_reward": 2.209040764192797, "episode": 57.0, "batch_reward": 0.010600659943185747, "critic_loss": 0.0025646510098231374, "actor_loss": -28.129905816435812, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.58316659927368, "step": 57000}
{"episode_reward": 2.7489053515400315, "episode": 58.0, "batch_reward": 0.01081531810737215, "critic_loss": 0.0021486451115852105, "actor_loss": -28.48721021068096, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.81505584716797, "step": 58000}
{"episode_reward": 2.2525946683280846, "episode": 59.0, "batch_reward": 0.01028400562168099, "critic_loss": 0.0028102949730091495, "actor_loss": -28.029953344762326, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.190213441848755, "step": 59000}
{"episode_reward": 2.6902465269886227, "episode": 60.0, "batch_reward": 0.01056042139895726, "critic_loss": 0.002564711955405073, "actor_loss": -28.27702796459198, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.610441207885742, "step": 60000}
{"episode_reward": 2.6812943527169475, "episode": 61.0, "batch_reward": 0.010342100420501084, "critic_loss": 0.0026774504370623616, "actor_loss": -27.850424401551486, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.80529761314392, "step": 61000}
{"episode_reward": 3.1903580548536876, "episode": 62.0, "batch_reward": 0.010320536198676563, "critic_loss": 0.0020886620216915616, "actor_loss": -29.23681791806221, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.65766930580139, "step": 62000}
{"episode_reward": 2.43849039106214, "episode": 63.0, "batch_reward": 0.010105240924167447, "critic_loss": 0.0025786757842870428, "actor_loss": -28.382175438523294, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.310715675354004, "step": 63000}
{"episode_reward": 3.399988375066311, "episode": 64.0, "batch_reward": 0.009648251499747858, "critic_loss": 0.0022109577330338653, "actor_loss": -28.48804998242855, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.730212688446045, "step": 64000}
{"episode_reward": 3.4312744795924814, "episode": 65.0, "batch_reward": 0.009721316637587733, "critic_loss": 0.0024181968118573423, "actor_loss": -28.018194995194673, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.83880376815796, "step": 65000}
{"episode_reward": 3.3327518807201844, "episode": 66.0, "batch_reward": 0.009844687817385421, "critic_loss": 0.0020071706910312057, "actor_loss": -28.232143401801586, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.316900491714478, "step": 66000}
{"episode_reward": 2.5383060528333115, "episode": 67.0, "batch_reward": 0.00966855465946719, "critic_loss": 0.002079279378434876, "actor_loss": -28.49538794794679, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.771373510360718, "step": 67000}
{"episode_reward": 3.181053335618832, "episode": 68.0, "batch_reward": 0.00952525949536357, "critic_loss": 0.0025519601446649175, "actor_loss": -28.00103832629323, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.13651156425476, "step": 68000}
{"episode_reward": 2.4675672141613605, "episode": 69.0, "batch_reward": 0.009262398382881657, "critic_loss": 0.002624202678402071, "actor_loss": -28.273490485846995, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.476417303085327, "step": 69000}
{"episode_reward": 2.6018985690878447, "episode": 70.0, "batch_reward": 0.009534166296594777, "critic_loss": 0.002105471261231287, "actor_loss": -28.564073673665522, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.943857669830322, "step": 70000}
{"episode_reward": 2.6122295910558004, "episode": 71.0, "batch_reward": 0.009496949744061567, "critic_loss": 0.0032112924669891072, "actor_loss": -28.84885134705901, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.69471740722656, "step": 71000}
{"episode_reward": 2.228142275317059, "episode": 72.0, "batch_reward": 0.009255771798780187, "critic_loss": 0.002066646652587224, "actor_loss": -28.909701117157937, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.93923592567444, "step": 72000}
{"episode_reward": 2.628180689002775, "episode": 73.0, "batch_reward": 0.00915714455395937, "critic_loss": 0.002274368367383431, "actor_loss": -28.399867357343435, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.890968561172485, "step": 73000}
{"episode_reward": 3.860928787531032, "episode": 74.0, "batch_reward": 0.008957045006332919, "critic_loss": 0.002008144369472575, "actor_loss": -29.032104430437087, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.781554222106934, "step": 74000}
{"episode_reward": 3.865352057222899, "episode": 75.0, "batch_reward": 0.008794733277522027, "critic_loss": 0.001863900107651716, "actor_loss": -28.238833527475595, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.442312002182007, "step": 75000}
{"episode_reward": 2.2679502390899495, "episode": 76.0, "batch_reward": 0.008872246857499703, "critic_loss": 0.0023651015102368546, "actor_loss": -28.597684174329043, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.740718364715576, "step": 76000}
{"episode_reward": 3.2802384829080347, "episode": 77.0, "batch_reward": 0.008678802828653716, "critic_loss": 0.002128192388925527, "actor_loss": -27.81847985550761, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.31930661201477, "step": 77000}
{"episode_reward": 2.212870852890308, "episode": 78.0, "batch_reward": 0.008981058236095123, "critic_loss": 0.0017867329557120684, "actor_loss": -28.671144779115917, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.226356744766235, "step": 78000}
{"episode_reward": 2.5093374462281233, "episode": 79.0, "batch_reward": 0.008415931817842648, "critic_loss": 0.0018713027369085466, "actor_loss": -26.746561147600413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.927040576934814, "step": 79000}
{"episode_reward": 3.205846555459374, "episode": 80.0, "batch_reward": 0.008419755599228666, "critic_loss": 0.001613243284275086, "actor_loss": -27.863080510675907, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.12653136253357, "step": 80000}
{"episode_reward": 2.374547656559561, "episode": 81.0, "batch_reward": 0.008270583448698745, "critic_loss": 0.002477973422839568, "actor_loss": -27.63620914106071, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.571478843688965, "step": 81000}
{"episode_reward": 3.912278944427677, "episode": 82.0, "batch_reward": 0.008502918935962953, "critic_loss": 0.0016292523722804618, "actor_loss": -29.213065321937204, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.362996101379395, "step": 82000}
{"episode_reward": 2.5302190185613043, "episode": 83.0, "batch_reward": 0.008180639426456764, "critic_loss": 0.0015429580728596193, "actor_loss": -27.63961515341699, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.18007755279541, "step": 83000}
{"episode_reward": 2.8583997296240296, "episode": 84.0, "batch_reward": 0.008037255732808262, "critic_loss": 0.0017387219276861288, "actor_loss": -29.75443715378642, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.50697612762451, "step": 84000}
{"episode_reward": 3.5452689129437354, "episode": 85.0, "batch_reward": 0.008322134617133997, "critic_loss": 0.0017801862352280296, "actor_loss": -28.42244819919765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.788827896118164, "step": 85000}
{"episode_reward": 2.802631824281688, "episode": 86.0, "batch_reward": 0.0081254511581501, "critic_loss": 0.0011952121618160163, "actor_loss": -28.33390901619196, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.819611072540283, "step": 86000}
{"episode_reward": 3.5793134813071443, "episode": 87.0, "batch_reward": 0.008139222439727745, "critic_loss": 0.0011646792837891552, "actor_loss": -28.134976883038878, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.874626874923706, "step": 87000}
{"episode_reward": 2.573593656343527, "episode": 88.0, "batch_reward": 0.008116758660762571, "critic_loss": 0.001935897110575752, "actor_loss": -27.42156914819777, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.829174280166626, "step": 88000}
{"episode_reward": 3.0359093625338787, "episode": 89.0, "batch_reward": 0.008130599832162261, "critic_loss": 0.0015576673917385051, "actor_loss": -28.420422563403847, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.45506191253662, "step": 89000}
{"episode_reward": 2.8695860954112637, "episode": 90.0, "batch_reward": 0.007921248009661213, "critic_loss": 0.002140098536354344, "actor_loss": -28.834640723645688, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.104345560073853, "step": 90000}
{"episode_reward": 3.3248032712354094, "episode": 91.0, "batch_reward": 0.008096255867159926, "critic_loss": 0.0013762315426974964, "actor_loss": -28.342878020524978, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.856807708740234, "step": 91000}
{"episode_reward": 2.205392931773808, "episode": 92.0, "batch_reward": 0.008027068652561866, "critic_loss": 0.0021613339122704927, "actor_loss": -28.125659352138637, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.33709692955017, "step": 92000}
{"episode_reward": 3.1946645687575956, "episode": 93.0, "batch_reward": 0.007741695956443436, "critic_loss": 0.0013374007309248555, "actor_loss": -27.620107823759316, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.44263505935669, "step": 93000}
{"episode_reward": 3.672946875356698, "episode": 94.0, "batch_reward": 0.007622098732972518, "critic_loss": 0.0016463461055609514, "actor_loss": -28.26348824901879, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.0312979221344, "step": 94000}
{"episode_reward": 2.360755906918658, "episode": 95.0, "batch_reward": 0.007662569196196273, "critic_loss": 0.002036680897945189, "actor_loss": -28.54767675179243, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.695597887039185, "step": 95000}
{"episode_reward": 2.555023729338144, "episode": 96.0, "batch_reward": 0.0076320735161425545, "critic_loss": 0.0013833103570541426, "actor_loss": -28.73987238994241, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.666266202926636, "step": 96000}
{"episode_reward": 3.806101839690931, "episode": 97.0, "batch_reward": 0.007654884874937124, "critic_loss": 0.0016418166732291865, "actor_loss": -29.183342237487434, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.92896842956543, "step": 97000}
{"episode_reward": 2.7017720563716012, "episode": 98.0, "batch_reward": 0.00758235631044954, "critic_loss": 0.0016938584163399355, "actor_loss": -28.598381928741933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.23244071006775, "step": 98000}
{"episode_reward": 2.6895361680236487, "episode": 99.0, "batch_reward": 0.007361969691351987, "critic_loss": 0.0013208110692248737, "actor_loss": -27.67593359081447, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.26637077331543, "step": 99000}
{"episode_reward": 3.208811998281175, "episode": 100.0, "batch_reward": 0.007512573405518196, "critic_loss": 0.0014425217889693159, "actor_loss": -28.311932915255426, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.229846239089966, "step": 100000}
{"episode_reward": 2.898137363498231, "episode": 101.0, "batch_reward": 0.0072637587945209815, "critic_loss": 0.0017123183884723402, "actor_loss": -28.37503493504226, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.63358211517334, "step": 101000}
{"episode_reward": 2.4242551501784675, "episode": 102.0, "batch_reward": 0.007440468292334117, "critic_loss": 0.0016753904131728632, "actor_loss": -27.818434380292892, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.97098159790039, "step": 102000}
{"episode_reward": 2.6957133899721994, "episode": 103.0, "batch_reward": 0.007434826612705365, "critic_loss": 0.0020145081684422622, "actor_loss": -28.663223407670856, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.70717692375183, "step": 103000}
{"episode_reward": 2.5606171235175474, "episode": 104.0, "batch_reward": 0.007318802984314971, "critic_loss": 0.0013844664563184778, "actor_loss": -28.724410663083194, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.44933581352234, "step": 104000}
{"episode_reward": 2.3018798775909115, "episode": 105.0, "batch_reward": 0.007328086208319292, "critic_loss": 0.001978005766894057, "actor_loss": -28.588612793460488, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.900558471679688, "step": 105000}
{"episode_reward": 3.1245344731385956, "episode": 106.0, "batch_reward": 0.0072345686221960935, "critic_loss": 0.001327387478599121, "actor_loss": -28.24118509362638, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.361475944519043, "step": 106000}
{"episode_reward": 2.557755520736266, "episode": 107.0, "batch_reward": 0.007230295303510502, "critic_loss": 0.0019001537983349408, "actor_loss": -28.7958730064407, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.305965900421143, "step": 107000}
{"episode_reward": 2.5072296346621803, "episode": 108.0, "batch_reward": 0.007050841731135733, "critic_loss": 0.002007735634902929, "actor_loss": -28.301771259084344, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.262986183166504, "step": 108000}
{"episode_reward": 2.2638820283905923, "episode": 109.0, "batch_reward": 0.006896220458322205, "critic_loss": 0.001347388991387561, "actor_loss": -29.699950831621884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.81147050857544, "step": 109000}
{"episode_reward": 2.4805853887951845, "episode": 110.0, "batch_reward": 0.0068658796973759305, "critic_loss": 0.0018944983394321752, "actor_loss": -29.066208598427476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.59766435623169, "step": 110000}
{"episode_reward": 3.2549372041305107, "episode": 111.0, "batch_reward": 0.006931995510240085, "critic_loss": 0.0020687124217547534, "actor_loss": -29.01140061570704, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.327693700790405, "step": 111000}
{"episode_reward": 2.3686555131322518, "episode": 112.0, "batch_reward": 0.006993929275777191, "critic_loss": 0.0015280281577215646, "actor_loss": -27.79955211095512, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.24341654777527, "step": 112000}
{"episode_reward": 3.129484084500281, "episode": 113.0, "batch_reward": 0.006752959313686005, "critic_loss": 0.0018442034698928182, "actor_loss": -28.709436063840986, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.217952728271484, "step": 113000}
{"episode_reward": 2.4168658818694766, "episode": 114.0, "batch_reward": 0.0067388877114281055, "critic_loss": 0.0014680483850279416, "actor_loss": -29.346486061364413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.5318284034729, "step": 114000}
{"episode_reward": 2.8630159668302606, "episode": 115.0, "batch_reward": 0.006795905258506536, "critic_loss": 0.0014864687193075952, "actor_loss": -28.496698593750597, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.448997974395752, "step": 115000}
{"episode_reward": 2.871814449710577, "episode": 116.0, "batch_reward": 0.006831889239605516, "critic_loss": 0.0013936258978719707, "actor_loss": -28.440580180346966, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.771671056747437, "step": 116000}
{"episode_reward": 2.9270206945994506, "episode": 117.0, "batch_reward": 0.006935412118560635, "critic_loss": 0.0018481738503796806, "actor_loss": -28.094356453888118, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.013689279556274, "step": 117000}
{"episode_reward": 2.7294884291833377, "episode": 118.0, "batch_reward": 0.006846549631212838, "critic_loss": 0.0017189150367194089, "actor_loss": -28.65114751443267, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.974645614624023, "step": 118000}
{"episode_reward": 2.0434918336186354, "episode": 119.0, "batch_reward": 0.006647662841831334, "critic_loss": 0.001481794777111645, "actor_loss": -28.779827593758704, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.596046209335327, "step": 119000}
{"episode_reward": 2.2455636098351244, "episode": 120.0, "batch_reward": 0.006526548049063422, "critic_loss": 0.001474835030745453, "actor_loss": -28.427509828791024, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.39367127418518, "step": 120000}
{"episode_reward": 3.107746326919916, "episode": 121.0, "batch_reward": 0.006640771719161421, "critic_loss": 0.0012083288675967196, "actor_loss": -27.72197740803659, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.48152756690979, "step": 121000}
{"episode_reward": 3.830500275428532, "episode": 122.0, "batch_reward": 0.006608437520102598, "critic_loss": 0.0014102147808189329, "actor_loss": -28.138086722165347, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.35935664176941, "step": 122000}
{"episode_reward": 2.60791737849331, "episode": 123.0, "batch_reward": 0.006577882089302875, "critic_loss": 0.0016211605080061417, "actor_loss": -27.080052195951342, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.004352569580078, "step": 123000}
{"episode_reward": 2.082662961289215, "episode": 124.0, "batch_reward": 0.006515305057750083, "critic_loss": 0.0016536501747541478, "actor_loss": -27.933535648867487, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.391608953475952, "step": 124000}
{"episode_reward": 3.156734684207895, "episode": 125.0, "batch_reward": 0.00637579639814794, "critic_loss": 0.0014772669709236651, "actor_loss": -27.639143812164665, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.264894008636475, "step": 125000}
{"episode_reward": 3.1298754471682595, "episode": 126.0, "batch_reward": 0.006340731176431291, "critic_loss": 0.0012081129817706824, "actor_loss": -27.00226843097061, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.024126768112183, "step": 126000}
{"episode_reward": 3.2560821822068267, "episode": 127.0, "batch_reward": 0.0064501197257777676, "critic_loss": 0.0015964437043658108, "actor_loss": -28.337679539293052, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.36746573448181, "step": 127000}
{"episode_reward": 2.545491872688213, "episode": 128.0, "batch_reward": 0.006386747610988095, "critic_loss": 0.0010013124969955242, "actor_loss": -29.25231041241437, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.275546312332153, "step": 128000}
{"episode_reward": 3.1139443464598546, "episode": 129.0, "batch_reward": 0.006441627713153139, "critic_loss": 0.0017454130849473585, "actor_loss": -28.48647325850278, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.452181816101074, "step": 129000}
{"episode_reward": 3.2798820886604894, "episode": 130.0, "batch_reward": 0.006342616485897451, "critic_loss": 0.0012513296542128955, "actor_loss": -28.3426458401829, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.224306106567383, "step": 130000}
{"episode_reward": 2.9455157783781263, "episode": 131.0, "batch_reward": 0.006360362223465927, "critic_loss": 0.0017618440575497517, "actor_loss": -29.04003482286632, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.75432062149048, "step": 131000}
{"episode_reward": 2.7976905763614504, "episode": 132.0, "batch_reward": 0.006399890074157156, "critic_loss": 0.0014452540665624838, "actor_loss": -28.900588075846432, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.759983777999878, "step": 132000}
{"episode_reward": 3.883961443228989, "episode": 133.0, "batch_reward": 0.006296212053624913, "critic_loss": 0.0014447111387235053, "actor_loss": -28.482082864902914, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.50174379348755, "step": 133000}
{"episode_reward": 2.8798370386711842, "episode": 134.0, "batch_reward": 0.006412444676971063, "critic_loss": 0.0015298859841750528, "actor_loss": -28.902991076380015, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.18587303161621, "step": 134000}
{"episode_reward": 3.5231815415851435, "episode": 135.0, "batch_reward": 0.0062989519884577025, "critic_loss": 0.0018076407992157328, "actor_loss": -28.609648471526803, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.101945161819458, "step": 135000}
{"episode_reward": 2.2157095129903883, "episode": 136.0, "batch_reward": 0.006084212823072448, "critic_loss": 0.0027272265077990594, "actor_loss": -29.66003166102618, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.481398344039917, "step": 136000}
{"episode_reward": 2.7014047672550476, "episode": 137.0, "batch_reward": 0.006217667445424012, "critic_loss": 0.003139329122241179, "actor_loss": -28.822173702597617, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.212915658950806, "step": 137000}
{"episode_reward": 2.674852681031888, "episode": 138.0, "batch_reward": 0.006364964207867161, "critic_loss": 0.0032469529494883317, "actor_loss": -27.146878906205295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.26552152633667, "step": 138000}
{"episode_reward": 3.0246102218779893, "episode": 139.0, "batch_reward": 0.006240525469765998, "critic_loss": 0.001772989574175881, "actor_loss": -27.695484592586755, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.447554111480713, "step": 139000}
{"episode_reward": 2.966046708082641, "episode": 140.0, "batch_reward": 0.006078543491777964, "critic_loss": 0.0016110706161016424, "actor_loss": -27.062530956223608, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.56033229827881, "step": 140000}
{"episode_reward": 3.039870662923324, "episode": 141.0, "batch_reward": 0.006122524806996808, "critic_loss": 0.001784697874878475, "actor_loss": -27.619701856285335, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.891772985458374, "step": 141000}
{"episode_reward": 2.787164778847733, "episode": 142.0, "batch_reward": 0.006126895587425679, "critic_loss": 0.0017886053910115152, "actor_loss": -27.901245590142906, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.779104709625244, "step": 142000}
{"episode_reward": 2.645273842955928, "episode": 143.0, "batch_reward": 0.006046271858969703, "critic_loss": 0.002149438717515295, "actor_loss": -28.458354305572808, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.30116605758667, "step": 143000}
{"episode_reward": 4.050793048740385, "episode": 144.0, "batch_reward": 0.00624353966396302, "critic_loss": 0.0024871152567866373, "actor_loss": -28.255839002013207, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.499905347824097, "step": 144000}
{"episode_reward": 2.8609358221581136, "episode": 145.0, "batch_reward": 0.0059769333170261235, "critic_loss": 0.0013265800629342266, "actor_loss": -27.89146017022431, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.881175756454468, "step": 145000}
{"episode_reward": 2.096617647403695, "episode": 146.0, "batch_reward": 0.00601032555103302, "critic_loss": 0.001099849967256887, "actor_loss": -27.69746102768928, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.10447072982788, "step": 146000}
{"episode_reward": 3.9413459564709448, "episode": 147.0, "batch_reward": 0.005913868499104865, "critic_loss": 0.0007663730294698326, "actor_loss": -28.798869617760182, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.67365026473999, "step": 147000}
{"episode_reward": 1.9873865571539462, "episode": 148.0, "batch_reward": 0.006297167164972052, "critic_loss": 0.0015321439131439547, "actor_loss": -27.905945878483355, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.581011533737183, "step": 148000}
{"episode_reward": 2.5921585055032406, "episode": 149.0, "batch_reward": 0.005929540626937523, "critic_loss": 0.0010480295914239832, "actor_loss": -28.314665248252453, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.32537841796875, "step": 149000}
{"episode_reward": 2.721219134243649, "episode": 150.0, "batch_reward": 0.0058797615073854104, "critic_loss": 0.0010897692635990098, "actor_loss": -27.33401717416942, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
