{"episode": 1.0, "duration": 16.433914184570312, "episode_reward": 5.1931688606333894, "step": 1000}
{"episode": 2.0, "duration": 1.4405410289764404, "episode_reward": 454.7514538707513, "step": 2000}
{"episode": 3.0, "batch_reward": 0.23013642891401767, "critic_loss": 0.19790571347927402, "actor_loss": -45.317839985420434, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 79.55433416366577, "episode_reward": 255.6612838199836, "step": 3000}
{"episode": 4.0, "batch_reward": 0.24980341377854348, "critic_loss": 0.21813413208723068, "actor_loss": -45.35863484954834, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 24.9244704246521, "episode_reward": 280.32277023531583, "step": 4000}
{"episode": 5.0, "batch_reward": 0.26049270208179953, "critic_loss": 0.2203179013207555, "actor_loss": -45.5626410446167, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 25.267173528671265, "episode_reward": 383.6904718210636, "step": 5000}
{"episode": 6.0, "batch_reward": 0.2732628093659878, "critic_loss": 0.2294050675109029, "actor_loss": -45.93598383331299, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 25.20691967010498, "episode_reward": 253.80777121909054, "step": 6000}
{"episode": 7.0, "batch_reward": 0.27649409680068493, "critic_loss": 0.21476095025241376, "actor_loss": -45.8525005569458, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.686941862106323, "episode_reward": 356.2179485203756, "step": 7000}
{"episode": 8.0, "batch_reward": 0.287871871650219, "critic_loss": 0.20777088168263436, "actor_loss": -46.24153385162354, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 25.51501989364624, "episode_reward": 369.5999816937484, "step": 8000}
{"episode": 9.0, "batch_reward": 0.2945325113236904, "critic_loss": 0.2506924869120121, "actor_loss": -46.45154103088379, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 25.304752111434937, "episode_reward": 326.0432710089194, "step": 9000}
{"episode": 10.0, "batch_reward": 0.2985078125, "critic_loss": 0.26756804041564464, "actor_loss": -39.370402954101564, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 3574.8328869342804, "episode_reward": 350.17713436301653, "step": 10000}
{"episode": 11.0, "batch_reward": 0.30519830510020257, "critic_loss": 0.20449637456983327, "actor_loss": -39.666964073181155, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.73643445968628, "episode_reward": 356.57499798947117, "step": 11000}
{"episode": 12.0, "batch_reward": 0.31010210907459257, "critic_loss": 0.19887552830576896, "actor_loss": -36.37614172363281, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 426.82921171188354, "episode_reward": 329.7951915344742, "step": 12000}
{"episode": 13.0, "batch_reward": 0.31020177817344663, "critic_loss": 0.20059326629340649, "actor_loss": -36.24883578491211, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.807369470596313, "episode_reward": 327.1076585884829, "step": 13000}
{"episode": 14.0, "batch_reward": 0.3069666066467762, "critic_loss": 0.2073645090162754, "actor_loss": -33.992951961517335, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 376.8890132904053, "episode_reward": 114.42208580658497, "step": 14000}
{"episode": 15.0, "batch_reward": 0.29912121507525447, "critic_loss": 0.1841934860870242, "actor_loss": -33.04590572357178, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.85917854309082, "episode_reward": 371.7987101395502, "step": 15000}
{"episode": 16.0, "batch_reward": 0.30392341086268426, "critic_loss": 0.17622265622764827, "actor_loss": -32.45488571929932, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 432.58832716941833, "episode_reward": 366.2082337385127, "step": 16000}
{"episode": 17.0, "batch_reward": 0.3075883302390575, "critic_loss": 0.1832561100050807, "actor_loss": -32.622221935272215, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 25.381356477737427, "episode_reward": 366.04838361874675, "step": 17000}
