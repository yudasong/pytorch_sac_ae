{"episode": 1.0, "duration": 20.563458681106567, "episode_reward": 5.1931688606333894, "step": 1000}
{"episode": 2.0, "duration": 1.6716463565826416, "episode_reward": 454.7514538707513, "step": 2000}
{"episode": 3.0, "batch_reward": 0.21806157865785447, "actor_loss": -45.31269693546364, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 55.02331852912903, "episode_reward": 40.690310003824465, "step": 3000}
{"episode": 4.0, "batch_reward": 0.15112638612836599, "actor_loss": -40.70713233947754, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.03862714767456, "episode_reward": 52.496236151187595, "step": 4000}
{"episode": 5.0, "batch_reward": 0.13078189555555583, "actor_loss": -39.115888671875, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.436853885650635, "episode_reward": 73.64539592644262, "step": 5000}
{"episode": 6.0, "batch_reward": 0.12078346601128578, "actor_loss": -38.08102365875244, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.4411039352417, "episode_reward": 88.21662397061368, "step": 6000}
{"episode": 7.0, "batch_reward": 0.120830133497715, "actor_loss": -38.07447946929932, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.358948945999146, "episode_reward": 131.9972341223638, "step": 7000}
{"episode": 8.0, "batch_reward": 0.12081507482379675, "actor_loss": -37.65043634033203, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.848106861114502, "episode_reward": 85.203127561959, "step": 8000}
{"episode": 9.0, "batch_reward": 0.11373555751889944, "actor_loss": -35.62164152526856, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.894768238067627, "episode_reward": 44.57923725992504, "step": 9000}
{"episode": 10.0, "batch_reward": 0.10593762438744307, "actor_loss": -29.274741695404053, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 3999.6742177009583, "episode_reward": 30.82225732492181, "step": 10000}
{"episode": 11.0, "batch_reward": 0.10480010757595301, "actor_loss": -28.532261405944823, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.77024960517883, "episode_reward": 163.64629398076386, "step": 11000}
{"episode": 12.0, "batch_reward": 0.11039582509547471, "actor_loss": -25.755104496002197, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.64957880973816, "episode_reward": 139.70546184789828, "step": 12000}
{"episode": 13.0, "batch_reward": 0.11056999967247248, "actor_loss": -25.179313793182374, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.726717710494995, "episode_reward": 89.19494874791546, "step": 13000}
{"episode": 14.0, "batch_reward": 0.11348131296783685, "actor_loss": -23.413497039794922, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 438.1690390110016, "episode_reward": 293.85177572958094, "step": 14000}
{"episode": 15.0, "batch_reward": 0.1274880054667592, "actor_loss": -24.681044384002686, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.43821096420288, "episode_reward": 311.1166662132894, "step": 15000}
{"episode": 16.0, "batch_reward": 0.13914739768207074, "actor_loss": -24.44840534210205, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.19431471824646, "episode_reward": 326.3694512112848, "step": 16000}
{"episode": 17.0, "batch_reward": 0.14763738748431204, "actor_loss": -25.07457634353638, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.6784086227417, "episode_reward": 129.25194020634683, "step": 17000}
{"episode": 18.0, "batch_reward": 0.148661909006536, "actor_loss": -24.233971694946288, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 432.666068315506, "episode_reward": 308.274912366977, "step": 18000}
{"episode": 19.0, "batch_reward": 0.15869441136717796, "actor_loss": -25.239968482971193, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.858166456222534, "episode_reward": 338.47936802066073, "step": 19000}
{"episode": 20.0, "batch_reward": 0.16822060027718544, "actor_loss": -25.542912509918214, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 426.8703043460846, "episode_reward": 338.64151304738374, "step": 20000}
{"episode": 21.0, "batch_reward": 0.1753128779232502, "actor_loss": -26.286496112823485, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 40.28789448738098, "episode_reward": 305.1233006315911, "step": 21000}
{"episode": 22.0, "batch_reward": 0.18068354894220828, "actor_loss": -26.516181182861327, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.4684250354767, "episode_reward": 302.7952057609698, "step": 22000}
{"episode": 23.0, "batch_reward": 0.18616912028193475, "actor_loss": -26.9512707862854, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.20363974571228, "episode_reward": 325.9777187877424, "step": 23000}
{"episode": 24.0, "batch_reward": 0.19383450800180435, "actor_loss": -27.198528175354003, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.6575255393982, "episode_reward": 373.50010121208715, "step": 24000}
{"episode": 25.0, "batch_reward": 0.19937220637500286, "actor_loss": -27.592668281555177, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.902722358703613, "episode_reward": 381.9097532238595, "step": 25000}
{"episode": 26.0, "batch_reward": 0.20839946587383748, "actor_loss": -28.51619459915161, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 443.83594608306885, "episode_reward": 361.36072521405083, "step": 26000}
{"episode": 27.0, "batch_reward": 0.2135671538859606, "actor_loss": -28.987602172851563, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.797993421554565, "episode_reward": 354.5991534202676, "step": 27000}
{"episode": 28.0, "batch_reward": 0.2196312114149332, "actor_loss": -29.105192527770996, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 430.49265480041504, "episode_reward": 362.17036793340776, "step": 28000}
{"episode": 29.0, "batch_reward": 0.22426717323064804, "actor_loss": -29.447377918243408, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.502049446105957, "episode_reward": 371.05884481617494, "step": 29000}
{"episode": 30.0, "batch_reward": 0.22918201072514058, "actor_loss": -29.34889694595337, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 441.25589895248413, "episode_reward": 384.01354188558605, "step": 30000}
{"episode": 31.0, "batch_reward": 0.23395060062408446, "actor_loss": -29.866851894378662, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.20829200744629, "episode_reward": 349.0414441744252, "step": 31000}
{"episode": 32.0, "batch_reward": 0.23738105992972852, "actor_loss": -29.58924752807617, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.1414301395416, "episode_reward": 395.1455218962753, "step": 32000}
{"episode": 33.0, "batch_reward": 0.24189618362486362, "actor_loss": -30.060319232940675, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.48066806793213, "episode_reward": 357.1353328980899, "step": 33000}
{"episode": 34.0, "batch_reward": 0.2454853485375643, "actor_loss": -29.67634715652466, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 439.73701000213623, "episode_reward": 343.70118231524043, "step": 34000}
{"episode": 35.0, "batch_reward": 0.2486457949578762, "actor_loss": -29.997784103393556, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.7847580909729, "episode_reward": 387.8738603337186, "step": 35000}
{"episode": 36.0, "batch_reward": 0.2520959796756506, "actor_loss": -29.016901428222656, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 440.6751685142517, "episode_reward": 359.2668812084218, "step": 36000}
{"episode": 37.0, "batch_reward": 0.25540691359341144, "actor_loss": -29.30941367340088, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.150370597839355, "episode_reward": 361.35921135937105, "step": 37000}
{"episode": 38.0, "batch_reward": 0.25947107984125617, "actor_loss": -28.655475620269776, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 433.39451813697815, "episode_reward": 371.65231016784526, "step": 38000}
{"episode": 39.0, "batch_reward": 0.26182844246923925, "actor_loss": -29.0821243057251, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.492949724197388, "episode_reward": 385.29866195304845, "step": 39000}
{"episode": 40.0, "batch_reward": 0.2645524694174528, "actor_loss": -28.1318426322937, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 439.433043718338, "episode_reward": 367.6893119310317, "step": 40000}
{"episode": 41.0, "batch_reward": 0.26631811128556726, "actor_loss": -28.221789669036866, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.085808992385864, "episode_reward": 287.0930300261315, "step": 41000}
{"episode": 42.0, "batch_reward": 0.26687816666066644, "actor_loss": -28.485138847351074, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 430.7626280784607, "episode_reward": 339.820582121812, "step": 42000}
{"episode": 43.0, "batch_reward": 0.2690427744835615, "actor_loss": -28.710124156951903, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.722679138183594, "episode_reward": 351.7535615974297, "step": 43000}
{"episode": 44.0, "batch_reward": 0.2708509983718395, "actor_loss": -27.981111614227295, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 442.8300054073334, "episode_reward": 297.8249629409886, "step": 44000}
{"episode": 45.0, "batch_reward": 0.2714626362621784, "actor_loss": -28.053828044891358, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.22533893585205, "episode_reward": 321.60661943241126, "step": 45000}
{"episode": 46.0, "batch_reward": 0.27277770699560644, "actor_loss": -27.747007846832275, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 428.87290954589844, "episode_reward": 346.7721164440116, "step": 46000}
{"episode": 47.0, "batch_reward": 0.274813571870327, "actor_loss": -27.88723789215088, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.27186155319214, "episode_reward": 352.17406623494014, "step": 47000}
{"episode": 48.0, "batch_reward": 0.27582184194028375, "actor_loss": -27.473687404632567, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 440.7490267753601, "episode_reward": 326.63312194052935, "step": 48000}
{"episode": 49.0, "batch_reward": 0.2768637769818306, "actor_loss": -27.60845008087158, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.830316066741943, "episode_reward": 331.36025095471064, "step": 49000}
{"episode": 50.0, "batch_reward": 0.2773193547129631, "actor_loss": -27.71458229827881, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 426.9689795970917, "episode_reward": 315.3102359946545, "step": 50000}
{"episode": 51.0, "batch_reward": 0.2784144638031721, "actor_loss": -27.847911937713622, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.489781618118286, "episode_reward": 332.51690130474265, "step": 51000}
{"episode": 52.0, "batch_reward": 0.2793394828438759, "actor_loss": -28.066470291137694, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 437.2769010066986, "episode_reward": 252.7195605033895, "step": 52000}
{"episode": 53.0, "batch_reward": 0.27903161430358886, "actor_loss": -27.9635911026001, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.12064027786255, "episode_reward": 311.0480897561737, "step": 53000}
{"episode": 54.0, "batch_reward": 0.28037444612383844, "actor_loss": -28.193607498168944, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 433.6905837059021, "episode_reward": 336.79631762361817, "step": 54000}
{"episode": 55.0, "batch_reward": 0.2805622667521238, "actor_loss": -28.350198600769044, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.175658464431763, "episode_reward": 316.40720258350717, "step": 55000}
{"episode": 56.0, "batch_reward": 0.28177944706380365, "actor_loss": -28.668177421569823, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.83214807510376, "episode_reward": 332.7111669898332, "step": 56000}
{"episode": 57.0, "batch_reward": 0.28210060139000415, "actor_loss": -28.67106055831909, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.779393434524536, "episode_reward": 345.8727319029323, "step": 57000}
{"episode": 58.0, "batch_reward": 0.283966758325696, "actor_loss": -28.508554443359376, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 421.48014640808105, "episode_reward": 361.17368124246326, "step": 58000}
{"episode": 59.0, "batch_reward": 0.2846335225254297, "actor_loss": -28.603814121246337, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.712284564971924, "episode_reward": 351.3892542023313, "step": 59000}
{"episode": 60.0, "batch_reward": 0.2869841936826706, "actor_loss": -28.981237071990968, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.8973252773285, "episode_reward": 331.6046556821724, "step": 60000}
{"episode": 61.0, "batch_reward": 0.28577583296597003, "actor_loss": -28.938957187652587, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.06255221366882, "episode_reward": 261.47427171065493, "step": 61000}
{"episode": 62.0, "batch_reward": 0.2863454320877791, "actor_loss": -29.026472213745116, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 425.349796295166, "episode_reward": 231.811007131584, "step": 62000}
{"episode": 63.0, "batch_reward": 0.28433041033148765, "actor_loss": -28.94346422576904, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.774229049682617, "episode_reward": 267.26760967131435, "step": 63000}
{"episode": 64.0, "batch_reward": 0.28524478328228, "actor_loss": -29.02460354614258, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 416.7719888687134, "episode_reward": 291.0171902411694, "step": 64000}
{"episode": 65.0, "batch_reward": 0.28469255416095257, "actor_loss": -28.972246829986574, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.74386692047119, "episode_reward": 286.5002913204532, "step": 65000}
{"episode": 66.0, "batch_reward": 0.28540150295197964, "actor_loss": -29.493711654663088, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 424.8528347015381, "episode_reward": 357.24531643439815, "step": 66000}
{"episode": 67.0, "batch_reward": 0.2859431332945824, "actor_loss": -29.444603393554686, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.79049277305603, "episode_reward": 353.334723433231, "step": 67000}
{"episode": 68.0, "batch_reward": 0.2877573471665382, "actor_loss": -30.087214584350587, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.8226583003998, "episode_reward": 334.71189701781833, "step": 68000}
{"episode": 69.0, "batch_reward": 0.28772907714545726, "actor_loss": -30.081313606262206, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.857311248779297, "episode_reward": 391.17182979021027, "step": 69000}
{"episode": 70.0, "batch_reward": 0.2896277878433466, "actor_loss": -30.10040616607666, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 421.6797389984131, "episode_reward": 399.1388998664452, "step": 70000}
{"episode": 71.0, "batch_reward": 0.29105920988321304, "actor_loss": -30.25341979980469, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 31.813414812088013, "episode_reward": 392.132336348041, "step": 71000}
{"episode": 72.0, "batch_reward": 0.29367244508862494, "actor_loss": -30.638358867645263, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 417.98807096481323, "episode_reward": 400.9251121387144, "step": 72000}
{"episode": 73.0, "batch_reward": 0.29417166566848757, "actor_loss": -30.784697410583497, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.193689346313477, "episode_reward": 406.9810822119164, "step": 73000}
{"episode": 74.0, "batch_reward": 0.29591767098009586, "actor_loss": -30.98392269897461, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 423.0907471179962, "episode_reward": 359.2328885442706, "step": 74000}
{"episode": 75.0, "batch_reward": 0.2971359462738037, "actor_loss": -30.997841735839845, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.30865979194641, "episode_reward": 363.42563410754127, "step": 75000}
{"episode": 76.0, "batch_reward": 0.2957647852897644, "actor_loss": -30.78659218978882, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 425.8243794441223, "episode_reward": 290.82225818573204, "step": 76000}
{"episode": 77.0, "batch_reward": 0.2973221840262413, "actor_loss": -31.032174697875977, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.760327577590942, "episode_reward": 334.0653874906855, "step": 77000}
{"episode": 78.0, "batch_reward": 0.2983226768672466, "actor_loss": -31.456301303863526, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 423.23382806777954, "episode_reward": 357.21721648284574, "step": 78000}
{"episode": 79.0, "batch_reward": 0.29859514367580414, "actor_loss": -31.490614448547362, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.906056642532349, "episode_reward": 353.1492264248486, "step": 79000}
{"episode": 80.0, "batch_reward": 0.2991817587912083, "actor_loss": -31.635112594604493, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.1407673358917, "episode_reward": 351.7805075835471, "step": 80000}
{"episode": 81.0, "batch_reward": 0.2996074226498604, "actor_loss": -31.756935745239257, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.9849910736084, "episode_reward": 353.68936198251146, "step": 81000}
{"episode": 82.0, "batch_reward": 0.30040258142352105, "actor_loss": -31.78576229095459, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 422.3840317726135, "episode_reward": 379.40459540447694, "step": 82000}
{"episode": 83.0, "batch_reward": 0.3022974833846092, "actor_loss": -31.898692977905274, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.496045351028442, "episode_reward": 371.40710013444834, "step": 83000}
{"episode": 84.0, "batch_reward": 0.30235186237096784, "actor_loss": -32.265796596527096, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.8562915325165, "episode_reward": 345.9182049386501, "step": 84000}
{"episode": 85.0, "batch_reward": 0.30280360659956934, "actor_loss": -32.372160194396976, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.21552085876465, "episode_reward": 364.26271159246556, "step": 85000}
{"episode": 86.0, "batch_reward": 0.30458662953972815, "actor_loss": -32.45741040802002, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.30253314971924, "episode_reward": 372.9657910153241, "step": 86000}
{"episode": 87.0, "batch_reward": 0.30394294133782385, "actor_loss": -32.388095989227295, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.981207132339478, "episode_reward": 383.4810491837698, "step": 87000}
{"episode": 88.0, "batch_reward": 0.30622085151076317, "actor_loss": -32.75810609054565, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 441.8471827507019, "episode_reward": 406.77972148912335, "step": 88000}
{"episode": 89.0, "batch_reward": 0.3058294585943222, "actor_loss": -32.61915480041504, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.415159702301025, "episode_reward": 385.69850326006576, "step": 89000}
{"episode": 90.0, "batch_reward": 0.3077261727154255, "actor_loss": -33.11482792663574, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.5364646911621, "episode_reward": 401.00547969195975, "step": 90000}
{"episode": 91.0, "batch_reward": 0.30872648423910143, "actor_loss": -33.17612000656128, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 39.314510345458984, "episode_reward": 414.4636645170948, "step": 91000}
{"episode": 92.0, "batch_reward": 0.30917273205518725, "actor_loss": -33.753486335754395, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.50425028800964, "episode_reward": 407.3149266387225, "step": 92000}
{"episode": 93.0, "batch_reward": 0.3107060402333736, "actor_loss": -33.88489828872681, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.594106674194336, "episode_reward": 433.36216113294097, "step": 93000}
{"episode": 94.0, "batch_reward": 0.3119513470828533, "actor_loss": -34.17160685729981, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 432.24149680137634, "episode_reward": 407.1701222035968, "step": 94000}
{"episode": 95.0, "batch_reward": 0.3139496439397335, "actor_loss": -34.31204175186157, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.913071632385254, "episode_reward": 343.3466270455494, "step": 95000}
{"episode": 96.0, "batch_reward": 0.314138590157032, "actor_loss": -34.46628912734985, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.4757652282715, "episode_reward": 432.3970398563311, "step": 96000}
{"episode": 97.0, "batch_reward": 0.31506411480903623, "actor_loss": -34.44634103012085, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.670809745788574, "episode_reward": 424.41229855825475, "step": 97000}
{"episode": 98.0, "batch_reward": 0.3153744612932205, "actor_loss": -34.65168279266357, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 434.27130007743835, "episode_reward": 408.5180271651862, "step": 98000}
{"episode": 99.0, "batch_reward": 0.3165640220940113, "actor_loss": -34.80571690750122, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.45027756690979, "episode_reward": 414.678348970396, "step": 99000}
{"episode": 100.0, "batch_reward": 0.31761330223083495, "actor_loss": -34.82425292205811, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 435.447425365448, "episode_reward": 423.5809993780253, "step": 100000}
{"episode": 101.0, "batch_reward": 0.3186788740456104, "actor_loss": -34.92205545043945, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.502923250198364, "episode_reward": 411.03025977689015, "step": 101000}
{"episode": 102.0, "batch_reward": 0.32013954809308054, "actor_loss": -34.94293909835815, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 430.29014134407043, "episode_reward": 412.376031990654, "step": 102000}
{"episode": 103.0, "batch_reward": 0.3209967096447945, "actor_loss": -35.1011329498291, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.783632278442383, "episode_reward": 409.5164009907429, "step": 103000}
{"episode": 104.0, "batch_reward": 0.32132306808233263, "actor_loss": -34.923950229644774, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 438.1975944042206, "episode_reward": 438.07927867550995, "step": 104000}
{"episode": 105.0, "batch_reward": 0.322695425927639, "actor_loss": -35.10108225631714, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.754098892211914, "episode_reward": 441.0741576061729, "step": 105000}
{"episode": 106.0, "batch_reward": 0.32366707640886305, "actor_loss": -35.6912617111206, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.7794907093048, "episode_reward": 417.28130493214934, "step": 106000}
{"episode": 107.0, "batch_reward": 0.32475517186522485, "actor_loss": -35.68604852676392, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.275162935256958, "episode_reward": 441.50005686080056, "step": 107000}
{"episode": 108.0, "batch_reward": 0.32612581527233125, "actor_loss": -35.72618006134033, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 435.170574426651, "episode_reward": 451.6477681702893, "step": 108000}
{"episode": 109.0, "batch_reward": 0.32709419897198677, "actor_loss": -35.76328659057617, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.664165496826172, "episode_reward": 473.6145588509576, "step": 109000}
{"episode": 110.0, "batch_reward": 0.3285725838840008, "actor_loss": -35.697691257476805, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 433.4717001914978, "episode_reward": 446.83379455963984, "step": 110000}
{"episode": 111.0, "batch_reward": 0.3297032805979252, "actor_loss": -35.8001446685791, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.173781394958496, "episode_reward": 454.17145917506826, "step": 111000}
{"episode": 112.0, "batch_reward": 0.3303021180033684, "actor_loss": -35.30277827072143, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 433.3136920928955, "episode_reward": 450.75908549358195, "step": 112000}
{"episode": 113.0, "batch_reward": 0.3319660274386406, "actor_loss": -35.43565145111084, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.664292335510254, "episode_reward": 440.8732751142316, "step": 113000}
{"episode": 114.0, "batch_reward": 0.33276580050587656, "actor_loss": -35.12300849533081, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.85340547561646, "episode_reward": 420.9482852875942, "step": 114000}
{"episode": 115.0, "batch_reward": 0.333643462151289, "actor_loss": -35.07816777420044, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.024412155151367, "episode_reward": 416.65830015471363, "step": 115000}
{"episode": 116.0, "batch_reward": 0.3337269018292427, "actor_loss": -34.62272104644775, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 434.04291009902954, "episode_reward": 424.0014870877432, "step": 116000}
{"episode": 117.0, "batch_reward": 0.3349340923130512, "actor_loss": -34.725588928222656, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.90711259841919, "episode_reward": 415.1570300082207, "step": 117000}
{"episode": 118.0, "batch_reward": 0.33532174012064936, "actor_loss": -34.44049211502075, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 434.4288446903229, "episode_reward": 432.64001237887936, "step": 118000}
{"episode": 119.0, "batch_reward": 0.3364222527742386, "actor_loss": -34.53275605392456, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.98627781867981, "episode_reward": 433.12107489845806, "step": 119000}
{"episode": 120.0, "batch_reward": 0.33759108662605286, "actor_loss": -34.69288553619385, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.8612184524536, "episode_reward": 431.27154194659136, "step": 120000}
{"episode": 121.0, "batch_reward": 0.3379662915170193, "actor_loss": -34.71057670593262, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.60109829902649, "episode_reward": 450.944802065051, "step": 121000}
{"episode": 122.0, "batch_reward": 0.3389878334105015, "actor_loss": -34.70530251312256, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 441.9658877849579, "episode_reward": 443.98508559331475, "step": 122000}
{"episode": 123.0, "batch_reward": 0.3392537350952625, "actor_loss": -34.70671017456055, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.10686707496643, "episode_reward": 425.8027304639384, "step": 123000}
{"episode": 124.0, "batch_reward": 0.3404145813584328, "actor_loss": -34.84651287460327, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 437.03318309783936, "episode_reward": 424.28425566843526, "step": 124000}
{"episode": 125.0, "batch_reward": 0.34087954825162886, "actor_loss": -34.97665024185181, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.06073260307312, "episode_reward": 417.53294240385344, "step": 125000}
{"episode": 126.0, "batch_reward": 0.34225891083478927, "actor_loss": -34.98435690307617, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 440.92822527885437, "episode_reward": 423.329620184169, "step": 126000}
{"episode": 127.0, "batch_reward": 0.3424212870299816, "actor_loss": -35.00804219818115, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.558808088302612, "episode_reward": 413.49851062464757, "step": 127000}
{"episode": 128.0, "batch_reward": 0.34287787359952926, "actor_loss": -35.378585964202884, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 441.45821166038513, "episode_reward": 411.35361979666044, "step": 128000}
{"episode": 129.0, "batch_reward": 0.3436264492869377, "actor_loss": -35.46269896316528, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.672366619110107, "episode_reward": 418.85631780227374, "step": 129000}
{"episode": 130.0, "batch_reward": 0.34385707074403765, "actor_loss": -35.22348157501221, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.3232161998749, "episode_reward": 372.7161384418492, "step": 130000}
{"episode": 131.0, "batch_reward": 0.34388402262330053, "actor_loss": -35.22076132965088, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.27112436294556, "episode_reward": 363.91965519027633, "step": 131000}
{"episode": 132.0, "batch_reward": 0.3445707697570324, "actor_loss": -35.53165279388428, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 432.45404505729675, "episode_reward": 404.04690501510163, "step": 132000}
{"episode": 133.0, "batch_reward": 0.34428101661801336, "actor_loss": -35.57604539489746, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.057387113571167, "episode_reward": 406.77217941320055, "step": 133000}
{"episode": 134.0, "batch_reward": 0.3457766705155373, "actor_loss": -35.278688591003416, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 432.79550886154175, "episode_reward": 391.53528792567016, "step": 134000}
{"episode": 135.0, "batch_reward": 0.34508545565605164, "actor_loss": -35.24396838378906, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.44907283782959, "episode_reward": 377.2364414544773, "step": 135000}
{"episode": 136.0, "batch_reward": 0.34598374292254447, "actor_loss": -35.77661669158935, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 432.29282546043396, "episode_reward": 373.6918687376956, "step": 136000}
{"episode": 137.0, "batch_reward": 0.3458102432787418, "actor_loss": -35.80369968414307, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.272345781326294, "episode_reward": 384.7436892611186, "step": 137000}
{"episode": 138.0, "batch_reward": 0.3465213529765606, "actor_loss": -35.56642472839356, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 433.2979874610901, "episode_reward": 404.4931064918609, "step": 138000}
{"episode": 139.0, "batch_reward": 0.34668958821892737, "actor_loss": -35.70450169754028, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.603623628616333, "episode_reward": 394.8115721079474, "step": 139000}
{"episode": 140.0, "batch_reward": 0.34734227710962295, "actor_loss": -35.604396724700926, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 436.61290097236633, "episode_reward": 407.17209780175114, "step": 140000}
{"episode": 141.0, "batch_reward": 0.3475124821960926, "actor_loss": -35.73555149841309, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.551182985305786, "episode_reward": 424.9496991799119, "step": 141000}
{"episode": 142.0, "batch_reward": 0.34759991666674617, "actor_loss": -36.01384375, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 432.6144917011261, "episode_reward": 446.66737343433965, "step": 142000}
{"episode": 143.0, "batch_reward": 0.34877701780200004, "actor_loss": -36.1309495010376, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.76219344139099, "episode_reward": 453.5445335851364, "step": 143000}
{"episode": 144.0, "batch_reward": 0.3494797728359699, "actor_loss": -36.236618171691894, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 425.1458594799042, "episode_reward": 447.60471382346896, "step": 144000}
{"episode": 145.0, "batch_reward": 0.3505288369655609, "actor_loss": -36.388989379882815, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.638895750045776, "episode_reward": 464.05974178350147, "step": 145000}
{"episode": 146.0, "batch_reward": 0.3516717771589756, "actor_loss": -36.448616264343265, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 419.11635637283325, "episode_reward": 477.55149792686865, "step": 146000}
{"episode": 147.0, "batch_reward": 0.3522816544175148, "actor_loss": -36.49726544952392, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.351715087890625, "episode_reward": 460.36356335912325, "step": 147000}
{"episode": 148.0, "batch_reward": 0.35267010110616687, "actor_loss": -36.19849278259277, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.6659152507782, "episode_reward": 451.5718083187988, "step": 148000}
{"episode": 149.0, "batch_reward": 0.3529426255822182, "actor_loss": -36.19340595245362, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.80158042907715, "episode_reward": 437.4800887647119, "step": 149000}
{"episode": 150.0, "batch_reward": 0.3531417512893677, "actor_loss": -36.187501022338864, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "step": 150000}
