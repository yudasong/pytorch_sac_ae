{"episode_reward": 0.0, "episode": 1.0, "duration": 18.55139970779419, "step": 1000}
{"episode_reward": 5.1931688606333894, "episode": 2.0, "duration": 1.656357765197754, "step": 2000}
{"episode_reward": 454.7514538707513, "episode": 3.0, "batch_reward": 0.21581397411295794, "critic_loss": 0.024709997764267164, "actor_loss": -20.903942475224337, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 68.94133949279785, "step": 3000}
{"episode_reward": 1.8987638686577653, "episode": 4.0, "batch_reward": 0.13374288622289895, "critic_loss": 0.01989013280486688, "actor_loss": -22.211660817146303, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.033262968063354, "step": 4000}
{"episode_reward": 1.8547966700736271, "episode": 5.0, "batch_reward": 0.10286699477955699, "critic_loss": 0.019829645910300314, "actor_loss": -18.094777757644653, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.4695885181427, "step": 5000}
{"episode_reward": 1.335406056230557, "episode": 6.0, "batch_reward": 0.08417693158611655, "critic_loss": 0.019444605478784068, "actor_loss": -18.90135289287567, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.29738163948059, "step": 6000}
{"episode_reward": 1.9576678703607282, "episode": 7.0, "batch_reward": 0.07153485664725304, "critic_loss": 0.022149326256010682, "actor_loss": -18.745530870437623, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.316619396209717, "step": 7000}
{"episode_reward": 1.5489950895281432, "episode": 8.0, "batch_reward": 0.062643504941836, "critic_loss": 0.020329538927879184, "actor_loss": -17.63451212501526, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.416327238082886, "step": 8000}
{"episode_reward": 1.7422945166826462, "episode": 9.0, "batch_reward": 0.055168265897780656, "critic_loss": 0.02692315797600895, "actor_loss": -18.86251313304901, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.92696237564087, "step": 9000}
{"episode_reward": 1.9085083836263004, "episode": 10.0, "batch_reward": 0.050078367341309785, "critic_loss": 0.021975784143200144, "actor_loss": -17.688138875961304, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.258563995361328, "step": 10000}
{"episode_reward": 1.6680770247289383, "episode": 11.0, "batch_reward": 0.045453031563200054, "critic_loss": 0.025417392821982503, "actor_loss": -17.52915303659439, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.924561738967896, "step": 11000}
{"episode_reward": 1.6226408290220888, "episode": 12.0, "batch_reward": 0.041893399739637974, "critic_loss": 0.021701173408306206, "actor_loss": -17.374459731578828, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.793413400650024, "step": 12000}
{"episode_reward": 2.0506186606741847, "episode": 13.0, "batch_reward": 0.03841353258024901, "critic_loss": 0.0189673279793933, "actor_loss": -17.200289347171783, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.620184659957886, "step": 13000}
{"episode_reward": 2.9817093180519123, "episode": 14.0, "batch_reward": 0.035291807155124845, "critic_loss": 0.023027297675609588, "actor_loss": -15.196265960216522, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.914398670196533, "step": 14000}
{"episode_reward": 1.7055628980639452, "episode": 15.0, "batch_reward": 0.033490725846029815, "critic_loss": 0.01942870070145, "actor_loss": -17.075449191093444, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.137571811676025, "step": 15000}
{"episode_reward": 2.718865764873714, "episode": 16.0, "batch_reward": 0.031059391035931184, "critic_loss": 0.01504813413938973, "actor_loss": -17.072418348789213, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.655150175094604, "step": 16000}
{"episode_reward": 1.8394807781996079, "episode": 17.0, "batch_reward": 0.029442270695231854, "critic_loss": 0.016257747624535114, "actor_loss": -15.439437018871308, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.472482442855835, "step": 17000}
{"episode_reward": 2.1834208246722637, "episode": 18.0, "batch_reward": 0.028093885101843626, "critic_loss": 0.011227950320055243, "actor_loss": -15.570537864208221, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.90228247642517, "step": 18000}
{"episode_reward": 2.8115309835865316, "episode": 19.0, "batch_reward": 0.02664245743304491, "critic_loss": 0.009869076721020975, "actor_loss": -16.664189215660095, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.195801973342896, "step": 19000}
{"episode_reward": 5.015507745986939, "episode": 20.0, "batch_reward": 0.02509215967054479, "critic_loss": 0.01404886830871692, "actor_loss": -16.816701102256776, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.391043663024902, "step": 20000}
{"episode_reward": 1.5491647987657802, "episode": 21.0, "batch_reward": 0.02456250900309533, "critic_loss": 0.011371247260191012, "actor_loss": -16.870862458229066, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.31341552734375, "step": 21000}
{"episode_reward": 1.7223739518053105, "episode": 22.0, "batch_reward": 0.023224645724869333, "critic_loss": 0.01120944806811167, "actor_loss": -16.231092675685883, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.320186376571655, "step": 22000}
{"episode_reward": 2.3630455778336756, "episode": 23.0, "batch_reward": 0.022732087277574465, "critic_loss": 0.007894159925461282, "actor_loss": -14.784383356332778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.9441020488739, "step": 23000}
{"episode_reward": 2.4998902413515856, "episode": 24.0, "batch_reward": 0.021402959004277362, "critic_loss": 0.012738922852353425, "actor_loss": -15.945631266117095, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.290279626846313, "step": 24000}
{"episode_reward": 2.8454569432794683, "episode": 25.0, "batch_reward": 0.020659538734122178, "critic_loss": 0.01012830846462748, "actor_loss": -14.465479833126068, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.187052011489868, "step": 25000}
{"episode_reward": 1.7810931444921994, "episode": 26.0, "batch_reward": 0.0199721699237125, "critic_loss": 0.00733286603007582, "actor_loss": -15.051088066577911, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.38792634010315, "step": 26000}
{"episode_reward": 1.7315581360300927, "episode": 27.0, "batch_reward": 0.01951015841087792, "critic_loss": 0.007729305369924987, "actor_loss": -15.005066990852356, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.672293663024902, "step": 27000}
{"episode_reward": 2.221214188850869, "episode": 28.0, "batch_reward": 0.018718992826878094, "critic_loss": 0.00835945590256597, "actor_loss": -14.473289415359497, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.874744415283203, "step": 28000}
{"episode_reward": 2.51081606916319, "episode": 29.0, "batch_reward": 0.018794058551080524, "critic_loss": 0.00638530940603232, "actor_loss": -15.409256076335907, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.397175788879395, "step": 29000}
{"episode_reward": 1.8343136713183918, "episode": 30.0, "batch_reward": 0.017461048676050268, "critic_loss": 0.006401708496079663, "actor_loss": -14.725108396530151, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.563533306121826, "step": 30000}
{"episode_reward": 2.157096277066548, "episode": 31.0, "batch_reward": 0.01705129803379532, "critic_loss": 0.007569683813693701, "actor_loss": -14.619412598133087, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.535672187805176, "step": 31000}
{"episode_reward": 1.912596008158669, "episode": 32.0, "batch_reward": 0.01636965561960824, "critic_loss": 0.007515861185005633, "actor_loss": -16.054339396715164, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.722137212753296, "step": 32000}
{"episode_reward": 1.501931615539724, "episode": 33.0, "batch_reward": 0.0161923030638136, "critic_loss": 0.00649275129706075, "actor_loss": -15.661906808376312, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.59503483772278, "step": 33000}
{"episode_reward": 4.162213032392688, "episode": 34.0, "batch_reward": 0.015717593456618488, "critic_loss": 0.004316434641135856, "actor_loss": -12.679063523888589, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.81834626197815, "step": 34000}
{"episode_reward": 1.7400298535788217, "episode": 35.0, "batch_reward": 0.015223400740069337, "critic_loss": 0.006131860217574285, "actor_loss": -14.855604841589928, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.852753400802612, "step": 35000}
{"episode_reward": 2.0210509292047965, "episode": 36.0, "batch_reward": 0.014878606111626141, "critic_loss": 0.005519601770094596, "actor_loss": -14.323887960314751, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.09944438934326, "step": 36000}
{"episode_reward": 3.7273759317444175, "episode": 37.0, "batch_reward": 0.014692208875901997, "critic_loss": 0.005759286626489484, "actor_loss": -13.666497905015946, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.966570138931274, "step": 37000}
{"episode_reward": 2.855845739433606, "episode": 38.0, "batch_reward": 0.014401086872443557, "critic_loss": 0.004697838870204578, "actor_loss": -13.767215721964837, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.855265855789185, "step": 38000}
{"episode_reward": 1.8214524377741055, "episode": 39.0, "batch_reward": 0.013877267687348649, "critic_loss": 0.003704213209413865, "actor_loss": -15.697134140849114, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.56445288658142, "step": 39000}
{"episode_reward": 1.9160510910810673, "episode": 40.0, "batch_reward": 0.01375928294856567, "critic_loss": 0.003352627368105459, "actor_loss": -14.317978239655496, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.66890573501587, "step": 40000}
{"episode_reward": 1.9075477091454918, "episode": 41.0, "batch_reward": 0.01349948593764566, "critic_loss": 0.004305518149783893, "actor_loss": -14.678135574698448, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.001373291015625, "step": 41000}
{"episode_reward": 1.7248463475687767, "episode": 42.0, "batch_reward": 0.01282929492346011, "critic_loss": 0.004288794896798208, "actor_loss": -14.373108832001686, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.4371178150177, "step": 42000}
{"episode_reward": 1.4445947955334995, "episode": 43.0, "batch_reward": 0.012871906964224763, "critic_loss": 0.005011304202635074, "actor_loss": -14.43475282919407, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.529391050338745, "step": 43000}
{"episode_reward": 2.6247708060465786, "episode": 44.0, "batch_reward": 0.012513888358487747, "critic_loss": 0.003273180103584309, "actor_loss": -13.722067031621933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.291409969329834, "step": 44000}
{"episode_reward": 1.7897178868050705, "episode": 45.0, "batch_reward": 0.012533295930596069, "critic_loss": 0.003523663991196372, "actor_loss": -15.527425711870194, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.913288354873657, "step": 45000}
{"episode_reward": 1.3201086973621587, "episode": 46.0, "batch_reward": 0.012044175616581925, "critic_loss": 0.003280129328253679, "actor_loss": -14.962164581894875, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.86023497581482, "step": 46000}
{"episode_reward": 1.6934401860343098, "episode": 47.0, "batch_reward": 0.0122528317195829, "critic_loss": 0.0027909012937452644, "actor_loss": -14.410223863601685, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.97651481628418, "step": 47000}
{"episode_reward": 1.6247577285967814, "episode": 48.0, "batch_reward": 0.011598729087389074, "critic_loss": 0.002382491744167055, "actor_loss": -13.907737781465054, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.412488222122192, "step": 48000}
{"episode_reward": 1.592105158959003, "episode": 49.0, "batch_reward": 0.011679872087785042, "critic_loss": 0.0039750559515014175, "actor_loss": -14.768710710823536, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.497763633728027, "step": 49000}
{"episode_reward": 1.4612855782834526, "episode": 50.0, "batch_reward": 0.011050846841535532, "critic_loss": 0.004222817427304107, "actor_loss": -14.463516298770905, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.950276136398315, "step": 50000}
{"episode_reward": 2.5640066102441184, "episode": 51.0, "batch_reward": 0.011003545450395905, "critic_loss": 0.0019294928178533156, "actor_loss": -13.304083528459072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.70278549194336, "step": 51000}
{"episode_reward": 1.933482633950001, "episode": 52.0, "batch_reward": 0.010992997190682217, "critic_loss": 0.0036228128727307193, "actor_loss": -15.117355430483817, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.987932920455933, "step": 52000}
{"episode_reward": 2.499569771764236, "episode": 53.0, "batch_reward": 0.01096502489654813, "critic_loss": 0.00217581052220703, "actor_loss": -14.965019972026347, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.382577419281006, "step": 53000}
{"episode_reward": 1.8832981475933708, "episode": 54.0, "batch_reward": 0.010332536924397572, "critic_loss": 0.002458281646780961, "actor_loss": -14.67321402078867, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.694313526153564, "step": 54000}
{"episode_reward": 1.9955298827262715, "episode": 55.0, "batch_reward": 0.010531902285525575, "critic_loss": 0.003409213226652355, "actor_loss": -13.156649026215076, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.17109251022339, "step": 55000}
{"episode_reward": 1.766060104226577, "episode": 56.0, "batch_reward": 0.01030564953875728, "critic_loss": 0.002321155032455863, "actor_loss": -14.711642350494861, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.963895559310913, "step": 56000}
{"episode_reward": 2.1541783638414427, "episode": 57.0, "batch_reward": 0.009887977652368136, "critic_loss": 0.0030197099917786546, "actor_loss": -14.764564682245254, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.398097038269043, "step": 57000}
{"episode_reward": 1.7078202793524375, "episode": 58.0, "batch_reward": 0.010092202834319323, "critic_loss": 0.0023348473356090836, "actor_loss": -12.848225582063199, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.061591148376465, "step": 58000}
{"episode_reward": 1.8799283588968734, "episode": 59.0, "batch_reward": 0.009544856478692963, "critic_loss": 0.003650258754078095, "actor_loss": -14.245802143096924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.596301317214966, "step": 59000}
{"episode_reward": 1.9737733608404286, "episode": 60.0, "batch_reward": 0.009842671475489624, "critic_loss": 0.002758009249926545, "actor_loss": -14.882029721140862, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.736079931259155, "step": 60000}
{"episode_reward": 1.9102627080321875, "episode": 61.0, "batch_reward": 0.009672238725353964, "critic_loss": 0.0025650084314074776, "actor_loss": -13.850024111449718, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.748841524124146, "step": 61000}
{"episode_reward": 1.764309860691783, "episode": 62.0, "batch_reward": 0.00959882435691543, "critic_loss": 0.001789071224102372, "actor_loss": -13.342678757995367, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.279332160949707, "step": 62000}
{"episode_reward": 1.5163598171626191, "episode": 63.0, "batch_reward": 0.009371066196006722, "critic_loss": 0.003644258584379713, "actor_loss": -13.66927759680152, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.553839445114136, "step": 63000}
{"episode_reward": 1.70054383014102, "episode": 64.0, "batch_reward": 0.008904996878351085, "critic_loss": 0.0020649813601048665, "actor_loss": -13.703075598657131, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.066718578338623, "step": 64000}
{"episode_reward": 2.0357620371164984, "episode": 65.0, "batch_reward": 0.008979615919990466, "critic_loss": 0.0029031824454214075, "actor_loss": -14.319854494959117, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.813785076141357, "step": 65000}
{"episode_reward": 1.9223722290500245, "episode": 66.0, "batch_reward": 0.00909926744637778, "critic_loss": 0.002718674290656054, "actor_loss": -14.329442394554615, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.400657653808594, "step": 66000}
{"episode_reward": 2.1672883152236357, "episode": 67.0, "batch_reward": 0.008957032961770892, "critic_loss": 0.0031900774654350242, "actor_loss": -15.612170045167208, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.07520341873169, "step": 67000}
{"episode_reward": 2.4799360394996093, "episode": 68.0, "batch_reward": 0.008763831985299475, "critic_loss": 0.003983028281454608, "actor_loss": -13.065469353049993, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.98956799507141, "step": 68000}
{"episode_reward": 1.4031995780247362, "episode": 69.0, "batch_reward": 0.008525753360590898, "critic_loss": 0.003037455941655935, "actor_loss": -14.435486925512553, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.194982528686523, "step": 69000}
{"episode_reward": 1.825703973640415, "episode": 70.0, "batch_reward": 0.008758708538021893, "critic_loss": 0.0019968168042760227, "actor_loss": -14.846765074908733, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.80955219268799, "step": 70000}
{"episode_reward": 1.7182711112680262, "episode": 71.0, "batch_reward": 0.008752019556937739, "critic_loss": 0.002138460598704114, "actor_loss": -13.203605755150319, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.02806758880615, "step": 71000}
{"episode_reward": 1.7112995042873926, "episode": 72.0, "batch_reward": 0.008465601241099649, "critic_loss": 0.004169607729905692, "actor_loss": -13.598413626343012, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.336211681365967, "step": 72000}
{"episode_reward": 1.5140258557608968, "episode": 73.0, "batch_reward": 0.008373373095411807, "critic_loss": 0.0024103560038711293, "actor_loss": -13.919929134190083, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.43424153327942, "step": 73000}
{"episode_reward": 1.6357963146446444, "episode": 74.0, "batch_reward": 0.008124339633854106, "critic_loss": 0.002266466888027935, "actor_loss": -14.472703987002372, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.703964948654175, "step": 74000}
{"episode_reward": 1.6865252990512962, "episode": 75.0, "batch_reward": 0.008031653973273933, "critic_loss": 0.0013452255537413293, "actor_loss": -13.906375332355498, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.786303281784058, "step": 75000}
{"episode_reward": 1.6185515989213948, "episode": 76.0, "batch_reward": 0.008136863136198371, "critic_loss": 0.0024066424410666516, "actor_loss": -13.83758278901875, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.478541135787964, "step": 76000}
{"episode_reward": 5.140572353050016, "episode": 77.0, "batch_reward": 0.007947430647793225, "critic_loss": 0.0019583963570657943, "actor_loss": -13.966102893322706, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.585965871810913, "step": 77000}
{"episode_reward": 2.8477696112645634, "episode": 78.0, "batch_reward": 0.008270729511976242, "critic_loss": 0.002208769936263707, "actor_loss": -14.445562378168106, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.166078329086304, "step": 78000}
{"episode_reward": 2.883538352629011, "episode": 79.0, "batch_reward": 0.00769791558792349, "critic_loss": 0.00241980064289055, "actor_loss": -13.266417380198837, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.338224172592163, "step": 79000}
{"episode_reward": 1.7692759957844213, "episode": 80.0, "batch_reward": 0.007704017866984941, "critic_loss": 0.0019344585700091558, "actor_loss": -13.328478279978038, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.74373984336853, "step": 80000}
{"episode_reward": 1.636565633496518, "episode": 81.0, "batch_reward": 0.00753700434055645, "critic_loss": 0.0036612659201236968, "actor_loss": -14.409726085945964, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.14170861244202, "step": 81000}
{"episode_reward": 1.620910424974738, "episode": 82.0, "batch_reward": 0.007724626718321815, "critic_loss": 0.0038438389111543074, "actor_loss": -14.42322471959889, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.978256225585938, "step": 82000}
{"episode_reward": 1.5734149776241153, "episode": 83.0, "batch_reward": 0.007419837467023171, "critic_loss": 0.0019961298284324586, "actor_loss": -14.274072743743657, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.980043649673462, "step": 83000}
{"episode_reward": 2.2946085107684624, "episode": 84.0, "batch_reward": 0.007278295699274168, "critic_loss": 0.002701521232160303, "actor_loss": -14.746588780537248, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.143177032470703, "step": 84000}
{"episode_reward": 2.1909138098866876, "episode": 85.0, "batch_reward": 0.007542816910892725, "critic_loss": 0.003014609055224355, "actor_loss": -14.75257596129179, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.277918815612793, "step": 85000}
{"episode_reward": 1.6184509574435948, "episode": 86.0, "batch_reward": 0.007334452350391075, "critic_loss": 0.002883034202026465, "actor_loss": -14.119074130162597, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.095667839050293, "step": 86000}
{"episode_reward": 1.5149524921916544, "episode": 87.0, "batch_reward": 0.007346643817494623, "critic_loss": 0.0010681550032677477, "actor_loss": -13.239124342530966, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.623587608337402, "step": 87000}
{"episode_reward": 1.429245928123494, "episode": 88.0, "batch_reward": 0.00737664990965277, "critic_loss": 0.002525581623227481, "actor_loss": -12.818867824852466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.54587173461914, "step": 88000}
{"episode_reward": 2.806351165718022, "episode": 89.0, "batch_reward": 0.007370182414073497, "critic_loss": 0.0016127799023815895, "actor_loss": -13.639027881085873, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.704654455184937, "step": 89000}
{"episode_reward": 1.9050903709138358, "episode": 90.0, "batch_reward": 0.007129450090695173, "critic_loss": 0.0015367441631005932, "actor_loss": -14.119077338844537, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.821447134017944, "step": 90000}
{"episode_reward": 1.8713192660848117, "episode": 91.0, "batch_reward": 0.007307932667084969, "critic_loss": 0.002215751178782739, "actor_loss": -13.350609946787356, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.670949935913086, "step": 91000}
{"episode_reward": 2.6599886819409235, "episode": 92.0, "batch_reward": 0.007243658535298891, "critic_loss": 0.001812747589776336, "actor_loss": -14.185079230837523, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.858397960662842, "step": 92000}
{"episode_reward": 1.404547826190088, "episode": 93.0, "batch_reward": 0.006947729190695099, "critic_loss": 0.00162030819258689, "actor_loss": -13.27999391822517, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.28616166114807, "step": 93000}
{"episode_reward": 1.6775672483906419, "episode": 94.0, "batch_reward": 0.006834835200454108, "critic_loss": 0.002267370423551256, "actor_loss": -13.689485978685319, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.510404109954834, "step": 94000}
{"episode_reward": 1.6287913207556695, "episode": 95.0, "batch_reward": 0.0068667904981411995, "critic_loss": 0.001546076050706688, "actor_loss": -14.580737912259996, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.91735816001892, "step": 95000}
{"episode_reward": 1.3978011723133656, "episode": 96.0, "batch_reward": 0.006802015970344655, "critic_loss": 0.0013949286152674177, "actor_loss": -13.748423095829786, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.653865337371826, "step": 96000}
{"episode_reward": 1.8942339714458867, "episode": 97.0, "batch_reward": 0.006862748657236807, "critic_loss": 0.0012761744894269214, "actor_loss": -14.383508425556123, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.093669414520264, "step": 97000}
{"episode_reward": 1.8469566460049216, "episode": 98.0, "batch_reward": 0.006726230453699827, "critic_loss": 0.0019308633254568122, "actor_loss": -13.88105752567947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.113131999969482, "step": 98000}
{"episode_reward": 1.631748306518896, "episode": 99.0, "batch_reward": 0.00655911724397447, "critic_loss": 0.0017271071949071484, "actor_loss": -13.751352600120008, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.241947650909424, "step": 99000}
{"episode_reward": 2.8865935091860004, "episode": 100.0, "batch_reward": 0.006734021132579073, "critic_loss": 0.0020419447191070503, "actor_loss": -13.27162639209628, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.690529584884644, "step": 100000}
{"episode_reward": 2.097908060809805, "episode": 101.0, "batch_reward": 0.006443215444101952, "critic_loss": 0.0013519963564485806, "actor_loss": -14.561840862512588, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.34711527824402, "step": 101000}
{"episode_reward": 1.2100281323258668, "episode": 102.0, "batch_reward": 0.006614569655503146, "critic_loss": 0.0013310033950037904, "actor_loss": -14.290921326667071, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.67807412147522, "step": 102000}
{"episode_reward": 1.9077245987422862, "episode": 103.0, "batch_reward": 0.006623932030517608, "critic_loss": 0.0021065091039235997, "actor_loss": -14.156128785811365, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.590518474578857, "step": 103000}
{"episode_reward": 1.7039089682576556, "episode": 104.0, "batch_reward": 0.006514785231091082, "critic_loss": 0.0017987008249510837, "actor_loss": -14.216293199673295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.998908042907715, "step": 104000}
{"episode_reward": 1.8627804614036478, "episode": 105.0, "batch_reward": 0.006518318731104955, "critic_loss": 0.0015935176654056705, "actor_loss": -12.572201454468072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.117549419403076, "step": 105000}
{"episode_reward": 1.7071621837644582, "episode": 106.0, "batch_reward": 0.006424836166319437, "critic_loss": 0.0013209530279400497, "actor_loss": -12.025131582811476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.203179597854614, "step": 106000}
{"episode_reward": 1.8478478362366118, "episode": 107.0, "batch_reward": 0.006420414456399157, "critic_loss": 0.0016941367021991026, "actor_loss": -13.738059843279421, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.047420978546143, "step": 107000}
{"episode_reward": 1.640717089022021, "episode": 108.0, "batch_reward": 0.006248522123205476, "critic_loss": 0.0020721905273367158, "actor_loss": -14.27489150660485, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.970134258270264, "step": 108000}
{"episode_reward": 1.634217386298096, "episode": 109.0, "batch_reward": 0.006120828389422968, "critic_loss": 0.001999016167028458, "actor_loss": -13.655432532384992, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.361485481262207, "step": 109000}
{"episode_reward": 2.9036319610433283, "episode": 110.0, "batch_reward": 0.006092722362605855, "critic_loss": 0.001553844063428187, "actor_loss": -14.343813556872309, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.857908725738525, "step": 110000}
{"episode_reward": 2.058550723340632, "episode": 111.0, "batch_reward": 0.006097107652400155, "critic_loss": 0.002389786042040214, "actor_loss": -13.804549846276641, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.97654891014099, "step": 111000}
{"episode_reward": 2.9916087034269925, "episode": 112.0, "batch_reward": 0.006213404460228048, "critic_loss": 0.001780487492433167, "actor_loss": -13.510814311645925, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.29753565788269, "step": 112000}
{"episode_reward": 1.7499800832524812, "episode": 113.0, "batch_reward": 0.005945488512166776, "critic_loss": 0.0015848964092620007, "actor_loss": -13.716615343466401, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.206949472427368, "step": 113000}
{"episode_reward": 2.4205733616858534, "episode": 114.0, "batch_reward": 0.005957053195335902, "critic_loss": 0.0013160445593184704, "actor_loss": -14.951394533909857, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.094398498535156, "step": 114000}
{"episode_reward": 1.8001530542984348, "episode": 115.0, "batch_reward": 0.00604057137586642, "critic_loss": 0.002314483465537705, "actor_loss": -13.918273127820342, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.526026725769043, "step": 115000}
{"episode_reward": 2.4887278760982, "episode": 116.0, "batch_reward": 0.006002382335835136, "critic_loss": 0.001417222129612128, "actor_loss": -14.677638324044645, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.083324432373047, "step": 116000}
{"episode_reward": 1.6108083820447012, "episode": 117.0, "batch_reward": 0.00614392762305215, "critic_loss": 0.0024628998092011897, "actor_loss": -13.867864247567951, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.672844409942627, "step": 117000}
{"episode_reward": 1.6072382483520615, "episode": 118.0, "batch_reward": 0.006063941931817681, "critic_loss": 0.0022559184445435677, "actor_loss": -15.094583867073059, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.350948333740234, "step": 118000}
{"episode_reward": 1.9831058687607956, "episode": 119.0, "batch_reward": 0.005886883507599123, "critic_loss": 0.0016976459445795626, "actor_loss": -13.577593855958431, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.220574140548706, "step": 119000}
{"episode_reward": 2.4012190561108073, "episode": 120.0, "batch_reward": 0.005756047963048331, "critic_loss": 0.001385399304483144, "actor_loss": -12.610439096990973, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.634299993515015, "step": 120000}
{"episode_reward": 2.151821567505932, "episode": 121.0, "batch_reward": 0.005865465978742577, "critic_loss": 0.0015846114231717365, "actor_loss": -13.22948166603595, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.89675331115723, "step": 121000}
{"episode_reward": 2.249274487189785, "episode": 122.0, "batch_reward": 0.005859141394146718, "critic_loss": 0.0017950345945137087, "actor_loss": -13.638023355979472, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.943379640579224, "step": 122000}
{"episode_reward": 2.1509350834430405, "episode": 123.0, "batch_reward": 0.0057910788127919655, "critic_loss": 0.0015245848031026981, "actor_loss": -12.811310549546032, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.95067834854126, "step": 123000}
{"episode_reward": 1.8511198350006535, "episode": 124.0, "batch_reward": 0.005718149645719677, "critic_loss": 0.0012563440917765546, "actor_loss": -13.780867237921804, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.206319093704224, "step": 124000}
{"episode_reward": 2.2374515663715453, "episode": 125.0, "batch_reward": 0.00560212553886231, "critic_loss": 0.002200732718069048, "actor_loss": -13.01857580269873, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.92237901687622, "step": 125000}
{"episode_reward": 1.9422734114829088, "episode": 126.0, "batch_reward": 0.005520127127761953, "critic_loss": 0.0020032940351720754, "actor_loss": -14.247081277627498, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.691834449768066, "step": 126000}
{"episode_reward": 1.3580140801535674, "episode": 127.0, "batch_reward": 0.005655682632583194, "critic_loss": 0.001385844015669136, "actor_loss": -14.038434449065477, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.35111665725708, "step": 127000}
{"episode_reward": 2.0881694431788005, "episode": 128.0, "batch_reward": 0.0055761872196453625, "critic_loss": 0.0014679431252880022, "actor_loss": -14.514920785080642, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.8834490776062, "step": 128000}
{"episode_reward": 1.6739323233397596, "episode": 129.0, "batch_reward": 0.0055938539040507745, "critic_loss": 0.002365801115471186, "actor_loss": -14.859286167185754, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.105249881744385, "step": 129000}
{"episode_reward": 2.316250479264208, "episode": 130.0, "batch_reward": 0.005543472833465785, "critic_loss": 0.0016732928698365867, "actor_loss": -13.55097376325354, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.486491203308105, "step": 130000}
{"episode_reward": 1.8002645026753752, "episode": 131.0, "batch_reward": 0.0055565354080172255, "critic_loss": 0.0019359246200638153, "actor_loss": -13.964290331210941, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.153300285339355, "step": 131000}
{"episode_reward": 2.3084272993552784, "episode": 132.0, "batch_reward": 0.005591233226412442, "critic_loss": 0.0018235941067796374, "actor_loss": -13.627145890723915, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.803911209106445, "step": 132000}
{"episode_reward": 2.637689851602438, "episode": 133.0, "batch_reward": 0.005496838826686144, "critic_loss": 0.0011829659346358312, "actor_loss": -14.855178515188395, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.06404948234558, "step": 133000}
{"episode_reward": 1.7323318008736908, "episode": 134.0, "batch_reward": 0.005624774080351926, "critic_loss": 0.001444417465261722, "actor_loss": -15.399754115562885, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.19477939605713, "step": 134000}
{"episode_reward": 2.0046997020606, "episode": 135.0, "batch_reward": 0.005486800849204883, "critic_loss": 0.0008430608844537346, "actor_loss": -13.680726418059319, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.353649377822876, "step": 135000}
{"episode_reward": 1.7452117489474637, "episode": 136.0, "batch_reward": 0.005300649237120524, "critic_loss": 0.0017272962539263972, "actor_loss": -13.95793314279616, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.036635398864746, "step": 136000}
{"episode_reward": 1.2125653662554208, "episode": 137.0, "batch_reward": 0.005413986004772596, "critic_loss": 0.0012307109343546471, "actor_loss": -13.412317521888763, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.726513862609863, "step": 137000}
{"episode_reward": 2.1935480604929287, "episode": 138.0, "batch_reward": 0.00552450825960841, "critic_loss": 0.001689166393578489, "actor_loss": -12.660199027223513, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.563483953475952, "step": 138000}
{"episode_reward": 2.6577835128330767, "episode": 139.0, "batch_reward": 0.00542245488835033, "critic_loss": 0.0008112239084948669, "actor_loss": -13.076051708383485, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.461024284362793, "step": 139000}
{"episode_reward": 1.4954199364604173, "episode": 140.0, "batch_reward": 0.005279823142103851, "critic_loss": 0.0009920643833593204, "actor_loss": -12.864561729012058, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.83600163459778, "step": 140000}
{"episode_reward": 1.796259999615589, "episode": 141.0, "batch_reward": 0.005293338036863133, "critic_loss": 0.0011590815903327893, "actor_loss": -12.655223629886285, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.16204762458801, "step": 141000}
{"episode_reward": 2.162902629252776, "episode": 142.0, "batch_reward": 0.005285477184690535, "critic_loss": 0.0008513288637896039, "actor_loss": -13.133267850244417, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.50105094909668, "step": 142000}
{"episode_reward": 1.6103665722848481, "episode": 143.0, "batch_reward": 0.0052532802217174325, "critic_loss": 0.0013951961416805716, "actor_loss": -12.991026715038345, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.790730714797974, "step": 143000}
{"episode_reward": 2.715737607269424, "episode": 144.0, "batch_reward": 0.00542417374253273, "critic_loss": 0.0024155372210279895, "actor_loss": -12.727511467637495, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.541812419891357, "step": 144000}
{"episode_reward": 2.190141766920303, "episode": 145.0, "batch_reward": 0.005166460506035946, "critic_loss": 0.0010550331838658166, "actor_loss": -13.12564919938147, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.384408950805664, "step": 145000}
{"episode_reward": 1.8488010950238332, "episode": 146.0, "batch_reward": 0.0051796256297966465, "critic_loss": 0.0012224092952192222, "actor_loss": -14.300074624011293, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.07990264892578, "step": 146000}
{"episode_reward": 1.828694176723975, "episode": 147.0, "batch_reward": 0.005080709608679171, "critic_loss": 0.0006861273251051899, "actor_loss": -13.522785932859406, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.3664493560791, "step": 147000}
{"episode_reward": 3.111548472378172, "episode": 148.0, "batch_reward": 0.005505333524895832, "critic_loss": 0.001111012170167669, "actor_loss": -13.343722356008366, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.236472606658936, "step": 148000}
{"episode_reward": 2.2953941627949597, "episode": 149.0, "batch_reward": 0.005173509174375795, "critic_loss": 0.0009618878101236987, "actor_loss": -13.3990015813075, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.962828159332275, "step": 149000}
{"episode_reward": 2.1490394802914903, "episode": 150.0, "batch_reward": 0.005067506665247493, "critic_loss": 0.0007892021129191563, "actor_loss": -13.60648512791656, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
