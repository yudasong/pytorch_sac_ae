{"episode_reward": 0.0, "episode": 1.0, "duration": 19.107705116271973, "step": 1000}
{"episode_reward": 5.1931688606333894, "episode": 2.0, "duration": 1.5054841041564941, "step": 2000}
{"episode_reward": 454.7514538707513, "episode": 3.0, "batch_reward": 0.21583591895466767, "critic_loss": 0.024221783023633165, "actor_loss": -16.51468222082651, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 68.25092220306396, "step": 3000}
{"episode_reward": 2.1739150405785743, "episode": 4.0, "batch_reward": 0.133950350753963, "critic_loss": 0.02086618452006951, "actor_loss": -18.516375601768495, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.796038389205933, "step": 4000}
{"episode_reward": 2.7949061304115506, "episode": 5.0, "batch_reward": 0.10320557528734207, "critic_loss": 0.019430684988386928, "actor_loss": -15.774289695739746, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.437418460845947, "step": 5000}
{"episode_reward": 1.8655699925057196, "episode": 6.0, "batch_reward": 0.08450458851456642, "critic_loss": 0.020906575402710587, "actor_loss": -15.572307321548461, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.038752555847168, "step": 6000}
{"episode_reward": 2.168674654913151, "episode": 7.0, "batch_reward": 0.07184697256237269, "critic_loss": 0.024050002571428194, "actor_loss": -15.543955183982849, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.163645029067993, "step": 7000}
{"episode_reward": 1.590109822890513, "episode": 8.0, "batch_reward": 0.06291214773058891, "critic_loss": 0.021348736838903277, "actor_loss": -14.586904880523681, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.04632258415222, "step": 8000}
{"episode_reward": 1.7866799709451757, "episode": 9.0, "batch_reward": 0.055406436597928405, "critic_loss": 0.025311480770353226, "actor_loss": -15.581993872642517, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.8962082862854, "step": 9000}
{"episode_reward": 1.86509989117567, "episode": 10.0, "batch_reward": 0.05029906392470002, "critic_loss": 0.02071388555970043, "actor_loss": -13.777211656570435, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.348305463790894, "step": 10000}
{"episode_reward": 1.5664943438508727, "episode": 11.0, "batch_reward": 0.04564097445365042, "critic_loss": 0.02074729871947784, "actor_loss": -15.119492185115813, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.79266929626465, "step": 11000}
{"episode_reward": 1.6770990970703654, "episode": 12.0, "batch_reward": 0.04205407923553139, "critic_loss": 0.017678097043535672, "actor_loss": -14.435493570804596, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.486149549484253, "step": 12000}
{"episode_reward": 1.9555681033593377, "episode": 13.0, "batch_reward": 0.038509618278592825, "critic_loss": 0.015711554257257374, "actor_loss": -14.105721554279327, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.97916531562805, "step": 13000}
{"episode_reward": 2.2640846072435514, "episode": 14.0, "batch_reward": 0.03538123607542366, "critic_loss": 0.01699625116836978, "actor_loss": -12.556906478881835, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.66211462020874, "step": 14000}
{"episode_reward": 1.7055628980639452, "episode": 15.0, "batch_reward": 0.03357633480941877, "critic_loss": 0.01997541478276253, "actor_loss": -14.124517622947693, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.427643299102783, "step": 15000}
{"episode_reward": 2.718865764873714, "episode": 16.0, "batch_reward": 0.03112425218289718, "critic_loss": 0.01634738968894817, "actor_loss": -13.812675313949585, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.278842449188232, "step": 16000}
{"episode_reward": 1.8141103154534628, "episode": 17.0, "batch_reward": 0.029510818729177116, "critic_loss": 0.02004407123546116, "actor_loss": -12.833655997276306, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.872087240219116, "step": 17000}
{"episode_reward": 2.1834208246722637, "episode": 18.0, "batch_reward": 0.02817343812738545, "critic_loss": 0.013434985671890899, "actor_loss": -11.805645090579986, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.80651354789734, "step": 18000}
{"episode_reward": 2.8115309835865316, "episode": 19.0, "batch_reward": 0.026703299295157194, "critic_loss": 0.013102548410766758, "actor_loss": -13.802115540504456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.32794690132141, "step": 19000}
{"episode_reward": 5.015507745986939, "episode": 20.0, "batch_reward": 0.025146741765085606, "critic_loss": 0.020167241628281773, "actor_loss": -13.164946959018707, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.845354318618774, "step": 20000}
{"episode_reward": 1.5491647987657802, "episode": 21.0, "batch_reward": 0.02462897222628817, "critic_loss": 0.017514482134894933, "actor_loss": -14.107093256950378, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.19997191429138, "step": 21000}
{"episode_reward": 1.7223739518053105, "episode": 22.0, "batch_reward": 0.023273775209556334, "critic_loss": 0.019281747145112604, "actor_loss": -12.551485107660294, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.86957287788391, "step": 22000}
{"episode_reward": 2.3630455778336756, "episode": 23.0, "batch_reward": 0.022781921204179524, "critic_loss": 0.011062290142202983, "actor_loss": -10.966075989246368, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.233623504638672, "step": 23000}
{"episode_reward": 2.4998902413515856, "episode": 24.0, "batch_reward": 0.0214927102511283, "critic_loss": 0.021685642348486, "actor_loss": -12.099254339456559, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.562448501586914, "step": 24000}
{"episode_reward": 3.777320793396947, "episode": 25.0, "batch_reward": 0.020757427747594193, "critic_loss": 0.020984603166027228, "actor_loss": -11.795663381576539, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.21802020072937, "step": 25000}
{"episode_reward": 1.7651745449939635, "episode": 26.0, "batch_reward": 0.020043845305801368, "critic_loss": 0.013048604624113069, "actor_loss": -11.837892002105713, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.547010898590088, "step": 26000}
{"episode_reward": 1.7315581360300927, "episode": 27.0, "batch_reward": 0.019589604310691358, "critic_loss": 0.009859760626248317, "actor_loss": -11.705498915433884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.918216228485107, "step": 27000}
{"episode_reward": 2.221214188850869, "episode": 28.0, "batch_reward": 0.0188032001623651, "critic_loss": 0.00967687610554276, "actor_loss": -11.824934322357178, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.614396810531616, "step": 28000}
{"episode_reward": 2.51081606916319, "episode": 29.0, "batch_reward": 0.01885426957102027, "critic_loss": 0.010787314455272281, "actor_loss": -11.74490926027298, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.66198444366455, "step": 29000}
{"episode_reward": 1.8343136713183918, "episode": 30.0, "batch_reward": 0.01753089735319372, "critic_loss": 0.006778303427163337, "actor_loss": -11.5866322183609, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.3587007522583, "step": 30000}
{"episode_reward": 2.157096277066548, "episode": 31.0, "batch_reward": 0.017107907445402817, "critic_loss": 0.011163906128203963, "actor_loss": -11.141174906492234, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.70629000663757, "step": 31000}
{"episode_reward": 1.912596008158669, "episode": 32.0, "batch_reward": 0.01643318654107861, "critic_loss": 0.010515631978341844, "actor_loss": -12.431482920885086, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.155949592590332, "step": 32000}
{"episode_reward": 1.501931615539724, "episode": 33.0, "batch_reward": 0.016248579827486537, "critic_loss": 0.008373119717551163, "actor_loss": -11.69658072757721, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.137372970581055, "step": 33000}
{"episode_reward": 4.162213032392688, "episode": 34.0, "batch_reward": 0.015769696729723365, "critic_loss": 0.005783781695950892, "actor_loss": -9.717967750549317, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.715944528579712, "step": 34000}
{"episode_reward": 1.7400298535788217, "episode": 35.0, "batch_reward": 0.01528439772373531, "critic_loss": 0.008025049080650206, "actor_loss": -11.342950987458229, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.397963523864746, "step": 35000}
{"episode_reward": 2.0210509292047965, "episode": 36.0, "batch_reward": 0.014929403606336563, "critic_loss": 0.006272882844685228, "actor_loss": -11.286180220484734, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.753052473068237, "step": 36000}
{"episode_reward": 3.7273759317444175, "episode": 37.0, "batch_reward": 0.014752208560123109, "critic_loss": 0.006857710184296593, "actor_loss": -10.683758940815926, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.10682702064514, "step": 37000}
{"episode_reward": 2.855845739433606, "episode": 38.0, "batch_reward": 0.014465680839610286, "critic_loss": 0.005292744248632516, "actor_loss": -10.110285979509353, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.648601531982422, "step": 38000}
{"episode_reward": 1.8214524377741055, "episode": 39.0, "batch_reward": 0.013920512159937061, "critic_loss": 0.004553170760809735, "actor_loss": -12.032031015872956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.466411113739014, "step": 39000}
{"episode_reward": 1.9160510910810673, "episode": 40.0, "batch_reward": 0.013807698344928213, "critic_loss": 0.003469322295022721, "actor_loss": -11.720687487006188, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.49734377861023, "step": 40000}
{"episode_reward": 1.9075477091454918, "episode": 41.0, "batch_reward": 0.013546196789597162, "critic_loss": 0.005518242001373437, "actor_loss": -11.876271971225739, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.4029586315155, "step": 41000}
{"episode_reward": 1.7248463475687767, "episode": 42.0, "batch_reward": 0.012874950053170324, "critic_loss": 0.005626964106792002, "actor_loss": -9.94202035832405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.43392324447632, "step": 42000}
{"episode_reward": 1.4445947955334995, "episode": 43.0, "batch_reward": 0.012919267190154642, "critic_loss": 0.005894168065628037, "actor_loss": -10.477195107221604, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.01940679550171, "step": 43000}
{"episode_reward": 2.6247708060465786, "episode": 44.0, "batch_reward": 0.012565284474403597, "critic_loss": 0.003985533799910627, "actor_loss": -10.767658790707587, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.649091243743896, "step": 44000}
{"episode_reward": 1.7897178868050705, "episode": 45.0, "batch_reward": 0.012571049808873795, "critic_loss": 0.004339835448830854, "actor_loss": -11.565732999920845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.25150465965271, "step": 45000}
{"episode_reward": 1.3201086973621587, "episode": 46.0, "batch_reward": 0.012087893013027496, "critic_loss": 0.003054119192624057, "actor_loss": -11.869461331486702, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.25641369819641, "step": 46000}
{"episode_reward": 1.6934401860343098, "episode": 47.0, "batch_reward": 0.012298009917372838, "critic_loss": 0.0034923533717374083, "actor_loss": -10.909567274808884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.844712734222412, "step": 47000}
{"episode_reward": 1.6247577285967814, "episode": 48.0, "batch_reward": 0.01164623557718005, "critic_loss": 0.002552371038320416, "actor_loss": -10.47889952814579, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.638072729110718, "step": 48000}
{"episode_reward": 1.592105158959003, "episode": 49.0, "batch_reward": 0.011723765852395445, "critic_loss": 0.005196332408922899, "actor_loss": -11.40629419130087, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.979965686798096, "step": 49000}
{"episode_reward": 1.4612855782834526, "episode": 50.0, "batch_reward": 0.011096005955361761, "critic_loss": 0.004935451959965576, "actor_loss": -10.618235123932362, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.136170625686646, "step": 50000}
{"episode_reward": 2.5640066102441184, "episode": 51.0, "batch_reward": 0.011040671067894437, "critic_loss": 0.0026529203992286055, "actor_loss": -9.72952435696125, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.79255127906799, "step": 51000}
{"episode_reward": 1.933482633950001, "episode": 52.0, "batch_reward": 0.011032358565134927, "critic_loss": 0.00406975773418526, "actor_loss": -11.806152637302876, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.82395887374878, "step": 52000}
{"episode_reward": 2.499569771764236, "episode": 53.0, "batch_reward": 0.011003288276144304, "critic_loss": 0.0025577416637279386, "actor_loss": -11.519018426179885, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.02583146095276, "step": 53000}
{"episode_reward": 1.8832981475933708, "episode": 54.0, "batch_reward": 0.010368587206001393, "critic_loss": 0.002696362708633387, "actor_loss": -10.469922761023044, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.732969045639038, "step": 54000}
{"episode_reward": 1.9955298827262715, "episode": 55.0, "batch_reward": 0.010570953850168735, "critic_loss": 0.004269338908467034, "actor_loss": -9.835472796976566, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.374703884124756, "step": 55000}
{"episode_reward": 1.766060104226577, "episode": 56.0, "batch_reward": 0.010340026252320968, "critic_loss": 0.0023940861611299624, "actor_loss": -10.299140775024892, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.49630308151245, "step": 56000}
{"episode_reward": 2.1541783638414427, "episode": 57.0, "batch_reward": 0.009925303930649533, "critic_loss": 0.003011015190175385, "actor_loss": -11.167943186581136, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.368812561035156, "step": 57000}
{"episode_reward": 1.7078202793524375, "episode": 58.0, "batch_reward": 0.010131316574057565, "critic_loss": 0.0025511703508818754, "actor_loss": -9.042358500659466, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.34166955947876, "step": 58000}
{"episode_reward": 1.8799283588968734, "episode": 59.0, "batch_reward": 0.009581238822895102, "critic_loss": 0.003612460757329245, "actor_loss": -11.669475921988488, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.2640380859375, "step": 59000}
{"episode_reward": 1.9737733608404286, "episode": 60.0, "batch_reward": 0.009866128315276, "critic_loss": 0.0024863653726024497, "actor_loss": -11.07280494236946, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.60131525993347, "step": 60000}
{"episode_reward": 1.9102627080321875, "episode": 61.0, "batch_reward": 0.009706578313140198, "critic_loss": 0.003302727668415173, "actor_loss": -10.379884297311307, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.681495904922485, "step": 61000}
{"episode_reward": 1.764309860691783, "episode": 62.0, "batch_reward": 0.009634595362236723, "critic_loss": 0.0017740179154352518, "actor_loss": -8.983743607580662, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.938419818878174, "step": 62000}
{"episode_reward": 1.5163598171626191, "episode": 63.0, "batch_reward": 0.009399242849322037, "critic_loss": 0.004005076475481473, "actor_loss": -10.403522282838821, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.79380440711975, "step": 63000}
{"episode_reward": 1.70054383014102, "episode": 64.0, "batch_reward": 0.00893557979760226, "critic_loss": 0.0017122718678983801, "actor_loss": -10.099617952644826, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.75171184539795, "step": 64000}
{"episode_reward": 2.0357620371164984, "episode": 65.0, "batch_reward": 0.009012541833682917, "critic_loss": 0.004109496702480101, "actor_loss": -10.645328272104264, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.51754117012024, "step": 65000}
{"episode_reward": 1.9223722290500245, "episode": 66.0, "batch_reward": 0.009128090918879025, "critic_loss": 0.002266641469705064, "actor_loss": -10.654745459884404, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.327823638916016, "step": 66000}
{"episode_reward": 2.1672883152236357, "episode": 67.0, "batch_reward": 0.00898020759189967, "critic_loss": 0.002704937095717469, "actor_loss": -11.799403259307146, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.317472219467163, "step": 67000}
{"episode_reward": 2.4799360394996093, "episode": 68.0, "batch_reward": 0.008793981271213851, "critic_loss": 0.002719902533717686, "actor_loss": -10.002331603586674, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.194604635238647, "step": 68000}
{"episode_reward": 1.4031995780247362, "episode": 69.0, "batch_reward": 0.008554227758198977, "critic_loss": 0.003205870073848928, "actor_loss": -9.866654823124408, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.416615962982178, "step": 69000}
{"episode_reward": 1.825703973640415, "episode": 70.0, "batch_reward": 0.00880115021020174, "critic_loss": 0.0017792069683491719, "actor_loss": -10.891035974860191, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.841429471969604, "step": 70000}
{"episode_reward": 1.7182711112680262, "episode": 71.0, "batch_reward": 0.008781099784420803, "critic_loss": 0.003517309705428488, "actor_loss": -10.064551235079765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.7181658744812, "step": 71000}
{"episode_reward": 1.7112995042873926, "episode": 72.0, "batch_reward": 0.008492315885960125, "critic_loss": 0.0046363664774871725, "actor_loss": -10.396180362224579, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.405991077423096, "step": 72000}
{"episode_reward": 1.5140258557608968, "episode": 73.0, "batch_reward": 0.00840091589349322, "critic_loss": 0.002648200863157399, "actor_loss": -10.64294551679492, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.573206186294556, "step": 73000}
{"episode_reward": 1.6357963146446444, "episode": 74.0, "batch_reward": 0.008155579925165512, "critic_loss": 0.0021269980054676126, "actor_loss": -10.848845126569271, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.187033653259277, "step": 74000}
{"episode_reward": 1.6865252990512962, "episode": 75.0, "batch_reward": 0.008058511666138657, "critic_loss": 0.0016867972682339315, "actor_loss": -10.147401967644692, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.65943670272827, "step": 75000}
{"episode_reward": 1.6185515989213948, "episode": 76.0, "batch_reward": 0.008167498044902459, "critic_loss": 0.002657386933791713, "actor_loss": -10.282825357049704, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.24846386909485, "step": 76000}
{"episode_reward": 5.140572353050016, "episode": 77.0, "batch_reward": 0.007975942251621746, "critic_loss": 0.002046273724303319, "actor_loss": -10.338859790146351, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.412591457366943, "step": 77000}
{"episode_reward": 2.8477696112645634, "episode": 78.0, "batch_reward": 0.008293238142621704, "critic_loss": 0.0021637085839338395, "actor_loss": -10.548037811428308, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.093377590179443, "step": 78000}
{"episode_reward": 2.883538352629011, "episode": 79.0, "batch_reward": 0.007722117741126567, "critic_loss": 0.0027773643489563257, "actor_loss": -9.986464951947331, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.617316722869873, "step": 79000}
{"episode_reward": 1.7692759957844213, "episode": 80.0, "batch_reward": 0.007724270249484107, "critic_loss": 0.0017228937505169596, "actor_loss": -9.769131029695272, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.524364709854126, "step": 80000}
{"episode_reward": 1.636565633496518, "episode": 81.0, "batch_reward": 0.0075593372357543554, "critic_loss": 0.0032942390146054094, "actor_loss": -10.854529203921556, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.90939450263977, "step": 81000}
{"episode_reward": 1.620910424974738, "episode": 82.0, "batch_reward": 0.007744350420543924, "critic_loss": 0.0036032355475381335, "actor_loss": -10.66660708808899, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.751648664474487, "step": 82000}
{"episode_reward": 1.5734149776241153, "episode": 83.0, "batch_reward": 0.007449001961271279, "critic_loss": 0.002150923666171366, "actor_loss": -11.060421417549252, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.342015743255615, "step": 83000}
{"episode_reward": 2.2946085107684624, "episode": 84.0, "batch_reward": 0.007308550822315738, "critic_loss": 0.003324374903761054, "actor_loss": -10.782508714199066, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.78689169883728, "step": 84000}
{"episode_reward": 2.1909138098866876, "episode": 85.0, "batch_reward": 0.007568553283112124, "critic_loss": 0.0028934141148820344, "actor_loss": -10.167794056132436, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.37786030769348, "step": 85000}
{"episode_reward": 1.6184509574435948, "episode": 86.0, "batch_reward": 0.007358900975552388, "critic_loss": 0.002693560909729058, "actor_loss": -10.568270956397056, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.083242177963257, "step": 86000}
{"episode_reward": 1.5149524921916544, "episode": 87.0, "batch_reward": 0.007369656663155183, "critic_loss": 0.0011983143944198672, "actor_loss": -10.304259984090924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.452932119369507, "step": 87000}
{"episode_reward": 1.429245928123494, "episode": 88.0, "batch_reward": 0.007399744217924308, "critic_loss": 0.003010825895546077, "actor_loss": -10.22434470398724, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.083001375198364, "step": 88000}
{"episode_reward": 2.806351165718022, "episode": 89.0, "batch_reward": 0.007395476782694459, "critic_loss": 0.0017211193377952442, "actor_loss": -9.400268671572208, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.004587650299072, "step": 89000}
{"episode_reward": 1.9050903709138358, "episode": 90.0, "batch_reward": 0.007155831265612506, "critic_loss": 0.001852500624347158, "actor_loss": -10.696823703378438, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.61517024040222, "step": 90000}
{"episode_reward": 1.8713192660848117, "episode": 91.0, "batch_reward": 0.007334124107030221, "critic_loss": 0.00205053356069584, "actor_loss": -9.52310664613545, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.79400682449341, "step": 91000}
{"episode_reward": 2.6599886819409235, "episode": 92.0, "batch_reward": 0.0072635545858647675, "critic_loss": 0.0015793049682160927, "actor_loss": -11.313857782959937, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.301490545272827, "step": 92000}
{"episode_reward": 1.404547826190088, "episode": 93.0, "batch_reward": 0.006977564534288831, "critic_loss": 0.0016265349490695372, "actor_loss": -9.92427820044756, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.886255264282227, "step": 93000}
{"episode_reward": 1.6775672483906419, "episode": 94.0, "batch_reward": 0.0068620446271961556, "critic_loss": 0.00250877017640596, "actor_loss": -11.15413790550828, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.762122869491577, "step": 94000}
{"episode_reward": 1.6287913207556695, "episode": 95.0, "batch_reward": 0.006885693370597437, "critic_loss": 0.0023669309808647086, "actor_loss": -10.949797774888575, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.974034070968628, "step": 95000}
{"episode_reward": 1.3978011723133656, "episode": 96.0, "batch_reward": 0.006829749747063033, "critic_loss": 0.0017166696881322423, "actor_loss": -10.664239358961582, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.965636491775513, "step": 96000}
{"episode_reward": 1.8942339714458867, "episode": 97.0, "batch_reward": 0.006877043496351689, "critic_loss": 0.0013557837437856506, "actor_loss": -10.473716392338275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.779383659362793, "step": 97000}
{"episode_reward": 1.8469566460049216, "episode": 98.0, "batch_reward": 0.006746332002105192, "critic_loss": 0.002379343949311078, "actor_loss": -10.99825241253525, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.803377866744995, "step": 98000}
{"episode_reward": 1.631748306518896, "episode": 99.0, "batch_reward": 0.006576532494742424, "critic_loss": 0.0023338983580797504, "actor_loss": -10.459712337940932, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.86298441886902, "step": 99000}
{"episode_reward": 2.8865935091860004, "episode": 100.0, "batch_reward": 0.00675781364669092, "critic_loss": 0.0022383582331567597, "actor_loss": -10.255340479381383, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.793875217437744, "step": 100000}
{"episode_reward": 2.097908060809805, "episode": 101.0, "batch_reward": 0.0064652366988593715, "critic_loss": 0.001564861143435337, "actor_loss": -11.202591208800673, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.31081819534302, "step": 101000}
{"episode_reward": 1.2100281323258668, "episode": 102.0, "batch_reward": 0.006632730916957371, "critic_loss": 0.0011317072485526297, "actor_loss": -10.862001118302345, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.99861979484558, "step": 102000}
{"episode_reward": 1.9077245987422862, "episode": 103.0, "batch_reward": 0.006643728587194346, "critic_loss": 0.0025827351114676277, "actor_loss": -10.662275979600846, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.199443340301514, "step": 103000}
{"episode_reward": 1.7039089682576556, "episode": 104.0, "batch_reward": 0.006530935080256313, "critic_loss": 0.0019521048273127234, "actor_loss": -10.51241366390884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.53437614440918, "step": 104000}
{"episode_reward": 1.8627804614036478, "episode": 105.0, "batch_reward": 0.0065403879431542005, "critic_loss": 0.001646760462801467, "actor_loss": -9.552510720677674, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.229573726654053, "step": 105000}
{"episode_reward": 1.7071621837644582, "episode": 106.0, "batch_reward": 0.006443803230533376, "critic_loss": 0.0014900969279806305, "actor_loss": -8.867421554356813, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.406869411468506, "step": 106000}
{"episode_reward": 1.8478478362366118, "episode": 107.0, "batch_reward": 0.006441749391262419, "critic_loss": 0.0018545013454422587, "actor_loss": -9.962031002663077, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.23805546760559, "step": 107000}
{"episode_reward": 1.640717089022021, "episode": 108.0, "batch_reward": 0.006270973006845452, "critic_loss": 0.002083099377794497, "actor_loss": -10.840176652356982, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.572943449020386, "step": 108000}
{"episode_reward": 1.634217386298096, "episode": 109.0, "batch_reward": 0.0061422957754693926, "critic_loss": 0.0017405467609387414, "actor_loss": -10.771835479781032, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.698315143585205, "step": 109000}
{"episode_reward": 2.9036319610433283, "episode": 110.0, "batch_reward": 0.006114867324242369, "critic_loss": 0.0013809606678514682, "actor_loss": -10.704679866503923, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.473111391067505, "step": 110000}
{"episode_reward": 2.058550723340632, "episode": 111.0, "batch_reward": 0.006110294410027564, "critic_loss": 0.0016432306705100928, "actor_loss": -10.713741993855685, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.29395508766174, "step": 111000}
{"episode_reward": 2.9916087034269925, "episode": 112.0, "batch_reward": 0.006230417088721879, "critic_loss": 0.0016156511617973592, "actor_loss": -10.698554575596004, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.22664189338684, "step": 112000}
{"episode_reward": 1.7499800832524812, "episode": 113.0, "batch_reward": 0.005962445987155661, "critic_loss": 0.0017737551380414517, "actor_loss": -10.836759166322649, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.174821615219116, "step": 113000}
{"episode_reward": 2.4205733616858534, "episode": 114.0, "batch_reward": 0.005971701450995169, "critic_loss": 0.0012925233684582054, "actor_loss": -10.973988372709602, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.98016929626465, "step": 114000}
{"episode_reward": 1.8001530542984348, "episode": 115.0, "batch_reward": 0.006058268830412999, "critic_loss": 0.0016787311282278096, "actor_loss": -9.872860120330007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.623450756072998, "step": 115000}
{"episode_reward": 2.4887278760982, "episode": 116.0, "batch_reward": 0.006023421957506798, "critic_loss": 0.0016466607909642334, "actor_loss": -10.631953415609896, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.683629751205444, "step": 116000}
{"episode_reward": 1.6108083820447012, "episode": 117.0, "batch_reward": 0.006161671192618087, "critic_loss": 0.0023046089759991444, "actor_loss": -10.919078096006066, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.22298526763916, "step": 117000}
{"episode_reward": 1.6072382483520615, "episode": 118.0, "batch_reward": 0.006082796563394367, "critic_loss": 0.0016908088724194386, "actor_loss": -11.323136358637363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.78834319114685, "step": 118000}
{"episode_reward": 1.9831058687607956, "episode": 119.0, "batch_reward": 0.005903505523689091, "critic_loss": 0.0018416422773498199, "actor_loss": -9.737649005688727, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.245748043060303, "step": 119000}
{"episode_reward": 2.4012190561108073, "episode": 120.0, "batch_reward": 0.005772674634936265, "critic_loss": 0.001024850031495589, "actor_loss": -9.317599656641484, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.421631574630737, "step": 120000}
{"episode_reward": 2.151821567505932, "episode": 121.0, "batch_reward": 0.005886089232633822, "critic_loss": 0.0014526680435155868, "actor_loss": -9.179206861153245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.100327491760254, "step": 121000}
{"episode_reward": 2.249274487189785, "episode": 122.0, "batch_reward": 0.005873355829273351, "critic_loss": 0.001753051497078559, "actor_loss": -9.591977635201067, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.299566745758057, "step": 122000}
{"episode_reward": 2.1509350834430405, "episode": 123.0, "batch_reward": 0.0058053379718912765, "critic_loss": 0.0017443059138440731, "actor_loss": -10.134734156936407, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.781054258346558, "step": 123000}
{"episode_reward": 1.8511198350006535, "episode": 124.0, "batch_reward": 0.005729727782891132, "critic_loss": 0.001113445888759088, "actor_loss": -10.348771792065353, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.71481204032898, "step": 124000}
{"episode_reward": 2.2374515663715453, "episode": 125.0, "batch_reward": 0.00561344657454174, "critic_loss": 0.0017314511199801928, "actor_loss": -9.929688424356282, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.19085431098938, "step": 125000}
{"episode_reward": 1.9422734114829088, "episode": 126.0, "batch_reward": 0.0055382522057043386, "critic_loss": 0.0016538565894970816, "actor_loss": -10.341550582271068, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.48580050468445, "step": 126000}
{"episode_reward": 1.3580140801535674, "episode": 127.0, "batch_reward": 0.005674865834065713, "critic_loss": 0.001351018705392562, "actor_loss": -10.74669476357475, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.197905778884888, "step": 127000}
{"episode_reward": 2.0881694431788005, "episode": 128.0, "batch_reward": 0.005590320887567941, "critic_loss": 0.0013133523169290128, "actor_loss": -11.223924394143745, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.359840631484985, "step": 128000}
{"episode_reward": 1.6739323233397596, "episode": 129.0, "batch_reward": 0.005613874719012529, "critic_loss": 0.0020424995220855637, "actor_loss": -12.0477519993037, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.188602447509766, "step": 129000}
{"episode_reward": 2.316250479264208, "episode": 130.0, "batch_reward": 0.005559550703153946, "critic_loss": 0.001789767009946445, "actor_loss": -10.259686075473205, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.671465396881104, "step": 130000}
{"episode_reward": 1.8002645026753752, "episode": 131.0, "batch_reward": 0.00557406030036509, "critic_loss": 0.0014842906541634876, "actor_loss": -11.088116577520967, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.0851776599884, "step": 131000}
{"episode_reward": 2.3084272993552784, "episode": 132.0, "batch_reward": 0.005605050455080345, "critic_loss": 0.001132870149331211, "actor_loss": -9.849762824540958, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.69925570487976, "step": 132000}
{"episode_reward": 2.637689851602438, "episode": 133.0, "batch_reward": 0.005511678439681418, "critic_loss": 0.001112867832154734, "actor_loss": -11.492609486496075, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.86425495147705, "step": 133000}
{"episode_reward": 1.7323318008736908, "episode": 134.0, "batch_reward": 0.005638784828945063, "critic_loss": 0.0011059954485172056, "actor_loss": -11.627334389286116, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.0796902179718, "step": 134000}
{"episode_reward": 2.0046997020606, "episode": 135.0, "batch_reward": 0.005501529947738163, "critic_loss": 0.0011709172502851288, "actor_loss": -10.729594410696999, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.89344310760498, "step": 135000}
{"episode_reward": 1.7452117489474637, "episode": 136.0, "batch_reward": 0.005312172636156902, "critic_loss": 0.0015591970059613232, "actor_loss": -10.799516773432494, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.96553611755371, "step": 136000}
{"episode_reward": 1.2125653662554208, "episode": 137.0, "batch_reward": 0.005424529726151377, "critic_loss": 0.0012505985525258438, "actor_loss": -9.913722372259945, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.911922454833984, "step": 137000}
{"episode_reward": 2.1935480604929287, "episode": 138.0, "batch_reward": 0.005541029863408767, "critic_loss": 0.00167246686785802, "actor_loss": -9.436305841714145, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.88145399093628, "step": 138000}
{"episode_reward": 2.6577835128330767, "episode": 139.0, "batch_reward": 0.005439012679969892, "critic_loss": 0.0007258502456734277, "actor_loss": -10.194407054577022, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.811185359954834, "step": 139000}
{"episode_reward": 1.4954199364604173, "episode": 140.0, "batch_reward": 0.0052931987805059175, "critic_loss": 0.0009059448039042764, "actor_loss": -10.39601564305462, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.49423360824585, "step": 140000}
{"episode_reward": 1.796259999615589, "episode": 141.0, "batch_reward": 0.005307818496716209, "critic_loss": 0.0010036828664397035, "actor_loss": -9.498701263409108, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.93255019187927, "step": 141000}
{"episode_reward": 2.162902629252776, "episode": 142.0, "batch_reward": 0.005297498927335255, "critic_loss": 0.0011100701714203752, "actor_loss": -9.222167282447218, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.41180658340454, "step": 142000}
{"episode_reward": 1.6103665722848481, "episode": 143.0, "batch_reward": 0.005264080720953643, "critic_loss": 0.0010718229905360204, "actor_loss": -10.107729627870023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.5669002532959, "step": 143000}
{"episode_reward": 2.715737607269424, "episode": 144.0, "batch_reward": 0.005437554821255617, "critic_loss": 0.001177919286312317, "actor_loss": -9.427428015803919, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.195867776870728, "step": 144000}
{"episode_reward": 2.190141766920303, "episode": 145.0, "batch_reward": 0.005176348447683267, "critic_loss": 0.0006793040568554715, "actor_loss": -9.13616111424379, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.257425785064697, "step": 145000}
{"episode_reward": 1.8488010950238332, "episode": 146.0, "batch_reward": 0.005198356713866815, "critic_loss": 0.0008251986914438021, "actor_loss": -10.449870920715854, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.56582736968994, "step": 146000}
{"episode_reward": 1.828694176723975, "episode": 147.0, "batch_reward": 0.005095201034331694, "critic_loss": 0.0006835465054718952, "actor_loss": -9.950506159203128, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.996464014053345, "step": 147000}
{"episode_reward": 3.111548472378172, "episode": 148.0, "batch_reward": 0.0055195072774076835, "critic_loss": 0.0010077761711763742, "actor_loss": -10.113376895371825, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.342880249023438, "step": 148000}
{"episode_reward": 2.2953941627949597, "episode": 149.0, "batch_reward": 0.005184298536973074, "critic_loss": 0.0009136584851203224, "actor_loss": -9.347479917200282, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.74245595932007, "step": 149000}
{"episode_reward": 2.1490394802914903, "episode": 150.0, "batch_reward": 0.005079652580316178, "critic_loss": 0.0008547926476439898, "actor_loss": -9.897286714306102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
