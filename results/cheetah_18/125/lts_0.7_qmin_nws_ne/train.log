{"episode_reward": 0.0, "episode": 1.0, "duration": 19.393972158432007, "step": 1000}
{"episode_reward": 5.1931688606333894, "episode": 2.0, "duration": 1.584730625152588, "step": 2000}
{"episode_reward": 454.7514538707513, "episode": 3.0, "batch_reward": 0.21783488115874, "critic_loss": 0.02616664008178418, "actor_loss": -32.41491240045043, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 67.73406982421875, "step": 3000}
{"episode_reward": 20.471502582169776, "episode": 4.0, "batch_reward": 0.13972464967519044, "critic_loss": 0.01305240261205472, "actor_loss": -25.234700222969057, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.876304149627686, "step": 4000}
{"episode_reward": 5.574350500439259, "episode": 5.0, "batch_reward": 0.10836150418967008, "critic_loss": 0.012479026017012075, "actor_loss": -22.569367490768432, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.746907711029053, "step": 5000}
{"episode_reward": 4.465135767437722, "episode": 6.0, "batch_reward": 0.08929772625118494, "critic_loss": 0.012878525206120685, "actor_loss": -23.325625844955443, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.113297939300537, "step": 6000}
{"episode_reward": 4.678934083081794, "episode": 7.0, "batch_reward": 0.07625700987875461, "critic_loss": 0.013624693115241825, "actor_loss": -25.13520836353302, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.064124822616577, "step": 7000}
{"episode_reward": 4.741526674370875, "episode": 8.0, "batch_reward": 0.06704141076281667, "critic_loss": 0.009431821899139322, "actor_loss": -23.56951622581482, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.1767098903656, "step": 8000}
{"episode_reward": 4.56923399869957, "episode": 9.0, "batch_reward": 0.0594074719119817, "critic_loss": 0.010498116619768552, "actor_loss": -23.100938464641573, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.65438485145569, "step": 9000}
{"episode_reward": 3.9965492581478474, "episode": 10.0, "batch_reward": 0.054004362778738144, "critic_loss": 0.008717233299976215, "actor_loss": -22.945799201011656, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.681361198425293, "step": 10000}
{"episode_reward": 4.269178443099342, "episode": 11.0, "batch_reward": 0.04933689580950886, "critic_loss": 0.010009980725357309, "actor_loss": -23.274410204410554, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.48570108413696, "step": 11000}
{"episode_reward": 3.6847537461630706, "episode": 12.0, "batch_reward": 0.04561595947109163, "critic_loss": 0.008020627899793908, "actor_loss": -22.34039525604248, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.783392429351807, "step": 12000}
{"episode_reward": 5.573872864334282, "episode": 13.0, "batch_reward": 0.04203516188822687, "critic_loss": 0.007828277915657964, "actor_loss": -22.484718203544617, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.480573892593384, "step": 13000}
{"episode_reward": 3.8495467990665753, "episode": 14.0, "batch_reward": 0.03880374587327242, "critic_loss": 0.00823224545730045, "actor_loss": -21.524912415504456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.033820390701294, "step": 14000}
{"episode_reward": 3.3144543914765743, "episode": 15.0, "batch_reward": 0.036876292665489016, "critic_loss": 0.007596861733065453, "actor_loss": -24.139910507440568, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.771157026290894, "step": 15000}
{"episode_reward": 6.610660120138683, "episode": 16.0, "batch_reward": 0.034380956329405306, "critic_loss": 0.006484412923280615, "actor_loss": -22.876875920534133, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.594350337982178, "step": 16000}
{"episode_reward": 4.973912997934644, "episode": 17.0, "batch_reward": 0.032821416467428206, "critic_loss": 0.00594232561980607, "actor_loss": -21.928710951566696, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.714348554611206, "step": 17000}
{"episode_reward": 4.375469037202666, "episode": 18.0, "batch_reward": 0.031361439124681055, "critic_loss": 0.005088592639804119, "actor_loss": -22.145359293222427, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.066229820251465, "step": 18000}
{"episode_reward": 4.5327800950737736, "episode": 19.0, "batch_reward": 0.02970541262347251, "critic_loss": 0.005188606755051296, "actor_loss": -21.882981255531313, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.853135347366333, "step": 19000}
{"episode_reward": 2.566824390323864, "episode": 20.0, "batch_reward": 0.027986045989673584, "critic_loss": 0.005805692209949484, "actor_loss": -22.667195639371872, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.321092128753662, "step": 20000}
{"episode_reward": 2.999192812147805, "episode": 21.0, "batch_reward": 0.027484432561323047, "critic_loss": 0.00647694980862434, "actor_loss": -22.79944689106941, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.92619824409485, "step": 21000}
{"episode_reward": 4.899522235873936, "episode": 22.0, "batch_reward": 0.026114107327070087, "critic_loss": 0.004674124198703794, "actor_loss": -22.127849108457564, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.804820775985718, "step": 22000}
{"episode_reward": 3.661386447732136, "episode": 23.0, "batch_reward": 0.025523633878445254, "critic_loss": 0.0038813192695961334, "actor_loss": -22.757111261606216, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.885066032409668, "step": 23000}
{"episode_reward": 5.41132261460771, "episode": 24.0, "batch_reward": 0.024186792047228664, "critic_loss": 0.0038645841481338723, "actor_loss": -22.62380015730858, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.969123125076294, "step": 24000}
{"episode_reward": 5.138902317827213, "episode": 25.0, "batch_reward": 0.02345174450147897, "critic_loss": 0.0048598812451818955, "actor_loss": -21.542999922275545, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.31604790687561, "step": 25000}
{"episode_reward": 4.1499118777086785, "episode": 26.0, "batch_reward": 0.022795025430619717, "critic_loss": 0.0031887311676400713, "actor_loss": -21.92702165222168, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.964439153671265, "step": 26000}
{"episode_reward": 3.969794407719575, "episode": 27.0, "batch_reward": 0.022254222935065626, "critic_loss": 0.00434819161039195, "actor_loss": -22.140162028312684, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.306989669799805, "step": 27000}
{"episode_reward": 4.453787859052284, "episode": 28.0, "batch_reward": 0.02143184442166239, "critic_loss": 0.0031053348441491837, "actor_loss": -21.42539081454277, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.057335138320923, "step": 28000}
{"episode_reward": 5.232159433911443, "episode": 29.0, "batch_reward": 0.02159982418594882, "critic_loss": 0.004071391062956536, "actor_loss": -22.24316603553295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.254571437835693, "step": 29000}
{"episode_reward": 5.709726451840519, "episode": 30.0, "batch_reward": 0.02023777388664894, "critic_loss": 0.0031407305875472955, "actor_loss": -21.31460687983036, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.375287771224976, "step": 30000}
{"episode_reward": 3.959474033184079, "episode": 31.0, "batch_reward": 0.019758916905149818, "critic_loss": 0.003301809998229146, "actor_loss": -22.212010874032973, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.09019947052002, "step": 31000}
{"episode_reward": 4.207814764599079, "episode": 32.0, "batch_reward": 0.019068062307313085, "critic_loss": 0.003007223265856737, "actor_loss": -21.4309675180912, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.00385355949402, "step": 32000}
{"episode_reward": 4.050015499205245, "episode": 33.0, "batch_reward": 0.01879448626865633, "critic_loss": 0.0038248057450400667, "actor_loss": -22.530906129717827, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.84076428413391, "step": 33000}
{"episode_reward": 3.8748417798326336, "episode": 34.0, "batch_reward": 0.018353364253649487, "critic_loss": 0.0023230044723459285, "actor_loss": -21.232515467762948, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.505573511123657, "step": 34000}
{"episode_reward": 4.642602560424038, "episode": 35.0, "batch_reward": 0.0178561457619071, "critic_loss": 0.003398382866071188, "actor_loss": -22.181531887769697, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.736748933792114, "step": 35000}
{"episode_reward": 4.714798735412827, "episode": 36.0, "batch_reward": 0.017501045485725626, "critic_loss": 0.002378771931209485, "actor_loss": -20.643769959926605, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.94644260406494, "step": 36000}
{"episode_reward": 2.9264808014943764, "episode": 37.0, "batch_reward": 0.017155486026545985, "critic_loss": 0.0035374435884441484, "actor_loss": -21.62108775353432, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.651133060455322, "step": 37000}
{"episode_reward": 3.079681331737634, "episode": 38.0, "batch_reward": 0.016893669589422643, "critic_loss": 0.0029488496293779464, "actor_loss": -22.12877250432968, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.524978399276733, "step": 38000}
{"episode_reward": 4.634421592070644, "episode": 39.0, "batch_reward": 0.01643673235666938, "critic_loss": 0.0031084751332818997, "actor_loss": -22.306560134053232, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.463120222091675, "step": 39000}
{"episode_reward": 5.116350734689416, "episode": 40.0, "batch_reward": 0.016298862332710995, "critic_loss": 0.0026682191506697565, "actor_loss": -22.512149245142936, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.606714487075806, "step": 40000}
{"episode_reward": 3.762323036400173, "episode": 41.0, "batch_reward": 0.01597392465546727, "critic_loss": 0.0028516562047007027, "actor_loss": -22.13012251871824, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.00867676734924, "step": 41000}
{"episode_reward": 4.791261765855109, "episode": 42.0, "batch_reward": 0.015320864414796234, "critic_loss": 0.00287436286706361, "actor_loss": -21.277292362749577, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.695409774780273, "step": 42000}
{"episode_reward": 4.227351348330976, "episode": 43.0, "batch_reward": 0.015330504975048825, "critic_loss": 0.003440003661788069, "actor_loss": -21.38093747240305, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.133970022201538, "step": 43000}
{"episode_reward": 5.030044140792817, "episode": 44.0, "batch_reward": 0.015099239823641256, "critic_loss": 0.003256829714315245, "actor_loss": -20.988146061122418, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.411513805389404, "step": 44000}
{"episode_reward": 4.6044218487262825, "episode": 45.0, "batch_reward": 0.015080033641774208, "critic_loss": 0.0021233113172929733, "actor_loss": -21.84946812325716, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.936301469802856, "step": 45000}
{"episode_reward": 4.02373126846399, "episode": 46.0, "batch_reward": 0.014519187701167538, "critic_loss": 0.0028369285001826937, "actor_loss": -21.290365634977817, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.454923391342163, "step": 46000}
{"episode_reward": 4.801657130122669, "episode": 47.0, "batch_reward": 0.014786900822771714, "critic_loss": 0.0021057032681128475, "actor_loss": -21.956616991817953, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.180455446243286, "step": 47000}
{"episode_reward": 4.908479000968998, "episode": 48.0, "batch_reward": 0.014141210577683524, "critic_loss": 0.002051091126690153, "actor_loss": -20.43363399553299, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.995793104171753, "step": 48000}
{"episode_reward": 5.10258915935516, "episode": 49.0, "batch_reward": 0.014234374452847988, "critic_loss": 0.0030286765589262357, "actor_loss": -22.04929174619913, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.06009602546692, "step": 49000}
{"episode_reward": 3.9467099408032005, "episode": 50.0, "batch_reward": 0.013636344172758982, "critic_loss": 0.0022602560488157904, "actor_loss": -21.893646345853806, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.510454654693604, "step": 50000}
{"episode_reward": 5.945399413245947, "episode": 51.0, "batch_reward": 0.01355841190717183, "critic_loss": 0.0017988346697675297, "actor_loss": -21.79209930318594, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.502893924713135, "step": 51000}
{"episode_reward": 5.22452427077843, "episode": 52.0, "batch_reward": 0.013638555245939642, "critic_loss": 0.0028054467393958476, "actor_loss": -21.899352494418622, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.35274624824524, "step": 52000}
{"episode_reward": 3.649303102431215, "episode": 53.0, "batch_reward": 0.013531872640131042, "critic_loss": 0.0023503576856310246, "actor_loss": -21.552123779654504, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.83118176460266, "step": 53000}
{"episode_reward": 4.894890358357429, "episode": 54.0, "batch_reward": 0.012908808227162808, "critic_loss": 0.0016001977958658244, "actor_loss": -21.909800718605517, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.02327013015747, "step": 54000}
{"episode_reward": 4.4839027281952815, "episode": 55.0, "batch_reward": 0.013127345614368097, "critic_loss": 0.0030673053933132906, "actor_loss": -21.9716532779932, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.13587260246277, "step": 55000}
{"episode_reward": 4.398586559739409, "episode": 56.0, "batch_reward": 0.01292501751286909, "critic_loss": 0.0017766698331106454, "actor_loss": -22.13813844937086, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.255823850631714, "step": 56000}
{"episode_reward": 4.503748545907632, "episode": 57.0, "batch_reward": 0.012496544745052234, "critic_loss": 0.002161537243126077, "actor_loss": -21.778115731030702, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.368743658065796, "step": 57000}
{"episode_reward": 4.665997023746192, "episode": 58.0, "batch_reward": 0.012656341652618721, "critic_loss": 0.001679908171587158, "actor_loss": -21.238162779808043, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.505819082260132, "step": 58000}
{"episode_reward": 4.546687711166863, "episode": 59.0, "batch_reward": 0.01217806643107906, "critic_loss": 0.00257951220900577, "actor_loss": -21.01393517112732, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.702015161514282, "step": 59000}
{"episode_reward": 3.404579686664059, "episode": 60.0, "batch_reward": 0.012416891359724104, "critic_loss": 0.0020537802052567715, "actor_loss": -21.836158348560332, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.131267070770264, "step": 60000}
{"episode_reward": 4.90924576567038, "episode": 61.0, "batch_reward": 0.012259065127465874, "critic_loss": 0.0020344668945472223, "actor_loss": -21.37862798511982, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.27255988121033, "step": 61000}
{"episode_reward": 5.687482233415601, "episode": 62.0, "batch_reward": 0.012216362592298538, "critic_loss": 0.0020026712190156105, "actor_loss": -21.999299998939037, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.599018812179565, "step": 62000}
{"episode_reward": 4.648500838697628, "episode": 63.0, "batch_reward": 0.01200332528213039, "critic_loss": 0.0020632939300412544, "actor_loss": -21.18732971456647, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.980369806289673, "step": 63000}
{"episode_reward": 4.88405991996555, "episode": 64.0, "batch_reward": 0.01153969350666739, "critic_loss": 0.002099893923586933, "actor_loss": -21.649858302921057, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.429563283920288, "step": 64000}
{"episode_reward": 3.870623748750624, "episode": 65.0, "batch_reward": 0.011550537056522444, "critic_loss": 0.0016823285139544169, "actor_loss": -21.62145432758331, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.849183320999146, "step": 65000}
{"episode_reward": 3.9285601646126875, "episode": 66.0, "batch_reward": 0.011710796283092349, "critic_loss": 0.0018986928809972597, "actor_loss": -21.701793975532055, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.854964017868042, "step": 66000}
{"episode_reward": 3.89600040756712, "episode": 67.0, "batch_reward": 0.011559127371525393, "critic_loss": 0.0016400344489811687, "actor_loss": -21.97085652682185, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.14608907699585, "step": 67000}
{"episode_reward": 4.8987946804916005, "episode": 68.0, "batch_reward": 0.011402376900427044, "critic_loss": 0.0020134025074512465, "actor_loss": -20.694819569855927, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.2034695148468, "step": 68000}
{"episode_reward": 3.57215809979812, "episode": 69.0, "batch_reward": 0.011095482801552863, "critic_loss": 0.002218377905737725, "actor_loss": -20.68668064108491, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.05673122406006, "step": 69000}
{"episode_reward": 3.9603648730597047, "episode": 70.0, "batch_reward": 0.01136378953489475, "critic_loss": 0.0015337409060011851, "actor_loss": -21.90865263918042, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.434975624084473, "step": 70000}
{"episode_reward": 4.2242174862641, "episode": 71.0, "batch_reward": 0.011305631622904912, "critic_loss": 0.0016408653981779934, "actor_loss": -21.554182139366866, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.6902973651886, "step": 71000}
{"episode_reward": 3.865937674999433, "episode": 72.0, "batch_reward": 0.011057064201449975, "critic_loss": 0.001611368667872739, "actor_loss": -21.179515720009803, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.87225580215454, "step": 72000}
{"episode_reward": 4.162871101424403, "episode": 73.0, "batch_reward": 0.010900476685492322, "critic_loss": 0.0016103264164703433, "actor_loss": -21.363364941656588, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.699950695037842, "step": 73000}
{"episode_reward": 5.172420520305436, "episode": 74.0, "batch_reward": 0.01071655320818536, "critic_loss": 0.001381570646102773, "actor_loss": -21.887307991430163, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.327461004257202, "step": 74000}
{"episode_reward": 4.989804975685766, "episode": 75.0, "batch_reward": 0.010619939070893451, "critic_loss": 0.0015615653739514527, "actor_loss": -21.75649735213816, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.669957637786865, "step": 75000}
{"episode_reward": 4.354297523925878, "episode": 76.0, "batch_reward": 0.010658525499515235, "critic_loss": 0.0018594700562680373, "actor_loss": -22.236628702059388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.829919815063477, "step": 76000}
{"episode_reward": 5.006522262546473, "episode": 77.0, "batch_reward": 0.0105297346082516, "critic_loss": 0.0012534470359096303, "actor_loss": -21.145251798823477, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.022967100143433, "step": 77000}
{"episode_reward": 3.7763828761382037, "episode": 78.0, "batch_reward": 0.010754834477673284, "critic_loss": 0.001487269402598031, "actor_loss": -21.62653749895096, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.741817235946655, "step": 78000}
{"episode_reward": 4.06986466168755, "episode": 79.0, "batch_reward": 0.010160310249659233, "critic_loss": 0.001569618587003788, "actor_loss": -20.27938288910687, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.58344602584839, "step": 79000}
{"episode_reward": 3.403607322212955, "episode": 80.0, "batch_reward": 0.010175490385619924, "critic_loss": 0.0014285325318051036, "actor_loss": -21.180569307267668, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.249134302139282, "step": 80000}
{"episode_reward": 3.448386286627011, "episode": 81.0, "batch_reward": 0.010097630088916048, "critic_loss": 0.001772906058904482, "actor_loss": -21.358330984190108, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.172852516174316, "step": 81000}
{"episode_reward": 5.058687593820506, "episode": 82.0, "batch_reward": 0.010229537719395011, "critic_loss": 0.0014627119660726748, "actor_loss": -22.484287342965604, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.358705282211304, "step": 82000}
{"episode_reward": 5.416667060840987, "episode": 83.0, "batch_reward": 0.009923610258614645, "critic_loss": 0.0013532348783119232, "actor_loss": -20.533569835051892, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.339121103286743, "step": 83000}
{"episode_reward": 4.911742116090393, "episode": 84.0, "batch_reward": 0.009795182634377852, "critic_loss": 0.0016282217157422565, "actor_loss": -21.963128161408008, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.473328590393066, "step": 84000}
{"episode_reward": 5.23830670239155, "episode": 85.0, "batch_reward": 0.010104973716661334, "critic_loss": 0.001348208238465304, "actor_loss": -21.666678895272316, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.642387628555298, "step": 85000}
{"episode_reward": 4.887838623346601, "episode": 86.0, "batch_reward": 0.009882993895094842, "critic_loss": 0.001417201884170936, "actor_loss": -21.167488429665564, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.23294997215271, "step": 86000}
{"episode_reward": 4.53822246156133, "episode": 87.0, "batch_reward": 0.009898382627405226, "critic_loss": 0.0010444568111925036, "actor_loss": -21.07028839126229, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.856451988220215, "step": 87000}
{"episode_reward": 4.581254238985735, "episode": 88.0, "batch_reward": 0.00989664209028706, "critic_loss": 0.0016534484656949644, "actor_loss": -20.371205336436628, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.668410301208496, "step": 88000}
{"episode_reward": 3.99736418195877, "episode": 89.0, "batch_reward": 0.009901507581351325, "critic_loss": 0.0012290505630953702, "actor_loss": -21.785517733722926, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.412484407424927, "step": 89000}
{"episode_reward": 2.8246659415852973, "episode": 90.0, "batch_reward": 0.009647352769039571, "critic_loss": 0.0011129938649901306, "actor_loss": -22.46889737261087, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.24692130088806, "step": 90000}
{"episode_reward": 5.168119012660584, "episode": 91.0, "batch_reward": 0.009871845123823731, "critic_loss": 0.00115010386218637, "actor_loss": -21.298636416219175, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.05998086929321, "step": 91000}
{"episode_reward": 5.946735380877795, "episode": 92.0, "batch_reward": 0.009745702643413097, "critic_loss": 0.0014918874944996787, "actor_loss": -20.829467730171977, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.60133409500122, "step": 92000}
{"episode_reward": 4.728277887649469, "episode": 93.0, "batch_reward": 0.009561806444311514, "critic_loss": 0.0011356018892402062, "actor_loss": -20.871966827519238, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.675109386444092, "step": 93000}
{"episode_reward": 5.232503713191745, "episode": 94.0, "batch_reward": 0.009413611395284533, "critic_loss": 0.0011682924858032492, "actor_loss": -20.944259442701934, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.813673496246338, "step": 94000}
{"episode_reward": 2.9718786150627636, "episode": 95.0, "batch_reward": 0.009451529800891876, "critic_loss": 0.0015171088644419797, "actor_loss": -22.217763160303235, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.705246686935425, "step": 95000}
{"episode_reward": 4.967181635217857, "episode": 96.0, "batch_reward": 0.009374468406429514, "critic_loss": 0.0010624952552316246, "actor_loss": -22.128635274596512, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.085049152374268, "step": 96000}
{"episode_reward": 4.268601254139028, "episode": 97.0, "batch_reward": 0.009316443782532588, "critic_loss": 0.00116572153744346, "actor_loss": -22.47281981986761, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.588080167770386, "step": 97000}
{"episode_reward": 3.6598361894609415, "episode": 98.0, "batch_reward": 0.009280832654796541, "critic_loss": 0.0014603883482632227, "actor_loss": -20.930727291315794, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.587395906448364, "step": 98000}
{"episode_reward": 5.306177999857545, "episode": 99.0, "batch_reward": 0.009110438268166035, "critic_loss": 0.0010349321083704126, "actor_loss": -20.988923225432636, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.75296187400818, "step": 99000}
{"episode_reward": 3.7056467039494647, "episode": 100.0, "batch_reward": 0.009175486048916355, "critic_loss": 0.0011570861891232198, "actor_loss": -21.232548068746926, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.68087100982666, "step": 100000}
{"episode_reward": 5.890129330839069, "episode": 101.0, "batch_reward": 0.00903022224700544, "critic_loss": 0.0010643742678366835, "actor_loss": -21.65788481693715, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.762805700302124, "step": 101000}
{"episode_reward": 5.050374799512744, "episode": 102.0, "batch_reward": 0.009186224239645525, "critic_loss": 0.0013168973213105346, "actor_loss": -21.408760810784994, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.540969848632812, "step": 102000}
{"episode_reward": 2.922241057535058, "episode": 103.0, "batch_reward": 0.009152066581649706, "critic_loss": 0.001420011155118118, "actor_loss": -21.290875433743, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.61637592315674, "step": 103000}
{"episode_reward": 4.938838833632696, "episode": 104.0, "batch_reward": 0.009030740138143301, "critic_loss": 0.000946255224509514, "actor_loss": -21.284919317029416, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.66443133354187, "step": 104000}
{"episode_reward": 4.652965732618372, "episode": 105.0, "batch_reward": 0.009090490083443, "critic_loss": 0.0011493977991340217, "actor_loss": -21.30662490774691, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.29285478591919, "step": 105000}
{"episode_reward": 2.937379419654026, "episode": 106.0, "batch_reward": 0.00894765157927759, "critic_loss": 0.0012773796671026502, "actor_loss": -20.565935833007096, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.63827395439148, "step": 106000}
{"episode_reward": 4.20065443626079, "episode": 107.0, "batch_reward": 0.008986818291945383, "critic_loss": 0.0009794111998780864, "actor_loss": -21.177490578114988, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.856741905212402, "step": 107000}
{"episode_reward": 6.005134851811033, "episode": 108.0, "batch_reward": 0.008791492561344058, "critic_loss": 0.0012629380493890494, "actor_loss": -21.09803612536937, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.194989442825317, "step": 108000}
{"episode_reward": 4.100509024538885, "episode": 109.0, "batch_reward": 0.008631270771380513, "critic_loss": 0.0009305713888461468, "actor_loss": -21.887750014856458, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.494759798049927, "step": 109000}
{"episode_reward": 4.048734873822124, "episode": 110.0, "batch_reward": 0.008636893715942279, "critic_loss": 0.0012347270982209012, "actor_loss": -21.175581994846464, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.271100759506226, "step": 110000}
{"episode_reward": 5.040244324156577, "episode": 111.0, "batch_reward": 0.008648342438507825, "critic_loss": 0.0013243805586826056, "actor_loss": -22.26987511844933, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.786192893981934, "step": 111000}
{"episode_reward": 4.753157077215541, "episode": 112.0, "batch_reward": 0.008743112629512325, "critic_loss": 0.001011054286682338, "actor_loss": -21.15695162008703, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.12228798866272, "step": 112000}
{"episode_reward": 4.41959074886956, "episode": 113.0, "batch_reward": 0.008435510468669236, "critic_loss": 0.0015516936948333751, "actor_loss": -21.14165964680165, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.39295792579651, "step": 113000}
{"episode_reward": 5.8525843431342475, "episode": 114.0, "batch_reward": 0.008519349991343916, "critic_loss": 0.0009085028166009579, "actor_loss": -21.871433031499386, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.645212650299072, "step": 114000}
{"episode_reward": 4.883271490096334, "episode": 115.0, "batch_reward": 0.008528498851926997, "critic_loss": 0.0009145354455758934, "actor_loss": -21.272090074054898, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.803738117218018, "step": 115000}
{"episode_reward": 4.087162213691729, "episode": 116.0, "batch_reward": 0.008541345187462867, "critic_loss": 0.0009519241405796492, "actor_loss": -21.464919109158217, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.668692111968994, "step": 116000}
{"episode_reward": 5.452898312974316, "episode": 117.0, "batch_reward": 0.008673667604918592, "critic_loss": 0.0011654723189567448, "actor_loss": -21.277283009551464, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.577418327331543, "step": 117000}
{"episode_reward": 5.9594343397907785, "episode": 118.0, "batch_reward": 0.008620736490236595, "critic_loss": 0.0013047211066659656, "actor_loss": -21.582457512952388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.28315758705139, "step": 118000}
{"episode_reward": 3.9843158105538583, "episode": 119.0, "batch_reward": 0.008440399353625254, "critic_loss": 0.0010541920604082407, "actor_loss": -21.76711361590773, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.444375038146973, "step": 119000}
{"episode_reward": 3.3846320320081325, "episode": 120.0, "batch_reward": 0.008311148533131927, "critic_loss": 0.001205248725542333, "actor_loss": -20.80576539194584, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.258840084075928, "step": 120000}
{"episode_reward": 5.997806339544728, "episode": 121.0, "batch_reward": 0.008417831850238144, "critic_loss": 0.000985830780780816, "actor_loss": -21.1062374060452, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.05329251289368, "step": 121000}
{"episode_reward": 5.6072682015247, "episode": 122.0, "batch_reward": 0.008432364066364243, "critic_loss": 0.001188972038857173, "actor_loss": -20.965759109891952, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.786784410476685, "step": 122000}
{"episode_reward": 4.7465856832739775, "episode": 123.0, "batch_reward": 0.008399567845510318, "critic_loss": 0.0011773910421325126, "actor_loss": -19.447348501861097, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.085525274276733, "step": 123000}
{"episode_reward": 5.310269912423394, "episode": 124.0, "batch_reward": 0.008379231847124174, "critic_loss": 0.0011005980275513138, "actor_loss": -20.936965956307947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.708168506622314, "step": 124000}
{"episode_reward": 4.995225235823542, "episode": 125.0, "batch_reward": 0.008174653172958643, "critic_loss": 0.0014490733002239721, "actor_loss": -21.038632083304226, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.807097911834717, "step": 125000}
{"episode_reward": 5.294679289653223, "episode": 126.0, "batch_reward": 0.00813524951832369, "critic_loss": 0.001176911125810875, "actor_loss": -21.038462343491613, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.882227659225464, "step": 126000}
{"episode_reward": 5.6556357572126945, "episode": 127.0, "batch_reward": 0.008303133950103075, "critic_loss": 0.0012822568087285618, "actor_loss": -20.906437042228877, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.254791498184204, "step": 127000}
{"episode_reward": 5.204511482361773, "episode": 128.0, "batch_reward": 0.008218234772095457, "critic_loss": 0.0009696565800622921, "actor_loss": -21.91622757621482, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.18188762664795, "step": 128000}
{"episode_reward": 4.374544099719639, "episode": 129.0, "batch_reward": 0.008229514714330435, "critic_loss": 0.001198705955161131, "actor_loss": -21.980608344133945, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.21714162826538, "step": 129000}
{"episode_reward": 5.45393170843133, "episode": 130.0, "batch_reward": 0.008116282648174091, "critic_loss": 0.0010771410845118227, "actor_loss": -21.208780370809137, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.129662036895752, "step": 130000}
{"episode_reward": 3.4330994974366082, "episode": 131.0, "batch_reward": 0.008108855332247913, "critic_loss": 0.0013284760256137816, "actor_loss": -22.485490842882545, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.05179452896118, "step": 131000}
{"episode_reward": 4.767260662803943, "episode": 132.0, "batch_reward": 0.008199532555649057, "critic_loss": 0.0009324063670937904, "actor_loss": -21.436459922268988, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.912550687789917, "step": 132000}
{"episode_reward": 4.002706865515868, "episode": 133.0, "batch_reward": 0.008047922948608175, "critic_loss": 0.0009121597290140926, "actor_loss": -21.666105246577413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.553950309753418, "step": 133000}
{"episode_reward": 3.9199404756951477, "episode": 134.0, "batch_reward": 0.008217512179398909, "critic_loss": 0.001222833252111741, "actor_loss": -21.78937427087128, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.69468879699707, "step": 134000}
{"episode_reward": 5.581635394141723, "episode": 135.0, "batch_reward": 0.008057774698361754, "critic_loss": 0.0008308743968373165, "actor_loss": -21.9134770841524, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.09851837158203, "step": 135000}
{"episode_reward": 4.050579179641291, "episode": 136.0, "batch_reward": 0.007869999737245961, "critic_loss": 0.0009092404677139711, "actor_loss": -22.720384741790593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.65862512588501, "step": 136000}
{"episode_reward": 3.573315927302337, "episode": 137.0, "batch_reward": 0.008061010523699223, "critic_loss": 0.0011327847708162153, "actor_loss": -21.254472247585653, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.294599056243896, "step": 137000}
{"episode_reward": 4.328198856052906, "episode": 138.0, "batch_reward": 0.00810846074006986, "critic_loss": 0.0009315646619579639, "actor_loss": -20.784628881841897, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.64099431037903, "step": 138000}
{"episode_reward": 6.047808527612052, "episode": 139.0, "batch_reward": 0.008028625810984522, "critic_loss": 0.0008758727892636671, "actor_loss": -20.954637020304798, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.33237075805664, "step": 139000}
{"episode_reward": 3.8069649128095167, "episode": 140.0, "batch_reward": 0.007800440577790141, "critic_loss": 0.0009994485595016158, "actor_loss": -20.76094088514149, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.09965229034424, "step": 140000}
{"episode_reward": 5.033880690343734, "episode": 141.0, "batch_reward": 0.007870028902776539, "critic_loss": 0.0008944078874264961, "actor_loss": -20.44571995484084, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.071128606796265, "step": 141000}
{"episode_reward": 4.595276550627853, "episode": 142.0, "batch_reward": 0.007937397819245235, "critic_loss": 0.0010419141249949462, "actor_loss": -21.30253310242668, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.9002845287323, "step": 142000}
{"episode_reward": 3.814583237707594, "episode": 143.0, "batch_reward": 0.007823351237224415, "critic_loss": 0.000945777345346869, "actor_loss": -21.372257566444574, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.064754724502563, "step": 143000}
{"episode_reward": 3.917609787640718, "episode": 144.0, "batch_reward": 0.007984427289105952, "critic_loss": 0.001512373437450151, "actor_loss": -21.606323256373404, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.461921453475952, "step": 144000}
{"episode_reward": 5.04855306933506, "episode": 145.0, "batch_reward": 0.007729164307238534, "critic_loss": 0.0008117733377875993, "actor_loss": -21.244543362084777, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.12067437171936, "step": 145000}
{"episode_reward": 4.765516297121253, "episode": 146.0, "batch_reward": 0.007761778219603002, "critic_loss": 0.0010281965409812983, "actor_loss": -21.006697725977748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.786199808120728, "step": 146000}
{"episode_reward": 5.426264517849846, "episode": 147.0, "batch_reward": 0.0076669906682800505, "critic_loss": 0.0009226302108072559, "actor_loss": -21.458269876860083, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.16402530670166, "step": 147000}
{"episode_reward": 5.18377404998967, "episode": 148.0, "batch_reward": 0.008005578110227361, "critic_loss": 0.001049409079401812, "actor_loss": -20.537010040909053, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.949062824249268, "step": 148000}
{"episode_reward": 4.8325371379242865, "episode": 149.0, "batch_reward": 0.007697532806545496, "critic_loss": 0.0011765719206232462, "actor_loss": -21.0561250738129, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.457940340042114, "step": 149000}
{"episode_reward": 5.291002406033924, "episode": 150.0, "batch_reward": 0.007707213705638424, "critic_loss": 0.000770937447414326, "actor_loss": -21.00365622539073, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
