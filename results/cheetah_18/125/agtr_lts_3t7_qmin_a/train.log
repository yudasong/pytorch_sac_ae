{"episode_reward": 0.0, "episode": 1.0, "duration": 13.334864377975464, "step": 1000}
{"episode_reward": 5.1931688606333894, "episode": 2.0, "duration": 1.1880507469177246, "step": 2000}
{"episode_reward": 454.7514538707513, "episode": 3.0, "batch_reward": 0.21844676948609293, "critic_loss": 0.05145388129985543, "actor_loss": -32.203596525290955, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 72.14484548568726, "step": 3000}
{"episode_reward": 41.02647324897337, "episode": 4.0, "batch_reward": 0.14965306720137597, "critic_loss": 0.04682202119845897, "actor_loss": -25.69085703842342, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.24227499961853, "step": 4000}
{"episode_reward": 22.742854560177, "episode": 5.0, "batch_reward": 0.1174463652074337, "critic_loss": 0.03518370369821787, "actor_loss": -27.47397427099943, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.80034327507019, "step": 5000}
{"episode_reward": 12.81479169167931, "episode": 6.0, "batch_reward": 0.09914058333262801, "critic_loss": 0.0433966461122036, "actor_loss": -25.265030891299247, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.595273971557617, "step": 6000}
{"episode_reward": 27.961873314838623, "episode": 7.0, "batch_reward": 0.09206458070874214, "critic_loss": 0.04656299879774451, "actor_loss": -24.103803569756447, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.708617210388184, "step": 7000}
{"episode_reward": 85.18355124834433, "episode": 8.0, "batch_reward": 0.09392263030260802, "critic_loss": 0.061025729505345225, "actor_loss": -24.593907679356633, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.8904025554657, "step": 8000}
{"episode_reward": 128.14894713124775, "episode": 9.0, "batch_reward": 0.10185106132179499, "critic_loss": 0.09514732187241316, "actor_loss": -22.816167500175535, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.586376190185547, "step": 9000}
{"episode_reward": 180.74891255379256, "episode": 10.0, "batch_reward": 0.10928247208893299, "critic_loss": 0.10182986771687866, "actor_loss": -23.85337732049823, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.840965032577515, "step": 10000}
{"episode_reward": 106.38490199318322, "episode": 11.0, "batch_reward": 0.10386118138581514, "critic_loss": 0.10337217848375439, "actor_loss": -22.386907219052315, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 37.75473237037659, "step": 11000}
{"episode_reward": 28.474238239149315, "episode": 12.0, "batch_reward": 0.1004137576520443, "critic_loss": 0.11284193420037628, "actor_loss": -21.12880727291107, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.495271682739258, "step": 12000}
{"episode_reward": 74.11265532024062, "episode": 13.0, "batch_reward": 0.09630522079020738, "critic_loss": 0.10877180344238878, "actor_loss": -20.045867296934127, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.003612279891968, "step": 13000}
{"episode_reward": 43.091541864349466, "episode": 14.0, "batch_reward": 0.09608593280240893, "critic_loss": 0.12379103337973356, "actor_loss": -21.268782520294188, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.540228605270386, "step": 14000}
{"episode_reward": 169.87234924431326, "episode": 15.0, "batch_reward": 0.1007200903967023, "critic_loss": 0.15608721858263017, "actor_loss": -19.43966488838196, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.229027271270752, "step": 15000}
{"episode_reward": 96.8579506532162, "episode": 16.0, "batch_reward": 0.09807683150097728, "critic_loss": 0.17083010389655828, "actor_loss": -18.64208229970932, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.625324487686157, "step": 16000}
{"episode_reward": 52.50995770161064, "episode": 17.0, "batch_reward": 0.09356992267817259, "critic_loss": 0.15868622448667885, "actor_loss": -18.94832084560394, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.483632802963257, "step": 17000}
{"episode_reward": 22.842570770618764, "episode": 18.0, "batch_reward": 0.09372558767348528, "critic_loss": 0.18183541743084788, "actor_loss": -18.613630043029787, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.932806491851807, "step": 18000}
{"episode_reward": 102.96128408031046, "episode": 19.0, "batch_reward": 0.09194237338006496, "critic_loss": 0.21710481437295676, "actor_loss": -17.697721340179445, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.674065351486206, "step": 19000}
{"episode_reward": 57.72036230678596, "episode": 20.0, "batch_reward": 0.08918140056729316, "critic_loss": 0.25670193653553725, "actor_loss": -17.137072135925294, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.103636980056763, "step": 20000}
{"episode_reward": 46.092954090515505, "episode": 21.0, "batch_reward": 0.0867195590287447, "critic_loss": 0.27122171720117333, "actor_loss": -16.52485468673706, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.207828998565674, "step": 21000}
{"episode_reward": 22.492234146781982, "episode": 22.0, "batch_reward": 0.0839708121754229, "critic_loss": 0.3269273835942149, "actor_loss": -16.88541840362549, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.927786111831665, "step": 22000}
{"episode_reward": 32.42870171328766, "episode": 23.0, "batch_reward": 0.0817320763990283, "critic_loss": 0.2730747704654932, "actor_loss": -16.98848963737488, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.543651342391968, "step": 23000}
{"episode_reward": 25.43937533316644, "episode": 24.0, "batch_reward": 0.07896402179822326, "critic_loss": 0.2674581169784069, "actor_loss": -16.026791931152342, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.484739065170288, "step": 24000}
{"episode_reward": 20.51715055448792, "episode": 25.0, "batch_reward": 0.0765751760341227, "critic_loss": 0.2896785584613681, "actor_loss": -16.44354396247864, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.01749897003174, "step": 25000}
{"episode_reward": 21.914265678560884, "episode": 26.0, "batch_reward": 0.07658629406243563, "critic_loss": 0.3657852771878243, "actor_loss": -16.53463751888275, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.458096265792847, "step": 26000}
{"episode_reward": 75.54931404797637, "episode": 27.0, "batch_reward": 0.07596087037399411, "critic_loss": 0.3759513174071908, "actor_loss": -16.38311071872711, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.39087724685669, "step": 27000}
{"episode_reward": 65.13910415766722, "episode": 28.0, "batch_reward": 0.07842817643284798, "critic_loss": 0.3584103052467108, "actor_loss": -16.62990348625183, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.991849422454834, "step": 28000}
{"episode_reward": 236.94225274585932, "episode": 29.0, "batch_reward": 0.0844611871689558, "critic_loss": 0.3534665838927031, "actor_loss": -16.341130396842956, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.463173866271973, "step": 29000}
{"episode_reward": 278.76855426147284, "episode": 30.0, "batch_reward": 0.09048598364368081, "critic_loss": 0.34192592754215, "actor_loss": -17.09718507385254, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.93579864501953, "step": 30000}
{"episode_reward": 157.130921871209, "episode": 31.0, "batch_reward": 0.09018928095698357, "critic_loss": 0.38397212494164706, "actor_loss": -16.541650456428528, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.869701623916626, "step": 31000}
{"episode_reward": 54.17110395377265, "episode": 32.0, "batch_reward": 0.09037751154229044, "critic_loss": 0.3980900590419769, "actor_loss": -16.674750749588014, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.224748373031616, "step": 32000}
{"episode_reward": 133.94958890423374, "episode": 33.0, "batch_reward": 0.09098892148956657, "critic_loss": 0.39346705290675166, "actor_loss": -16.461801481246948, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.27792739868164, "step": 33000}
{"episode_reward": 72.80314392792344, "episode": 34.0, "batch_reward": 0.08993079591915011, "critic_loss": 0.38259141059219837, "actor_loss": -16.936218280792236, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.995317935943604, "step": 34000}
{"episode_reward": 56.1259296981723, "episode": 35.0, "batch_reward": 0.09255013062804937, "critic_loss": 0.4168324511423707, "actor_loss": -16.21863544559479, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.038058519363403, "step": 35000}
{"episode_reward": 250.76241042736947, "episode": 36.0, "batch_reward": 0.09237576757371425, "critic_loss": 0.36740559243410825, "actor_loss": -16.833255128860472, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.962048768997192, "step": 36000}
{"episode_reward": 38.09533723214805, "episode": 37.0, "batch_reward": 0.09502428295463324, "critic_loss": 0.41059742683172223, "actor_loss": -16.807661129951477, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.196642637252808, "step": 37000}
{"episode_reward": 352.22224472746456, "episode": 38.0, "batch_reward": 0.09860138542205095, "critic_loss": 0.4048925949335098, "actor_loss": -16.814459659576418, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.246700286865234, "step": 38000}
{"episode_reward": 29.333889677469568, "episode": 39.0, "batch_reward": 0.09759226670861244, "critic_loss": 0.4374179698079824, "actor_loss": -16.17994091796875, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.16695237159729, "step": 39000}
{"episode_reward": 89.53733253209188, "episode": 40.0, "batch_reward": 0.09681003730371594, "critic_loss": 0.3867911810055375, "actor_loss": -16.22085646915436, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.237963438034058, "step": 40000}
{"episode_reward": 56.3843091669253, "episode": 41.0, "batch_reward": 0.09510060022771359, "critic_loss": 0.43402024866640565, "actor_loss": -15.862403552055358, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.51978302001953, "step": 41000}
{"episode_reward": 24.525108135810207, "episode": 42.0, "batch_reward": 0.0948582027927041, "critic_loss": 0.4098932617306709, "actor_loss": -16.06246327972412, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.02021098136902, "step": 42000}
{"episode_reward": 133.386171405058, "episode": 43.0, "batch_reward": 0.09693758393079042, "critic_loss": 0.4474916852787137, "actor_loss": -16.0854447183609, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.447221279144287, "step": 43000}
{"episode_reward": 224.27224888534346, "episode": 44.0, "batch_reward": 0.09821028729900717, "critic_loss": 0.46371852843463424, "actor_loss": -16.454788274765015, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.252552270889282, "step": 44000}
{"episode_reward": 80.54719261975603, "episode": 45.0, "batch_reward": 0.10007301729544997, "critic_loss": 0.48698520374298093, "actor_loss": -15.964936666488647, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.265856981277466, "step": 45000}
{"episode_reward": 138.9006716762525, "episode": 46.0, "batch_reward": 0.09857464614883066, "critic_loss": 0.507311148956418, "actor_loss": -16.20417972946167, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.673254013061523, "step": 46000}
{"episode_reward": 76.74885259751763, "episode": 47.0, "batch_reward": 0.09873290836811066, "critic_loss": 0.535243156120181, "actor_loss": -15.958450657844544, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.88265824317932, "step": 47000}
{"episode_reward": 97.0981240333077, "episode": 48.0, "batch_reward": 0.09902017044275999, "critic_loss": 0.5216811960786581, "actor_loss": -16.130629512786864, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.342713356018066, "step": 48000}
{"episode_reward": 92.83334805252828, "episode": 49.0, "batch_reward": 0.09906368962302804, "critic_loss": 0.48689172354340554, "actor_loss": -15.548853641510009, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.537474632263184, "step": 49000}
{"episode_reward": 111.5698150291959, "episode": 50.0, "batch_reward": 0.09778460165858269, "critic_loss": 0.44988884484767916, "actor_loss": -15.498084818840027, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.62360906600952, "step": 50000}
{"episode_reward": 60.80876242769504, "episode": 51.0, "batch_reward": 0.09820062857493758, "critic_loss": 0.4754986175596714, "actor_loss": -15.584606937408447, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.355003356933594, "step": 51000}
{"episode_reward": 84.15994573540725, "episode": 52.0, "batch_reward": 0.09952517299354076, "critic_loss": 0.5562072905004024, "actor_loss": -15.848857559204102, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.26281499862671, "step": 52000}
{"episode_reward": 363.7170372156086, "episode": 53.0, "batch_reward": 0.10086705237254501, "critic_loss": 0.6838355811536312, "actor_loss": -17.138142890930176, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.08723545074463, "step": 53000}
{"episode_reward": 4.1374652217494985, "episode": 54.0, "batch_reward": 0.09912438073754311, "critic_loss": 0.7324668167233467, "actor_loss": -17.970839235305785, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.573898792266846, "step": 54000}
{"episode_reward": 4.859195825595448, "episode": 55.0, "batch_reward": 0.09800941747799516, "critic_loss": 0.6922313577830791, "actor_loss": -18.888533027648926, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.306124687194824, "step": 55000}
{"episode_reward": 3.457418490734047, "episode": 56.0, "batch_reward": 0.09578054367378354, "critic_loss": 0.6397755032777787, "actor_loss": -19.961200550079347, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.139277935028076, "step": 56000}
{"episode_reward": 3.169793060837656, "episode": 57.0, "batch_reward": 0.09394285149499775, "critic_loss": 0.5890962103456259, "actor_loss": -21.10134348678589, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.754494428634644, "step": 57000}
{"episode_reward": 3.6807980580417317, "episode": 58.0, "batch_reward": 0.09264267686754465, "critic_loss": 0.5803281735479832, "actor_loss": -22.18278667449951, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.502306699752808, "step": 58000}
{"episode_reward": 4.814063750880053, "episode": 59.0, "batch_reward": 0.09135723492130637, "critic_loss": 0.6599044882953167, "actor_loss": -24.173223426818847, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.224486112594604, "step": 59000}
{"episode_reward": 8.231326791515707, "episode": 60.0, "batch_reward": 0.08995684410259128, "critic_loss": 0.5725816923379898, "actor_loss": -25.717533210754393, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.42674207687378, "step": 60000}
{"episode_reward": 13.730180739498119, "episode": 61.0, "batch_reward": 0.08942511101067066, "critic_loss": 0.4828489032238722, "actor_loss": -27.012948436737062, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.54332375526428, "step": 61000}
{"episode_reward": 19.317473534331725, "episode": 62.0, "batch_reward": 0.0879067658111453, "critic_loss": 0.4150768668651581, "actor_loss": -28.02367831039429, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.11767029762268, "step": 62000}
{"episode_reward": 10.17231400746349, "episode": 63.0, "batch_reward": 0.0860337715782225, "critic_loss": 0.3219666147232056, "actor_loss": -28.094555618286133, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.82019543647766, "step": 63000}
{"episode_reward": 11.373293020765379, "episode": 64.0, "batch_reward": 0.08488139382749796, "critic_loss": 0.30155828266590834, "actor_loss": -28.08927751541138, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.64723539352417, "step": 64000}
{"episode_reward": 13.139254085997845, "episode": 65.0, "batch_reward": 0.08432692183926702, "critic_loss": 0.28793063456565143, "actor_loss": -27.97199773788452, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.520006895065308, "step": 65000}
{"episode_reward": 13.260965896151998, "episode": 66.0, "batch_reward": 0.08393699496611953, "critic_loss": 0.29078551968932154, "actor_loss": -27.82022958755493, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.007824659347534, "step": 66000}
{"episode_reward": 21.595646425510367, "episode": 67.0, "batch_reward": 0.08202583457529544, "critic_loss": 0.21248556419461967, "actor_loss": -27.46891397857666, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.054864168167114, "step": 67000}
{"episode_reward": 34.704687690881606, "episode": 68.0, "batch_reward": 0.08188742607086896, "critic_loss": 0.24007204703241586, "actor_loss": -26.97549353790283, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.441112756729126, "step": 68000}
{"episode_reward": 152.63090523800142, "episode": 69.0, "batch_reward": 0.08395870708301663, "critic_loss": 0.2187647066041827, "actor_loss": -26.506056217193603, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.306610822677612, "step": 69000}
{"episode_reward": 266.20355833380324, "episode": 70.0, "batch_reward": 0.08689049589261412, "critic_loss": 0.2110249014571309, "actor_loss": -26.31902053451538, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.758049249649048, "step": 70000}
{"episode_reward": 281.12714096156657, "episode": 71.0, "batch_reward": 0.0897061974182725, "critic_loss": 0.22550294633954762, "actor_loss": -26.109140933990478, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.69631862640381, "step": 71000}
{"episode_reward": 349.8216993313422, "episode": 72.0, "batch_reward": 0.09376955457776785, "critic_loss": 0.22527157746255397, "actor_loss": -25.93383222579956, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.978691577911377, "step": 72000}
{"episode_reward": 370.4100889262234, "episode": 73.0, "batch_reward": 0.09733323783427476, "critic_loss": 0.2564435629919171, "actor_loss": -25.88387714767456, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.918822288513184, "step": 73000}
{"episode_reward": 164.24252829178252, "episode": 74.0, "batch_reward": 0.09855143178999425, "critic_loss": 0.25470010308921337, "actor_loss": -25.771476303100584, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.05939245223999, "step": 74000}
{"episode_reward": 374.7233471614868, "episode": 75.0, "batch_reward": 0.10275960604846478, "critic_loss": 0.27855567810684445, "actor_loss": -25.790805370330812, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.166144371032715, "step": 75000}
{"episode_reward": 421.3945442165533, "episode": 76.0, "batch_reward": 0.1064506691917777, "critic_loss": 0.2631733056753874, "actor_loss": -25.70984822463989, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.095083475112915, "step": 76000}
{"episode_reward": 388.1356038804448, "episode": 77.0, "batch_reward": 0.1099308764114976, "critic_loss": 0.26556356076151133, "actor_loss": -25.82830954360962, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.76390552520752, "step": 77000}
{"episode_reward": 414.88818078330684, "episode": 78.0, "batch_reward": 0.11498760363459587, "critic_loss": 0.25587613071501253, "actor_loss": -25.70934090423584, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.923550605773926, "step": 78000}
{"episode_reward": 324.4681408627309, "episode": 79.0, "batch_reward": 0.1163695322573185, "critic_loss": 0.2549048142656684, "actor_loss": -25.67241284561157, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.153579711914062, "step": 79000}
{"episode_reward": 453.8786584083539, "episode": 80.0, "batch_reward": 0.12160995424538851, "critic_loss": 0.25191548059135677, "actor_loss": -25.57323138809204, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.825811862945557, "step": 80000}
{"episode_reward": 474.3696244004796, "episode": 81.0, "batch_reward": 0.12609292878955602, "critic_loss": 0.27084034617990255, "actor_loss": -25.634361877441407, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.581727266311646, "step": 81000}
{"episode_reward": 344.4583247056722, "episode": 82.0, "batch_reward": 0.1291734576895833, "critic_loss": 0.26655201154947283, "actor_loss": -25.574741024017335, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.385873079299927, "step": 82000}
{"episode_reward": 461.48723260694624, "episode": 83.0, "batch_reward": 0.13350129605084657, "critic_loss": 0.27084566026180984, "actor_loss": -25.887938835144045, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.977890253067017, "step": 83000}
{"episode_reward": 461.90431543479684, "episode": 84.0, "batch_reward": 0.13692786994576453, "critic_loss": 0.2584221987277269, "actor_loss": -25.660212226867674, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.851008415222168, "step": 84000}
{"episode_reward": 453.1195221179712, "episode": 85.0, "batch_reward": 0.14107766188681126, "critic_loss": 0.2767001011893153, "actor_loss": -25.81914741897583, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.391785860061646, "step": 85000}
{"episode_reward": 448.8514096310551, "episode": 86.0, "batch_reward": 0.14470584732294084, "critic_loss": 0.2665980417653918, "actor_loss": -26.114844810485838, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.878920793533325, "step": 86000}
{"episode_reward": 489.3145685822842, "episode": 87.0, "batch_reward": 0.14819298116862775, "critic_loss": 0.28684993883222343, "actor_loss": -26.299702095031737, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.21430993080139, "step": 87000}
{"episode_reward": 482.5714111706738, "episode": 88.0, "batch_reward": 0.15115946733951569, "critic_loss": 0.27655321061611177, "actor_loss": -26.624199794769286, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.24070644378662, "step": 88000}
{"episode_reward": 339.1015110242078, "episode": 89.0, "batch_reward": 0.1544987252280116, "critic_loss": 0.28479623037576673, "actor_loss": -26.332310214996337, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.52507972717285, "step": 89000}
{"episode_reward": 497.5075345034362, "episode": 90.0, "batch_reward": 0.15797474391758443, "critic_loss": 0.27929524207115175, "actor_loss": -26.4099079208374, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.471678733825684, "step": 90000}
{"episode_reward": 484.7515514670995, "episode": 91.0, "batch_reward": 0.16115240331739186, "critic_loss": 0.2741018674001098, "actor_loss": -26.862864078521728, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.75074601173401, "step": 91000}
{"episode_reward": 473.38386341762396, "episode": 92.0, "batch_reward": 0.16461933129280806, "critic_loss": 0.26347564036399124, "actor_loss": -27.11393972015381, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.25863766670227, "step": 92000}
{"episode_reward": 481.6008129601843, "episode": 93.0, "batch_reward": 0.16807164137810468, "critic_loss": 0.27952442881464956, "actor_loss": -27.29362577819824, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.136685371398926, "step": 93000}
{"episode_reward": 466.63400991960106, "episode": 94.0, "batch_reward": 0.17158223552256824, "critic_loss": 0.2893967884555459, "actor_loss": -27.418987529754638, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.985576391220093, "step": 94000}
{"episode_reward": 400.06949271699546, "episode": 95.0, "batch_reward": 0.17361347203701735, "critic_loss": 0.2888848520219326, "actor_loss": -27.148823791503908, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.442188024520874, "step": 95000}
{"episode_reward": 444.9036099875822, "episode": 96.0, "batch_reward": 0.17676214869320392, "critic_loss": 0.31183340863138437, "actor_loss": -27.34597537612915, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.10718870162964, "step": 96000}
{"episode_reward": 456.941463731802, "episode": 97.0, "batch_reward": 0.17905509163439273, "critic_loss": 0.27937165212631226, "actor_loss": -27.48432472229004, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.235517024993896, "step": 97000}
{"episode_reward": 465.72310389502667, "episode": 98.0, "batch_reward": 0.18202069991081954, "critic_loss": 0.30764287189394235, "actor_loss": -27.999156959533693, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.2512104511261, "step": 98000}
{"episode_reward": 482.35975405483566, "episode": 99.0, "batch_reward": 0.18651631735265256, "critic_loss": 0.2925569605007768, "actor_loss": -28.242457111358643, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.171876668930054, "step": 99000}
{"episode_reward": 496.43744998896784, "episode": 100.0, "batch_reward": 0.18898256896436214, "critic_loss": 0.30818852157890797, "actor_loss": -28.22553249359131, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.992761850357056, "step": 100000}
{"episode_reward": 460.14245349123047, "episode": 101.0, "batch_reward": 0.1915010947585106, "critic_loss": 0.285657313361764, "actor_loss": -28.143909423828124, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.235737323760986, "step": 101000}
{"episode_reward": 494.78844124248945, "episode": 102.0, "batch_reward": 0.19357034879922866, "critic_loss": 0.2742292676642537, "actor_loss": -28.42666396713257, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.740453958511353, "step": 102000}
{"episode_reward": 514.7614153083672, "episode": 103.0, "batch_reward": 0.19718045864999295, "critic_loss": 0.2932243233695626, "actor_loss": -28.60844631958008, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.105584859848022, "step": 103000}
{"episode_reward": 466.64025929557, "episode": 104.0, "batch_reward": 0.20057306450605392, "critic_loss": 0.2809259743690491, "actor_loss": -28.814339092254638, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.741886854171753, "step": 104000}
{"episode_reward": 468.2337965574314, "episode": 105.0, "batch_reward": 0.2022842103987932, "critic_loss": 0.28161171624064446, "actor_loss": -28.79480584335327, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.107309103012085, "step": 105000}
{"episode_reward": 481.8512490320926, "episode": 106.0, "batch_reward": 0.20513408836722374, "critic_loss": 0.30064757911115886, "actor_loss": -29.20454242324829, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.598270893096924, "step": 106000}
{"episode_reward": 486.1374591274856, "episode": 107.0, "batch_reward": 0.20887861941754818, "critic_loss": 0.27578259494155644, "actor_loss": -29.340026592254638, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.677810668945312, "step": 107000}
{"episode_reward": 502.09232826938097, "episode": 108.0, "batch_reward": 0.21050287148356436, "critic_loss": 0.29182405123114585, "actor_loss": -29.447238033294678, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.698737621307373, "step": 108000}
{"episode_reward": 498.1822190381698, "episode": 109.0, "batch_reward": 0.21373487655818463, "critic_loss": 0.2832344264239073, "actor_loss": -29.444001594543458, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.252820014953613, "step": 109000}
{"episode_reward": 501.4345532813538, "episode": 110.0, "batch_reward": 0.21606561924517154, "critic_loss": 0.2832434141635895, "actor_loss": -29.82609635925293, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.78120756149292, "step": 110000}
{"episode_reward": 504.85241430426896, "episode": 111.0, "batch_reward": 0.2182947277724743, "critic_loss": 0.27369013634324074, "actor_loss": -29.70252942276001, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.649821758270264, "step": 111000}
{"episode_reward": 524.5527055200237, "episode": 112.0, "batch_reward": 0.22115957225859165, "critic_loss": 0.2561006373986602, "actor_loss": -30.1766082611084, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.987682819366455, "step": 112000}
{"episode_reward": 467.37896661764177, "episode": 113.0, "batch_reward": 0.22396583299338818, "critic_loss": 0.2920417619794607, "actor_loss": -30.281046112060547, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.32112431526184, "step": 113000}
{"episode_reward": 480.6463968936529, "episode": 114.0, "batch_reward": 0.22617935641109943, "critic_loss": 0.2578914147168398, "actor_loss": -30.18774641418457, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.032415628433228, "step": 114000}
{"episode_reward": 512.351522575773, "episode": 115.0, "batch_reward": 0.2282011311352253, "critic_loss": 0.284143852494657, "actor_loss": -30.567937644958498, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.308228969573975, "step": 115000}
{"episode_reward": 489.88414118079305, "episode": 116.0, "batch_reward": 0.23200714184343815, "critic_loss": 0.26561272341012954, "actor_loss": -30.696580001831055, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.00477647781372, "step": 116000}
{"episode_reward": 474.79154032885447, "episode": 117.0, "batch_reward": 0.23393846303224564, "critic_loss": 0.27341733649373057, "actor_loss": -30.92438591003418, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.59048581123352, "step": 117000}
{"episode_reward": 514.207612054985, "episode": 118.0, "batch_reward": 0.23547692389786243, "critic_loss": 0.2708323794677854, "actor_loss": -30.908259147644042, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.386478662490845, "step": 118000}
{"episode_reward": 509.39478737262203, "episode": 119.0, "batch_reward": 0.23814347256720067, "critic_loss": 0.26959965463727714, "actor_loss": -31.09631061553955, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.757257223129272, "step": 119000}
{"episode_reward": 485.5899025320704, "episode": 120.0, "batch_reward": 0.23942054677009583, "critic_loss": 0.2531068528071046, "actor_loss": -31.52137017440796, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.316629886627197, "step": 120000}
{"episode_reward": 522.7204423865024, "episode": 121.0, "batch_reward": 0.24198562946915628, "critic_loss": 0.25873297210782764, "actor_loss": -31.489480060577392, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.154114723205566, "step": 121000}
{"episode_reward": 473.70761883773196, "episode": 122.0, "batch_reward": 0.24436570209264755, "critic_loss": 0.25324976636469365, "actor_loss": -31.826645694732665, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.975187063217163, "step": 122000}
{"episode_reward": 497.0862396418825, "episode": 123.0, "batch_reward": 0.24606661762297152, "critic_loss": 0.25837665478885175, "actor_loss": -32.29680307006836, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.176550149917603, "step": 123000}
{"episode_reward": 504.0452784614279, "episode": 124.0, "batch_reward": 0.24808686077594758, "critic_loss": 0.2571736104786396, "actor_loss": -32.12032983398438, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.12462615966797, "step": 124000}
{"episode_reward": 517.4763720585202, "episode": 125.0, "batch_reward": 0.24976994253695012, "critic_loss": 0.27202487041056156, "actor_loss": -32.00283964157104, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.548181772232056, "step": 125000}
{"episode_reward": 322.3884702081096, "episode": 126.0, "batch_reward": 0.25017363856732844, "critic_loss": 0.2747106656879187, "actor_loss": -32.090353885650636, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.933555603027344, "step": 126000}
{"episode_reward": 512.1603149356745, "episode": 127.0, "batch_reward": 0.2530954833328724, "critic_loss": 0.2622190278768539, "actor_loss": -32.376300571441654, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.929442167282104, "step": 127000}
{"episode_reward": 520.4273556102165, "episode": 128.0, "batch_reward": 0.25503041687607764, "critic_loss": 0.2673275879472494, "actor_loss": -32.12217890930176, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.408036947250366, "step": 128000}
{"episode_reward": 515.1086263046942, "episode": 129.0, "batch_reward": 0.25680889236927035, "critic_loss": 0.2699464382752776, "actor_loss": -32.32535913467407, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.14997935295105, "step": 129000}
{"episode_reward": 495.3545776660078, "episode": 130.0, "batch_reward": 0.2593008302003145, "critic_loss": 0.2530915629714727, "actor_loss": -32.75424811553955, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.391874313354492, "step": 130000}
{"episode_reward": 520.3710857755532, "episode": 131.0, "batch_reward": 0.2614773478358984, "critic_loss": 0.28417263498902323, "actor_loss": -32.58506653594971, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.71997594833374, "step": 131000}
{"episode_reward": 519.560202191836, "episode": 132.0, "batch_reward": 0.2633351303488016, "critic_loss": 0.259320658929646, "actor_loss": -33.11645760345459, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.370857000350952, "step": 132000}
{"episode_reward": 502.12405570778594, "episode": 133.0, "batch_reward": 0.26323840735852716, "critic_loss": 0.2812248220294714, "actor_loss": -33.13010615539551, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.14244246482849, "step": 133000}
{"episode_reward": 102.39967322882269, "episode": 134.0, "batch_reward": 0.26278011229634285, "critic_loss": 0.2720866101756692, "actor_loss": -33.156940074920655, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.324049472808838, "step": 134000}
{"episode_reward": 515.8119015345663, "episode": 135.0, "batch_reward": 0.26552361498773097, "critic_loss": 0.27303412345796824, "actor_loss": -33.29198239898682, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.600575923919678, "step": 135000}
{"episode_reward": 535.6079012986039, "episode": 136.0, "batch_reward": 0.26722136870026586, "critic_loss": 0.270949684612453, "actor_loss": -33.14669694519043, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.499417066574097, "step": 136000}
{"episode_reward": 545.9294870398891, "episode": 137.0, "batch_reward": 0.26895774391293525, "critic_loss": 0.2615953765884042, "actor_loss": -33.82551202392578, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.484649896621704, "step": 137000}
{"episode_reward": 490.7848238281258, "episode": 138.0, "batch_reward": 0.2710790890455246, "critic_loss": 0.2675636436864734, "actor_loss": -34.20857570266724, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.21135425567627, "step": 138000}
{"episode_reward": 528.2606981921197, "episode": 139.0, "batch_reward": 0.2735032194852829, "critic_loss": 0.2719766340404749, "actor_loss": -34.31511823272705, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.827978372573853, "step": 139000}
{"episode_reward": 547.5196631680687, "episode": 140.0, "batch_reward": 0.2735209189504385, "critic_loss": 0.2407421832755208, "actor_loss": -34.347873783111574, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.825093507766724, "step": 140000}
{"episode_reward": 525.8297523610914, "episode": 141.0, "batch_reward": 0.27587207695841787, "critic_loss": 0.2459832140058279, "actor_loss": -34.668018707275394, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.51907563209534, "step": 141000}
{"episode_reward": 528.9532416225794, "episode": 142.0, "batch_reward": 0.2777299003303051, "critic_loss": 0.2617195782139897, "actor_loss": -34.50796004104614, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.344130754470825, "step": 142000}
{"episode_reward": 527.6138318547991, "episode": 143.0, "batch_reward": 0.2801013373583555, "critic_loss": 0.25624388724565506, "actor_loss": -34.66812392044067, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.35406255722046, "step": 143000}
{"episode_reward": 529.5516780943997, "episode": 144.0, "batch_reward": 0.28152037659287454, "critic_loss": 0.2554905522838235, "actor_loss": -34.75441351318359, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.471564531326294, "step": 144000}
{"episode_reward": 209.187766432102, "episode": 145.0, "batch_reward": 0.2820415828675032, "critic_loss": 0.2669725620225072, "actor_loss": -34.88256528091431, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.004401922225952, "step": 145000}
{"episode_reward": 522.3760439592778, "episode": 146.0, "batch_reward": 0.28161466255784035, "critic_loss": 0.2793065013512969, "actor_loss": -34.94733696746826, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.659924030303955, "step": 146000}
{"episode_reward": 153.11961980960268, "episode": 147.0, "batch_reward": 0.2814647079110146, "critic_loss": 0.2613560780510306, "actor_loss": -34.650545413970946, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.94562530517578, "step": 147000}
{"episode_reward": 504.0439727157566, "episode": 148.0, "batch_reward": 0.28411373491585257, "critic_loss": 0.26839346809685233, "actor_loss": -35.16314030075073, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.44895029067993, "step": 148000}
{"episode_reward": 520.5142071165718, "episode": 149.0, "batch_reward": 0.28595425583422185, "critic_loss": 0.2993272981271148, "actor_loss": -35.13429196929932, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.8162522315979, "step": 149000}
{"episode_reward": 514.2073918226897, "episode": 150.0, "batch_reward": 0.2875200241953135, "critic_loss": 0.2584598096758127, "actor_loss": -35.25600201034546, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "step": 150000}
