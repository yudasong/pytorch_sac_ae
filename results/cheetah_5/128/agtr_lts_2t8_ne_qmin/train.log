{"episode_reward": 0.0, "episode": 1.0, "duration": 15.075302600860596, "step": 1000}
{"episode_reward": 5.218604035389536, "episode": 2.0, "duration": 1.3627967834472656, "step": 2000}
{"episode_reward": 548.2612263124905, "episode": 3.0, "batch_reward": 0.2869265190201149, "critic_loss": 0.21945081694935434, "actor_loss": -45.6849916907681, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 73.18886852264404, "step": 3000}
{"episode_reward": 526.1919623633046, "episode": 4.0, "batch_reward": 0.3703639057278633, "critic_loss": 0.2809166822135448, "actor_loss": -50.33761416625977, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.009182453155518, "step": 4000}
{"episode_reward": 524.2179816483525, "episode": 5.0, "batch_reward": 0.3878157133460045, "critic_loss": 0.33750130073726176, "actor_loss": -50.04965598297119, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.03187131881714, "step": 5000}
{"episode_reward": 190.36430400516082, "episode": 6.0, "batch_reward": 0.36785359153151514, "critic_loss": 0.37793643327057364, "actor_loss": -46.673882301330565, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.097745418548584, "step": 6000}
{"episode_reward": 429.13398293376554, "episode": 7.0, "batch_reward": 0.37933798417449, "critic_loss": 0.5480225497931241, "actor_loss": -46.18803239440918, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.947035312652588, "step": 7000}
{"episode_reward": 505.5406928625334, "episode": 8.0, "batch_reward": 0.3912364670932293, "critic_loss": 0.5352754416912794, "actor_loss": -46.28311247253418, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.728610277175903, "step": 8000}
{"episode_reward": 489.8503494585449, "episode": 9.0, "batch_reward": 0.4019297459125519, "critic_loss": 0.559480427235365, "actor_loss": -47.54268214416504, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.629656553268433, "step": 9000}
{"episode_reward": 454.97577539510416, "episode": 10.0, "batch_reward": 0.4109762037396431, "critic_loss": 0.5930121484696865, "actor_loss": -48.34853909301758, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.343383073806763, "step": 10000}
{"episode_reward": 552.9490112006266, "episode": 11.0, "batch_reward": 0.42551415029168127, "critic_loss": 0.9088259566426277, "actor_loss": -49.751097412109374, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.75841212272644, "step": 11000}
{"episode_reward": 577.9177354779221, "episode": 12.0, "batch_reward": 0.4405047034919262, "critic_loss": 1.222667970776558, "actor_loss": -51.15429736328125, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.91635036468506, "step": 12000}
{"episode_reward": 604.719566577605, "episode": 13.0, "batch_reward": 0.4510307395458221, "critic_loss": 2.607566514611244, "actor_loss": -52.68165328216553, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.198466539382935, "step": 13000}
{"episode_reward": 523.6591621129735, "episode": 14.0, "batch_reward": 0.4408582719564438, "critic_loss": 3.526795017242432, "actor_loss": -54.232095275878905, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.278244018554688, "step": 14000}
{"episode_reward": 3.6800976657036513, "episode": 15.0, "batch_reward": 0.40962196433544157, "critic_loss": 3.832142684221268, "actor_loss": -53.898587760925295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.264878749847412, "step": 15000}
{"episode_reward": 0.9501001046708879, "episode": 16.0, "batch_reward": 0.3831878454089165, "critic_loss": 8.49149712753296, "actor_loss": -55.85594289398193, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.856581926345825, "step": 16000}
{"episode_reward": 24.685502048175813, "episode": 17.0, "batch_reward": 0.3616800961792469, "critic_loss": 8.326986348390578, "actor_loss": -58.70585705566406, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.16231417655945, "step": 17000}
{"episode_reward": 37.51493745421553, "episode": 18.0, "batch_reward": 0.343393163472414, "critic_loss": 7.824274900913238, "actor_loss": -59.71306452178955, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.182988166809082, "step": 18000}
{"episode_reward": 18.495591087936635, "episode": 19.0, "batch_reward": 0.32614589008688927, "critic_loss": 7.751150161743164, "actor_loss": -59.47056761932373, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.962052822113037, "step": 19000}
{"episode_reward": 9.029914152165553, "episode": 20.0, "batch_reward": 0.3094809948951006, "critic_loss": 8.408883023262025, "actor_loss": -61.48473446655274, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.85083532333374, "step": 20000}
{"episode_reward": 0.8125115563483176, "episode": 21.0, "batch_reward": 0.2928495508879423, "critic_loss": 9.877957822799683, "actor_loss": -63.05643224334717, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 36.77455401420593, "step": 21000}
{"episode_reward": 4.448115608979626, "episode": 22.0, "batch_reward": 0.28086888301372526, "critic_loss": 13.46850184583664, "actor_loss": -68.37861867523193, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.13426423072815, "step": 22000}
{"episode_reward": 2.5980002905180903, "episode": 23.0, "batch_reward": 0.26854772968590257, "critic_loss": 12.833482971668243, "actor_loss": -69.81958228302003, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.788992404937744, "step": 23000}
{"episode_reward": 5.64191639972229, "episode": 24.0, "batch_reward": 0.2579809227734804, "critic_loss": 12.603253388404847, "actor_loss": -71.16453927612305, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.574393033981323, "step": 24000}
{"episode_reward": 3.1628253775134376, "episode": 25.0, "batch_reward": 0.24678210170567036, "critic_loss": 13.269411199569703, "actor_loss": -79.16669609069824, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.3028347492218, "step": 25000}
{"episode_reward": 1.8650236041566184, "episode": 26.0, "batch_reward": 0.23665102821588516, "critic_loss": 13.850688364505768, "actor_loss": -80.19709783172607, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.016953229904175, "step": 26000}
{"episode_reward": 2.9135451845854226, "episode": 27.0, "batch_reward": 0.22882723166048527, "critic_loss": 14.615726016044617, "actor_loss": -86.40568635559082, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.793980598449707, "step": 27000}
{"episode_reward": 6.455916270515642, "episode": 28.0, "batch_reward": 0.21931665906310083, "critic_loss": 14.158817885398864, "actor_loss": -92.76285958862304, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.86493754386902, "step": 28000}
{"episode_reward": 4.857386745471018, "episode": 29.0, "batch_reward": 0.2134254036396742, "critic_loss": 12.988668971538544, "actor_loss": -95.2604439163208, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.994014501571655, "step": 29000}
{"episode_reward": 1.7751156695685801, "episode": 30.0, "batch_reward": 0.20486625047028065, "critic_loss": 12.548503779888152, "actor_loss": -94.43812285614014, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.145644903182983, "step": 30000}
{"episode_reward": 1.7792719741601335, "episode": 31.0, "batch_reward": 0.19902818003296852, "critic_loss": 11.815840130329132, "actor_loss": -101.99895832824707, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.20618939399719, "step": 31000}
{"episode_reward": 2.7255482254285113, "episode": 32.0, "batch_reward": 0.19444097016751766, "critic_loss": 12.184818554878236, "actor_loss": -101.77593437957763, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.78804111480713, "step": 32000}
{"episode_reward": 62.92792073217146, "episode": 33.0, "batch_reward": 0.1891264826208353, "critic_loss": 12.4231975274086, "actor_loss": -105.22648085784913, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.617927312850952, "step": 33000}
{"episode_reward": 49.97063080468103, "episode": 34.0, "batch_reward": 0.18415343881398438, "critic_loss": 11.786854836940766, "actor_loss": -106.27072692871094, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.81358051300049, "step": 34000}
{"episode_reward": 67.32966695704653, "episode": 35.0, "batch_reward": 0.183121621966362, "critic_loss": 10.428743334770203, "actor_loss": -99.15384323120117, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.785632610321045, "step": 35000}
