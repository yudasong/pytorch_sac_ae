{"episode_reward": 0.0, "episode": 1.0, "duration": 17.782610654830933, "step": 1000}
{"episode_reward": 7.579367547095394, "episode": 2.0, "duration": 1.5271306037902832, "step": 2000}
{"episode_reward": 587.3438790414032, "episode": 3.0, "batch_reward": 0.28054938704600213, "critic_loss": 0.03777106439697247, "actor_loss": -29.753076054087625, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 66.91743922233582, "step": 3000}
{"episode_reward": 16.297290735641578, "episode": 4.0, "batch_reward": 0.18576335585862397, "critic_loss": 0.05556338644586503, "actor_loss": -18.479509020373225, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.7733952999115, "step": 4000}
{"episode_reward": 47.64743346062551, "episode": 5.0, "batch_reward": 0.1542077631354332, "critic_loss": 0.05477468984201551, "actor_loss": -13.92835496725142, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.30730652809143, "step": 5000}
{"episode_reward": 83.10209082700827, "episode": 6.0, "batch_reward": 0.13484113615006207, "critic_loss": 0.044206361198797825, "actor_loss": -15.504948905810714, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.33092474937439, "step": 6000}
{"episode_reward": 7.803746083123037, "episode": 7.0, "batch_reward": 0.11543202811479568, "critic_loss": 0.037569893596693875, "actor_loss": -18.241411112636328, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.981656312942505, "step": 7000}
{"episode_reward": 9.369239805295084, "episode": 8.0, "batch_reward": 0.10233774651959539, "critic_loss": 0.03598257388360798, "actor_loss": -17.68458347001672, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.116002321243286, "step": 8000}
{"episode_reward": 7.4915050004105845, "episode": 9.0, "batch_reward": 0.09039490485563874, "critic_loss": 0.04215167544409633, "actor_loss": -17.297519377470017, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.936097860336304, "step": 9000}
{"episode_reward": 8.42814951962853, "episode": 10.0, "batch_reward": 0.08221395406126976, "critic_loss": 0.034530050725676116, "actor_loss": -18.144819761544465, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.027362823486328, "step": 10000}
{"episode_reward": 4.000052390814286, "episode": 11.0, "batch_reward": 0.07472748748958111, "critic_loss": 0.039161393522284926, "actor_loss": -19.00694843161106, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.79357647895813, "step": 11000}
{"episode_reward": 6.546210895148659, "episode": 12.0, "batch_reward": 0.0689651651531458, "critic_loss": 0.03688908510375768, "actor_loss": -17.57254179418087, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.26803970336914, "step": 12000}
{"episode_reward": 4.862623218302551, "episode": 13.0, "batch_reward": 0.0637056997846812, "critic_loss": 0.033274366976693275, "actor_loss": -18.144472380042075, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.68701720237732, "step": 13000}
{"episode_reward": 17.5959825710738, "episode": 14.0, "batch_reward": 0.06031875165365636, "critic_loss": 0.04549259628076106, "actor_loss": -16.35106299483776, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.93286943435669, "step": 14000}
{"episode_reward": 22.553223672405906, "episode": 15.0, "batch_reward": 0.058012210059911015, "critic_loss": 0.037195313821546734, "actor_loss": -19.538199097394944, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.286518812179565, "step": 15000}
{"episode_reward": 17.6601766747683, "episode": 16.0, "batch_reward": 0.05621872645616531, "critic_loss": 0.04528143572807312, "actor_loss": -19.31906395864487, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.08163595199585, "step": 16000}
{"episode_reward": 57.06724593841394, "episode": 17.0, "batch_reward": 0.057619122080504896, "critic_loss": 0.07156618858501315, "actor_loss": -18.00735173356533, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.35314917564392, "step": 17000}
{"episode_reward": 76.59164101795709, "episode": 18.0, "batch_reward": 0.05885120348259806, "critic_loss": 0.10213042121380568, "actor_loss": -17.935432364284992, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.1604220867157, "step": 18000}
{"episode_reward": 117.37296853101114, "episode": 19.0, "batch_reward": 0.06139391251653433, "critic_loss": 0.15355169425159693, "actor_loss": -17.14810912922025, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.69564652442932, "step": 19000}
{"episode_reward": 48.25251951203775, "episode": 20.0, "batch_reward": 0.0617760633379221, "critic_loss": 0.18966567867994308, "actor_loss": -18.558713561564684, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.633012294769287, "step": 20000}
{"episode_reward": 85.04188867700825, "episode": 21.0, "batch_reward": 0.06377398800477385, "critic_loss": 0.1627360685877502, "actor_loss": -18.45049171566963, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.69893717765808, "step": 21000}
{"episode_reward": 116.86088017650235, "episode": 22.0, "batch_reward": 0.06576771145313978, "critic_loss": 0.19215693505108355, "actor_loss": -17.93070920062065, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.568061351776123, "step": 22000}
{"episode_reward": 125.91143427757758, "episode": 23.0, "batch_reward": 0.07010976240038871, "critic_loss": 0.24035306821018457, "actor_loss": -18.10345120704174, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.992908000946045, "step": 23000}
{"episode_reward": 162.35067447526313, "episode": 24.0, "batch_reward": 0.07252717459946871, "critic_loss": 0.25048317646980284, "actor_loss": -19.54853811046481, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.25767183303833, "step": 24000}
{"episode_reward": 158.54417937646213, "episode": 25.0, "batch_reward": 0.0765441214479506, "critic_loss": 0.2963391290679574, "actor_loss": -19.228000213727356, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.76441717147827, "step": 25000}
{"episode_reward": 195.57000070207962, "episode": 26.0, "batch_reward": 0.08180555599182844, "critic_loss": 0.30921011975407603, "actor_loss": -19.702065787792204, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.79611849784851, "step": 26000}
{"episode_reward": 212.36683894762328, "episode": 27.0, "batch_reward": 0.08652781000733376, "critic_loss": 0.3321581847965717, "actor_loss": -21.177764439582823, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.959031343460083, "step": 27000}
{"episode_reward": 144.53325158964512, "episode": 28.0, "batch_reward": 0.08919906044006348, "critic_loss": 0.3015641153007746, "actor_loss": -20.597345163345338, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.005167484283447, "step": 28000}
{"episode_reward": 200.98200000643925, "episode": 29.0, "batch_reward": 0.09361189656704665, "critic_loss": 0.3234617082476616, "actor_loss": -21.768121321201324, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.803683042526245, "step": 29000}
{"episode_reward": 174.34659826090098, "episode": 30.0, "batch_reward": 0.09401940895617009, "critic_loss": 0.31757272623479366, "actor_loss": -21.281110137939454, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.538224458694458, "step": 30000}
{"episode_reward": 70.45760452660734, "episode": 31.0, "batch_reward": 0.0939284818470478, "critic_loss": 0.26549209594726564, "actor_loss": -21.215830542564394, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.36365532875061, "step": 31000}
{"episode_reward": 121.36292508167215, "episode": 32.0, "batch_reward": 0.09485610948503018, "critic_loss": 0.2706271257698536, "actor_loss": -21.60957832288742, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.229358196258545, "step": 32000}
{"episode_reward": 96.77195928526046, "episode": 33.0, "batch_reward": 0.0959883274808526, "critic_loss": 0.2902217787876725, "actor_loss": -21.675094898700713, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.983476400375366, "step": 33000}
{"episode_reward": 190.06866804862602, "episode": 34.0, "batch_reward": 0.09805636405944824, "critic_loss": 0.30606604155898093, "actor_loss": -20.27330912542343, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.238360166549683, "step": 34000}
{"episode_reward": 165.50369735945645, "episode": 35.0, "batch_reward": 0.10108439745754004, "critic_loss": 0.30153652898967265, "actor_loss": -22.022646034240722, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.014206886291504, "step": 35000}
{"episode_reward": 212.3747741066607, "episode": 36.0, "batch_reward": 0.1023034192994237, "critic_loss": 0.3059610627144575, "actor_loss": -20.573231363296507, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.753440618515015, "step": 36000}
{"episode_reward": 85.63270446721397, "episode": 37.0, "batch_reward": 0.10245447328686715, "critic_loss": 0.29965938787162305, "actor_loss": -20.892937188148498, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.0477614402771, "step": 37000}
{"episode_reward": 100.09653129218708, "episode": 38.0, "batch_reward": 0.10231610018759967, "critic_loss": 0.28637335294485095, "actor_loss": -20.818837984085082, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.005823850631714, "step": 38000}
{"episode_reward": 96.71112773914298, "episode": 39.0, "batch_reward": 0.10340513260662555, "critic_loss": 0.30688535670936107, "actor_loss": -21.48172924041748, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.442802906036377, "step": 39000}
{"episode_reward": 226.7520477030245, "episode": 40.0, "batch_reward": 0.10535054493695498, "critic_loss": 0.32498541663587094, "actor_loss": -21.35949816417694, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.11978554725647, "step": 40000}
{"episode_reward": 131.2847028465688, "episode": 41.0, "batch_reward": 0.10655916606634855, "critic_loss": 0.3050841483697295, "actor_loss": -21.778391925811768, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.884751081466675, "step": 41000}
{"episode_reward": 165.2571859850078, "episode": 42.0, "batch_reward": 0.1072804621309042, "critic_loss": 0.30357129825651646, "actor_loss": -21.259259550094605, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.495436429977417, "step": 42000}
{"episode_reward": 102.28770093788307, "episode": 43.0, "batch_reward": 0.10738528484106064, "critic_loss": 0.29182578510046003, "actor_loss": -21.315848813056945, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.86775803565979, "step": 43000}
{"episode_reward": 85.88371395417896, "episode": 44.0, "batch_reward": 0.10617274737358093, "critic_loss": 0.30471592453122137, "actor_loss": -20.740354806900026, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.65251898765564, "step": 44000}
{"episode_reward": 79.97552198159026, "episode": 45.0, "batch_reward": 0.10772865504771471, "critic_loss": 0.3319338014572859, "actor_loss": -22.04034183883667, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.56348180770874, "step": 45000}
{"episode_reward": 297.885488334534, "episode": 46.0, "batch_reward": 0.10970476031303406, "critic_loss": 0.3565617741346359, "actor_loss": -21.638981505393982, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.66833519935608, "step": 46000}
{"episode_reward": 71.53546992543816, "episode": 47.0, "batch_reward": 0.11161022202670574, "critic_loss": 0.37302771136164664, "actor_loss": -21.637384143829344, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.977628469467163, "step": 47000}
{"episode_reward": 228.20343813684127, "episode": 48.0, "batch_reward": 0.11238646807521582, "critic_loss": 0.3526072999686003, "actor_loss": -21.245281295776365, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.477097988128662, "step": 48000}
{"episode_reward": 215.67682882197326, "episode": 49.0, "batch_reward": 0.11576477520912885, "critic_loss": 0.38839937026798727, "actor_loss": -22.516533984184264, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.178941011428833, "step": 49000}
{"episode_reward": 280.5492730662761, "episode": 50.0, "batch_reward": 0.11754584401845931, "critic_loss": 0.40298853972554205, "actor_loss": -22.520323937416077, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.432769536972046, "step": 50000}
{"episode_reward": 160.40807972467118, "episode": 51.0, "batch_reward": 0.11953693301975728, "critic_loss": 0.4035781530588865, "actor_loss": -22.09399439048767, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.796406507492065, "step": 51000}
{"episode_reward": 215.4596340711244, "episode": 52.0, "batch_reward": 0.1199370708167553, "critic_loss": 0.47265863144397735, "actor_loss": -22.99431053066254, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.105369806289673, "step": 52000}
{"episode_reward": 74.24722297759001, "episode": 53.0, "batch_reward": 0.12122395483404398, "critic_loss": 0.4435706568956375, "actor_loss": -22.441800518989563, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.24538516998291, "step": 53000}
{"episode_reward": 326.1493794790966, "episode": 54.0, "batch_reward": 0.1245207853987813, "critic_loss": 0.5518189414590597, "actor_loss": -23.101034717559813, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.387505531311035, "step": 54000}
{"episode_reward": 258.3070784604879, "episode": 55.0, "batch_reward": 0.12579088047146797, "critic_loss": 0.49450238031148913, "actor_loss": -22.51409246635437, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.500027656555176, "step": 55000}
{"episode_reward": 129.29379882106244, "episode": 56.0, "batch_reward": 0.12675720343738794, "critic_loss": 0.49827612437307833, "actor_loss": -23.411586079597473, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.896788597106934, "step": 56000}
{"episode_reward": 178.72446547230987, "episode": 57.0, "batch_reward": 0.12644985773414374, "critic_loss": 0.5036000097692013, "actor_loss": -22.656078349113464, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.714489459991455, "step": 57000}
{"episode_reward": 56.901230088603334, "episode": 58.0, "batch_reward": 0.1264282678887248, "critic_loss": 0.5195554564446211, "actor_loss": -22.151719564437865, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.009026050567627, "step": 58000}
{"episode_reward": 77.08092994742009, "episode": 59.0, "batch_reward": 0.12418861547112466, "critic_loss": 0.5598539564758539, "actor_loss": -21.738092288017274, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.107558488845825, "step": 59000}
{"episode_reward": 56.537366709788806, "episode": 60.0, "batch_reward": 0.1244584671407938, "critic_loss": 0.511272132769227, "actor_loss": -22.568128534317015, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.425454139709473, "step": 60000}
{"episode_reward": 176.87540994459604, "episode": 61.0, "batch_reward": 0.12437941242754459, "critic_loss": 0.4903598529994488, "actor_loss": -22.310264850616456, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.19273781776428, "step": 61000}
{"episode_reward": 65.1986782367834, "episode": 62.0, "batch_reward": 0.12441512040048838, "critic_loss": 0.470921103939414, "actor_loss": -22.022024251937868, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.192688703536987, "step": 62000}
{"episode_reward": 323.3098778735283, "episode": 63.0, "batch_reward": 0.12801352204382418, "critic_loss": 0.5442108033746481, "actor_loss": -22.347399927139282, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.628710746765137, "step": 63000}
{"episode_reward": 381.2096816362241, "episode": 64.0, "batch_reward": 0.13173765125870704, "critic_loss": 0.594628309622407, "actor_loss": -22.937588914871217, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.02955675125122, "step": 64000}
{"episode_reward": 227.15306581124332, "episode": 65.0, "batch_reward": 0.1332190084978938, "critic_loss": 0.5702379368841648, "actor_loss": -22.86100140953064, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.091991186141968, "step": 65000}
{"episode_reward": 217.30459280906527, "episode": 66.0, "batch_reward": 0.13455068150162697, "critic_loss": 0.5515614027380943, "actor_loss": -23.223999212265014, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.11610507965088, "step": 66000}
{"episode_reward": 111.69536534979615, "episode": 67.0, "batch_reward": 0.13264071599394084, "critic_loss": 0.5517757948487997, "actor_loss": -23.4294056186676, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.43980574607849, "step": 67000}
{"episode_reward": 59.82922064798771, "episode": 68.0, "batch_reward": 0.13278023567795755, "critic_loss": 0.5653340813219547, "actor_loss": -22.441113306045533, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.76908588409424, "step": 68000}
{"episode_reward": 324.8532433194122, "episode": 69.0, "batch_reward": 0.13565168980509043, "critic_loss": 0.5513251605480909, "actor_loss": -23.075492361068726, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.738654851913452, "step": 69000}
{"episode_reward": 195.93062483889952, "episode": 70.0, "batch_reward": 0.13542562384158374, "critic_loss": 0.50358104544878, "actor_loss": -23.26516952896118, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.96658444404602, "step": 70000}
{"episode_reward": 79.43161046311484, "episode": 71.0, "batch_reward": 0.13695403204113246, "critic_loss": 0.49234359653294085, "actor_loss": -22.77417865753174, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.701637268066406, "step": 71000}
{"episode_reward": 388.4765797753557, "episode": 72.0, "batch_reward": 0.1390217764824629, "critic_loss": 0.5214611375331879, "actor_loss": -23.02534938621521, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.419515132904053, "step": 72000}
{"episode_reward": 288.9122346066649, "episode": 73.0, "batch_reward": 0.140917565792799, "critic_loss": 0.5148362304717302, "actor_loss": -23.075055953979493, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.577110528945923, "step": 73000}
{"episode_reward": 109.49430580781743, "episode": 74.0, "batch_reward": 0.14180684604495764, "critic_loss": 0.5405263056308031, "actor_loss": -23.592900369644166, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.222076177597046, "step": 74000}
{"episode_reward": 338.5703220934, "episode": 75.0, "batch_reward": 0.14416889131069183, "critic_loss": 0.5323034839034081, "actor_loss": -23.949756170272828, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.49954080581665, "step": 75000}
{"episode_reward": 307.02539298987193, "episode": 76.0, "batch_reward": 0.14650859936326743, "critic_loss": 0.5550135204046964, "actor_loss": -23.751677656173705, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.87856698036194, "step": 76000}
{"episode_reward": 383.4607343801629, "episode": 77.0, "batch_reward": 0.14825111225247384, "critic_loss": 0.4974216843247414, "actor_loss": -23.785647960662843, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.98604917526245, "step": 77000}
{"episode_reward": 41.92429612276201, "episode": 78.0, "batch_reward": 0.1476535171046853, "critic_loss": 0.5285763228386641, "actor_loss": -23.884296327590942, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.692830562591553, "step": 78000}
{"episode_reward": 147.43150510980277, "episode": 79.0, "batch_reward": 0.1475164767280221, "critic_loss": 0.48908243244886396, "actor_loss": -22.628318880081178, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.411123514175415, "step": 79000}
{"episode_reward": 204.78093302508623, "episode": 80.0, "batch_reward": 0.14690853275358676, "critic_loss": 0.45161727564036847, "actor_loss": -22.942200386047364, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.694108963012695, "step": 80000}
{"episode_reward": 78.77630555040624, "episode": 81.0, "batch_reward": 0.1469044572636485, "critic_loss": 0.4766502565890551, "actor_loss": -23.06767746543884, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 43.83938503265381, "step": 81000}
{"episode_reward": 181.99084263678898, "episode": 82.0, "batch_reward": 0.1476702010035515, "critic_loss": 0.4992353232949972, "actor_loss": -23.8325655708313, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.69642925262451, "step": 82000}
{"episode_reward": 165.63512241045498, "episode": 83.0, "batch_reward": 0.14791526582092046, "critic_loss": 0.4479737593084574, "actor_loss": -22.874392738342284, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.519713401794434, "step": 83000}
{"episode_reward": 258.8732333131456, "episode": 84.0, "batch_reward": 0.14866378879547118, "critic_loss": 0.4778907770216465, "actor_loss": -23.649032430648802, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.623125076293945, "step": 84000}
{"episode_reward": 118.07479131827324, "episode": 85.0, "batch_reward": 0.14958806966245175, "critic_loss": 0.4682992002218962, "actor_loss": -23.454213823318483, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.034632682800293, "step": 85000}
{"episode_reward": 235.55733632693355, "episode": 86.0, "batch_reward": 0.14909761568903923, "critic_loss": 0.45853885827958585, "actor_loss": -22.506817972183228, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.65368938446045, "step": 86000}
{"episode_reward": 83.98079751505247, "episode": 87.0, "batch_reward": 0.14863303433358668, "critic_loss": 0.4905359547436237, "actor_loss": -22.952765146255494, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.076509475708008, "step": 87000}
{"episode_reward": 170.88837645342377, "episode": 88.0, "batch_reward": 0.14955541217327117, "critic_loss": 0.45744125680625436, "actor_loss": -22.65478288459778, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.610650300979614, "step": 88000}
{"episode_reward": 172.0439098098155, "episode": 89.0, "batch_reward": 0.14816858009248973, "critic_loss": 0.4531430242061615, "actor_loss": -22.643232732772827, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.22155213356018, "step": 89000}
{"episode_reward": 65.49775190888558, "episode": 90.0, "batch_reward": 0.14726245645433664, "critic_loss": 0.43318685886263847, "actor_loss": -22.797699714660645, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.38280153274536, "step": 90000}
{"episode_reward": 118.08952413155447, "episode": 91.0, "batch_reward": 0.14860435042530298, "critic_loss": 0.4569199627637863, "actor_loss": -22.27391665840149, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.96995258331299, "step": 91000}
{"episode_reward": 117.26097726139609, "episode": 92.0, "batch_reward": 0.14810799612104894, "critic_loss": 0.4216528241187334, "actor_loss": -22.317223051071167, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.943453073501587, "step": 92000}
{"episode_reward": 121.01512330204622, "episode": 93.0, "batch_reward": 0.14796259306371212, "critic_loss": 0.4669032653719187, "actor_loss": -22.217091722488405, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.126022338867188, "step": 93000}
{"episode_reward": 384.25026534334654, "episode": 94.0, "batch_reward": 0.15019981317967177, "critic_loss": 0.41936073878407476, "actor_loss": -22.484651344299316, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.83412766456604, "step": 94000}
{"episode_reward": 251.3731658142973, "episode": 95.0, "batch_reward": 0.1527842741161585, "critic_loss": 0.4082780598104, "actor_loss": -23.112316232681273, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.20831036567688, "step": 95000}
{"episode_reward": 400.7736532939323, "episode": 96.0, "batch_reward": 0.1537507536187768, "critic_loss": 0.43369235000014306, "actor_loss": -22.84111710548401, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.84104609489441, "step": 96000}
{"episode_reward": 150.71654434504288, "episode": 97.0, "batch_reward": 0.15438893823325633, "critic_loss": 0.39291728381812574, "actor_loss": -23.334904537200927, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.910917282104492, "step": 97000}
{"episode_reward": 374.13293663850607, "episode": 98.0, "batch_reward": 0.15703114958107472, "critic_loss": 0.39547803096473216, "actor_loss": -22.831758516311645, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.82203197479248, "step": 98000}
{"episode_reward": 411.9334903268814, "episode": 99.0, "batch_reward": 0.1585574601814151, "critic_loss": 0.4229697537869215, "actor_loss": -23.132057651519776, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.5488383769989, "step": 99000}
{"episode_reward": 400.2638001261703, "episode": 100.0, "batch_reward": 0.16213593780994415, "critic_loss": 0.44895090827345846, "actor_loss": -23.05881822013855, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.380099296569824, "step": 100000}
{"episode_reward": 413.2746910531836, "episode": 101.0, "batch_reward": 0.16444297877699138, "critic_loss": 0.41802023926377296, "actor_loss": -23.628611110687256, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.83352541923523, "step": 101000}
{"episode_reward": 417.362144398251, "episode": 102.0, "batch_reward": 0.16559574899077414, "critic_loss": 0.38603156225383284, "actor_loss": -23.578000198364258, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.404063940048218, "step": 102000}
{"episode_reward": 353.0996809472638, "episode": 103.0, "batch_reward": 0.1683619297593832, "critic_loss": 0.4171687595844269, "actor_loss": -23.754119836807252, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.599907398223877, "step": 103000}
{"episode_reward": 414.6745213567943, "episode": 104.0, "batch_reward": 0.17033381780982018, "critic_loss": 0.4401014402359724, "actor_loss": -23.901998542785645, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.889216899871826, "step": 104000}
{"episode_reward": 145.97664388129854, "episode": 105.0, "batch_reward": 0.170655976369977, "critic_loss": 0.41168353375792505, "actor_loss": -23.7914833278656, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.568782567977905, "step": 105000}
{"episode_reward": 387.48369925327574, "episode": 106.0, "batch_reward": 0.17174712381511928, "critic_loss": 0.3927707089781761, "actor_loss": -23.726638544082643, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.464898824691772, "step": 106000}
{"episode_reward": 407.6418432720081, "episode": 107.0, "batch_reward": 0.17354895162582398, "critic_loss": 0.35515665923058987, "actor_loss": -23.94244396972656, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.508196353912354, "step": 107000}
{"episode_reward": 384.1960105774761, "episode": 108.0, "batch_reward": 0.1771078840792179, "critic_loss": 0.38384128057956696, "actor_loss": -24.357128841400147, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.340324878692627, "step": 108000}
{"episode_reward": 413.6298990815505, "episode": 109.0, "batch_reward": 0.17885385438799858, "critic_loss": 0.42493672215938566, "actor_loss": -24.765994832992554, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.89135432243347, "step": 109000}
{"episode_reward": 211.23970829302758, "episode": 110.0, "batch_reward": 0.18012587581574918, "critic_loss": 0.36922812143713235, "actor_loss": -24.61837137413025, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.704049587249756, "step": 110000}
{"episode_reward": 446.5947842670776, "episode": 111.0, "batch_reward": 0.18151073002815246, "critic_loss": 0.43975927348434923, "actor_loss": -25.11399195098877, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.65229058265686, "step": 111000}
{"episode_reward": 384.75049373756536, "episode": 112.0, "batch_reward": 0.18350105002522468, "critic_loss": 0.39192913778126237, "actor_loss": -24.778744596481324, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.129565954208374, "step": 112000}
{"episode_reward": 450.58087499969554, "episode": 113.0, "batch_reward": 0.18559868401288987, "critic_loss": 0.38910189461708067, "actor_loss": -24.751338241577148, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.99321937561035, "step": 113000}
{"episode_reward": 417.8493243325269, "episode": 114.0, "batch_reward": 0.1889828010648489, "critic_loss": 0.36854225520789624, "actor_loss": -26.08520768737793, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.3729190826416, "step": 114000}
{"episode_reward": 449.46994635473146, "episode": 115.0, "batch_reward": 0.19071924525499345, "critic_loss": 0.3928919177353382, "actor_loss": -25.692523780822754, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.150896787643433, "step": 115000}
{"episode_reward": 326.6347027718797, "episode": 116.0, "batch_reward": 0.19182834129035473, "critic_loss": 0.3400626157671213, "actor_loss": -26.11796878051758, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.184430837631226, "step": 116000}
{"episode_reward": 348.0460171130597, "episode": 117.0, "batch_reward": 0.19269487226009369, "critic_loss": 0.3613646115362644, "actor_loss": -25.18312763595581, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.621824026107788, "step": 117000}
{"episode_reward": 435.8232530363128, "episode": 118.0, "batch_reward": 0.19603084410727023, "critic_loss": 0.35973103930056094, "actor_loss": -26.209573253631593, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.91043496131897, "step": 118000}
{"episode_reward": 489.5631951304859, "episode": 119.0, "batch_reward": 0.19779889076948165, "critic_loss": 0.34893041402101516, "actor_loss": -26.489307497024537, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.19564127922058, "step": 119000}
{"episode_reward": 461.98488188075476, "episode": 120.0, "batch_reward": 0.19812366539239884, "critic_loss": 0.36403987542539834, "actor_loss": -26.110241054534914, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.246498346328735, "step": 120000}
{"episode_reward": 26.66060330591552, "episode": 121.0, "batch_reward": 0.19906676034629345, "critic_loss": 0.39761059860885145, "actor_loss": -26.17836276435852, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 37.82999300956726, "step": 121000}
{"episode_reward": 463.4232459739749, "episode": 122.0, "batch_reward": 0.20177732203900814, "critic_loss": 0.3651457945108414, "actor_loss": -26.454919635772704, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.970386028289795, "step": 122000}
{"episode_reward": 467.51424674383554, "episode": 123.0, "batch_reward": 0.20350858980417252, "critic_loss": 0.3752314976900816, "actor_loss": -25.71840080833435, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.112179279327393, "step": 123000}
{"episode_reward": 490.682619160144, "episode": 124.0, "batch_reward": 0.20407876381278037, "critic_loss": 0.37612767617404463, "actor_loss": -26.605538513183593, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.52973508834839, "step": 124000}
{"episode_reward": 410.8255749386872, "episode": 125.0, "batch_reward": 0.20708192974328996, "critic_loss": 0.3774249143004417, "actor_loss": -26.642921325683595, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.617897033691406, "step": 125000}
{"episode_reward": 515.4151048445066, "episode": 126.0, "batch_reward": 0.20927660529315473, "critic_loss": 0.3737068282067776, "actor_loss": -27.312817462921142, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.60709810256958, "step": 126000}
{"episode_reward": 487.19644840239454, "episode": 127.0, "batch_reward": 0.21170030470192433, "critic_loss": 0.37122678324580194, "actor_loss": -27.284162158966065, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.755875825881958, "step": 127000}
{"episode_reward": 479.3098889808948, "episode": 128.0, "batch_reward": 0.21355314120650293, "critic_loss": 0.38399927063286304, "actor_loss": -27.47714465522766, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.137479543685913, "step": 128000}
{"episode_reward": 494.95057489075947, "episode": 129.0, "batch_reward": 0.2150405285656452, "critic_loss": 0.38254388834536074, "actor_loss": -27.967598838806154, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.024535417556763, "step": 129000}
{"episode_reward": 496.1718579286378, "episode": 130.0, "batch_reward": 0.21835461400449277, "critic_loss": 0.37570823524892333, "actor_loss": -28.257878412246704, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.40295958518982, "step": 130000}
{"episode_reward": 491.6208236395906, "episode": 131.0, "batch_reward": 0.2213817826360464, "critic_loss": 0.37985800896584987, "actor_loss": -28.676279071807862, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.020655393600464, "step": 131000}
{"episode_reward": 513.1659262520296, "episode": 132.0, "batch_reward": 0.22260700465738772, "critic_loss": 0.3982943013161421, "actor_loss": -28.610435110092162, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.35058355331421, "step": 132000}
{"episode_reward": 506.72140687826834, "episode": 133.0, "batch_reward": 0.2234629786014557, "critic_loss": 0.3802093283832073, "actor_loss": -28.546871519088747, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.75650429725647, "step": 133000}
{"episode_reward": 383.83387712682116, "episode": 134.0, "batch_reward": 0.22541441930830478, "critic_loss": 0.3780398046523333, "actor_loss": -28.995900230407713, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.606333255767822, "step": 134000}
{"episode_reward": 466.7924029405278, "episode": 135.0, "batch_reward": 0.2270958716124296, "critic_loss": 0.3973311996161938, "actor_loss": -29.19401916885376, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.215492963790894, "step": 135000}
{"episode_reward": 500.504926632367, "episode": 136.0, "batch_reward": 0.22943195727467536, "critic_loss": 0.3941785697788, "actor_loss": -29.630600242614747, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.563010454177856, "step": 136000}
{"episode_reward": 495.80428511090827, "episode": 137.0, "batch_reward": 0.23098247762024401, "critic_loss": 0.3996601184308529, "actor_loss": -29.387450229644774, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.801650524139404, "step": 137000}
{"episode_reward": 493.3216608477494, "episode": 138.0, "batch_reward": 0.23348416662216187, "critic_loss": 0.3967069595903158, "actor_loss": -28.9920753326416, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.788338661193848, "step": 138000}
{"episode_reward": 475.0081777672678, "episode": 139.0, "batch_reward": 0.23533018220961094, "critic_loss": 0.3900303634256124, "actor_loss": -29.338719760894776, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.647340297698975, "step": 139000}
{"episode_reward": 486.3712286684436, "episode": 140.0, "batch_reward": 0.23548849515616893, "critic_loss": 0.38530342392623423, "actor_loss": -29.242552396774293, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.331825256347656, "step": 140000}
{"episode_reward": 465.592276719266, "episode": 141.0, "batch_reward": 0.23814609558880329, "critic_loss": 0.3869542274326086, "actor_loss": -29.237802852630615, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.71162295341492, "step": 141000}
{"episode_reward": 507.45969617404194, "episode": 142.0, "batch_reward": 0.23979189030826092, "critic_loss": 0.35810501012206075, "actor_loss": -29.87866190338135, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.797120571136475, "step": 142000}
{"episode_reward": 494.21389709485396, "episode": 143.0, "batch_reward": 0.24216604554653168, "critic_loss": 0.40953487011790274, "actor_loss": -29.981899509429933, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.079184770584106, "step": 143000}
{"episode_reward": 332.0081826759247, "episode": 144.0, "batch_reward": 0.24293176405131817, "critic_loss": 0.389560270383954, "actor_loss": -30.404641716003418, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.281906604766846, "step": 144000}
{"episode_reward": 333.6464608159625, "episode": 145.0, "batch_reward": 0.2437928985953331, "critic_loss": 0.3731119099408388, "actor_loss": -30.08596156692505, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.971975564956665, "step": 145000}
{"episode_reward": 499.61973570616647, "episode": 146.0, "batch_reward": 0.24540876451134683, "critic_loss": 0.4119999490827322, "actor_loss": -30.406946765899658, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.459266185760498, "step": 146000}
{"episode_reward": 505.9973818363848, "episode": 147.0, "batch_reward": 0.24665650799870492, "critic_loss": 0.3883590794056654, "actor_loss": -30.788805374145507, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.677409648895264, "step": 147000}
{"episode_reward": 468.640453149285, "episode": 148.0, "batch_reward": 0.2492010208517313, "critic_loss": 0.3741014014482498, "actor_loss": -30.54476625061035, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.263816833496094, "step": 148000}
{"episode_reward": 538.6324632047293, "episode": 149.0, "batch_reward": 0.25092260918021203, "critic_loss": 0.3850852707773447, "actor_loss": -30.649098812103272, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.49693727493286, "step": 149000}
{"episode_reward": 528.0196303988039, "episode": 150.0, "batch_reward": 0.25264788541197775, "critic_loss": 0.3683999114632607, "actor_loss": -31.254971855163575, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
