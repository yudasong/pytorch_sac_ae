{"episode": 1.0, "duration": 17.767269134521484, "episode_reward": 7.579367547095394, "step": 1000}
{"episode": 2.0, "duration": 1.4428842067718506, "episode_reward": 587.3438790414032, "step": 2000}
{"episode": 3.0, "batch_reward": 0.2954647293604386, "critic_loss": 0.17912180952556375, "actor_loss": -50.853448320551365, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 72.45332074165344, "episode_reward": 329.91968113008113, "step": 3000}
{"episode": 4.0, "batch_reward": 0.3090470479875803, "critic_loss": 0.30489805513620377, "actor_loss": -50.89318084716797, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.05628800392151, "episode_reward": 381.54013749633947, "step": 4000}
{"episode": 5.0, "batch_reward": 0.34307952272892, "critic_loss": 0.3644159257560968, "actor_loss": -52.56323680877686, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.960402727127075, "episode_reward": 525.2967066259313, "step": 5000}
{"episode": 6.0, "batch_reward": 0.3790219615101814, "critic_loss": 0.4510385339409113, "actor_loss": -54.12001350402832, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 30.5119571685791, "episode_reward": 550.822558683689, "step": 6000}
{"episode": 7.0, "batch_reward": 0.4016922455430031, "critic_loss": 0.45409610265493394, "actor_loss": -55.17689057922363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 30.599061012268066, "episode_reward": 519.1628857418518, "step": 7000}
{"episode": 8.0, "batch_reward": 0.4119065759181976, "critic_loss": 0.5245184266865254, "actor_loss": -55.652637550354, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 30.132914543151855, "episode_reward": 413.5520147701022, "step": 8000}
{"episode": 9.0, "batch_reward": 0.4164129523038864, "critic_loss": 0.5284431529939174, "actor_loss": -55.9393570022583, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 29.625112533569336, "episode_reward": 497.1484211769621, "step": 9000}
{"episode": 10.0, "batch_reward": 0.4259952377080917, "critic_loss": 0.6315909932255745, "actor_loss": -50.77948528289795, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 4995.705105781555, "episode_reward": 528.4856529360613, "step": 10000}
{"episode": 11.0, "batch_reward": 0.43557236486673356, "critic_loss": 0.6357884393632411, "actor_loss": -51.307748565673826, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 37.40233039855957, "episode_reward": 512.3006279090201, "step": 11000}
{"episode": 12.0, "batch_reward": 0.44278720900416374, "critic_loss": 0.683406054109335, "actor_loss": -48.073697265625, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 523.1260435581207, "episode_reward": 518.1477507676863, "step": 12000}
{"episode": 13.0, "batch_reward": 0.4469117407798767, "critic_loss": 0.6431986881196499, "actor_loss": -48.51480311584473, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 29.821093320846558, "episode_reward": 522.4693342600326, "step": 13000}
{"episode": 14.0, "batch_reward": 0.45263517704606054, "critic_loss": 0.627672108322382, "actor_loss": -46.605071182250974, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 514.578102350235, "episode_reward": 486.78275133114846, "step": 14000}
{"episode": 15.0, "batch_reward": 0.45507788833975793, "critic_loss": 0.6146485593914985, "actor_loss": -47.027466468811035, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.758395433425903, "episode_reward": 500.20524873123696, "step": 15000}
{"episode": 16.0, "batch_reward": 0.4580783626437187, "critic_loss": 0.6362285071313382, "actor_loss": -45.28643111419678, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 542.5639424324036, "episode_reward": 495.56794274215804, "step": 16000}
{"episode": 17.0, "batch_reward": 0.46042112746834757, "critic_loss": 0.6158301827013493, "actor_loss": -45.63826974487305, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.888129949569702, "episode_reward": 483.13433224279794, "step": 17000}
{"episode": 18.0, "batch_reward": 0.461171401232481, "critic_loss": 0.6052256355881691, "actor_loss": -44.34186183166504, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 502.2401373386383, "episode_reward": 492.42182330603333, "step": 18000}
{"episode": 19.0, "batch_reward": 0.464406926035881, "critic_loss": 0.5708394364118576, "actor_loss": -44.7921576385498, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.767515182495117, "episode_reward": 512.7310045347284, "step": 19000}
