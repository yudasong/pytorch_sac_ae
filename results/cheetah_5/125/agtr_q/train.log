{"episode": 1.0, "duration": 23.015723943710327, "episode_reward": 7.579367547095394, "step": 1000}
{"episode": 2.0, "duration": 1.7385108470916748, "episode_reward": 587.3438790414032, "step": 2000}
{"episode": 3.0, "batch_reward": 0.28051714523111276, "actor_loss": -48.169257256735705, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 54.67317080497742, "episode_reward": 17.30022099831924, "step": 3000}
{"episode": 4.0, "batch_reward": 0.18542642876505852, "actor_loss": -42.57673175048828, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.596308946609497, "episode_reward": 68.97838018458339, "step": 4000}
{"episode": 5.0, "batch_reward": 0.16245499896258117, "actor_loss": -39.68470696258545, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.34963893890381, "episode_reward": 118.4626747084465, "step": 5000}
{"episode": 6.0, "batch_reward": 0.15611781916022302, "actor_loss": -39.278368995666504, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.570979118347168, "episode_reward": 126.29966274602985, "step": 6000}
{"episode": 7.0, "batch_reward": 0.1504891173020005, "actor_loss": -37.81805319213867, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.423426866531372, "episode_reward": 82.93759570170886, "step": 7000}
{"episode": 8.0, "batch_reward": 0.14703742688149213, "actor_loss": -36.79499184417725, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.481090784072876, "episode_reward": 182.76417361166878, "step": 8000}
{"episode": 9.0, "batch_reward": 0.14687775044888257, "actor_loss": -37.08179539489746, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.792476415634155, "episode_reward": 123.17016379185084, "step": 9000}
{"episode": 10.0, "batch_reward": 0.14438991686701774, "actor_loss": -32.47081927490235, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 3935.785924911499, "episode_reward": 70.69867871773774, "step": 10000}
{"episode": 11.0, "batch_reward": 0.14073339270800353, "actor_loss": -31.418224113464355, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.78042960166931, "episode_reward": 202.59875183906584, "step": 11000}
{"episode": 12.0, "batch_reward": 0.14302250626683236, "actor_loss": -28.28308246231079, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 428.1653378009796, "episode_reward": 73.99359660233428, "step": 12000}
{"episode": 13.0, "batch_reward": 0.1370477235764265, "actor_loss": -27.030587326049805, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.80068850517273, "episode_reward": 66.56101096371084, "step": 13000}
{"episode": 14.0, "batch_reward": 0.13407266800850628, "actor_loss": -24.336972637176515, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.31428837776184, "episode_reward": 148.44712769852347, "step": 14000}
{"episode": 15.0, "batch_reward": 0.1375697994232178, "actor_loss": -24.378364906311035, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.40002179145813, "episode_reward": 260.8706038705885, "step": 15000}
{"episode": 16.0, "batch_reward": 0.14528380125015974, "actor_loss": -24.036269428253174, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 426.21963024139404, "episode_reward": 260.7794980606623, "step": 16000}
{"episode": 17.0, "batch_reward": 0.15478056985139846, "actor_loss": -24.72107807159424, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.09267520904541, "episode_reward": 350.55888056160217, "step": 17000}
{"episode": 18.0, "batch_reward": 0.16027536184340715, "actor_loss": -24.082907234191893, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 424.1591856479645, "episode_reward": 72.56161039953544, "step": 18000}
{"episode": 19.0, "batch_reward": 0.15841133324801923, "actor_loss": -23.746845153808593, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 20.122796773910522, "episode_reward": 248.63064763432817, "step": 19000}
{"episode": 20.0, "batch_reward": 0.1650240837484598, "actor_loss": -24.057480533599854, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.2286651134491, "episode_reward": 296.0514226348816, "step": 20000}
{"episode": 21.0, "batch_reward": 0.1694072187691927, "actor_loss": -24.59205124282837, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.06763434410095, "episode_reward": 211.01850387985294, "step": 21000}
{"episode": 22.0, "batch_reward": 0.16801386852562428, "actor_loss": -23.924928031921386, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 435.90573048591614, "episode_reward": 74.59623797403502, "step": 22000}
{"episode": 23.0, "batch_reward": 0.1675979798436165, "actor_loss": -23.50780635070801, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.028931379318237, "episode_reward": 232.3876858638688, "step": 23000}
{"episode": 24.0, "batch_reward": 0.17021896003186704, "actor_loss": -23.09048569869995, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 443.7368257045746, "episode_reward": 183.5260769196424, "step": 24000}
{"episode": 25.0, "batch_reward": 0.16932917691767216, "actor_loss": -22.898845333099366, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.05034112930298, "episode_reward": 244.44850684466545, "step": 25000}
{"episode": 26.0, "batch_reward": 0.17389239227771758, "actor_loss": -22.74368431854248, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 429.55131673812866, "episode_reward": 195.84568480985902, "step": 26000}
{"episode": 27.0, "batch_reward": 0.17508327174186705, "actor_loss": -22.69984507369995, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.746443510055542, "episode_reward": 351.5681839594253, "step": 27000}
{"episode": 28.0, "batch_reward": 0.1834385072439909, "actor_loss": -22.817680603027345, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 421.1576638221741, "episode_reward": 367.32759633528354, "step": 28000}
{"episode": 29.0, "batch_reward": 0.18729063260555268, "actor_loss": -23.032872200012207, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.340551137924194, "episode_reward": 296.0288553171982, "step": 29000}
{"episode": 30.0, "batch_reward": 0.1917690576016903, "actor_loss": -22.86188557434082, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 405.95503878593445, "episode_reward": 360.05913453924666, "step": 30000}
{"episode": 31.0, "batch_reward": 0.19789641900360586, "actor_loss": -23.347566028594972, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.88040471076965, "episode_reward": 360.24786347110216, "step": 31000}
{"episode": 32.0, "batch_reward": 0.2018343220204115, "actor_loss": -23.028663261413573, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 420.84099531173706, "episode_reward": 162.69025394831908, "step": 32000}
{"episode": 33.0, "batch_reward": 0.20171272285282613, "actor_loss": -22.821776618957518, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.03740668296814, "episode_reward": 229.91332945510115, "step": 33000}
{"episode": 34.0, "batch_reward": 0.20114079421758652, "actor_loss": -22.013783283233643, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 426.00150847435, "episode_reward": 167.22109709842022, "step": 34000}
{"episode": 35.0, "batch_reward": 0.20195840129256248, "actor_loss": -21.795651378631593, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.424550771713257, "episode_reward": 282.76684171525113, "step": 35000}
{"episode": 36.0, "batch_reward": 0.20497196039557458, "actor_loss": -21.258726432800295, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.33705163002014, "episode_reward": 453.43022732074115, "step": 36000}
{"episode": 37.0, "batch_reward": 0.21267823195457458, "actor_loss": -21.802746803283693, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.869237899780273, "episode_reward": 439.91787138059124, "step": 37000}
{"episode": 38.0, "batch_reward": 0.21855141578614712, "actor_loss": -21.981631229400634, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.5832540988922, "episode_reward": 393.91295481744385, "step": 38000}
{"episode": 39.0, "batch_reward": 0.21901204533874988, "actor_loss": -22.171819862365723, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.64663863182068, "episode_reward": 219.2496996910062, "step": 39000}
{"episode": 40.0, "batch_reward": 0.22305224235355853, "actor_loss": -22.22076850891113, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.92070603370667, "episode_reward": 425.86517178357735, "step": 40000}
{"episode": 41.0, "batch_reward": 0.22687029527127742, "actor_loss": -22.61813631439209, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.18724989891052, "episode_reward": 393.7845786703461, "step": 41000}
{"episode": 42.0, "batch_reward": 0.22986682790517807, "actor_loss": -22.636875053405763, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.47284150123596, "episode_reward": 352.3366124892209, "step": 42000}
{"episode": 43.0, "batch_reward": 0.23271292199194432, "actor_loss": -23.021994178771973, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.66729474067688, "episode_reward": 247.64500462955914, "step": 43000}
{"episode": 44.0, "batch_reward": 0.23446500469744205, "actor_loss": -23.0304034614563, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 410.3322501182556, "episode_reward": 414.06084205398565, "step": 44000}
{"episode": 45.0, "batch_reward": 0.238892609462142, "actor_loss": -23.35007941818237, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.728273153305054, "episode_reward": 424.37091531804646, "step": 45000}
{"episode": 46.0, "batch_reward": 0.24300591610372066, "actor_loss": -23.51869151687622, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.25801396369934, "episode_reward": 427.4854621984395, "step": 46000}
{"episode": 47.0, "batch_reward": 0.24677150367200373, "actor_loss": -23.865553924560547, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.608668088912964, "episode_reward": 391.4655836617113, "step": 47000}
{"episode": 48.0, "batch_reward": 0.2500355079174042, "actor_loss": -24.250600002288817, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 406.003116607666, "episode_reward": 408.9425113228461, "step": 48000}
{"episode": 49.0, "batch_reward": 0.2530851629972458, "actor_loss": -24.44797314453125, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.652624130249023, "episode_reward": 432.04156037782394, "step": 49000}
{"episode": 50.0, "batch_reward": 0.25629405897855756, "actor_loss": -25.128250122070312, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 426.2341899871826, "episode_reward": 430.3029209769546, "step": 50000}
{"episode": 51.0, "batch_reward": 0.25962311409413813, "actor_loss": -25.450027919769287, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.7763135433197, "episode_reward": 461.4910931999061, "step": 51000}
{"episode": 52.0, "batch_reward": 0.2634211176633835, "actor_loss": -25.780603427886962, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 424.6652133464813, "episode_reward": 369.4492368323837, "step": 52000}
{"episode": 53.0, "batch_reward": 0.26588783486187456, "actor_loss": -25.99812760925293, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.142086029052734, "episode_reward": 420.8916342903602, "step": 53000}
{"episode": 54.0, "batch_reward": 0.26934033815562725, "actor_loss": -26.05960363769531, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 424.6439666748047, "episode_reward": 461.14292553768917, "step": 54000}
{"episode": 55.0, "batch_reward": 0.27153943924605844, "actor_loss": -26.45546784210205, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.23262858390808, "episode_reward": 440.87198387434347, "step": 55000}
{"episode": 56.0, "batch_reward": 0.2760264842361212, "actor_loss": -26.494860912323, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.46599674224854, "episode_reward": 442.118230354214, "step": 56000}
{"episode": 57.0, "batch_reward": 0.2778754162490368, "actor_loss": -26.805662670135497, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.945530891418457, "episode_reward": 425.7529937136584, "step": 57000}
{"episode": 58.0, "batch_reward": 0.28081363180279734, "actor_loss": -27.080295459747315, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.0777151584625, "episode_reward": 425.24092707491667, "step": 58000}
{"episode": 59.0, "batch_reward": 0.28365880362689494, "actor_loss": -27.30340161514282, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.770747184753418, "episode_reward": 449.01648698257196, "step": 59000}
{"episode": 60.0, "batch_reward": 0.28729044188559055, "actor_loss": -28.09963819503784, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.5879135131836, "episode_reward": 449.7103504017495, "step": 60000}
{"episode": 61.0, "batch_reward": 0.28730067363381384, "actor_loss": -27.985778720855713, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.20564317703247, "episode_reward": 377.15788929975855, "step": 61000}
{"episode": 62.0, "batch_reward": 0.291400849878788, "actor_loss": -27.774725536346434, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 422.1072247028351, "episode_reward": 431.02523905252184, "step": 62000}
{"episode": 63.0, "batch_reward": 0.2913707575052977, "actor_loss": -27.995252799987792, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.892849922180176, "episode_reward": 436.47241676109434, "step": 63000}
{"episode": 64.0, "batch_reward": 0.2957211399227381, "actor_loss": -28.27384117126465, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 429.06789088249207, "episode_reward": 469.8428574393963, "step": 64000}
{"episode": 65.0, "batch_reward": 0.29749414521455764, "actor_loss": -28.372701484680174, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.41929531097412, "episode_reward": 443.6429612232073, "step": 65000}
{"episode": 66.0, "batch_reward": 0.30044519728422164, "actor_loss": -28.300342220306398, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.1243648529053, "episode_reward": 485.2047111618618, "step": 66000}
{"episode": 67.0, "batch_reward": 0.30108329659700395, "actor_loss": -28.225402893066406, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.647443056106567, "episode_reward": 172.8822803387745, "step": 67000}
{"episode": 68.0, "batch_reward": 0.3016154788434505, "actor_loss": -27.99000969314575, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.57611083984375, "episode_reward": 461.68508408880274, "step": 68000}
{"episode": 69.0, "batch_reward": 0.3029085671454668, "actor_loss": -28.088224502563477, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.921894073486328, "episode_reward": 472.7733137823175, "step": 69000}
{"episode": 70.0, "batch_reward": 0.3063235741257668, "actor_loss": -28.277968105316162, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.5226140022278, "episode_reward": 480.90631679192273, "step": 70000}
{"episode": 71.0, "batch_reward": 0.3082460727393627, "actor_loss": -28.413582069396973, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.65427327156067, "episode_reward": 450.4024794030854, "step": 71000}
{"episode": 72.0, "batch_reward": 0.3097106848359108, "actor_loss": -28.48321957397461, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 424.6807894706726, "episode_reward": 444.76279238013416, "step": 72000}
{"episode": 73.0, "batch_reward": 0.3110438247025013, "actor_loss": -28.55936680984497, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.469817399978638, "episode_reward": 329.1364414223277, "step": 73000}
{"episode": 74.0, "batch_reward": 0.311381398499012, "actor_loss": -28.646155223846435, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 428.93742275238037, "episode_reward": 460.554494419044, "step": 74000}
{"episode": 75.0, "batch_reward": 0.31477550265192983, "actor_loss": -28.919556327819823, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.945322513580322, "episode_reward": 441.38161858834854, "step": 75000}
{"episode": 76.0, "batch_reward": 0.3156807264387608, "actor_loss": -28.981040992736816, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 424.95808839797974, "episode_reward": 462.8878831789391, "step": 76000}
{"episode": 77.0, "batch_reward": 0.31771752855181695, "actor_loss": -29.204270050048827, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.09587788581848, "episode_reward": 445.96230617664145, "step": 77000}
{"episode": 78.0, "batch_reward": 0.3192677381634712, "actor_loss": -29.240270175933837, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 428.6221079826355, "episode_reward": 472.19424432151936, "step": 78000}
{"episode": 79.0, "batch_reward": 0.3209379297494888, "actor_loss": -29.270014781951904, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.61383605003357, "episode_reward": 467.8648818500029, "step": 79000}
{"episode": 80.0, "batch_reward": 0.3236393373310566, "actor_loss": -29.69235676574707, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.3449811935425, "episode_reward": 512.752188194408, "step": 80000}
{"episode": 81.0, "batch_reward": 0.3249439258873463, "actor_loss": -29.728319358825683, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.27053141593933, "episode_reward": 503.3006564207734, "step": 81000}
{"episode": 82.0, "batch_reward": 0.3271914663016796, "actor_loss": -29.591725399017335, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 425.77783966064453, "episode_reward": 466.77508319979523, "step": 82000}
{"episode": 83.0, "batch_reward": 0.3302784662246704, "actor_loss": -29.906543533325195, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.53259301185608, "episode_reward": 473.00868488089594, "step": 83000}
{"episode": 84.0, "batch_reward": 0.3312096684575081, "actor_loss": -30.124545974731447, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.29344606399536, "episode_reward": 524.3498822234756, "step": 84000}
{"episode": 85.0, "batch_reward": 0.3332965928316116, "actor_loss": -30.393612174987794, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.527178287506104, "episode_reward": 372.5226302625833, "step": 85000}
{"episode": 86.0, "batch_reward": 0.3348283466696739, "actor_loss": -30.31299536514282, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 429.32608342170715, "episode_reward": 527.9856689656416, "step": 86000}
{"episode": 87.0, "batch_reward": 0.33532445642352104, "actor_loss": -30.39994330215454, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.165857553482056, "episode_reward": 501.62274006750806, "step": 87000}
{"episode": 88.0, "batch_reward": 0.33891401487588885, "actor_loss": -30.496635650634765, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 430.1795060634613, "episode_reward": 538.5508002878812, "step": 88000}
{"episode": 89.0, "batch_reward": 0.3399994883239269, "actor_loss": -30.555164447784424, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.697660446166992, "episode_reward": 531.9467024867857, "step": 89000}
{"episode": 90.0, "batch_reward": 0.34377431228756905, "actor_loss": -31.02127659225464, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 421.83768606185913, "episode_reward": 554.3501852650295, "step": 90000}
{"episode": 91.0, "batch_reward": 0.34563846892118455, "actor_loss": -31.11944199371338, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.73186993598938, "episode_reward": 538.36496343782, "step": 91000}
{"episode": 92.0, "batch_reward": 0.3465975014567375, "actor_loss": -31.0141510887146, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 409.8468277454376, "episode_reward": 476.9408278122389, "step": 92000}
{"episode": 93.0, "batch_reward": 0.3489207347035408, "actor_loss": -31.325252712249757, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.189996242523193, "episode_reward": 541.2518541076942, "step": 93000}
{"episode": 94.0, "batch_reward": 0.35054449757933615, "actor_loss": -31.386561847686767, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.087539434433, "episode_reward": 518.5033355045688, "step": 94000}
{"episode": 95.0, "batch_reward": 0.35240255418419836, "actor_loss": -31.59611514663696, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.10446333885193, "episode_reward": 526.8244349030923, "step": 95000}
{"episode": 96.0, "batch_reward": 0.3555668113827705, "actor_loss": -31.95513821411133, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 430.7357268333435, "episode_reward": 563.9623225827081, "step": 96000}
{"episode": 97.0, "batch_reward": 0.3567933854162693, "actor_loss": -32.05282077026367, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.147846937179565, "episode_reward": 548.6057519292465, "step": 97000}
{"episode": 98.0, "batch_reward": 0.3587772683799267, "actor_loss": -32.22934273910523, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 430.764151096344, "episode_reward": 540.540335082752, "step": 98000}
{"episode": 99.0, "batch_reward": 0.36044284036755564, "actor_loss": -32.468161685943606, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.251095294952393, "episode_reward": 534.1993609526671, "step": 99000}
{"episode": 100.0, "batch_reward": 0.3602922325730324, "actor_loss": -32.28103384017945, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 422.8929486274719, "episode_reward": 59.452919093487566, "step": 100000}
{"episode": 101.0, "batch_reward": 0.35897668489813805, "actor_loss": -32.176764835357666, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 35.082552671432495, "episode_reward": 541.8707426144247, "step": 101000}
{"episode": 102.0, "batch_reward": 0.36052006763219835, "actor_loss": -32.47052598190307, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 429.59793424606323, "episode_reward": 526.4650773726555, "step": 102000}
{"episode": 103.0, "batch_reward": 0.3629166306853294, "actor_loss": -32.64204375839233, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.92781662940979, "episode_reward": 517.4418460683466, "step": 103000}
{"episode": 104.0, "batch_reward": 0.36368061980605126, "actor_loss": -32.82498747634888, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 425.30760860443115, "episode_reward": 500.24323605704916, "step": 104000}
{"episode": 105.0, "batch_reward": 0.3652878458797932, "actor_loss": -32.89773622131348, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.56977939605713, "episode_reward": 529.00607172259, "step": 105000}
{"episode": 106.0, "batch_reward": 0.3650493851006031, "actor_loss": -32.98170685958862, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 426.9252498149872, "episode_reward": 158.51303059494055, "step": 106000}
{"episode": 107.0, "batch_reward": 0.3648849749863148, "actor_loss": -32.95509752655029, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.05374526977539, "episode_reward": 574.2845227647833, "step": 107000}
{"episode": 108.0, "batch_reward": 0.36841777577996254, "actor_loss": -33.54335318374634, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 425.0957715511322, "episode_reward": 554.7413715176245, "step": 108000}
{"episode": 109.0, "batch_reward": 0.3691045989692211, "actor_loss": -33.639755977630614, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.58489751815796, "episode_reward": 548.485910337941, "step": 109000}
{"episode": 110.0, "batch_reward": 0.3706822194457054, "actor_loss": -33.745514183044435, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 412.2895472049713, "episode_reward": 536.5092359845672, "step": 110000}
{"episode": 111.0, "batch_reward": 0.37203581947088243, "actor_loss": -33.83067951965332, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 36.554253816604614, "episode_reward": 573.8426705514535, "step": 111000}
{"episode": 112.0, "batch_reward": 0.3736926448047161, "actor_loss": -34.09552510452271, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.3857955932617, "episode_reward": 539.7889337709014, "step": 112000}
{"episode": 113.0, "batch_reward": 0.3759778534770012, "actor_loss": -34.28322547149658, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.327707290649414, "episode_reward": 573.9857617395235, "step": 113000}
{"episode": 114.0, "batch_reward": 0.3764957822561264, "actor_loss": -34.43545803070069, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 415.85424637794495, "episode_reward": 527.7051361815227, "step": 114000}
{"episode": 115.0, "batch_reward": 0.3779232321977615, "actor_loss": -34.55278366851807, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.418861865997314, "episode_reward": 510.08606761224473, "step": 115000}
{"episode": 116.0, "batch_reward": 0.3793249225616455, "actor_loss": -35.20549080276489, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 431.08408284187317, "episode_reward": 559.0082619969079, "step": 116000}
{"episode": 117.0, "batch_reward": 0.3809408101439476, "actor_loss": -35.36383353042603, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.952799558639526, "episode_reward": 557.5547421115841, "step": 117000}
{"episode": 118.0, "batch_reward": 0.38166586437821387, "actor_loss": -35.421133701324464, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 421.73297667503357, "episode_reward": 500.00818101484515, "step": 118000}
{"episode": 119.0, "batch_reward": 0.3828639578521252, "actor_loss": -35.62924111557007, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.538812398910522, "episode_reward": 523.6893256788628, "step": 119000}
{"episode": 120.0, "batch_reward": 0.38469362911581995, "actor_loss": -35.65396115493775, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 430.14422726631165, "episode_reward": 532.2762262854959, "step": 120000}
{"episode": 121.0, "batch_reward": 0.3855804197192192, "actor_loss": -35.70296185302735, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.270084857940674, "episode_reward": 504.9516862864685, "step": 121000}
{"episode": 122.0, "batch_reward": 0.38716931426525114, "actor_loss": -35.82103203201294, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 425.02253913879395, "episode_reward": 500.1087201187448, "step": 122000}
{"episode": 123.0, "batch_reward": 0.3867711750268936, "actor_loss": -35.93065991973877, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.48626208305359, "episode_reward": 490.52346012023816, "step": 123000}
{"episode": 124.0, "batch_reward": 0.38888994106650354, "actor_loss": -36.17369208526611, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 429.635036945343, "episode_reward": 526.7088581668838, "step": 124000}
{"episode": 125.0, "batch_reward": 0.3889630148112774, "actor_loss": -36.18283902359009, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.273056983947754, "episode_reward": 496.39655659158916, "step": 125000}
{"episode": 126.0, "batch_reward": 0.39086240592598914, "actor_loss": -36.07826346588135, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 427.45968198776245, "episode_reward": 508.6622703251339, "step": 126000}
{"episode": 127.0, "batch_reward": 0.39184603893756864, "actor_loss": -36.16922058105469, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.94105887413025, "episode_reward": 502.68156978369683, "step": 127000}
{"episode": 128.0, "batch_reward": 0.39297804221510885, "actor_loss": -36.196740386962894, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 409.3393144607544, "episode_reward": 513.6026131887514, "step": 128000}
{"episode": 129.0, "batch_reward": 0.39366994896531105, "actor_loss": -36.28693751907348, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.121514081954956, "episode_reward": 500.98651385603625, "step": 129000}
{"episode": 130.0, "batch_reward": 0.3936378549039364, "actor_loss": -36.53735646057129, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.83569598197937, "episode_reward": 491.7558004135948, "step": 130000}
{"episode": 131.0, "batch_reward": 0.3948960590362549, "actor_loss": -36.571592006683346, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.155311822891235, "episode_reward": 527.1454554856082, "step": 131000}
{"episode": 132.0, "batch_reward": 0.39620337018370627, "actor_loss": -36.314597854614256, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 407.97642278671265, "episode_reward": 514.0307570346871, "step": 132000}
{"episode": 133.0, "batch_reward": 0.39627726501226423, "actor_loss": -36.43493982696533, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.240976572036743, "episode_reward": 509.2920842464226, "step": 133000}
{"episode": 134.0, "batch_reward": 0.3988356249928474, "actor_loss": -36.83436245727539, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.4465608596802, "episode_reward": 539.3043138934802, "step": 134000}
{"episode": 135.0, "batch_reward": 0.39856345161795614, "actor_loss": -36.82116379928589, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.34997248649597, "episode_reward": 545.7030735472592, "step": 135000}
{"episode": 136.0, "batch_reward": 0.4005323621928692, "actor_loss": -36.91976252746582, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 409.5813910961151, "episode_reward": 532.0674469799498, "step": 136000}
{"episode": 137.0, "batch_reward": 0.4001047396957874, "actor_loss": -36.75211357879639, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.653212785720825, "episode_reward": 532.8354881325254, "step": 137000}
{"episode": 138.0, "batch_reward": 0.40165059387683866, "actor_loss": -36.663344551086425, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 429.6910071372986, "episode_reward": 551.2476207619208, "step": 138000}
{"episode": 139.0, "batch_reward": 0.402672383338213, "actor_loss": -36.985714141845705, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.62442111968994, "episode_reward": 537.6688535846332, "step": 139000}
{"episode": 140.0, "batch_reward": 0.4043062537908554, "actor_loss": -37.17826294708252, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 413.5783038139343, "episode_reward": 565.3942026252852, "step": 140000}
{"episode": 141.0, "batch_reward": 0.4052008217573166, "actor_loss": -37.35263897705078, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 33.08658528327942, "episode_reward": 599.7055722730861, "step": 141000}
{"episode": 142.0, "batch_reward": 0.4047584403455257, "actor_loss": -37.070815994262695, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 404.3262722492218, "episode_reward": 586.9952847478979, "step": 142000}
{"episode": 143.0, "batch_reward": 0.40727346178889273, "actor_loss": -37.24478596496582, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.503556489944458, "episode_reward": 554.3484464084221, "step": 143000}
{"episode": 144.0, "batch_reward": 0.40780833953619006, "actor_loss": -37.208132465362546, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 406.3298091888428, "episode_reward": 526.3852319893113, "step": 144000}
{"episode": 145.0, "batch_reward": 0.4097781895697117, "actor_loss": -37.41601212310791, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.6201593875885, "episode_reward": 557.6068029001191, "step": 145000}
{"episode": 146.0, "batch_reward": 0.41023149964213373, "actor_loss": -37.47680067443848, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 414.20409631729126, "episode_reward": 525.5795274065756, "step": 146000}
{"episode": 147.0, "batch_reward": 0.41119026762247085, "actor_loss": -37.53461277770996, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.178380012512207, "episode_reward": 551.7324752504556, "step": 147000}
{"episode": 148.0, "batch_reward": 0.4129446761012077, "actor_loss": -37.53907192230225, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 411.8487243652344, "episode_reward": 586.8197694480506, "step": 148000}
{"episode": 149.0, "batch_reward": 0.41294833925366403, "actor_loss": -37.55048249053955, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.6351158618927, "episode_reward": 575.8802998205663, "step": 149000}
{"episode": 150.0, "batch_reward": 0.41329339173436164, "actor_loss": -37.66110461425781, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "step": 150000}
