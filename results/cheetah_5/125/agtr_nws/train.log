{"episode": 1.0, "duration": 12.06989598274231, "episode_reward": 7.579367547095394, "step": 1000}
{"episode": 2.0, "duration": 1.177567720413208, "episode_reward": 587.3438790414032, "step": 2000}
{"episode": 3.0, "batch_reward": 0.2843283224439732, "actor_loss": -49.54250198442872, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 47.78604435920715, "episode_reward": 92.68795495321298, "step": 3000}
{"episode": 4.0, "batch_reward": 0.2047055824548006, "actor_loss": -39.60608071899414, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.62121295928955, "episode_reward": 34.921739649579806, "step": 4000}
{"episode": 5.0, "batch_reward": 0.1672858279198408, "actor_loss": -38.22975861358643, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.697040319442749, "episode_reward": 89.66313732856959, "step": 5000}
{"episode": 6.0, "batch_reward": 0.15322588455677033, "actor_loss": -37.321263259887694, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.18742561340332, "episode_reward": 78.45797052752357, "step": 6000}
{"episode": 7.0, "batch_reward": 0.14084498580545188, "actor_loss": -36.27427095031738, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.681312561035156, "episode_reward": 55.6049883566393, "step": 7000}
{"episode": 8.0, "batch_reward": 0.12831406885385513, "actor_loss": -36.22801842880249, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.5455482006073, "episode_reward": 17.775469085942504, "step": 8000}
{"episode": 9.0, "batch_reward": 0.11469805995374918, "actor_loss": -36.14978025054932, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.469861030578613, "episode_reward": 35.26794967522698, "step": 9000}
{"episode": 10.0, "batch_reward": 0.10988043487071991, "actor_loss": -31.293950225830077, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 4801.58770942688, "episode_reward": 97.04891792780106, "step": 10000}
{"episode": 11.0, "batch_reward": 0.10923512008786201, "actor_loss": -31.440377365112305, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.93546438217163, "episode_reward": 82.32838601477195, "step": 11000}
{"episode": 12.0, "batch_reward": 0.10468524866551161, "actor_loss": -28.343759304046632, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.7241303920746, "episode_reward": 59.302933475041065, "step": 12000}
{"episode": 13.0, "batch_reward": 0.10334162282943725, "actor_loss": -28.693815017700196, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.947615623474121, "episode_reward": 101.00416196383348, "step": 13000}
{"episode": 14.0, "batch_reward": 0.10258805353194475, "actor_loss": -26.234671646118166, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 525.4994761943817, "episode_reward": 102.37750167474086, "step": 14000}
{"episode": 15.0, "batch_reward": 0.1039311207383871, "actor_loss": -26.47544491958618, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.672112226486206, "episode_reward": 128.65449067430438, "step": 15000}
{"episode": 16.0, "batch_reward": 0.1065073015615344, "actor_loss": -25.18907664489746, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 521.4201145172119, "episode_reward": 174.6579476029857, "step": 16000}
{"episode": 17.0, "batch_reward": 0.11159701307117939, "actor_loss": -25.659880489349366, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.414977550506592, "episode_reward": 173.56184327003626, "step": 17000}
{"episode": 18.0, "batch_reward": 0.11407995580881834, "actor_loss": -24.324663722991943, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.3755993843079, "episode_reward": 161.50410622900037, "step": 18000}
{"episode": 19.0, "batch_reward": 0.11914841604232788, "actor_loss": -24.769518306732177, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.011852979660034, "episode_reward": 171.53067332753506, "step": 19000}
{"episode": 20.0, "batch_reward": 0.11897717919945718, "actor_loss": -23.657117179870607, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.5011165142059, "episode_reward": 169.1326011842597, "step": 20000}
{"episode": 21.0, "batch_reward": 0.12230389168858528, "actor_loss": -24.05105207824707, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 28.178060293197632, "episode_reward": 191.2763391721614, "step": 21000}
{"episode": 22.0, "batch_reward": 0.12671769014000892, "actor_loss": -23.702149459838868, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.6794934272766, "episode_reward": 235.6324003495603, "step": 22000}
{"episode": 23.0, "batch_reward": 0.1308040735051036, "actor_loss": -24.17146635055542, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.732317924499512, "episode_reward": 207.62590560803483, "step": 23000}
{"episode": 24.0, "batch_reward": 0.13426821368187666, "actor_loss": -23.832690349578858, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.6806261539459, "episode_reward": 191.7187007365608, "step": 24000}
{"episode": 25.0, "batch_reward": 0.13655190248042345, "actor_loss": -24.167153739929198, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.785930871963501, "episode_reward": 223.7457901271141, "step": 25000}
{"episode": 26.0, "batch_reward": 0.13821462926268577, "actor_loss": -23.576855880737305, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.0417590141296, "episode_reward": 138.99235844034683, "step": 26000}
{"episode": 27.0, "batch_reward": 0.1395942374765873, "actor_loss": -23.61275178527832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.472914218902588, "episode_reward": 259.648852758002, "step": 27000}
{"episode": 28.0, "batch_reward": 0.14473529279232025, "actor_loss": -23.043173225402832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.1604199409485, "episode_reward": 226.24201841726105, "step": 28000}
{"episode": 29.0, "batch_reward": 0.14647234116494656, "actor_loss": -23.62277911758423, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.929704904556274, "episode_reward": 192.02442576220358, "step": 29000}
{"episode": 30.0, "batch_reward": 0.14846140249073506, "actor_loss": -22.971939922332762, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.1159958839417, "episode_reward": 233.33509054678933, "step": 30000}
{"episode": 31.0, "batch_reward": 0.1492363022491336, "actor_loss": -23.261747152328493, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.96370506286621, "episode_reward": 73.15846694600076, "step": 31000}
{"episode": 32.0, "batch_reward": 0.14850817023962737, "actor_loss": -22.187369382858275, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.986065864563, "episode_reward": 219.00072971279727, "step": 32000}
{"episode": 33.0, "batch_reward": 0.15137793550640344, "actor_loss": -22.55446231651306, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.951211929321289, "episode_reward": 198.6156785411862, "step": 33000}
{"episode": 34.0, "batch_reward": 0.15204762261360882, "actor_loss": -21.768275645256043, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.7118237018585, "episode_reward": 204.93684195478156, "step": 34000}
{"episode": 35.0, "batch_reward": 0.1539233788624406, "actor_loss": -22.003806154251098, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.873260259628296, "episode_reward": 224.37615459592612, "step": 35000}
{"episode": 36.0, "batch_reward": 0.15527229289710523, "actor_loss": -20.833115036010742, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 515.9680140018463, "episode_reward": 176.9367370777393, "step": 36000}
{"episode": 37.0, "batch_reward": 0.15674835474789142, "actor_loss": -20.91539796447754, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.569234609603882, "episode_reward": 162.1858980355162, "step": 37000}
{"episode": 38.0, "batch_reward": 0.15712576714903118, "actor_loss": -18.700991642475127, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.268717288971, "episode_reward": 139.80231114753934, "step": 38000}
{"episode": 39.0, "batch_reward": 0.15559670983254908, "actor_loss": -18.52137581539154, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.59402585029602, "episode_reward": 187.821170800108, "step": 39000}
{"episode": 40.0, "batch_reward": 0.15628751736879348, "actor_loss": -15.79947781944275, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.7434906959534, "episode_reward": 143.43612507677057, "step": 40000}
{"episode": 41.0, "batch_reward": 0.15659151743352412, "actor_loss": -16.42684644937515, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.91082501411438, "episode_reward": 194.41913350471336, "step": 41000}
{"episode": 42.0, "batch_reward": 0.157167809702456, "actor_loss": -14.282181870937347, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 508.7188913822174, "episode_reward": 185.6670023143085, "step": 42000}
{"episode": 43.0, "batch_reward": 0.15860221815109252, "actor_loss": -14.108804121017457, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.700326919555664, "episode_reward": 174.2506091529324, "step": 43000}
{"episode": 44.0, "batch_reward": 0.15922331178188323, "actor_loss": -13.53151004076004, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.4534747600555, "episode_reward": 225.93012312378087, "step": 44000}
{"episode": 45.0, "batch_reward": 0.16033204077184202, "actor_loss": -13.69590180015564, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.537731647491455, "episode_reward": 259.33634212280214, "step": 45000}
{"episode": 46.0, "batch_reward": 0.16239410166442395, "actor_loss": -13.219445528030395, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.4043092727661, "episode_reward": 180.49791139288214, "step": 46000}
{"episode": 47.0, "batch_reward": 0.16272325137257576, "actor_loss": -13.24373207139969, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.58784818649292, "episode_reward": 220.79964629306497, "step": 47000}
{"episode": 48.0, "batch_reward": 0.16293514947593213, "actor_loss": -12.59113380241394, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.7364478111267, "episode_reward": 102.51856858788263, "step": 48000}
{"episode": 49.0, "batch_reward": 0.16257315522432328, "actor_loss": -12.101943790197373, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.224608421325684, "episode_reward": 208.87308813154718, "step": 49000}
{"episode": 50.0, "batch_reward": 0.16305353677272796, "actor_loss": -10.96398949599266, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 517.463356256485, "episode_reward": 169.54016305194347, "step": 50000}
{"episode": 51.0, "batch_reward": 0.16280711188912392, "actor_loss": -11.082191112756728, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 28.00386953353882, "episode_reward": 155.95367166227663, "step": 51000}
{"episode": 52.0, "batch_reward": 0.16359648177027702, "actor_loss": -9.17529837846756, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 516.33065366745, "episode_reward": 167.65095105338787, "step": 52000}
{"episode": 53.0, "batch_reward": 0.1627959612160921, "actor_loss": -9.198519097089768, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.030670404434204, "episode_reward": 52.18503779999386, "step": 53000}
{"episode": 54.0, "batch_reward": 0.16209723576903343, "actor_loss": -7.354158015012741, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 521.5622198581696, "episode_reward": 183.25423335950802, "step": 54000}
{"episode": 55.0, "batch_reward": 0.16239934624731542, "actor_loss": -7.644983376026153, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.500815868377686, "episode_reward": 163.9321205195701, "step": 55000}
{"episode": 56.0, "batch_reward": 0.16238258838653563, "actor_loss": -7.816833688020706, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.1682105064392, "episode_reward": 201.81223849059884, "step": 56000}
{"episode": 57.0, "batch_reward": 0.16313086584210396, "actor_loss": -8.30071974158287, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.532134771347046, "episode_reward": 291.60299166732074, "step": 57000}
{"episode": 58.0, "batch_reward": 0.16494717425107955, "actor_loss": -10.020269441127777, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.6723499298096, "episode_reward": 257.65398940564035, "step": 58000}
{"episode": 59.0, "batch_reward": 0.16677518387138843, "actor_loss": -10.256498994350434, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.457281589508057, "episode_reward": 266.17716254348096, "step": 59000}
{"episode": 60.0, "batch_reward": 0.16869883684813977, "actor_loss": -12.53937063074112, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.5167446136475, "episode_reward": 236.8307231535765, "step": 60000}
{"episode": 61.0, "batch_reward": 0.1690439520329237, "actor_loss": -12.574591670513152, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.959599018096924, "episode_reward": 157.73547461446714, "step": 61000}
{"episode": 62.0, "batch_reward": 0.16920970073342323, "actor_loss": -12.652598020553588, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.24866938591, "episode_reward": 302.0026798782536, "step": 62000}
{"episode": 63.0, "batch_reward": 0.17141972768306732, "actor_loss": -12.852237632751464, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.069226026535034, "episode_reward": 280.95962703941046, "step": 63000}
{"episode": 64.0, "batch_reward": 0.17271697118878365, "actor_loss": -13.557555002212524, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.3953354358673, "episode_reward": 252.65616083033757, "step": 64000}
{"episode": 65.0, "batch_reward": 0.1746317021548748, "actor_loss": -13.879105688095093, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.732341289520264, "episode_reward": 256.50468844227555, "step": 65000}
{"episode": 66.0, "batch_reward": 0.17638061556220055, "actor_loss": -14.943181919097901, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.889570236206, "episode_reward": 291.7375190374519, "step": 66000}
{"episode": 67.0, "batch_reward": 0.17644510382413864, "actor_loss": -14.798158756256104, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.392535209655762, "episode_reward": 66.06310681762167, "step": 67000}
{"episode": 68.0, "batch_reward": 0.17686800993978977, "actor_loss": -14.910627737998963, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 516.7783761024475, "episode_reward": 378.52793541754954, "step": 68000}
{"episode": 69.0, "batch_reward": 0.180042421489954, "actor_loss": -15.225051427841187, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.515945196151733, "episode_reward": 383.8144994034664, "step": 69000}
{"episode": 70.0, "batch_reward": 0.18265610082447528, "actor_loss": -16.07006406211853, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.7403995990753, "episode_reward": 348.0890045579384, "step": 70000}
{"episode": 71.0, "batch_reward": 0.18462710379064082, "actor_loss": -16.278835762023927, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.640785455703735, "episode_reward": 337.6580651210119, "step": 71000}
{"episode": 72.0, "batch_reward": 0.187080652192235, "actor_loss": -17.325189149856566, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.8840935230255, "episode_reward": 366.36939712594125, "step": 72000}
{"episode": 73.0, "batch_reward": 0.18980260275304317, "actor_loss": -17.526001888275147, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.335657119750977, "episode_reward": 351.7933605599576, "step": 73000}
{"episode": 74.0, "batch_reward": 0.19201521776616573, "actor_loss": -18.472130479812623, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 523.5641973018646, "episode_reward": 392.05106394244206, "step": 74000}
{"episode": 75.0, "batch_reward": 0.19485398995876313, "actor_loss": -18.769492149353027, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.349742412567139, "episode_reward": 358.36517056942847, "step": 75000}
{"episode": 76.0, "batch_reward": 0.19656292064487935, "actor_loss": -19.850114139556887, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.7713973522186, "episode_reward": 390.8510348690541, "step": 76000}
{"episode": 77.0, "batch_reward": 0.19926694515347482, "actor_loss": -20.104544527053832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.627787351608276, "episode_reward": 360.8749544470331, "step": 77000}
{"episode": 78.0, "batch_reward": 0.20129385060071944, "actor_loss": -20.784528438568117, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 521.4007048606873, "episode_reward": 445.5019854316085, "step": 78000}
{"episode": 79.0, "batch_reward": 0.2037631947696209, "actor_loss": -21.137909915924073, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.966296672821045, "episode_reward": 318.13824664689974, "step": 79000}
{"episode": 80.0, "batch_reward": 0.20511945194005965, "actor_loss": -21.51981727600098, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.3079607486725, "episode_reward": 346.09565371262784, "step": 80000}
{"episode": 81.0, "batch_reward": 0.207189419478178, "actor_loss": -21.68822608566284, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.914510250091553, "episode_reward": 396.55990743980817, "step": 81000}
{"episode": 82.0, "batch_reward": 0.20988646943867206, "actor_loss": -21.996248302459716, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.4952800273895, "episode_reward": 397.83885024334404, "step": 82000}
{"episode": 83.0, "batch_reward": 0.2125225735157728, "actor_loss": -22.30950838470459, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.860898733139038, "episode_reward": 409.3786444756697, "step": 83000}
{"episode": 84.0, "batch_reward": 0.21471973000466824, "actor_loss": -22.502721130371093, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.5840964317322, "episode_reward": 419.35318216609835, "step": 84000}
{"episode": 85.0, "batch_reward": 0.21693019856512546, "actor_loss": -22.666567266464234, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.04802894592285, "episode_reward": 347.90051354822475, "step": 85000}
{"episode": 86.0, "batch_reward": 0.2189110144674778, "actor_loss": -23.40025824737549, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.4326736927032, "episode_reward": 356.69151977715796, "step": 86000}
{"episode": 87.0, "batch_reward": 0.2184940133392811, "actor_loss": -23.293630504608153, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.792053461074829, "episode_reward": 71.87712625655865, "step": 87000}
{"episode": 88.0, "batch_reward": 0.21896370209753513, "actor_loss": -23.726274532318115, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.0393152236938, "episode_reward": 342.2744606311157, "step": 88000}
{"episode": 89.0, "batch_reward": 0.21856584729254247, "actor_loss": -23.58876959991455, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.622196912765503, "episode_reward": 168.08917758716194, "step": 89000}
{"episode": 90.0, "batch_reward": 0.22009383775293828, "actor_loss": -23.76143608093262, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.47873711586, "episode_reward": 393.4771065841592, "step": 90000}
{"episode": 91.0, "batch_reward": 0.22019054594635964, "actor_loss": -23.773690155029296, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.779350757598877, "episode_reward": 113.64084439449715, "step": 91000}
{"episode": 92.0, "batch_reward": 0.22016086646914482, "actor_loss": -24.266980354309084, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 508.7383894920349, "episode_reward": 316.52265378429894, "step": 92000}
{"episode": 93.0, "batch_reward": 0.2214422627836466, "actor_loss": -24.20528628921509, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.249921321868896, "episode_reward": 402.11617038126377, "step": 93000}
{"episode": 94.0, "batch_reward": 0.22231591176986695, "actor_loss": -24.491564842224122, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 531.2604899406433, "episode_reward": 244.6312414431299, "step": 94000}
{"episode": 95.0, "batch_reward": 0.2232785675674677, "actor_loss": -24.55768207168579, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.4515438079834, "episode_reward": 424.1947799004215, "step": 95000}
{"episode": 96.0, "batch_reward": 0.22609211590886116, "actor_loss": -24.705716674804687, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 526.7040259838104, "episode_reward": 400.9379497667684, "step": 96000}
{"episode": 97.0, "batch_reward": 0.22772410793602466, "actor_loss": -24.854830947875975, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.999932050704956, "episode_reward": 441.4320497758587, "step": 97000}
{"episode": 98.0, "batch_reward": 0.22914659568667411, "actor_loss": -24.777522304534912, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 529.9950971603394, "episode_reward": 380.86361797901316, "step": 98000}
{"episode": 99.0, "batch_reward": 0.23079525263607503, "actor_loss": -25.034105285644532, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.99390983581543, "episode_reward": 433.5495383019464, "step": 99000}
{"episode": 100.0, "batch_reward": 0.23321844393014907, "actor_loss": -25.319315200805665, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 534.3609347343445, "episode_reward": 420.479199432188, "step": 100000}
{"episode": 101.0, "batch_reward": 0.23410207694768906, "actor_loss": -25.446541450500487, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 29.31498384475708, "episode_reward": 429.15171836835515, "step": 101000}
{"episode": 102.0, "batch_reward": 0.23682546657323839, "actor_loss": -25.80218645477295, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 559.4003388881683, "episode_reward": 428.4795826316693, "step": 102000}
{"episode": 103.0, "batch_reward": 0.24012285901606084, "actor_loss": -26.156263149261473, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.177618741989136, "episode_reward": 449.3301159374605, "step": 103000}
{"episode": 104.0, "batch_reward": 0.23965887032449246, "actor_loss": -25.796528728485107, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 554.5378000736237, "episode_reward": 135.44122118879554, "step": 104000}
{"episode": 105.0, "batch_reward": 0.23968192411959172, "actor_loss": -25.82578569793701, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.641700983047485, "episode_reward": 405.4124242374032, "step": 105000}
{"episode": 106.0, "batch_reward": 0.24122889113426207, "actor_loss": -25.882158863067627, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 556.6297996044159, "episode_reward": 389.08568829772366, "step": 106000}
{"episode": 107.0, "batch_reward": 0.24258018216490745, "actor_loss": -25.961714504241943, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.7562255859375, "episode_reward": 390.723519360916, "step": 107000}
{"episode": 108.0, "batch_reward": 0.24428071036934854, "actor_loss": -25.970494846343993, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 555.3407766819, "episode_reward": 392.47663219836136, "step": 108000}
{"episode": 109.0, "batch_reward": 0.245606580093503, "actor_loss": -26.08874895477295, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 22.402296781539917, "episode_reward": 408.28075234328907, "step": 109000}
{"episode": 110.0, "batch_reward": 0.24629930172860623, "actor_loss": -26.286435863494873, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 562.9025375843048, "episode_reward": 339.4924226148529, "step": 110000}
{"episode": 111.0, "batch_reward": 0.24839316968619823, "actor_loss": -26.561588581085203, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 30.270426750183105, "episode_reward": 423.35354645651324, "step": 111000}
{"episode": 112.0, "batch_reward": 0.24760885892808437, "actor_loss": -26.209633193969726, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 561.5295429229736, "episode_reward": 52.902596598261944, "step": 112000}
{"episode": 113.0, "batch_reward": 0.2482770652770996, "actor_loss": -26.217426944732665, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.222681760787964, "episode_reward": 292.09719419996986, "step": 113000}
{"episode": 114.0, "batch_reward": 0.24831747902929782, "actor_loss": -26.35659865951538, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 568.5008606910706, "episode_reward": 444.509118422122, "step": 114000}
{"episode": 115.0, "batch_reward": 0.2502609711587429, "actor_loss": -26.317859546661378, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.697893142700195, "episode_reward": 458.38821626848943, "step": 115000}
{"episode": 116.0, "batch_reward": 0.25083917553722856, "actor_loss": -26.204708599090576, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 564.5889325141907, "episode_reward": 479.41353901869854, "step": 116000}
{"episode": 117.0, "batch_reward": 0.25417104360461235, "actor_loss": -26.45359194946289, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.272538423538208, "episode_reward": 472.71601963080434, "step": 117000}
{"episode": 118.0, "batch_reward": 0.2553966851979494, "actor_loss": -26.238308101654052, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 559.3015677928925, "episode_reward": 413.459375096449, "step": 118000}
{"episode": 119.0, "batch_reward": 0.25656205308437346, "actor_loss": -26.27720544052124, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 19.062846183776855, "episode_reward": 434.1113893397024, "step": 119000}
{"episode": 120.0, "batch_reward": 0.25797423800826075, "actor_loss": -26.603612461090087, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 554.9298913478851, "episode_reward": 451.8141628248968, "step": 120000}
{"episode": 121.0, "batch_reward": 0.26024152600765227, "actor_loss": -26.78328885650635, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 34.550570487976074, "episode_reward": 485.82547967642296, "step": 121000}
{"episode": 122.0, "batch_reward": 0.261885184019804, "actor_loss": -27.00375145339966, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 560.7376971244812, "episode_reward": 449.9009601000146, "step": 122000}
{"episode": 123.0, "batch_reward": 0.2624656496345997, "actor_loss": -27.017934337615966, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.388072967529297, "episode_reward": 475.3024037375208, "step": 123000}
{"episode": 124.0, "batch_reward": 0.26528893035650253, "actor_loss": -27.502646453857423, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 557.5252492427826, "episode_reward": 462.4818411796047, "step": 124000}
{"episode": 125.0, "batch_reward": 0.2666526005864143, "actor_loss": -27.625503189086913, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.142175912857056, "episode_reward": 440.2287099940234, "step": 125000}
{"episode": 126.0, "batch_reward": 0.26814834091067313, "actor_loss": -27.56570464324951, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 557.4886202812195, "episode_reward": 464.04814345341, "step": 126000}
{"episode": 127.0, "batch_reward": 0.2702626199424267, "actor_loss": -27.700682693481447, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 18.151577949523926, "episode_reward": 506.21393103254195, "step": 127000}
{"episode": 128.0, "batch_reward": 0.27110111045837404, "actor_loss": -28.004567401885986, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 554.5396163463593, "episode_reward": 355.75626503661266, "step": 128000}
{"episode": 129.0, "batch_reward": 0.27245964397490025, "actor_loss": -28.034777568817137, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.257554531097412, "episode_reward": 474.37429163221407, "step": 129000}
{"episode": 130.0, "batch_reward": 0.27264153818786147, "actor_loss": -28.08187266921997, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 557.6183543205261, "episode_reward": 108.21070945885474, "step": 130000}
{"episode": 131.0, "batch_reward": 0.27202508081495763, "actor_loss": -28.131631042480468, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 42.27630567550659, "episode_reward": 492.79315560101276, "step": 131000}
{"episode": 132.0, "batch_reward": 0.27383307568728926, "actor_loss": -28.185432388305664, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 559.5833864212036, "episode_reward": 502.89465687207274, "step": 132000}
{"episode": 133.0, "batch_reward": 0.27577417093515394, "actor_loss": -28.369833374023436, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 23.172381162643433, "episode_reward": 474.60292233580714, "step": 133000}
{"episode": 134.0, "batch_reward": 0.2776971088796854, "actor_loss": -28.722530277252197, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 568.963259935379, "episode_reward": 465.5591516917414, "step": 134000}
{"episode": 135.0, "batch_reward": 0.2769681832641363, "actor_loss": -28.56655556488037, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.908632755279541, "episode_reward": 65.86087900203061, "step": 135000}
{"episode": 136.0, "batch_reward": 0.27797193560004235, "actor_loss": -28.646794818878174, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 564.7021515369415, "episode_reward": 479.3953182573578, "step": 136000}
{"episode": 137.0, "batch_reward": 0.27785692243278026, "actor_loss": -28.73177801513672, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.438220739364624, "episode_reward": 466.4183406674735, "step": 137000}
{"episode": 138.0, "batch_reward": 0.28005776478350164, "actor_loss": -29.233936866760253, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 560.7204687595367, "episode_reward": 481.9651145879345, "step": 138000}
{"episode": 139.0, "batch_reward": 0.2812763350009918, "actor_loss": -29.371701622009276, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.25768518447876, "episode_reward": 468.2893220492637, "step": 139000}
{"episode": 140.0, "batch_reward": 0.28256748726964, "actor_loss": -29.365243377685548, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 555.8799564838409, "episode_reward": 206.46749677813685, "step": 140000}
{"episode": 141.0, "batch_reward": 0.2826245755404234, "actor_loss": -29.327610050201415, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 41.44868540763855, "episode_reward": 501.7149305567597, "step": 141000}
{"episode": 142.0, "batch_reward": 0.2831058218628168, "actor_loss": -29.06457530975342, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 547.51833319664, "episode_reward": 430.76471381268215, "step": 142000}
{"episode": 143.0, "batch_reward": 0.28371059243381025, "actor_loss": -28.987962036132814, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 17.923014640808105, "episode_reward": 264.82784164474566, "step": 143000}
{"episode": 144.0, "batch_reward": 0.28417256245017053, "actor_loss": -28.9863885307312, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 554.5968315601349, "episode_reward": 451.0035836393742, "step": 144000}
{"episode": 145.0, "batch_reward": 0.28671114318072793, "actor_loss": -29.143273986816407, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.81739854812622, "episode_reward": 472.8707749325224, "step": 145000}
{"episode": 146.0, "batch_reward": 0.28685554026067256, "actor_loss": -29.158647987365722, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 558.0094079971313, "episode_reward": 303.7725202818279, "step": 146000}
{"episode": 147.0, "batch_reward": 0.28764190985262394, "actor_loss": -29.12686219024658, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 21.108300924301147, "episode_reward": 479.2673315283581, "step": 147000}
{"episode": 148.0, "batch_reward": 0.288945096924901, "actor_loss": -29.463963947296143, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 558.3382308483124, "episode_reward": 492.6606010988431, "step": 148000}
{"episode": 149.0, "batch_reward": 0.2893701598197222, "actor_loss": -29.441329723358155, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 24.397775650024414, "episode_reward": 497.26826502816033, "step": 149000}
{"episode": 150.0, "batch_reward": 0.2908988484889269, "actor_loss": -29.909164546966554, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "step": 150000}
