{"episode": 1.0, "duration": 12.06989598274231, "episode_reward": 7.579367547095394, "step": 1000}
{"episode": 2.0, "duration": 1.177567720413208, "episode_reward": 587.3438790414032, "step": 2000}
{"episode": 3.0, "batch_reward": 0.2843283224439732, "actor_loss": -49.54250198442872, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230756, "duration": 47.78604435920715, "episode_reward": 92.68795495321298, "step": 3000}
{"episode": 4.0, "batch_reward": 0.2047055824548006, "actor_loss": -39.60608071899414, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.62121295928955, "episode_reward": 34.921739649579806, "step": 4000}
{"episode": 5.0, "batch_reward": 0.1672858279198408, "actor_loss": -38.22975861358643, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.697040319442749, "episode_reward": 89.66313732856959, "step": 5000}
{"episode": 6.0, "batch_reward": 0.15322588455677033, "actor_loss": -37.321263259887694, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.18742561340332, "episode_reward": 78.45797052752357, "step": 6000}
{"episode": 7.0, "batch_reward": 0.14084498580545188, "actor_loss": -36.27427095031738, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.681312561035156, "episode_reward": 55.6049883566393, "step": 7000}
{"episode": 8.0, "batch_reward": 0.12831406885385513, "actor_loss": -36.22801842880249, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.5455482006073, "episode_reward": 17.775469085942504, "step": 8000}
{"episode": 9.0, "batch_reward": 0.11469805995374918, "actor_loss": -36.14978025054932, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.469861030578613, "episode_reward": 35.26794967522698, "step": 9000}
{"episode": 10.0, "batch_reward": 0.10988043487071991, "actor_loss": -31.293950225830077, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 4801.58770942688, "episode_reward": 97.04891792780106, "step": 10000}
{"episode": 11.0, "batch_reward": 0.10923512008786201, "actor_loss": -31.440377365112305, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.93546438217163, "episode_reward": 82.32838601477195, "step": 11000}
{"episode": 12.0, "batch_reward": 0.10468524866551161, "actor_loss": -28.343759304046632, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.7241303920746, "episode_reward": 59.302933475041065, "step": 12000}
{"episode": 13.0, "batch_reward": 0.10334162282943725, "actor_loss": -28.693815017700196, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.947615623474121, "episode_reward": 101.00416196383348, "step": 13000}
{"episode": 14.0, "batch_reward": 0.10258805353194475, "actor_loss": -26.234671646118166, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 525.4994761943817, "episode_reward": 102.37750167474086, "step": 14000}
{"episode": 15.0, "batch_reward": 0.1039311207383871, "actor_loss": -26.47544491958618, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.672112226486206, "episode_reward": 128.65449067430438, "step": 15000}
{"episode": 16.0, "batch_reward": 0.1065073015615344, "actor_loss": -25.18907664489746, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 521.4201145172119, "episode_reward": 174.6579476029857, "step": 16000}
{"episode": 17.0, "batch_reward": 0.11159701307117939, "actor_loss": -25.659880489349366, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.414977550506592, "episode_reward": 173.56184327003626, "step": 17000}
{"episode": 18.0, "batch_reward": 0.11407995580881834, "actor_loss": -24.324663722991943, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.3755993843079, "episode_reward": 161.50410622900037, "step": 18000}
{"episode": 19.0, "batch_reward": 0.11914841604232788, "actor_loss": -24.769518306732177, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.011852979660034, "episode_reward": 171.53067332753506, "step": 19000}
{"episode": 20.0, "batch_reward": 0.11897717919945718, "actor_loss": -23.657117179870607, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.5011165142059, "episode_reward": 169.1326011842597, "step": 20000}
{"episode": 21.0, "batch_reward": 0.12230389168858528, "actor_loss": -24.05105207824707, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 28.178060293197632, "episode_reward": 191.2763391721614, "step": 21000}
{"episode": 22.0, "batch_reward": 0.12671769014000892, "actor_loss": -23.702149459838868, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.6794934272766, "episode_reward": 235.6324003495603, "step": 22000}
{"episode": 23.0, "batch_reward": 0.1308040735051036, "actor_loss": -24.17146635055542, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.732317924499512, "episode_reward": 207.62590560803483, "step": 23000}
{"episode": 24.0, "batch_reward": 0.13426821368187666, "actor_loss": -23.832690349578858, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.6806261539459, "episode_reward": 191.7187007365608, "step": 24000}
{"episode": 25.0, "batch_reward": 0.13655190248042345, "actor_loss": -24.167153739929198, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.785930871963501, "episode_reward": 223.7457901271141, "step": 25000}
{"episode": 26.0, "batch_reward": 0.13821462926268577, "actor_loss": -23.576855880737305, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.0417590141296, "episode_reward": 138.99235844034683, "step": 26000}
{"episode": 27.0, "batch_reward": 0.1395942374765873, "actor_loss": -23.61275178527832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.472914218902588, "episode_reward": 259.648852758002, "step": 27000}
{"episode": 28.0, "batch_reward": 0.14473529279232025, "actor_loss": -23.043173225402832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.1604199409485, "episode_reward": 226.24201841726105, "step": 28000}
{"episode": 29.0, "batch_reward": 0.14647234116494656, "actor_loss": -23.62277911758423, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.929704904556274, "episode_reward": 192.02442576220358, "step": 29000}
{"episode": 30.0, "batch_reward": 0.14846140249073506, "actor_loss": -22.971939922332762, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.1159958839417, "episode_reward": 233.33509054678933, "step": 30000}
{"episode": 31.0, "batch_reward": 0.1492363022491336, "actor_loss": -23.261747152328493, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.96370506286621, "episode_reward": 73.15846694600076, "step": 31000}
{"episode": 32.0, "batch_reward": 0.14850817023962737, "actor_loss": -22.187369382858275, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.986065864563, "episode_reward": 219.00072971279727, "step": 32000}
{"episode": 33.0, "batch_reward": 0.15137793550640344, "actor_loss": -22.55446231651306, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.951211929321289, "episode_reward": 198.6156785411862, "step": 33000}
{"episode": 34.0, "batch_reward": 0.15204762261360882, "actor_loss": -21.768275645256043, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.7118237018585, "episode_reward": 204.93684195478156, "step": 34000}
{"episode": 35.0, "batch_reward": 0.1539233788624406, "actor_loss": -22.003806154251098, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.873260259628296, "episode_reward": 224.37615459592612, "step": 35000}
{"episode": 36.0, "batch_reward": 0.15527229289710523, "actor_loss": -20.833115036010742, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 515.9680140018463, "episode_reward": 176.9367370777393, "step": 36000}
{"episode": 37.0, "batch_reward": 0.15674835474789142, "actor_loss": -20.91539796447754, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.569234609603882, "episode_reward": 162.1858980355162, "step": 37000}
{"episode": 38.0, "batch_reward": 0.15712576714903118, "actor_loss": -18.700991642475127, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.268717288971, "episode_reward": 139.80231114753934, "step": 38000}
{"episode": 39.0, "batch_reward": 0.15559670983254908, "actor_loss": -18.52137581539154, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.59402585029602, "episode_reward": 187.821170800108, "step": 39000}
{"episode": 40.0, "batch_reward": 0.15628751736879348, "actor_loss": -15.79947781944275, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.7434906959534, "episode_reward": 143.43612507677057, "step": 40000}
{"episode": 41.0, "batch_reward": 0.15659151743352412, "actor_loss": -16.42684644937515, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.91082501411438, "episode_reward": 194.41913350471336, "step": 41000}
{"episode": 42.0, "batch_reward": 0.157167809702456, "actor_loss": -14.282181870937347, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 508.7188913822174, "episode_reward": 185.6670023143085, "step": 42000}
{"episode": 43.0, "batch_reward": 0.15860221815109252, "actor_loss": -14.108804121017457, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.700326919555664, "episode_reward": 174.2506091529324, "step": 43000}
{"episode": 44.0, "batch_reward": 0.15922331178188323, "actor_loss": -13.53151004076004, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.4534747600555, "episode_reward": 225.93012312378087, "step": 44000}
{"episode": 45.0, "batch_reward": 0.16033204077184202, "actor_loss": -13.69590180015564, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.537731647491455, "episode_reward": 259.33634212280214, "step": 45000}
{"episode": 46.0, "batch_reward": 0.16239410166442395, "actor_loss": -13.219445528030395, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.4043092727661, "episode_reward": 180.49791139288214, "step": 46000}
{"episode": 47.0, "batch_reward": 0.16272325137257576, "actor_loss": -13.24373207139969, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.58784818649292, "episode_reward": 220.79964629306497, "step": 47000}
{"episode": 48.0, "batch_reward": 0.16293514947593213, "actor_loss": -12.59113380241394, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.7364478111267, "episode_reward": 102.51856858788263, "step": 48000}
{"episode": 49.0, "batch_reward": 0.16257315522432328, "actor_loss": -12.101943790197373, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.224608421325684, "episode_reward": 208.87308813154718, "step": 49000}
{"episode": 50.0, "batch_reward": 0.16305353677272796, "actor_loss": -10.96398949599266, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 517.463356256485, "episode_reward": 169.54016305194347, "step": 50000}
{"episode": 51.0, "batch_reward": 0.16280711188912392, "actor_loss": -11.082191112756728, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 28.00386953353882, "episode_reward": 155.95367166227663, "step": 51000}
{"episode": 52.0, "batch_reward": 0.16359648177027702, "actor_loss": -9.17529837846756, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 516.33065366745, "episode_reward": 167.65095105338787, "step": 52000}
{"episode": 53.0, "batch_reward": 0.1627959612160921, "actor_loss": -9.198519097089768, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.030670404434204, "episode_reward": 52.18503779999386, "step": 53000}
{"episode": 54.0, "batch_reward": 0.16209723576903343, "actor_loss": -7.354158015012741, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 521.5622198581696, "episode_reward": 183.25423335950802, "step": 54000}
{"episode": 55.0, "batch_reward": 0.16239934624731542, "actor_loss": -7.644983376026153, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.500815868377686, "episode_reward": 163.9321205195701, "step": 55000}
{"episode": 56.0, "batch_reward": 0.16238258838653563, "actor_loss": -7.816833688020706, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.1682105064392, "episode_reward": 201.81223849059884, "step": 56000}
{"episode": 57.0, "batch_reward": 0.16313086584210396, "actor_loss": -8.30071974158287, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.532134771347046, "episode_reward": 291.60299166732074, "step": 57000}
{"episode": 58.0, "batch_reward": 0.16494717425107955, "actor_loss": -10.020269441127777, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.6723499298096, "episode_reward": 257.65398940564035, "step": 58000}
{"episode": 59.0, "batch_reward": 0.16677518387138843, "actor_loss": -10.256498994350434, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.457281589508057, "episode_reward": 266.17716254348096, "step": 59000}
{"episode": 60.0, "batch_reward": 0.16869883684813977, "actor_loss": -12.53937063074112, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.5167446136475, "episode_reward": 236.8307231535765, "step": 60000}
{"episode": 61.0, "batch_reward": 0.1690439520329237, "actor_loss": -12.574591670513152, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.959599018096924, "episode_reward": 157.73547461446714, "step": 61000}
{"episode": 62.0, "batch_reward": 0.16920970073342323, "actor_loss": -12.652598020553588, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.24866938591, "episode_reward": 302.0026798782536, "step": 62000}
{"episode": 63.0, "batch_reward": 0.17141972768306732, "actor_loss": -12.852237632751464, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.069226026535034, "episode_reward": 280.95962703941046, "step": 63000}
{"episode": 64.0, "batch_reward": 0.17271697118878365, "actor_loss": -13.557555002212524, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.3953354358673, "episode_reward": 252.65616083033757, "step": 64000}
{"episode": 65.0, "batch_reward": 0.1746317021548748, "actor_loss": -13.879105688095093, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.732341289520264, "episode_reward": 256.50468844227555, "step": 65000}
{"episode": 66.0, "batch_reward": 0.17638061556220055, "actor_loss": -14.943181919097901, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.889570236206, "episode_reward": 291.7375190374519, "step": 66000}
{"episode": 67.0, "batch_reward": 0.17644510382413864, "actor_loss": -14.798158756256104, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.392535209655762, "episode_reward": 66.06310681762167, "step": 67000}
{"episode": 68.0, "batch_reward": 0.17686800993978977, "actor_loss": -14.910627737998963, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 516.7783761024475, "episode_reward": 378.52793541754954, "step": 68000}
{"episode": 69.0, "batch_reward": 0.180042421489954, "actor_loss": -15.225051427841187, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.515945196151733, "episode_reward": 383.8144994034664, "step": 69000}
{"episode": 70.0, "batch_reward": 0.18265610082447528, "actor_loss": -16.07006406211853, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.7403995990753, "episode_reward": 348.0890045579384, "step": 70000}
{"episode": 71.0, "batch_reward": 0.18462710379064082, "actor_loss": -16.278835762023927, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.640785455703735, "episode_reward": 337.6580651210119, "step": 71000}
{"episode": 72.0, "batch_reward": 0.187080652192235, "actor_loss": -17.325189149856566, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.8840935230255, "episode_reward": 366.36939712594125, "step": 72000}
{"episode": 73.0, "batch_reward": 0.18980260275304317, "actor_loss": -17.526001888275147, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.335657119750977, "episode_reward": 351.7933605599576, "step": 73000}
{"episode": 74.0, "batch_reward": 0.19201521776616573, "actor_loss": -18.472130479812623, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 523.5641973018646, "episode_reward": 392.05106394244206, "step": 74000}
{"episode": 75.0, "batch_reward": 0.19485398995876313, "actor_loss": -18.769492149353027, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.349742412567139, "episode_reward": 358.36517056942847, "step": 75000}
{"episode": 76.0, "batch_reward": 0.19656292064487935, "actor_loss": -19.850114139556887, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.7713973522186, "episode_reward": 390.8510348690541, "step": 76000}
{"episode": 77.0, "batch_reward": 0.19926694515347482, "actor_loss": -20.104544527053832, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.627787351608276, "episode_reward": 360.8749544470331, "step": 77000}
{"episode": 78.0, "batch_reward": 0.20129385060071944, "actor_loss": -20.784528438568117, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 521.4007048606873, "episode_reward": 445.5019854316085, "step": 78000}
{"episode": 79.0, "batch_reward": 0.2037631947696209, "actor_loss": -21.137909915924073, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.966296672821045, "episode_reward": 318.13824664689974, "step": 79000}
{"episode": 80.0, "batch_reward": 0.20511945194005965, "actor_loss": -21.51981727600098, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.3079607486725, "episode_reward": 346.09565371262784, "step": 80000}
{"episode": 81.0, "batch_reward": 0.207189419478178, "actor_loss": -21.68822608566284, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.914510250091553, "episode_reward": 396.55990743980817, "step": 81000}
{"episode": 82.0, "batch_reward": 0.20988646943867206, "actor_loss": -21.996248302459716, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 518.4952800273895, "episode_reward": 397.83885024334404, "step": 82000}
{"episode": 83.0, "batch_reward": 0.2125225735157728, "actor_loss": -22.30950838470459, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.860898733139038, "episode_reward": 409.3786444756697, "step": 83000}
{"episode": 84.0, "batch_reward": 0.21471973000466824, "actor_loss": -22.502721130371093, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.5840964317322, "episode_reward": 419.35318216609835, "step": 84000}
{"episode": 85.0, "batch_reward": 0.21693019856512546, "actor_loss": -22.666567266464234, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.04802894592285, "episode_reward": 347.90051354822475, "step": 85000}
{"episode": 86.0, "batch_reward": 0.2189110144674778, "actor_loss": -23.40025824737549, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 522.4326736927032, "episode_reward": 356.69151977715796, "step": 86000}
{"episode": 87.0, "batch_reward": 0.2184940133392811, "actor_loss": -23.293630504608153, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.792053461074829, "episode_reward": 71.87712625655865, "step": 87000}
{"episode": 88.0, "batch_reward": 0.21896370209753513, "actor_loss": -23.726274532318115, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 519.0393152236938, "episode_reward": 342.2744606311157, "step": 88000}
{"episode": 89.0, "batch_reward": 0.21856584729254247, "actor_loss": -23.58876959991455, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 15.622196912765503, "episode_reward": 168.08917758716194, "step": 89000}
{"episode": 90.0, "batch_reward": 0.22009383775293828, "actor_loss": -23.76143608093262, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 520.47873711586, "episode_reward": 393.4771065841592, "step": 90000}
{"episode": 91.0, "batch_reward": 0.22019054594635964, "actor_loss": -23.773690155029296, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 27.779350757598877, "episode_reward": 113.64084439449715, "step": 91000}
{"episode": 92.0, "batch_reward": 0.22016086646914482, "actor_loss": -24.266980354309084, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 508.7383894920349, "episode_reward": 316.52265378429894, "step": 92000}
{"episode": 93.0, "batch_reward": 0.2214422627836466, "actor_loss": -24.20528628921509, "actor_target_entropy": -6.0, "alpha_value": 0.008529113283230454, "duration": 16.249921321868896, "episode_reward": 402.11617038126377, "step": 93000}
