{"episode_reward": 0.0, "episode": 1.0, "duration": 19.69581627845764, "step": 1000}
{"episode_reward": 4.859792814687425, "episode": 2.0, "duration": 1.857886552810669, "step": 2000}
{"episode_reward": 550.1572824113056, "episode": 3.0, "batch_reward": 0.26769400371357216, "critic_loss": 0.0365295266739095, "actor_loss": -13.292957509614054, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 69.85412669181824, "step": 3000}
{"episode_reward": 88.6302714994046, "episode": 4.0, "batch_reward": 0.19878504636883737, "critic_loss": 0.040015490487217906, "actor_loss": -10.843620587825775, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.963783264160156, "step": 4000}
{"episode_reward": 62.70266711348377, "episode": 5.0, "batch_reward": 0.17344507134705783, "critic_loss": 0.05314008101820946, "actor_loss": -9.56327905201912, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.63141965866089, "step": 5000}
{"episode_reward": 127.9908915729296, "episode": 6.0, "batch_reward": 0.16113250144571065, "critic_loss": 0.06279046940058469, "actor_loss": -10.128326650619506, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.14536428451538, "step": 6000}
{"episode_reward": 86.9677542296055, "episode": 7.0, "batch_reward": 0.1559473870843649, "critic_loss": 0.08019350930675864, "actor_loss": -9.723889194488525, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.64213490486145, "step": 7000}
{"episode_reward": 183.21498124240858, "episode": 8.0, "batch_reward": 0.15288780802488328, "critic_loss": 0.08315231618657708, "actor_loss": -10.483694874763488, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.0551860332489, "step": 8000}
{"episode_reward": 55.48073457173128, "episode": 9.0, "batch_reward": 0.14449839074164628, "critic_loss": 0.08160292448475957, "actor_loss": -10.814036816596985, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.070148944854736, "step": 9000}
{"episode_reward": 113.73058252148016, "episode": 10.0, "batch_reward": 0.13849975164979697, "critic_loss": 0.09032287278026342, "actor_loss": -10.907720944404602, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.506032943725586, "step": 10000}
{"episode_reward": 49.953243463072376, "episode": 11.0, "batch_reward": 0.13588869960606098, "critic_loss": 0.09270643303915858, "actor_loss": -11.876070119857788, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.40687370300293, "step": 11000}
{"episode_reward": 224.2425450641605, "episode": 12.0, "batch_reward": 0.1427449148222804, "critic_loss": 0.09449046960845589, "actor_loss": -12.554672288894654, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.161765098571777, "step": 12000}
{"episode_reward": 205.11395685035092, "episode": 13.0, "batch_reward": 0.14915671429038047, "critic_loss": 0.10557189241424203, "actor_loss": -12.628993278503417, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.33425283432007, "step": 13000}
{"episode_reward": 279.1433562588629, "episode": 14.0, "batch_reward": 0.16249459785968065, "critic_loss": 0.14160890378803015, "actor_loss": -14.043947514533997, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.489665508270264, "step": 14000}
{"episode_reward": 339.4446198522956, "episode": 15.0, "batch_reward": 0.1697232024371624, "critic_loss": 0.12912854006886482, "actor_loss": -14.619676181793213, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.912778854370117, "step": 15000}
{"episode_reward": 132.7991743345064, "episode": 16.0, "batch_reward": 0.16490918461233378, "critic_loss": 0.13507310366630554, "actor_loss": -14.807174171447754, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.953392505645752, "step": 16000}
{"episode_reward": 70.34547522202064, "episode": 17.0, "batch_reward": 0.15903332767635583, "critic_loss": 0.13414307687431573, "actor_loss": -14.624682783126831, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.565819263458252, "step": 17000}
{"episode_reward": 65.33466389173797, "episode": 18.0, "batch_reward": 0.15781694934517146, "critic_loss": 0.17392720708251, "actor_loss": -14.732346633911133, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.47319746017456, "step": 18000}
{"episode_reward": 246.94511415503018, "episode": 19.0, "batch_reward": 0.161017808444798, "critic_loss": 0.17906606967747213, "actor_loss": -15.058357051849365, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.451759815216064, "step": 19000}
{"episode_reward": 126.66979140705828, "episode": 20.0, "batch_reward": 0.16003922470659018, "critic_loss": 0.18790077616274356, "actor_loss": -15.554719799041749, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.642921686172485, "step": 20000}
{"episode_reward": 143.52719186240444, "episode": 21.0, "batch_reward": 0.1610916753411293, "critic_loss": 0.19380182231217621, "actor_loss": -15.625423637390137, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.9629282951355, "step": 21000}
{"episode_reward": 246.46447458431064, "episode": 22.0, "batch_reward": 0.16047853036224843, "critic_loss": 0.20123827473819256, "actor_loss": -16.459688709259034, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.941426277160645, "step": 22000}
{"episode_reward": 99.17116528151674, "episode": 23.0, "batch_reward": 0.15932285360991955, "critic_loss": 0.21664742525666952, "actor_loss": -16.121540313720704, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.23800253868103, "step": 23000}
{"episode_reward": 117.07202455352531, "episode": 24.0, "batch_reward": 0.15493919873982667, "critic_loss": 0.20490046916157006, "actor_loss": -16.23793500518799, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.2201828956604, "step": 24000}
{"episode_reward": 49.82182718786686, "episode": 25.0, "batch_reward": 0.1550915784910321, "critic_loss": 0.21014768365770578, "actor_loss": -16.201121099472047, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.429866790771484, "step": 25000}
{"episode_reward": 153.52865630469825, "episode": 26.0, "batch_reward": 0.15454060473293066, "critic_loss": 0.21538544883579017, "actor_loss": -16.250104364395142, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.639809131622314, "step": 26000}
{"episode_reward": 274.8909029706425, "episode": 27.0, "batch_reward": 0.15759281445294618, "critic_loss": 0.2563883261978626, "actor_loss": -16.740262742996215, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.370097875595093, "step": 27000}
{"episode_reward": 119.26335273227787, "episode": 28.0, "batch_reward": 0.158193100258708, "critic_loss": 0.2511940699070692, "actor_loss": -17.201394264221193, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.26112675666809, "step": 28000}
{"episode_reward": 185.41659199757797, "episode": 29.0, "batch_reward": 0.15858074939996003, "critic_loss": 0.2691101866960526, "actor_loss": -17.086478424072265, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 26.178240299224854, "step": 29000}
{"episode_reward": 244.46726991969783, "episode": 30.0, "batch_reward": 0.16405689320713282, "critic_loss": 0.2663728250712156, "actor_loss": -17.464495483398437, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.780883073806763, "step": 30000}
{"episode_reward": 340.2964381785491, "episode": 31.0, "batch_reward": 0.16883005918562413, "critic_loss": 0.25256745316088197, "actor_loss": -18.060031019210815, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.23686909675598, "step": 31000}
{"episode_reward": 299.57239860621615, "episode": 32.0, "batch_reward": 0.17292309415340423, "critic_loss": 0.26227487868070604, "actor_loss": -18.629713653564455, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.429832696914673, "step": 32000}
{"episode_reward": 209.60631768167113, "episode": 33.0, "batch_reward": 0.17245530931651593, "critic_loss": 0.2827046255320311, "actor_loss": -18.90144941520691, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.855356693267822, "step": 33000}
{"episode_reward": 144.53033363310973, "episode": 34.0, "batch_reward": 0.17190444007515907, "critic_loss": 0.2872878717035055, "actor_loss": -18.631152238845825, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.40639328956604, "step": 34000}
{"episode_reward": 178.24264064084716, "episode": 35.0, "batch_reward": 0.17068156693875788, "critic_loss": 0.28277465687692166, "actor_loss": -18.913675518035888, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.046926736831665, "step": 35000}
{"episode_reward": 142.8009588976819, "episode": 36.0, "batch_reward": 0.17217526853084564, "critic_loss": 0.302725809276104, "actor_loss": -19.15257207298279, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.994503021240234, "step": 36000}
{"episode_reward": 349.13978503092403, "episode": 37.0, "batch_reward": 0.1764732206761837, "critic_loss": 0.32906506417691705, "actor_loss": -19.39281990814209, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.895699739456177, "step": 37000}
{"episode_reward": 310.77043581252184, "episode": 38.0, "batch_reward": 0.18010838970541954, "critic_loss": 0.3272768506407738, "actor_loss": -19.71490737915039, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.7559175491333, "step": 38000}
{"episode_reward": 218.37071106700057, "episode": 39.0, "batch_reward": 0.18273698551952838, "critic_loss": 0.3306046627908945, "actor_loss": -20.00746726989746, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.504668712615967, "step": 39000}
{"episode_reward": 427.61974378424856, "episode": 40.0, "batch_reward": 0.1879266509562731, "critic_loss": 0.3461560306251049, "actor_loss": -20.276357791900633, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.832393407821655, "step": 40000}
{"episode_reward": 193.88155640613894, "episode": 41.0, "batch_reward": 0.18628633934259414, "critic_loss": 0.30175295208394526, "actor_loss": -20.220318168640137, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.51205611228943, "step": 41000}
{"episode_reward": 148.0076963149379, "episode": 42.0, "batch_reward": 0.18466650077700614, "critic_loss": 0.3179421870410442, "actor_loss": -20.295660152435303, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.253483295440674, "step": 42000}
{"episode_reward": 61.329198300682535, "episode": 43.0, "batch_reward": 0.18354516382515432, "critic_loss": 0.2993671124428511, "actor_loss": -20.18074012374878, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.09376287460327, "step": 43000}
{"episode_reward": 225.86797940255116, "episode": 44.0, "batch_reward": 0.1844223274588585, "critic_loss": 0.3115997385829687, "actor_loss": -20.393146919250487, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.031697988510132, "step": 44000}
{"episode_reward": 316.53294219657266, "episode": 45.0, "batch_reward": 0.18767009173333646, "critic_loss": 0.2953628239184618, "actor_loss": -20.594490604400633, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.20260500907898, "step": 45000}
{"episode_reward": 282.4237699830278, "episode": 46.0, "batch_reward": 0.18841892080008985, "critic_loss": 0.3106228743195534, "actor_loss": -20.564469352722167, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.87583327293396, "step": 46000}
{"episode_reward": 115.00135212240673, "episode": 47.0, "batch_reward": 0.18807490642368793, "critic_loss": 0.30181587575376034, "actor_loss": -20.393455978393554, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.83731245994568, "step": 47000}
{"episode_reward": 291.14304136199854, "episode": 48.0, "batch_reward": 0.1906038323044777, "critic_loss": 0.308087123349309, "actor_loss": -20.649464248657228, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.19999051094055, "step": 48000}
{"episode_reward": 328.48677305106713, "episode": 49.0, "batch_reward": 0.19323137174546717, "critic_loss": 0.3386423011422157, "actor_loss": -21.026317226409912, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.34418034553528, "step": 49000}
{"episode_reward": 167.60712996200925, "episode": 50.0, "batch_reward": 0.19443133699893952, "critic_loss": 0.3371895637214184, "actor_loss": -20.879722854614258, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.94993019104004, "step": 50000}
{"episode_reward": 428.9003778599213, "episode": 51.0, "batch_reward": 0.19692180821299554, "critic_loss": 0.3586614376455545, "actor_loss": -21.090582492828368, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.26160407066345, "step": 51000}
{"episode_reward": 161.04390904867273, "episode": 52.0, "batch_reward": 0.19725992393493652, "critic_loss": 0.3195213217586279, "actor_loss": -21.12095567321777, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.62160611152649, "step": 52000}
{"episode_reward": 196.5150682360571, "episode": 53.0, "batch_reward": 0.19760986751317977, "critic_loss": 0.333686520382762, "actor_loss": -21.20977347946167, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.105690717697144, "step": 53000}
{"episode_reward": 324.81836149499173, "episode": 54.0, "batch_reward": 0.19886748388409614, "critic_loss": 0.345587844774127, "actor_loss": -21.29440943145752, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.578795671463013, "step": 54000}
{"episode_reward": 152.10094259515782, "episode": 55.0, "batch_reward": 0.19872198033332825, "critic_loss": 0.3583132895380259, "actor_loss": -21.354198669433593, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.01825499534607, "step": 55000}
{"episode_reward": 382.2829175167833, "episode": 56.0, "batch_reward": 0.20251992379128933, "critic_loss": 0.3558167132586241, "actor_loss": -21.533892429351805, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.762848377227783, "step": 56000}
{"episode_reward": 440.82287875372936, "episode": 57.0, "batch_reward": 0.20531653314828874, "critic_loss": 0.3410117435157299, "actor_loss": -21.842619842529295, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.995607376098633, "step": 57000}
{"episode_reward": 297.4532380739174, "episode": 58.0, "batch_reward": 0.20794138829410075, "critic_loss": 0.3639449672996998, "actor_loss": -22.170030223846435, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.0638325214386, "step": 58000}
{"episode_reward": 313.2521538276582, "episode": 59.0, "batch_reward": 0.20797070720791816, "critic_loss": 0.34733206072449685, "actor_loss": -21.990023097991944, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.242100954055786, "step": 59000}
{"episode_reward": 125.4625258882799, "episode": 60.0, "batch_reward": 0.20717829766869544, "critic_loss": 0.35535117714107034, "actor_loss": -22.157706825256348, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.211188316345215, "step": 60000}
{"episode_reward": 315.5222992892775, "episode": 61.0, "batch_reward": 0.2108287972509861, "critic_loss": 0.36173281678557395, "actor_loss": -22.27945520019531, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.31089925765991, "step": 61000}
{"episode_reward": 445.4156155180123, "episode": 62.0, "batch_reward": 0.21332011817395688, "critic_loss": 0.34381357246637345, "actor_loss": -22.542514743804933, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.785892963409424, "step": 62000}
{"episode_reward": 104.5957172633636, "episode": 63.0, "batch_reward": 0.21222044529020787, "critic_loss": 0.34941772623360157, "actor_loss": -22.360277271270753, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.29319190979004, "step": 63000}
{"episode_reward": 292.23201173078155, "episode": 64.0, "batch_reward": 0.2110414290726185, "critic_loss": 0.34742114329338075, "actor_loss": -22.309221179962158, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.61902165412903, "step": 64000}
{"episode_reward": 43.85662983461648, "episode": 65.0, "batch_reward": 0.20976054844260217, "critic_loss": 0.35438474364578726, "actor_loss": -22.26258260345459, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.205676555633545, "step": 65000}
{"episode_reward": 275.372490463884, "episode": 66.0, "batch_reward": 0.21212700183689595, "critic_loss": 0.3430893761366606, "actor_loss": -22.349916736602783, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.831535577774048, "step": 66000}
{"episode_reward": 334.5406171487713, "episode": 67.0, "batch_reward": 0.21372196497023105, "critic_loss": 0.34474479995667934, "actor_loss": -22.58329680633545, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.215697765350342, "step": 67000}
{"episode_reward": 290.71477646908716, "episode": 68.0, "batch_reward": 0.21442520134150983, "critic_loss": 0.3815216737538576, "actor_loss": -22.555757232666014, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.504613399505615, "step": 68000}
{"episode_reward": 248.06377717793796, "episode": 69.0, "batch_reward": 0.21412868358194828, "critic_loss": 0.3749821128696203, "actor_loss": -22.295414821624757, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.97030735015869, "step": 69000}
{"episode_reward": 357.1496371954994, "episode": 70.0, "batch_reward": 0.21589555896818638, "critic_loss": 0.39788366812467574, "actor_loss": -22.55761930847168, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.0868022441864, "step": 70000}
{"episode_reward": 87.52750705680614, "episode": 71.0, "batch_reward": 0.2153908351957798, "critic_loss": 0.36989056257903574, "actor_loss": -22.517331798553467, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.58247399330139, "step": 71000}
{"episode_reward": 163.33921794536917, "episode": 72.0, "batch_reward": 0.21433560360968112, "critic_loss": 0.37620496515929697, "actor_loss": -22.25544105911255, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.950286149978638, "step": 72000}
{"episode_reward": 190.7263449726383, "episode": 73.0, "batch_reward": 0.21551914079487325, "critic_loss": 0.39948663108050825, "actor_loss": -22.413351596832275, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.778130531311035, "step": 73000}
{"episode_reward": 445.82951267411124, "episode": 74.0, "batch_reward": 0.21636755537986754, "critic_loss": 0.3858466095328331, "actor_loss": -22.49565007019043, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.34495711326599, "step": 74000}
{"episode_reward": 159.82793668534262, "episode": 75.0, "batch_reward": 0.21611320015788077, "critic_loss": 0.3720483319014311, "actor_loss": -22.363965560913087, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.56117010116577, "step": 75000}
{"episode_reward": 463.61588447717924, "episode": 76.0, "batch_reward": 0.2195120942145586, "critic_loss": 0.37398534585535526, "actor_loss": -22.585770126342773, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.603163480758667, "step": 76000}
{"episode_reward": 267.3369399399257, "episode": 77.0, "batch_reward": 0.21899987848103047, "critic_loss": 0.356202346727252, "actor_loss": -22.525647701263427, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.63118267059326, "step": 77000}
{"episode_reward": 71.45251570614212, "episode": 78.0, "batch_reward": 0.218777980864048, "critic_loss": 0.35432868817448615, "actor_loss": -22.471267349243163, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.06715154647827, "step": 78000}
{"episode_reward": 262.10675728189796, "episode": 79.0, "batch_reward": 0.2197185873836279, "critic_loss": 0.3629906579852104, "actor_loss": -22.546912250518798, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.90715217590332, "step": 79000}
{"episode_reward": 463.52989701415726, "episode": 80.0, "batch_reward": 0.22220689114928246, "critic_loss": 0.36853816056251526, "actor_loss": -22.734479038238526, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.198885202407837, "step": 80000}
{"episode_reward": 136.3605080671095, "episode": 81.0, "batch_reward": 0.221181543469429, "critic_loss": 0.3783558694571257, "actor_loss": -22.651590255737304, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.56365728378296, "step": 81000}
{"episode_reward": 217.2463111683447, "episode": 82.0, "batch_reward": 0.219563278272748, "critic_loss": 0.3947203602492809, "actor_loss": -22.25121868133545, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.513671398162842, "step": 82000}
{"episode_reward": 147.9265140879609, "episode": 83.0, "batch_reward": 0.22025780528783798, "critic_loss": 0.4081828863024712, "actor_loss": -22.457285785675047, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.496952772140503, "step": 83000}
{"episode_reward": 423.97160048224174, "episode": 84.0, "batch_reward": 0.22380113483965397, "critic_loss": 0.38220175382494925, "actor_loss": -22.628291065216064, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.248380661010742, "step": 84000}
{"episode_reward": 451.57576048695154, "episode": 85.0, "batch_reward": 0.22611665181815624, "critic_loss": 0.3929607172459364, "actor_loss": -22.857999340057372, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.22496747970581, "step": 85000}
{"episode_reward": 465.72662308497377, "episode": 86.0, "batch_reward": 0.22897891050577163, "critic_loss": 0.4040334736257791, "actor_loss": -23.095055103302002, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.791807413101196, "step": 86000}
{"episode_reward": 541.897265847468, "episode": 87.0, "batch_reward": 0.2303287181407213, "critic_loss": 0.3939850674122572, "actor_loss": -23.16241944503784, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.23478102684021, "step": 87000}
{"episode_reward": 66.31293044493569, "episode": 88.0, "batch_reward": 0.2300426290035248, "critic_loss": 0.3995843918919563, "actor_loss": -22.91158295059204, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.708661794662476, "step": 88000}
{"episode_reward": 381.84619556839345, "episode": 89.0, "batch_reward": 0.23128493855893612, "critic_loss": 0.39434688571095466, "actor_loss": -23.085477100372316, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.403345584869385, "step": 89000}
{"episode_reward": 414.2835721235466, "episode": 90.0, "batch_reward": 0.23502678307890892, "critic_loss": 0.41528718772530554, "actor_loss": -23.434346881866453, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.10421109199524, "step": 90000}
{"episode_reward": 376.6572590361712, "episode": 91.0, "batch_reward": 0.23546691347658635, "critic_loss": 0.39006391656398776, "actor_loss": -23.362557315826415, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.36215353012085, "step": 91000}
{"episode_reward": 191.59024869621283, "episode": 92.0, "batch_reward": 0.23578580547869205, "critic_loss": 0.3679595132768154, "actor_loss": -23.475549629211425, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.832384824752808, "step": 92000}
{"episode_reward": 464.11863395396153, "episode": 93.0, "batch_reward": 0.23740721395611764, "critic_loss": 0.40304968048632145, "actor_loss": -23.554113586425782, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.41433310508728, "step": 93000}
{"episode_reward": 407.85096382615103, "episode": 94.0, "batch_reward": 0.24053916001319886, "critic_loss": 0.40873689523339274, "actor_loss": -23.915023471832274, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.642856121063232, "step": 94000}
{"episode_reward": 512.6560829241809, "episode": 95.0, "batch_reward": 0.242998466655612, "critic_loss": 0.4173040868192911, "actor_loss": -24.19639117050171, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.084434509277344, "step": 95000}
{"episode_reward": 472.1698587106198, "episode": 96.0, "batch_reward": 0.24546853506565094, "critic_loss": 0.3839851757735014, "actor_loss": -24.19831554412842, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.03675103187561, "step": 96000}
{"episode_reward": 497.32611075694416, "episode": 97.0, "batch_reward": 0.24722866974771024, "critic_loss": 0.3922911764830351, "actor_loss": -24.48063435745239, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.305299997329712, "step": 97000}
{"episode_reward": 286.77507053483055, "episode": 98.0, "batch_reward": 0.2472584937363863, "critic_loss": 0.397748622238636, "actor_loss": -24.454888534545898, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.166817665100098, "step": 98000}
{"episode_reward": 427.6350196105555, "episode": 99.0, "batch_reward": 0.24970639342069625, "critic_loss": 0.41075813880562784, "actor_loss": -24.53745212173462, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.041780948638916, "step": 99000}
{"episode_reward": 309.79901480428816, "episode": 100.0, "batch_reward": 0.25019949808716774, "critic_loss": 0.42075990499556065, "actor_loss": -24.62633652114868, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.854085206985474, "step": 100000}
{"episode_reward": 356.8660818143227, "episode": 101.0, "batch_reward": 0.2518742697238922, "critic_loss": 0.40995461831986907, "actor_loss": -24.77204140472412, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.43274545669556, "step": 101000}
{"episode_reward": 517.5445536925425, "episode": 102.0, "batch_reward": 0.25491361385583877, "critic_loss": 0.41668451981246474, "actor_loss": -25.05518762588501, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.516204833984375, "step": 102000}
{"episode_reward": 555.1568588603752, "episode": 103.0, "batch_reward": 0.255743396833539, "critic_loss": 0.44501609598100184, "actor_loss": -24.94741548538208, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.327744007110596, "step": 103000}
{"episode_reward": 119.54515113875657, "episode": 104.0, "batch_reward": 0.2548552025258541, "critic_loss": 0.399189553886652, "actor_loss": -24.83983983230591, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.51353096961975, "step": 104000}
{"episode_reward": 112.1077886847854, "episode": 105.0, "batch_reward": 0.25409484864771364, "critic_loss": 0.4565951939225197, "actor_loss": -24.85240258026123, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.939629077911377, "step": 105000}
{"episode_reward": 245.17337005824533, "episode": 106.0, "batch_reward": 0.25396363909542563, "critic_loss": 0.4492864408791065, "actor_loss": -24.796207679748534, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.87360692024231, "step": 106000}
{"episode_reward": 469.67398441983596, "episode": 107.0, "batch_reward": 0.2568922482728958, "critic_loss": 0.44797532702982423, "actor_loss": -24.936365928649902, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.095898151397705, "step": 107000}
{"episode_reward": 436.1041326886565, "episode": 108.0, "batch_reward": 0.25780313402414323, "critic_loss": 0.44018985933065413, "actor_loss": -25.08811507034302, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.32893180847168, "step": 108000}
{"episode_reward": 225.27222879537612, "episode": 109.0, "batch_reward": 0.2578920305669308, "critic_loss": 0.45818902644515036, "actor_loss": -25.04599521636963, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.47018837928772, "step": 109000}
{"episode_reward": 493.26998047895756, "episode": 110.0, "batch_reward": 0.2594589898586273, "critic_loss": 0.42717606702446936, "actor_loss": -25.315720191955567, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.860511541366577, "step": 110000}
{"episode_reward": 504.8253173010094, "episode": 111.0, "batch_reward": 0.26100705233216287, "critic_loss": 0.4831109379380941, "actor_loss": -25.336163570404054, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.25543975830078, "step": 111000}
{"episode_reward": 275.5082969604904, "episode": 112.0, "batch_reward": 0.2622015177309513, "critic_loss": 0.46192971067130567, "actor_loss": -25.427249893188478, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.844769716262817, "step": 112000}
{"episode_reward": 569.1321124208737, "episode": 113.0, "batch_reward": 0.26501043190062046, "critic_loss": 0.45955617631971835, "actor_loss": -25.566539539337157, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.914352893829346, "step": 113000}
{"episode_reward": 419.1562724796863, "episode": 114.0, "batch_reward": 0.2670103132277727, "critic_loss": 0.4913907984942198, "actor_loss": -25.735905742645265, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.57513451576233, "step": 114000}
{"episode_reward": 287.44418552942534, "episode": 115.0, "batch_reward": 0.26787139911949637, "critic_loss": 0.5315691630393267, "actor_loss": -25.64154511642456, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.367820501327515, "step": 115000}
{"episode_reward": 460.6573527827626, "episode": 116.0, "batch_reward": 0.26867294956743715, "critic_loss": 0.48821588753163814, "actor_loss": -25.797109355926512, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.32584309577942, "step": 116000}
{"episode_reward": 462.062248605016, "episode": 117.0, "batch_reward": 0.2710134384781122, "critic_loss": 0.5116185460686684, "actor_loss": -25.891487564086916, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.493040084838867, "step": 117000}
{"episode_reward": 562.7801999860008, "episode": 118.0, "batch_reward": 0.27122950766980647, "critic_loss": 0.47601808682084085, "actor_loss": -25.858571117401123, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.311336994171143, "step": 118000}
{"episode_reward": 259.6135436503842, "episode": 119.0, "batch_reward": 0.27275610107183457, "critic_loss": 0.5269864831566811, "actor_loss": -25.938191871643067, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.911282300949097, "step": 119000}
{"episode_reward": 540.1609870170585, "episode": 120.0, "batch_reward": 0.27445826153457165, "critic_loss": 0.4989310722053051, "actor_loss": -26.155515125274658, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.82294487953186, "step": 120000}
{"episode_reward": 568.5081944382716, "episode": 121.0, "batch_reward": 0.2759632870852947, "critic_loss": 0.47569013632833956, "actor_loss": -26.195555156707762, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.96349811553955, "step": 121000}
{"episode_reward": 423.68956142152905, "episode": 122.0, "batch_reward": 0.278013444930315, "critic_loss": 0.46040334163606167, "actor_loss": -26.414673805236816, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.514569759368896, "step": 122000}
{"episode_reward": 324.1291932591658, "episode": 123.0, "batch_reward": 0.2794363746345043, "critic_loss": 0.49252534621953964, "actor_loss": -26.54500385284424, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.779884815216064, "step": 123000}
{"episode_reward": 594.5107487276183, "episode": 124.0, "batch_reward": 0.28280454824864865, "critic_loss": 0.4732680881470442, "actor_loss": -26.778600860595702, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.376110792160034, "step": 124000}
{"episode_reward": 533.2949301864636, "episode": 125.0, "batch_reward": 0.28268697786331176, "critic_loss": 0.46030542701482774, "actor_loss": -26.770869873046873, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.422876358032227, "step": 125000}
{"episode_reward": 334.99923270218875, "episode": 126.0, "batch_reward": 0.2842263562977314, "critic_loss": 0.45561259765923023, "actor_loss": -26.949961555480957, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.68495512008667, "step": 126000}
{"episode_reward": 388.50647439196706, "episode": 127.0, "batch_reward": 0.28414429967105387, "critic_loss": 0.4962851082533598, "actor_loss": -26.930696308135985, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.154754877090454, "step": 127000}
{"episode_reward": 482.9413153999445, "episode": 128.0, "batch_reward": 0.2861339439153671, "critic_loss": 0.4448714946210384, "actor_loss": -27.07886130142212, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.965161323547363, "step": 128000}
{"episode_reward": 507.9462162543187, "episode": 129.0, "batch_reward": 0.2882691125124693, "critic_loss": 0.4408477221429348, "actor_loss": -27.219356842041016, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.879153966903687, "step": 129000}
{"episode_reward": 475.17480144911906, "episode": 130.0, "batch_reward": 0.28817377892136575, "critic_loss": 0.5095782267004252, "actor_loss": -27.172773597717285, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.25872826576233, "step": 130000}
{"episode_reward": 281.3992370845262, "episode": 131.0, "batch_reward": 0.28986285138130186, "critic_loss": 0.47310579289495946, "actor_loss": -27.315013973236084, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 44.22071051597595, "step": 131000}
{"episode_reward": 570.0604275805647, "episode": 132.0, "batch_reward": 0.29074040450155736, "critic_loss": 0.43189000369608405, "actor_loss": -27.376844299316407, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.962488889694214, "step": 132000}
{"episode_reward": 360.95223089748663, "episode": 133.0, "batch_reward": 0.29160327710211276, "critic_loss": 0.4624922332763672, "actor_loss": -27.26922579574585, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.618114709854126, "step": 133000}
{"episode_reward": 511.73167783308804, "episode": 134.0, "batch_reward": 0.29339922735095025, "critic_loss": 0.4463618285357952, "actor_loss": -27.676738945007322, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.98268723487854, "step": 134000}
{"episode_reward": 494.0149650455331, "episode": 135.0, "batch_reward": 0.29575575172901153, "critic_loss": 0.4739539216905832, "actor_loss": -27.947911766052247, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.548167943954468, "step": 135000}
{"episode_reward": 562.2967417168319, "episode": 136.0, "batch_reward": 0.29676915042102336, "critic_loss": 0.4528761041164398, "actor_loss": -27.922190620422363, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.991737365722656, "step": 136000}
{"episode_reward": 245.37932235929313, "episode": 137.0, "batch_reward": 0.2969287628084421, "critic_loss": 0.46422525064647197, "actor_loss": -27.97013385772705, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.287567377090454, "step": 137000}
{"episode_reward": 484.91166056967387, "episode": 138.0, "batch_reward": 0.29718521268665793, "critic_loss": 0.43017687392234805, "actor_loss": -27.92044324874878, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.087417364120483, "step": 138000}
{"episode_reward": 451.12154855597424, "episode": 139.0, "batch_reward": 0.2993624901175499, "critic_loss": 0.47930927576124666, "actor_loss": -28.067375988006592, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.623523235321045, "step": 139000}
{"episode_reward": 543.0157698068854, "episode": 140.0, "batch_reward": 0.29984688356518746, "critic_loss": 0.4359047499448061, "actor_loss": -28.113838676452637, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.84054946899414, "step": 140000}
{"episode_reward": 570.3854123940916, "episode": 141.0, "batch_reward": 0.30344717483222483, "critic_loss": 0.4583461229801178, "actor_loss": -28.371879119873046, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.09196949005127, "step": 141000}
{"episode_reward": 214.69192208956505, "episode": 142.0, "batch_reward": 0.30270097649097444, "critic_loss": 0.46143245550990103, "actor_loss": -28.36682524108887, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.118815898895264, "step": 142000}
{"episode_reward": 580.0781580508569, "episode": 143.0, "batch_reward": 0.3034646626859903, "critic_loss": 0.4664138192385435, "actor_loss": -28.370689601898192, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.15739679336548, "step": 143000}
{"episode_reward": 529.2177536217517, "episode": 144.0, "batch_reward": 0.30595637242496015, "critic_loss": 0.46842295669019224, "actor_loss": -28.580241897583008, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.85543417930603, "step": 144000}
{"episode_reward": 593.1152426869189, "episode": 145.0, "batch_reward": 0.3074822152405977, "critic_loss": 0.49791252334415914, "actor_loss": -28.697960960388183, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.13314938545227, "step": 145000}
{"episode_reward": 587.3034327826073, "episode": 146.0, "batch_reward": 0.30826439522206783, "critic_loss": 0.4482568519115448, "actor_loss": -28.80658913421631, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.840360403060913, "step": 146000}
{"episode_reward": 535.2734478172382, "episode": 147.0, "batch_reward": 0.30981990253925323, "critic_loss": 0.4618643975108862, "actor_loss": -29.010241088867186, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.871357202529907, "step": 147000}
{"episode_reward": 571.1444343786418, "episode": 148.0, "batch_reward": 0.31373806691169737, "critic_loss": 0.4311718974858522, "actor_loss": -29.23080590057373, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.754205226898193, "step": 148000}
{"episode_reward": 572.4066928296562, "episode": 149.0, "batch_reward": 0.31419275924563406, "critic_loss": 0.4217399531304836, "actor_loss": -29.215065914154053, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.299794673919678, "step": 149000}
{"episode_reward": 197.26229294442638, "episode": 150.0, "batch_reward": 0.3133409992158413, "critic_loss": 0.467746992200613, "actor_loss": -29.26967960357666, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
