{"episode_reward": 0.0, "episode": 1.0, "duration": 13.89728569984436, "step": 1000}
{"episode_reward": 4.859792814687425, "episode": 2.0, "duration": 1.2322771549224854, "step": 2000}
{"episode_reward": 550.1572824113056, "episode": 3.0, "batch_reward": 0.2663073541671684, "critic_loss": 0.046723890597781426, "actor_loss": -23.726153451782398, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 74.26143884658813, "step": 3000}
{"episode_reward": 118.87190884708343, "episode": 4.0, "batch_reward": 0.2159691387563944, "critic_loss": 0.0711798391956836, "actor_loss": -20.19169929407537, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.160138368606567, "step": 4000}
{"episode_reward": 128.222227855664, "episode": 5.0, "batch_reward": 0.18921939641982316, "critic_loss": 0.0675625375173986, "actor_loss": -18.359551195032896, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.16080093383789, "step": 5000}
{"episode_reward": 105.69872545885491, "episode": 6.0, "batch_reward": 0.1725204305574298, "critic_loss": 0.05938123680464923, "actor_loss": -16.584291156999768, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.228606462478638, "step": 6000}
{"episode_reward": 72.62435387948167, "episode": 7.0, "batch_reward": 0.15894923423230647, "critic_loss": 0.057859269831329586, "actor_loss": -16.91473507938534, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.150331497192383, "step": 7000}
{"episode_reward": 80.58349687693013, "episode": 8.0, "batch_reward": 0.14641819483041763, "critic_loss": 0.05482284115999937, "actor_loss": -15.783727409042418, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.046371459960938, "step": 8000}
{"episode_reward": 69.92128204836476, "episode": 9.0, "batch_reward": 0.14082114461064338, "critic_loss": 0.052499170478433374, "actor_loss": -15.19003985633701, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.177053928375244, "step": 9000}
{"episode_reward": 151.791387944308, "episode": 10.0, "batch_reward": 0.13999433217197657, "critic_loss": 0.06509857200458646, "actor_loss": -15.628689213365316, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.165131330490112, "step": 10000}
{"episode_reward": 125.29723566690454, "episode": 11.0, "batch_reward": 0.13883169190585612, "critic_loss": 0.06364847796037792, "actor_loss": -15.516358465194703, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.788557052612305, "step": 11000}
{"episode_reward": 55.52372504693802, "episode": 12.0, "batch_reward": 0.1331282270923257, "critic_loss": 0.07769930167868734, "actor_loss": -15.313621447324753, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.81383967399597, "step": 12000}
{"episode_reward": 176.31083749857333, "episode": 13.0, "batch_reward": 0.13766874888539316, "critic_loss": 0.09267780079320073, "actor_loss": -16.101411338090898, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.605391263961792, "step": 13000}
{"episode_reward": 151.57231305204266, "episode": 14.0, "batch_reward": 0.13673749143630265, "critic_loss": 0.09533254495635629, "actor_loss": -15.981093810081482, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.715505838394165, "step": 14000}
{"episode_reward": 67.31289593413139, "episode": 15.0, "batch_reward": 0.13453175606578588, "critic_loss": 0.10440007714927196, "actor_loss": -15.111769862174988, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.76201868057251, "step": 15000}
{"episode_reward": 190.86377982317896, "episode": 16.0, "batch_reward": 0.14003621388971804, "critic_loss": 0.12149990192428231, "actor_loss": -16.082824696540833, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.85614848136902, "step": 16000}
{"episode_reward": 244.2807574719628, "episode": 17.0, "batch_reward": 0.1440038000792265, "critic_loss": 0.12737386185675859, "actor_loss": -16.302653184890747, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.34342885017395, "step": 17000}
{"episode_reward": 164.38703203063216, "episode": 18.0, "batch_reward": 0.14364246491342783, "critic_loss": 0.13232471833378076, "actor_loss": -16.130666354179382, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.110320806503296, "step": 18000}
{"episode_reward": 57.49648753279129, "episode": 19.0, "batch_reward": 0.1425506244674325, "critic_loss": 0.13951372666656972, "actor_loss": -16.3961933259964, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.997546672821045, "step": 19000}
{"episode_reward": 269.0684959969668, "episode": 20.0, "batch_reward": 0.14940959359705447, "critic_loss": 0.1666666580811143, "actor_loss": -16.227159845352173, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.118605375289917, "step": 20000}
{"episode_reward": 280.0002121780148, "episode": 21.0, "batch_reward": 0.1569237082004547, "critic_loss": 0.18524508498609066, "actor_loss": -18.260903820991516, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.456016302108765, "step": 21000}
{"episode_reward": 328.9224797220951, "episode": 22.0, "batch_reward": 0.16159060841053724, "critic_loss": 0.18322241465747358, "actor_loss": -17.110933102607728, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.147254943847656, "step": 22000}
{"episode_reward": 122.75947387897004, "episode": 23.0, "batch_reward": 0.16384129977226258, "critic_loss": 0.1735788258984685, "actor_loss": -17.982414598464967, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.640949249267578, "step": 23000}
{"episode_reward": 241.07900730306352, "episode": 24.0, "batch_reward": 0.16419942086189984, "critic_loss": 0.22674812493473293, "actor_loss": -18.020462613105774, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.881447076797485, "step": 24000}
{"episode_reward": 230.8817200801239, "episode": 25.0, "batch_reward": 0.16699473775923251, "critic_loss": 0.2347819533497095, "actor_loss": -18.072804010391234, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.768117427825928, "step": 25000}
{"episode_reward": 117.12148671857119, "episode": 26.0, "batch_reward": 0.16675236485153436, "critic_loss": 0.23237195080518722, "actor_loss": -18.32115885543823, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.916369199752808, "step": 26000}
{"episode_reward": 331.4197015593941, "episode": 27.0, "batch_reward": 0.16963743130862713, "critic_loss": 0.2287424121722579, "actor_loss": -18.884163875579834, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.01759624481201, "step": 27000}
{"episode_reward": 62.800209011396255, "episode": 28.0, "batch_reward": 0.1681736481487751, "critic_loss": 0.21711356177926064, "actor_loss": -18.380830354690552, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.832406997680664, "step": 28000}
{"episode_reward": 228.61769414591228, "episode": 29.0, "batch_reward": 0.16701036748290063, "critic_loss": 0.23146461514383554, "actor_loss": -18.697669677734375, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.401091814041138, "step": 29000}
{"episode_reward": 57.67623534989075, "episode": 30.0, "batch_reward": 0.16648569060862065, "critic_loss": 0.24810099904239177, "actor_loss": -19.291107265472412, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.118483066558838, "step": 30000}
{"episode_reward": 139.12819961038446, "episode": 31.0, "batch_reward": 0.16553809885680676, "critic_loss": 0.2669766582399607, "actor_loss": -18.678629007339477, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.3684868812561, "step": 31000}
{"episode_reward": 218.94843121855791, "episode": 32.0, "batch_reward": 0.16816245275735855, "critic_loss": 0.304410400301218, "actor_loss": -18.372242994308472, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.97576141357422, "step": 32000}
{"episode_reward": 172.43818000191942, "episode": 33.0, "batch_reward": 0.16681045711040496, "critic_loss": 0.3073391591310501, "actor_loss": -18.614560110092164, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.218189001083374, "step": 33000}
{"episode_reward": 207.51034493792585, "episode": 34.0, "batch_reward": 0.16809611187130213, "critic_loss": 0.30808627197146415, "actor_loss": -19.142856079101563, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.461058855056763, "step": 34000}
{"episode_reward": 143.6280426798856, "episode": 35.0, "batch_reward": 0.16600208619982004, "critic_loss": 0.3304458631426096, "actor_loss": -18.702234588623046, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.234620809555054, "step": 35000}
{"episode_reward": 131.62823748117134, "episode": 36.0, "batch_reward": 0.16812035512924195, "critic_loss": 0.3113372171372175, "actor_loss": -18.673036926269532, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.70918035507202, "step": 36000}
{"episode_reward": 347.7535248029741, "episode": 37.0, "batch_reward": 0.17177312667667866, "critic_loss": 0.32677911829948425, "actor_loss": -19.480763095855714, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.808210611343384, "step": 37000}
{"episode_reward": 173.20135934966095, "episode": 38.0, "batch_reward": 0.17304153440892697, "critic_loss": 0.3118659088015556, "actor_loss": -19.65081936264038, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.89958882331848, "step": 38000}
{"episode_reward": 298.45478298287577, "episode": 39.0, "batch_reward": 0.17673166872560978, "critic_loss": 0.3267998693287373, "actor_loss": -19.882227098464966, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.390368223190308, "step": 39000}
{"episode_reward": 358.42281230540584, "episode": 40.0, "batch_reward": 0.1793174039274454, "critic_loss": 0.3272194495499134, "actor_loss": -20.187329526901244, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.842578172683716, "step": 40000}
{"episode_reward": 164.81283514369403, "episode": 41.0, "batch_reward": 0.17986065903306006, "critic_loss": 0.3256506378799677, "actor_loss": -20.608426723480225, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.141592502593994, "step": 41000}
{"episode_reward": 312.7644995228555, "episode": 42.0, "batch_reward": 0.18223958837985993, "critic_loss": 0.3308595564514399, "actor_loss": -20.529131185531615, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.39371681213379, "step": 42000}
{"episode_reward": 123.33814114313424, "episode": 43.0, "batch_reward": 0.18245344485342502, "critic_loss": 0.3138790517002344, "actor_loss": -20.309917434692384, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.11406946182251, "step": 43000}
{"episode_reward": 390.82857559746174, "episode": 44.0, "batch_reward": 0.1865787283331156, "critic_loss": 0.3531480133831501, "actor_loss": -20.398364343643188, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.026980876922607, "step": 44000}
{"episode_reward": 315.10892729628335, "episode": 45.0, "batch_reward": 0.18819370730221271, "critic_loss": 0.3505293846130371, "actor_loss": -21.003574794769285, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.594271898269653, "step": 45000}
{"episode_reward": 82.11522297521148, "episode": 46.0, "batch_reward": 0.18499049505591392, "critic_loss": 0.3426068731546402, "actor_loss": -21.312794719696043, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.686999559402466, "step": 46000}
{"episode_reward": 91.13711533182902, "episode": 47.0, "batch_reward": 0.18402700333297253, "critic_loss": 0.3372960348725319, "actor_loss": -21.168138418197632, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.918009757995605, "step": 47000}
{"episode_reward": 300.17409954097997, "episode": 48.0, "batch_reward": 0.1878685939013958, "critic_loss": 0.3434198817461729, "actor_loss": -21.585020946502684, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.107030391693115, "step": 48000}
{"episode_reward": 288.13984625680337, "episode": 49.0, "batch_reward": 0.18955319923162461, "critic_loss": 0.3454069426655769, "actor_loss": -21.39271430015564, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.24699640274048, "step": 49000}
{"episode_reward": 196.91930808373715, "episode": 50.0, "batch_reward": 0.1896934385150671, "critic_loss": 0.34137294478714464, "actor_loss": -21.745370555877685, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.88918685913086, "step": 50000}
{"episode_reward": 211.50090428675486, "episode": 51.0, "batch_reward": 0.1891809815019369, "critic_loss": 0.3514587300866842, "actor_loss": -21.832802291870117, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.66383099555969, "step": 51000}
{"episode_reward": 122.22533180554518, "episode": 52.0, "batch_reward": 0.18847132924199103, "critic_loss": 0.3228112735748291, "actor_loss": -21.964343658447266, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.496633052825928, "step": 52000}
{"episode_reward": 119.43674659209651, "episode": 53.0, "batch_reward": 0.18845046238601207, "critic_loss": 0.318339950427413, "actor_loss": -21.74614130783081, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.40724778175354, "step": 53000}
{"episode_reward": 369.53767322453865, "episode": 54.0, "batch_reward": 0.19210829205811022, "critic_loss": 0.3311601437032223, "actor_loss": -21.834312744140625, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.436398029327393, "step": 54000}
{"episode_reward": 410.6179917066529, "episode": 55.0, "batch_reward": 0.19488566429913043, "critic_loss": 0.3572968692779541, "actor_loss": -22.499833602905273, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.17745041847229, "step": 55000}
{"episode_reward": 250.15553450289067, "episode": 56.0, "batch_reward": 0.19672260718047618, "critic_loss": 0.32366195191442965, "actor_loss": -22.6006537399292, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.089128971099854, "step": 56000}
{"episode_reward": 362.8413429069899, "episode": 57.0, "batch_reward": 0.20046139426529408, "critic_loss": 0.3464110653847456, "actor_loss": -22.918470851898192, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.072173595428467, "step": 57000}
{"episode_reward": 386.989384012953, "episode": 58.0, "batch_reward": 0.20255042289197445, "critic_loss": 0.3310884171426296, "actor_loss": -23.152017063140867, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.97847294807434, "step": 58000}
{"episode_reward": 272.14463985852416, "episode": 59.0, "batch_reward": 0.20360186192393304, "critic_loss": 0.33651362089812753, "actor_loss": -23.12994627761841, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.95700693130493, "step": 59000}
{"episode_reward": 411.2386476952495, "episode": 60.0, "batch_reward": 0.20613141338527202, "critic_loss": 0.3318701372742653, "actor_loss": -23.4023342666626, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.83992028236389, "step": 60000}
{"episode_reward": 188.94831375547633, "episode": 61.0, "batch_reward": 0.20731642256677152, "critic_loss": 0.35144412657618523, "actor_loss": -23.702444969177247, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 37.91388821601868, "step": 61000}
{"episode_reward": 349.8742724255278, "episode": 62.0, "batch_reward": 0.20938824802637102, "critic_loss": 0.3419629034996033, "actor_loss": -23.670674697875977, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.91347336769104, "step": 62000}
{"episode_reward": 359.33404686620327, "episode": 63.0, "batch_reward": 0.20903309804201126, "critic_loss": 0.33202972914278506, "actor_loss": -23.790903644561766, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.432205200195312, "step": 63000}
{"episode_reward": 54.04409480050626, "episode": 64.0, "batch_reward": 0.20933290153741838, "critic_loss": 0.33224213507771494, "actor_loss": -23.738973011016846, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.91387963294983, "step": 64000}
{"episode_reward": 357.20522324515093, "episode": 65.0, "batch_reward": 0.21123037128150463, "critic_loss": 0.34331351961195467, "actor_loss": -23.872979724884033, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.082629442214966, "step": 65000}
{"episode_reward": 323.4396389232399, "episode": 66.0, "batch_reward": 0.212448421895504, "critic_loss": 0.339330330863595, "actor_loss": -23.931468673706053, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.321981191635132, "step": 66000}
{"episode_reward": 226.80150035387803, "episode": 67.0, "batch_reward": 0.2126729526370764, "critic_loss": 0.34725816391408443, "actor_loss": -23.777790672302245, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.060919284820557, "step": 67000}
{"episode_reward": 132.80674390735086, "episode": 68.0, "batch_reward": 0.21057876804471015, "critic_loss": 0.33012725231051443, "actor_loss": -23.559571502685547, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.216495275497437, "step": 68000}
{"episode_reward": 94.11862760783553, "episode": 69.0, "batch_reward": 0.20832662113010883, "critic_loss": 0.32195053112506866, "actor_loss": -23.75544993209839, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.15898609161377, "step": 69000}
{"episode_reward": 177.43950594666265, "episode": 70.0, "batch_reward": 0.21105982026457787, "critic_loss": 0.3177095731943846, "actor_loss": -23.91207391357422, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.953896522521973, "step": 70000}
{"episode_reward": 377.13266287882016, "episode": 71.0, "batch_reward": 0.21235648088157177, "critic_loss": 0.31780917055904867, "actor_loss": -23.901753112792967, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.70685529708862, "step": 71000}
{"episode_reward": 412.46075082500136, "episode": 72.0, "batch_reward": 0.21526674658060074, "critic_loss": 0.3160864769667387, "actor_loss": -24.08967269897461, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.87815809249878, "step": 72000}
{"episode_reward": 378.54312327951226, "episode": 73.0, "batch_reward": 0.21695461669564248, "critic_loss": 0.3203288748264313, "actor_loss": -24.032478118896485, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.413326263427734, "step": 73000}
{"episode_reward": 146.9291472335822, "episode": 74.0, "batch_reward": 0.21509828262031078, "critic_loss": 0.3078795623034239, "actor_loss": -23.771752117156982, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.47997260093689, "step": 74000}
{"episode_reward": 190.6925394468298, "episode": 75.0, "batch_reward": 0.2156603910177946, "critic_loss": 0.32360664309561255, "actor_loss": -23.92401866912842, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.196077346801758, "step": 75000}
{"episode_reward": 411.1564955761904, "episode": 76.0, "batch_reward": 0.21774397361278533, "critic_loss": 0.3308877310156822, "actor_loss": -23.852305389404297, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.387416124343872, "step": 76000}
{"episode_reward": 238.7159076607995, "episode": 77.0, "batch_reward": 0.21808851306140423, "critic_loss": 0.31644635620713235, "actor_loss": -23.835571571350098, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.49663257598877, "step": 77000}
{"episode_reward": 117.5232375397565, "episode": 78.0, "batch_reward": 0.21748152121901512, "critic_loss": 0.33147621794044974, "actor_loss": -23.77582828903198, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.3121235370636, "step": 78000}
{"episode_reward": 359.43424515721796, "episode": 79.0, "batch_reward": 0.22068019178509712, "critic_loss": 0.3298817258030176, "actor_loss": -23.904636962890624, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.23818850517273, "step": 79000}
{"episode_reward": 508.0717709412085, "episode": 80.0, "batch_reward": 0.22321779215335846, "critic_loss": 0.3464571264088154, "actor_loss": -24.045023891448974, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.250443935394287, "step": 80000}
{"episode_reward": 207.91907311022945, "episode": 81.0, "batch_reward": 0.2240148919224739, "critic_loss": 0.31780374389886856, "actor_loss": -24.193996757507325, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.74998331069946, "step": 81000}
{"episode_reward": 436.2819587654856, "episode": 82.0, "batch_reward": 0.22567445981502532, "critic_loss": 0.3323801683932543, "actor_loss": -24.443122756958008, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.749696731567383, "step": 82000}
{"episode_reward": 322.29702608267223, "episode": 83.0, "batch_reward": 0.22722037872672082, "critic_loss": 0.34230893060564993, "actor_loss": -24.27457956314087, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.425370693206787, "step": 83000}
{"episode_reward": 483.62823365940477, "episode": 84.0, "batch_reward": 0.23057553251087665, "critic_loss": 0.36948442515730856, "actor_loss": -24.443310691833496, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.18795418739319, "step": 84000}
{"episode_reward": 438.24205428847296, "episode": 85.0, "batch_reward": 0.23144069342315196, "critic_loss": 0.35227555215358736, "actor_loss": -24.694590713500975, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.493980169296265, "step": 85000}
{"episode_reward": 276.85827661970393, "episode": 86.0, "batch_reward": 0.2330204712599516, "critic_loss": 0.36660004226863385, "actor_loss": -24.903426559448242, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.350170850753784, "step": 86000}
{"episode_reward": 311.06028796784426, "episode": 87.0, "batch_reward": 0.2332755031734705, "critic_loss": 0.37424703779816626, "actor_loss": -24.61406597518921, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.264857292175293, "step": 87000}
{"episode_reward": 253.8188058193485, "episode": 88.0, "batch_reward": 0.23310570935904978, "critic_loss": 0.37775365872681144, "actor_loss": -24.815927715301513, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.198792695999146, "step": 88000}
{"episode_reward": 329.2589769113591, "episode": 89.0, "batch_reward": 0.23471686199307443, "critic_loss": 0.3685443710386753, "actor_loss": -25.067967555999754, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.122414350509644, "step": 89000}
{"episode_reward": 302.82505067122736, "episode": 90.0, "batch_reward": 0.23781999008357524, "critic_loss": 0.38522934755682947, "actor_loss": -25.05543061065674, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.904676914215088, "step": 90000}
{"episode_reward": 331.9500191492156, "episode": 91.0, "batch_reward": 0.23740747195482254, "critic_loss": 0.37433761709928515, "actor_loss": -24.904916007995606, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.70271944999695, "step": 91000}
{"episode_reward": 466.83582049995994, "episode": 92.0, "batch_reward": 0.23952857188880444, "critic_loss": 0.3832318376749754, "actor_loss": -24.938704868316652, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.982891082763672, "step": 92000}
{"episode_reward": 235.59878739425622, "episode": 93.0, "batch_reward": 0.23911268950998782, "critic_loss": 0.3764634250253439, "actor_loss": -25.07355616760254, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.196638345718384, "step": 93000}
{"episode_reward": 453.83444251920497, "episode": 94.0, "batch_reward": 0.24215363842248916, "critic_loss": 0.4146042374968529, "actor_loss": -25.075283882141115, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.41623091697693, "step": 94000}
{"episode_reward": 347.7856667903826, "episode": 95.0, "batch_reward": 0.24241450805962086, "critic_loss": 0.4072086954712868, "actor_loss": -24.84649919128418, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.025920867919922, "step": 95000}
{"episode_reward": 462.2385672467268, "episode": 96.0, "batch_reward": 0.24581312093138694, "critic_loss": 0.412485296189785, "actor_loss": -25.326976875305174, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.15662670135498, "step": 96000}
{"episode_reward": 475.02464231085736, "episode": 97.0, "batch_reward": 0.24751159097254277, "critic_loss": 0.4154846606850624, "actor_loss": -25.74685485458374, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.21301007270813, "step": 97000}
{"episode_reward": 498.27668538169894, "episode": 98.0, "batch_reward": 0.24862904334068298, "critic_loss": 0.41984119471907616, "actor_loss": -25.25692671585083, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.254554271697998, "step": 98000}
{"episode_reward": 154.69444278149209, "episode": 99.0, "batch_reward": 0.24934954473376275, "critic_loss": 0.39911576676368715, "actor_loss": -25.593533336639403, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.451741456985474, "step": 99000}
{"episode_reward": 524.9643195807656, "episode": 100.0, "batch_reward": 0.25252306257188323, "critic_loss": 0.4279419267624617, "actor_loss": -25.755943042755128, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.173629999160767, "step": 100000}
{"episode_reward": 207.14532227998637, "episode": 101.0, "batch_reward": 0.2510571763217449, "critic_loss": 0.46011993536353113, "actor_loss": -25.843198474884034, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.53720569610596, "step": 101000}
{"episode_reward": 400.71904478935676, "episode": 102.0, "batch_reward": 0.2526915706694126, "critic_loss": 0.42705776713788507, "actor_loss": -25.706486515045167, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.105746507644653, "step": 102000}
{"episode_reward": 143.5671689750915, "episode": 103.0, "batch_reward": 0.2518682669401169, "critic_loss": 0.43683290159702304, "actor_loss": -25.565650165557862, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.275179147720337, "step": 103000}
{"episode_reward": 435.8366173684411, "episode": 104.0, "batch_reward": 0.2537049496173859, "critic_loss": 0.46057825796306134, "actor_loss": -25.872830898284914, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.4034264087677, "step": 104000}
{"episode_reward": 382.3680001294059, "episode": 105.0, "batch_reward": 0.25577090150117876, "critic_loss": 0.4330705899149179, "actor_loss": -26.091176498413088, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.444597959518433, "step": 105000}
{"episode_reward": 428.5570253284514, "episode": 106.0, "batch_reward": 0.2558920111656189, "critic_loss": 0.47148066836595537, "actor_loss": -26.101254776000978, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.909528970718384, "step": 106000}
{"episode_reward": 360.87397110920756, "episode": 107.0, "batch_reward": 0.2578827713131905, "critic_loss": 0.4610691795796156, "actor_loss": -26.28267158126831, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.321204900741577, "step": 107000}
{"episode_reward": 400.967841737977, "episode": 108.0, "batch_reward": 0.2590011755526066, "critic_loss": 0.46910808657109737, "actor_loss": -25.92085908126831, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.30023431777954, "step": 108000}
{"episode_reward": 387.4281258267785, "episode": 109.0, "batch_reward": 0.2614247824549675, "critic_loss": 0.5022798224538565, "actor_loss": -26.407792461395264, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.290106058120728, "step": 109000}
{"episode_reward": 595.482758829386, "episode": 110.0, "batch_reward": 0.2642626884132624, "critic_loss": 0.4836243087798357, "actor_loss": -26.363122413635253, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.87907600402832, "step": 110000}
{"episode_reward": 522.3475917737233, "episode": 111.0, "batch_reward": 0.26463404116034506, "critic_loss": 0.4327924258559942, "actor_loss": -26.841554592132567, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.27765488624573, "step": 111000}
{"episode_reward": 183.67515290784272, "episode": 112.0, "batch_reward": 0.264603722140193, "critic_loss": 0.4646812614500523, "actor_loss": -26.530896961212157, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.237770318984985, "step": 112000}
{"episode_reward": 590.2958030806074, "episode": 113.0, "batch_reward": 0.2684693245142698, "critic_loss": 0.4963217090368271, "actor_loss": -27.099875469207763, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.441282510757446, "step": 113000}
{"episode_reward": 444.45456757165977, "episode": 114.0, "batch_reward": 0.27035935236513614, "critic_loss": 0.48734579762816427, "actor_loss": -27.177579807281493, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.478598594665527, "step": 114000}
{"episode_reward": 525.1788090818927, "episode": 115.0, "batch_reward": 0.2723719104528427, "critic_loss": 0.5166043656915427, "actor_loss": -27.360620422363283, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.051092386245728, "step": 115000}
{"episode_reward": 559.8963405263169, "episode": 116.0, "batch_reward": 0.2743040542602539, "critic_loss": 0.4525050350278616, "actor_loss": -27.45874927520752, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.41861867904663, "step": 116000}
{"episode_reward": 535.2300762226513, "episode": 117.0, "batch_reward": 0.27593823824822905, "critic_loss": 0.4261187440007925, "actor_loss": -27.85327164840698, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.45621919631958, "step": 117000}
{"episode_reward": 242.5046751537398, "episode": 118.0, "batch_reward": 0.2757947903126478, "critic_loss": 0.4787255603969097, "actor_loss": -27.770287590026854, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.209158658981323, "step": 118000}
{"episode_reward": 498.371410605321, "episode": 119.0, "batch_reward": 0.2780933266878128, "critic_loss": 0.459283993512392, "actor_loss": -27.934124069213865, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.301819324493408, "step": 119000}
{"episode_reward": 503.0330843785593, "episode": 120.0, "batch_reward": 0.279224461838603, "critic_loss": 0.4809764762222767, "actor_loss": -28.010969734191896, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.074114084243774, "step": 120000}
{"episode_reward": 473.49609748600045, "episode": 121.0, "batch_reward": 0.2812475793212652, "critic_loss": 0.500607432320714, "actor_loss": -28.068417350769042, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.37262558937073, "step": 121000}
{"episode_reward": 554.5497221445249, "episode": 122.0, "batch_reward": 0.2839557634145021, "critic_loss": 0.4529476626962423, "actor_loss": -28.260419357299806, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.42863965034485, "step": 122000}
{"episode_reward": 536.6856882692445, "episode": 123.0, "batch_reward": 0.28640869234502314, "critic_loss": 0.4226118128746748, "actor_loss": -28.50999136352539, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.82675528526306, "step": 123000}
{"episode_reward": 526.5226344738738, "episode": 124.0, "batch_reward": 0.28839569810032845, "critic_loss": 0.4146769215762615, "actor_loss": -28.532580768585206, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.088983297348022, "step": 124000}
{"episode_reward": 554.1115693486363, "episode": 125.0, "batch_reward": 0.2900287355184555, "critic_loss": 0.4484688469320536, "actor_loss": -28.85483589553833, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.13219976425171, "step": 125000}
{"episode_reward": 453.31498231686, "episode": 126.0, "batch_reward": 0.29078299985826017, "critic_loss": 0.45425816360116006, "actor_loss": -28.81582968521118, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.055795192718506, "step": 126000}
{"episode_reward": 575.4593047576556, "episode": 127.0, "batch_reward": 0.2935216379910707, "critic_loss": 0.42646168580651284, "actor_loss": -28.995057765960695, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.222447633743286, "step": 127000}
{"episode_reward": 483.33626125189187, "episode": 128.0, "batch_reward": 0.29558207373321055, "critic_loss": 0.4312929540276527, "actor_loss": -29.233526874542235, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.013975858688354, "step": 128000}
{"episode_reward": 486.0536793859341, "episode": 129.0, "batch_reward": 0.2960085575282574, "critic_loss": 0.4223615863323212, "actor_loss": -29.37877502822876, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.026845932006836, "step": 129000}
{"episode_reward": 589.6089657075469, "episode": 130.0, "batch_reward": 0.29868280041217804, "critic_loss": 0.4199997799694538, "actor_loss": -29.634470706939698, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.68548846244812, "step": 130000}
{"episode_reward": 599.0063826347679, "episode": 131.0, "batch_reward": 0.30239870366454125, "critic_loss": 0.44130402347445485, "actor_loss": -30.05371323776245, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.5900239944458, "step": 131000}
{"episode_reward": 529.3093447309158, "episode": 132.0, "batch_reward": 0.30309092670679094, "critic_loss": 0.40334881147742274, "actor_loss": -29.90785182952881, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.003721237182617, "step": 132000}
{"episode_reward": 536.3378502737372, "episode": 133.0, "batch_reward": 0.3046790264248848, "critic_loss": 0.4105862988084555, "actor_loss": -30.296344570159913, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.074003219604492, "step": 133000}
{"episode_reward": 559.4477405807896, "episode": 134.0, "batch_reward": 0.3066128443777561, "critic_loss": 0.44292744171619414, "actor_loss": -30.32476344680786, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.134443044662476, "step": 134000}
{"episode_reward": 591.5359477115328, "episode": 135.0, "batch_reward": 0.3089805133342743, "critic_loss": 0.4412026524096727, "actor_loss": -30.430392517089842, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.165592908859253, "step": 135000}
{"episode_reward": 564.2846537119232, "episode": 136.0, "batch_reward": 0.3116500287204981, "critic_loss": 0.41164917294681075, "actor_loss": -30.64771110534668, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.65373992919922, "step": 136000}
{"episode_reward": 562.6248866895288, "episode": 137.0, "batch_reward": 0.3115446822196245, "critic_loss": 0.3992915631383657, "actor_loss": -30.787811408996582, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.93813729286194, "step": 137000}
{"episode_reward": 552.6284790001844, "episode": 138.0, "batch_reward": 0.31345759180188176, "critic_loss": 0.40211730213463304, "actor_loss": -30.812798503875733, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.356037139892578, "step": 138000}
{"episode_reward": 522.8636611558833, "episode": 139.0, "batch_reward": 0.31600263583660126, "critic_loss": 0.43958913876116273, "actor_loss": -31.281842166900635, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.5564181804657, "step": 139000}
{"episode_reward": 514.0008172212283, "episode": 140.0, "batch_reward": 0.3163597272336483, "critic_loss": 0.406980820029974, "actor_loss": -31.182664306640625, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.11064600944519, "step": 140000}
{"episode_reward": 550.3780316654824, "episode": 141.0, "batch_reward": 0.3189621222615242, "critic_loss": 0.42010272191464904, "actor_loss": -31.639651550292967, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 36.42470669746399, "step": 141000}
{"episode_reward": 570.9483093297171, "episode": 142.0, "batch_reward": 0.3206165963709354, "critic_loss": 0.41595036220550535, "actor_loss": -31.747522651672362, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.174031257629395, "step": 142000}
{"episode_reward": 400.59059422530845, "episode": 143.0, "batch_reward": 0.321302377641201, "critic_loss": 0.44357180073857305, "actor_loss": -31.996360820770263, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.440823793411255, "step": 143000}
{"episode_reward": 522.4419520027891, "episode": 144.0, "batch_reward": 0.32357784631848335, "critic_loss": 0.40672999000549315, "actor_loss": -31.920096725463868, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.599933862686157, "step": 144000}
{"episode_reward": 658.9793829878531, "episode": 145.0, "batch_reward": 0.3246331649422646, "critic_loss": 0.46693827015161515, "actor_loss": -32.000190212249755, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.27809453010559, "step": 145000}
{"episode_reward": 379.84263514670266, "episode": 146.0, "batch_reward": 0.32334741842746734, "critic_loss": 0.40232878732681276, "actor_loss": -32.17117128753662, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.0409517288208, "step": 146000}
{"episode_reward": 597.3325446878667, "episode": 147.0, "batch_reward": 0.327205902993679, "critic_loss": 0.434187826409936, "actor_loss": -32.42779651260376, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.917219400405884, "step": 147000}
{"episode_reward": 500.1512621262533, "episode": 148.0, "batch_reward": 0.329183996796608, "critic_loss": 0.41656147879362104, "actor_loss": -32.57137269973755, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.088403463363647, "step": 148000}
{"episode_reward": 507.58041070899674, "episode": 149.0, "batch_reward": 0.3303721552789211, "critic_loss": 0.40332506904006005, "actor_loss": -32.66396513748169, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.58315682411194, "step": 149000}
{"episode_reward": 558.6413243036162, "episode": 150.0, "batch_reward": 0.33074057364463805, "critic_loss": 0.4498550555408001, "actor_loss": -32.63952265930176, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
