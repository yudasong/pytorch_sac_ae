{"episode_reward": 0.0, "episode": 1.0, "duration": 19.788604974746704, "step": 1000}
{"episode_reward": 4.859792814687425, "episode": 2.0, "duration": 1.681333065032959, "step": 2000}
{"episode_reward": 550.1572824113056, "episode": 3.0, "batch_reward": 0.260217485829527, "critic_loss": 0.019974262323031757, "actor_loss": -26.170055469372073, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 70.65613555908203, "step": 3000}
{"episode_reward": 1.9619355808022223, "episode": 4.0, "batch_reward": 0.1613695397078991, "critic_loss": 0.010100052398163825, "actor_loss": -23.041315814971924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.451024770736694, "step": 4000}
{"episode_reward": 2.2748052455243144, "episode": 5.0, "batch_reward": 0.12504177171736955, "critic_loss": 0.013489985553082078, "actor_loss": -21.273203873872756, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.560314655303955, "step": 5000}
{"episode_reward": 2.3037357662248272, "episode": 6.0, "batch_reward": 0.10166879499331116, "critic_loss": 0.009566126218764112, "actor_loss": -21.600014209270476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.463992834091187, "step": 6000}
{"episode_reward": 2.021910323193442, "episode": 7.0, "batch_reward": 0.08688323887437582, "critic_loss": 0.011288002783549017, "actor_loss": -19.64745067834854, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.176035404205322, "step": 7000}
{"episode_reward": 2.31921275946241, "episode": 8.0, "batch_reward": 0.075773117268458, "critic_loss": 0.012213610363134649, "actor_loss": -20.32111294054985, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.906816720962524, "step": 8000}
{"episode_reward": 2.4036434383779293, "episode": 9.0, "batch_reward": 0.06681781901046634, "critic_loss": 0.013035844912868924, "actor_loss": -20.246299876213072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.501187324523926, "step": 9000}
{"episode_reward": 1.9220606711290724, "episode": 10.0, "batch_reward": 0.060357304556295274, "critic_loss": 0.013282993465312757, "actor_loss": -19.72156646156311, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.81536602973938, "step": 10000}
{"episode_reward": 1.970141794579969, "episode": 11.0, "batch_reward": 0.0556619368721731, "critic_loss": 0.01031473893095972, "actor_loss": -19.518722777366637, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.81879234313965, "step": 11000}
{"episode_reward": 3.0086445600506613, "episode": 12.0, "batch_reward": 0.04944990952732042, "critic_loss": 0.009983962908445393, "actor_loss": -19.54899370598793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.858907461166382, "step": 12000}
{"episode_reward": 3.15804249675911, "episode": 13.0, "batch_reward": 0.04611956342868507, "critic_loss": 0.014271649956004695, "actor_loss": -18.777059252262116, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.699883937835693, "step": 13000}
{"episode_reward": 2.874967887767724, "episode": 14.0, "batch_reward": 0.04342613205313683, "critic_loss": 0.009736320960219019, "actor_loss": -18.37994708800316, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.164377689361572, "step": 14000}
{"episode_reward": 3.6851592720477684, "episode": 15.0, "batch_reward": 0.040103752870112655, "critic_loss": 0.012355961738794576, "actor_loss": -19.489456892490388, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.51001286506653, "step": 15000}
{"episode_reward": 2.3400563274989725, "episode": 16.0, "batch_reward": 0.03786746419174597, "critic_loss": 0.013277436963515356, "actor_loss": -18.649588711738588, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.75185513496399, "step": 16000}
{"episode_reward": 1.9434252560770109, "episode": 17.0, "batch_reward": 0.03555681439279579, "critic_loss": 0.011770252433692804, "actor_loss": -18.33658141696453, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.619022130966187, "step": 17000}
{"episode_reward": 2.0581716134453805, "episode": 18.0, "batch_reward": 0.03455300214746967, "critic_loss": 0.008680363572377246, "actor_loss": -18.272494630813597, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.875871419906616, "step": 18000}
{"episode_reward": 3.450366781939305, "episode": 19.0, "batch_reward": 0.03237503709737211, "critic_loss": 0.010498535967286443, "actor_loss": -18.381398035764693, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.338019371032715, "step": 19000}
{"episode_reward": 2.4657233031629016, "episode": 20.0, "batch_reward": 0.03077546590846032, "critic_loss": 0.014001556305360282, "actor_loss": -19.33714575815201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.739837884902954, "step": 20000}
{"episode_reward": 1.9052691041439604, "episode": 21.0, "batch_reward": 0.02859259355976246, "critic_loss": 0.010578354018216487, "actor_loss": -17.070394933223724, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.41516923904419, "step": 21000}
{"episode_reward": 3.0171883043736782, "episode": 22.0, "batch_reward": 0.02826337614422664, "critic_loss": 0.013060211873234948, "actor_loss": -18.783527672052383, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.47576904296875, "step": 22000}
{"episode_reward": 3.7329415290821832, "episode": 23.0, "batch_reward": 0.026868770855711772, "critic_loss": 0.010165682756021852, "actor_loss": -17.681124752521516, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.642500638961792, "step": 23000}
{"episode_reward": 2.0364466935955905, "episode": 24.0, "batch_reward": 0.025746841959655285, "critic_loss": 0.009707070146512706, "actor_loss": -18.27783188676834, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.456089735031128, "step": 24000}
{"episode_reward": 2.205179288153676, "episode": 25.0, "batch_reward": 0.02537783662462607, "critic_loss": 0.012657986209407682, "actor_loss": -18.792877871990203, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.04966902732849, "step": 25000}
{"episode_reward": 2.990168523616827, "episode": 26.0, "batch_reward": 0.02411522403685376, "critic_loss": 0.011682161947042915, "actor_loss": -18.28522733259201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.480183839797974, "step": 26000}
{"episode_reward": 2.734883354953069, "episode": 27.0, "batch_reward": 0.02388041363656521, "critic_loss": 0.009773945740540512, "actor_loss": -17.537649785876273, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.727596521377563, "step": 27000}
{"episode_reward": 2.6500154867104326, "episode": 28.0, "batch_reward": 0.022907721064984798, "critic_loss": 0.008382936634865473, "actor_loss": -18.547652625083924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.043104648590088, "step": 28000}
{"episode_reward": 2.286445627288991, "episode": 29.0, "batch_reward": 0.02162356277590152, "critic_loss": 0.009576950944232521, "actor_loss": -17.499079075574876, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.112572193145752, "step": 29000}
{"episode_reward": 2.6702386782096283, "episode": 30.0, "batch_reward": 0.021300178746227173, "critic_loss": 0.00993389073688013, "actor_loss": -17.052361671328544, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.602080821990967, "step": 30000}
{"episode_reward": 3.0715253769176707, "episode": 31.0, "batch_reward": 0.02050399088463746, "critic_loss": 0.007093992905138294, "actor_loss": -17.33714779448509, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.78748345375061, "step": 31000}
{"episode_reward": 2.4172758265716285, "episode": 32.0, "batch_reward": 0.020055035441881044, "critic_loss": 0.010627715714712395, "actor_loss": -17.713509246587755, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.60133695602417, "step": 32000}
{"episode_reward": 2.4164902281141103, "episode": 33.0, "batch_reward": 0.019545236971927806, "critic_loss": 0.009724879484565463, "actor_loss": -17.615213475883007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.097662687301636, "step": 33000}
{"episode_reward": 2.6215303343913723, "episode": 34.0, "batch_reward": 0.019179411000106485, "critic_loss": 0.007683329340630735, "actor_loss": -17.29644119799137, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.58065438270569, "step": 34000}
{"episode_reward": 2.719833147317731, "episode": 35.0, "batch_reward": 0.017875406438717618, "critic_loss": 0.006375889427916263, "actor_loss": -17.687432265758513, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.32568335533142, "step": 35000}
{"episode_reward": 1.8692118624670662, "episode": 36.0, "batch_reward": 0.017612145495251752, "critic_loss": 0.006633213510838686, "actor_loss": -18.613044636189937, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.868407011032104, "step": 36000}
{"episode_reward": 2.929321368217695, "episode": 37.0, "batch_reward": 0.017673776110168547, "critic_loss": 0.01116362453099282, "actor_loss": -17.26606835180521, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.627599716186523, "step": 37000}
{"episode_reward": 2.9683602051577385, "episode": 38.0, "batch_reward": 0.017007663546828554, "critic_loss": 0.005488843903847737, "actor_loss": -17.242177145421504, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.253406524658203, "step": 38000}
{"episode_reward": 2.2575200073140946, "episode": 39.0, "batch_reward": 0.016902017920510843, "critic_loss": 0.008864056017671828, "actor_loss": -17.188352057993413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.895965576171875, "step": 39000}
{"episode_reward": 1.907519446522637, "episode": 40.0, "batch_reward": 0.016419439994497225, "critic_loss": 0.00689069178407226, "actor_loss": -17.555840050160885, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.378122568130493, "step": 40000}
{"episode_reward": 2.7281834424013462, "episode": 41.0, "batch_reward": 0.016021263882517813, "critic_loss": 0.008440605961994151, "actor_loss": -16.24722277981043, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.25060844421387, "step": 41000}
{"episode_reward": 2.3123499472319082, "episode": 42.0, "batch_reward": 0.015774658101028762, "critic_loss": 0.00629117198784661, "actor_loss": -17.0864085714221, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.43270468711853, "step": 42000}
{"episode_reward": 2.5745810221519987, "episode": 43.0, "batch_reward": 0.015243280662805773, "critic_loss": 0.009767192047314893, "actor_loss": -17.729256241261957, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.81745147705078, "step": 43000}
{"episode_reward": 2.111405030020732, "episode": 44.0, "batch_reward": 0.015271559118875302, "critic_loss": 0.005487679157260572, "actor_loss": -19.051669190645217, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.9941885471344, "step": 44000}
{"episode_reward": 3.0136334609828523, "episode": 45.0, "batch_reward": 0.014666451403871178, "critic_loss": 0.0077365348633611575, "actor_loss": -17.98562309807539, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.798946380615234, "step": 45000}
{"episode_reward": 3.351420810578337, "episode": 46.0, "batch_reward": 0.014272522811777889, "critic_loss": 0.006224475868788431, "actor_loss": -16.59669520741701, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.943804025650024, "step": 46000}
{"episode_reward": 2.620955117737939, "episode": 47.0, "batch_reward": 0.014123634686227888, "critic_loss": 0.006089796719592414, "actor_loss": -17.39127928239107, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.368286609649658, "step": 47000}
{"episode_reward": 2.9261789991242275, "episode": 48.0, "batch_reward": 0.0140641773941461, "critic_loss": 0.007506581330584595, "actor_loss": -17.196267217814924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.82483720779419, "step": 48000}
{"episode_reward": 2.8178012211957846, "episode": 49.0, "batch_reward": 0.014044023239053786, "critic_loss": 0.005928824209535378, "actor_loss": -18.612651857495308, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.558056592941284, "step": 49000}
{"episode_reward": 2.680019216995761, "episode": 50.0, "batch_reward": 0.013633027995703742, "critic_loss": 0.009008415176707786, "actor_loss": -18.098302778124808, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.376277446746826, "step": 50000}
{"episode_reward": 3.391744241170018, "episode": 51.0, "batch_reward": 0.013435968171106651, "critic_loss": 0.0059440418849189885, "actor_loss": -17.79750558972359, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.45591711997986, "step": 51000}
{"episode_reward": 2.33845305637397, "episode": 52.0, "batch_reward": 0.013298873543157242, "critic_loss": 0.0067464555035330704, "actor_loss": -16.838198120772837, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.897563934326172, "step": 52000}
{"episode_reward": 1.6314000115121012, "episode": 53.0, "batch_reward": 0.013069769973983058, "critic_loss": 0.006151703567862569, "actor_loss": -17.928468686819077, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.473586320877075, "step": 53000}
{"episode_reward": 1.8148608459034157, "episode": 54.0, "batch_reward": 0.012789266183041036, "critic_loss": 0.005643006328333286, "actor_loss": -18.842838390946387, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.96838903427124, "step": 54000}
{"episode_reward": 2.291137126121344, "episode": 55.0, "batch_reward": 0.01287882173084654, "critic_loss": 0.006594956269662362, "actor_loss": -17.259129424870014, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.262176275253296, "step": 55000}
{"episode_reward": 2.484918370218459, "episode": 56.0, "batch_reward": 0.012311329799122177, "critic_loss": 0.004476715086741024, "actor_loss": -17.559774253547193, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.64806818962097, "step": 56000}
{"episode_reward": 2.083576597842594, "episode": 57.0, "batch_reward": 0.012263646573061124, "critic_loss": 0.007339940834062872, "actor_loss": -17.393123220384123, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.209514617919922, "step": 57000}
{"episode_reward": 3.560711975742037, "episode": 58.0, "batch_reward": 0.012001608177437447, "critic_loss": 0.005273694964205788, "actor_loss": -17.566123285740613, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.591723918914795, "step": 58000}
{"episode_reward": 3.3298065553024476, "episode": 59.0, "batch_reward": 0.011828625362948514, "critic_loss": 0.004449940014339518, "actor_loss": -17.88332947835326, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.856765747070312, "step": 59000}
{"episode_reward": 2.2072556889819075, "episode": 60.0, "batch_reward": 0.01191986277536489, "critic_loss": 0.003308966850745492, "actor_loss": -18.16566623362899, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.877199172973633, "step": 60000}
{"episode_reward": 2.0288801767955524, "episode": 61.0, "batch_reward": 0.011851716211531312, "critic_loss": 0.005018759630569548, "actor_loss": -17.077464352220296, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 46.08519697189331, "step": 61000}
{"episode_reward": 1.7768095879477668, "episode": 62.0, "batch_reward": 0.011791495870915242, "critic_loss": 0.005743334577346104, "actor_loss": -18.308034917145967, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.959887981414795, "step": 62000}
{"episode_reward": 2.9884513557712324, "episode": 63.0, "batch_reward": 0.011287551366374828, "critic_loss": 0.005258507900674885, "actor_loss": -17.166546094477177, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.424921989440918, "step": 63000}
{"episode_reward": 1.5676918923614078, "episode": 64.0, "batch_reward": 0.011090658441884444, "critic_loss": 0.0032528403669275577, "actor_loss": -17.489763207495212, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.506630659103394, "step": 64000}
{"episode_reward": 1.3503716677516617, "episode": 65.0, "batch_reward": 0.010898943906067871, "critic_loss": 0.005148622565619008, "actor_loss": -17.665690302729608, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.96329617500305, "step": 65000}
{"episode_reward": 2.8713427516745025, "episode": 66.0, "batch_reward": 0.01075149286875967, "critic_loss": 0.005426145460165572, "actor_loss": -17.26344465070963, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.548861980438232, "step": 66000}
{"episode_reward": 1.871230642921349, "episode": 67.0, "batch_reward": 0.010848235969198868, "critic_loss": 0.0035008146904001477, "actor_loss": -18.114818225353957, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.378120183944702, "step": 67000}
{"episode_reward": 1.8573028735275483, "episode": 68.0, "batch_reward": 0.010541309104999527, "critic_loss": 0.005305867045783089, "actor_loss": -18.93923575529456, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.602022409439087, "step": 68000}
{"episode_reward": 3.129438294993311, "episode": 69.0, "batch_reward": 0.010581527321250178, "critic_loss": 0.004975173662220186, "actor_loss": -16.89394839632511, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.76532244682312, "step": 69000}
{"episode_reward": 1.9493759103471882, "episode": 70.0, "batch_reward": 0.010475364051992073, "critic_loss": 0.004787029875529697, "actor_loss": -16.95471941614151, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.37558388710022, "step": 70000}
{"episode_reward": 2.69331027766417, "episode": 71.0, "batch_reward": 0.010394817661494016, "critic_loss": 0.005021214949039858, "actor_loss": -16.61149289482832, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.66686463356018, "step": 71000}
{"episode_reward": 2.003632300382943, "episode": 72.0, "batch_reward": 0.010315226981649175, "critic_loss": 0.004007215645964607, "actor_loss": -17.065291600674392, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.747436046600342, "step": 72000}
{"episode_reward": 2.4565852939855066, "episode": 73.0, "batch_reward": 0.01018166368175298, "critic_loss": 0.005623131412336079, "actor_loss": -17.574592724084855, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.37229323387146, "step": 73000}
{"episode_reward": 1.7632737288220468, "episode": 74.0, "batch_reward": 0.009747107880655677, "critic_loss": 0.004245121951091278, "actor_loss": -17.392081915289165, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.356952905654907, "step": 74000}
{"episode_reward": 2.772635274343989, "episode": 75.0, "batch_reward": 0.009584844440105371, "critic_loss": 0.004505023673817049, "actor_loss": -17.2516596968174, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.39525818824768, "step": 75000}
{"episode_reward": 2.2052967441879145, "episode": 76.0, "batch_reward": 0.009928860708256252, "critic_loss": 0.00548076802577998, "actor_loss": -17.212915156185627, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.945120096206665, "step": 76000}
{"episode_reward": 3.0353807318094406, "episode": 77.0, "batch_reward": 0.009967674043495208, "critic_loss": 0.006171571767947171, "actor_loss": -17.448107732713222, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.36605668067932, "step": 77000}
{"episode_reward": 2.8184291025225923, "episode": 78.0, "batch_reward": 0.010030459432979115, "critic_loss": 0.00442055190911924, "actor_loss": -17.092346507787706, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.10360836982727, "step": 78000}
{"episode_reward": 3.0524267601238497, "episode": 79.0, "batch_reward": 0.009416671725688502, "critic_loss": 0.006444406190094014, "actor_loss": -17.771329202115535, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.06745409965515, "step": 79000}
{"episode_reward": 2.991673080565975, "episode": 80.0, "batch_reward": 0.009492619064054452, "critic_loss": 0.0035252125007973517, "actor_loss": -18.03336907878518, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.354119777679443, "step": 80000}
{"episode_reward": 2.369823523754639, "episode": 81.0, "batch_reward": 0.009055454205605202, "critic_loss": 0.0062623993075467295, "actor_loss": -17.63316048771143, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.683799743652344, "step": 81000}
{"episode_reward": 3.1026532337153236, "episode": 82.0, "batch_reward": 0.00907813727343455, "critic_loss": 0.00428254981210921, "actor_loss": -16.50627093231678, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.526198625564575, "step": 82000}
{"episode_reward": 2.2558142515249435, "episode": 83.0, "batch_reward": 0.009175562280695885, "critic_loss": 0.003942964344692882, "actor_loss": -17.912041224598884, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.345937252044678, "step": 83000}
{"episode_reward": 2.1657318767042595, "episode": 84.0, "batch_reward": 0.008977670045453124, "critic_loss": 0.004816873215509986, "actor_loss": -18.000746010035275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.849669694900513, "step": 84000}
{"episode_reward": 3.373628560789961, "episode": 85.0, "batch_reward": 0.008967667661374435, "critic_loss": 0.0050779266796926095, "actor_loss": -17.064436903595926, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.13645076751709, "step": 85000}
{"episode_reward": 2.5479684840342616, "episode": 86.0, "batch_reward": 0.009084196548094042, "critic_loss": 0.004424278211619822, "actor_loss": -16.828155606478454, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.51040267944336, "step": 86000}
{"episode_reward": 3.0030257414302746, "episode": 87.0, "batch_reward": 0.008900173953850754, "critic_loss": 0.003385350872456911, "actor_loss": -18.402242445647715, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.599162817001343, "step": 87000}
{"episode_reward": 3.047201420599913, "episode": 88.0, "batch_reward": 0.008458417904330417, "critic_loss": 0.003259786403788894, "actor_loss": -16.518029595434665, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.134281396865845, "step": 88000}
{"episode_reward": 3.0435403354881236, "episode": 89.0, "batch_reward": 0.008828780879499391, "critic_loss": 0.00461098725291231, "actor_loss": -16.302005689769985, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.97489643096924, "step": 89000}
{"episode_reward": 2.4227682376605313, "episode": 90.0, "batch_reward": 0.008831811940995976, "critic_loss": 0.004190580590286118, "actor_loss": -16.48020508405566, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.35244083404541, "step": 90000}
{"episode_reward": 2.8132489415436135, "episode": 91.0, "batch_reward": 0.008612427194719203, "critic_loss": 0.003346863305370789, "actor_loss": -16.293152780920266, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.34203505516052, "step": 91000}
{"episode_reward": 2.1302786820752537, "episode": 92.0, "batch_reward": 0.008453534786822274, "critic_loss": 0.004415679491379706, "actor_loss": -17.25802666479349, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.72489285469055, "step": 92000}
{"episode_reward": 3.292401830124011, "episode": 93.0, "batch_reward": 0.008365169227006846, "critic_loss": 0.003895269295971957, "actor_loss": -16.21661209948361, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.361684322357178, "step": 93000}
{"episode_reward": 2.838700440810163, "episode": 94.0, "batch_reward": 0.008301190123544074, "critic_loss": 0.004507837057528377, "actor_loss": -17.515737429499627, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.281853914260864, "step": 94000}
{"episode_reward": 3.277650101952145, "episode": 95.0, "batch_reward": 0.008168236169731245, "critic_loss": 0.0038673694995159168, "actor_loss": -18.587313393965363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.763561964035034, "step": 95000}
{"episode_reward": 3.3048608558308588, "episode": 96.0, "batch_reward": 0.008529049088014289, "critic_loss": 0.004635929535470496, "actor_loss": -17.113945115596056, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.372058629989624, "step": 96000}
{"episode_reward": 2.7063816143247297, "episode": 97.0, "batch_reward": 0.008322656163363718, "critic_loss": 0.003925697278682492, "actor_loss": -15.905730106443167, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.490942239761353, "step": 97000}
{"episode_reward": 2.667299353945528, "episode": 98.0, "batch_reward": 0.008008678739424796, "critic_loss": 0.0032087588129579674, "actor_loss": -18.798131806388497, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.29381036758423, "step": 98000}
{"episode_reward": 2.872904211790585, "episode": 99.0, "batch_reward": 0.008034936495590955, "critic_loss": 0.0027734099017179686, "actor_loss": -16.91354149326682, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.77758264541626, "step": 99000}
{"episode_reward": 2.192822816991359, "episode": 100.0, "batch_reward": 0.008121925743878818, "critic_loss": 0.0033479992278007556, "actor_loss": -16.965943345680834, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.39496636390686, "step": 100000}
{"episode_reward": 2.8327374096624425, "episode": 101.0, "batch_reward": 0.008181849171291106, "critic_loss": 0.0032884701579241663, "actor_loss": -16.583278002917766, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.07650303840637, "step": 101000}
{"episode_reward": 2.712812847923359, "episode": 102.0, "batch_reward": 0.008199288353091106, "critic_loss": 0.003636046381703636, "actor_loss": -17.60953067806363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.376017570495605, "step": 102000}
{"episode_reward": 2.674712195059109, "episode": 103.0, "batch_reward": 0.008209900931920856, "critic_loss": 0.002340362461240147, "actor_loss": -17.670967571392655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.43595027923584, "step": 103000}
{"episode_reward": 2.152997033847646, "episode": 104.0, "batch_reward": 0.008056441276916303, "critic_loss": 0.0031063718501027323, "actor_loss": -16.357298263534904, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.28068447113037, "step": 104000}
{"episode_reward": 2.8343931206495094, "episode": 105.0, "batch_reward": 0.007906160325743258, "critic_loss": 0.0027023051597279846, "actor_loss": -16.524368748798967, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.737964630126953, "step": 105000}
{"episode_reward": 1.92326423536411, "episode": 106.0, "batch_reward": 0.007466168432496488, "critic_loss": 0.0027292600899309037, "actor_loss": -16.727774632781745, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.37249517440796, "step": 106000}
{"episode_reward": 3.2595089506117754, "episode": 107.0, "batch_reward": 0.007748640678939409, "critic_loss": 0.003112842923190328, "actor_loss": -15.733393414780497, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.345938682556152, "step": 107000}
{"episode_reward": 2.6866379442245263, "episode": 108.0, "batch_reward": 0.008004654065938666, "critic_loss": 0.003306359210961091, "actor_loss": -18.457446579620242, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.65398359298706, "step": 108000}
{"episode_reward": 2.495434688537789, "episode": 109.0, "batch_reward": 0.007437592785456218, "critic_loss": 0.0022423227312901874, "actor_loss": -16.95220913541317, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.621583700180054, "step": 109000}
{"episode_reward": 3.1130549940319496, "episode": 110.0, "batch_reward": 0.007850372704677283, "critic_loss": 0.0025222355546247856, "actor_loss": -18.730940646320583, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.78820252418518, "step": 110000}
{"episode_reward": 2.160202337870946, "episode": 111.0, "batch_reward": 0.007504139159456827, "critic_loss": 0.003359679902532662, "actor_loss": -16.40008405137062, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.624576568603516, "step": 111000}
{"episode_reward": 2.009288988561818, "episode": 112.0, "batch_reward": 0.007579317677184008, "critic_loss": 0.003941173820960103, "actor_loss": -18.26446863490343, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.366031885147095, "step": 112000}
{"episode_reward": 3.038818560481954, "episode": 113.0, "batch_reward": 0.0075214615298900755, "critic_loss": 0.0024974911178942422, "actor_loss": -16.757959947898986, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.367464780807495, "step": 113000}
{"episode_reward": 2.7979150782076294, "episode": 114.0, "batch_reward": 0.007557459784089587, "critic_loss": 0.002408124417743238, "actor_loss": -17.541483087956905, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.736589670181274, "step": 114000}
{"episode_reward": 3.449010317545242, "episode": 115.0, "batch_reward": 0.007130691420985386, "critic_loss": 0.002871904986583104, "actor_loss": -17.452482139080765, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.391010284423828, "step": 115000}
{"episode_reward": 2.519831340007467, "episode": 116.0, "batch_reward": 0.007405055308830924, "critic_loss": 0.002611378525085456, "actor_loss": -17.476651942282913, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.559694051742554, "step": 116000}
{"episode_reward": 1.9765381640735105, "episode": 117.0, "batch_reward": 0.007209475465118885, "critic_loss": 0.0029315622786743917, "actor_loss": -16.508923671901226, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.287046432495117, "step": 117000}
{"episode_reward": 1.85472532360094, "episode": 118.0, "batch_reward": 0.007106279947678559, "critic_loss": 0.00197580879607267, "actor_loss": -16.665516136407852, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.662684202194214, "step": 118000}
{"episode_reward": 2.227486272175402, "episode": 119.0, "batch_reward": 0.007417162961792201, "critic_loss": 0.003057984599538031, "actor_loss": -16.56250585734844, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.38477349281311, "step": 119000}
{"episode_reward": 2.2948546672599943, "episode": 120.0, "batch_reward": 0.007370024320902303, "critic_loss": 0.002908222648111405, "actor_loss": -16.566293011114002, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.83347773551941, "step": 120000}
{"episode_reward": 2.2997639692472838, "episode": 121.0, "batch_reward": 0.006975703201605938, "critic_loss": 0.0025577420454646928, "actor_loss": -16.773158684760332, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.289867877960205, "step": 121000}
{"episode_reward": 2.5762673037117194, "episode": 122.0, "batch_reward": 0.0069972691785078495, "critic_loss": 0.0020921560417409635, "actor_loss": -17.348287759169935, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.374258279800415, "step": 122000}
{"episode_reward": 3.6838905036299527, "episode": 123.0, "batch_reward": 0.007078725011204369, "critic_loss": 0.002993597094187862, "actor_loss": -17.055687249824405, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.548844575881958, "step": 123000}
{"episode_reward": 2.371605229750095, "episode": 124.0, "batch_reward": 0.007276598121155985, "critic_loss": 0.0022808990404810173, "actor_loss": -18.31015444919467, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.262776374816895, "step": 124000}
{"episode_reward": 2.544964154718511, "episode": 125.0, "batch_reward": 0.006827019413351081, "critic_loss": 0.002080205768936139, "actor_loss": -17.03203889453411, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.371739864349365, "step": 125000}
{"episode_reward": 1.8725186101293383, "episode": 126.0, "batch_reward": 0.007063527069054544, "critic_loss": 0.0031478984149580355, "actor_loss": -17.680602982744574, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.981385707855225, "step": 126000}
{"episode_reward": 4.112794993566306, "episode": 127.0, "batch_reward": 0.006922365362290293, "critic_loss": 0.002483920055157796, "actor_loss": -17.795453785553576, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.16925597190857, "step": 127000}
{"episode_reward": 2.3678211085586827, "episode": 128.0, "batch_reward": 0.006908718526479788, "critic_loss": 0.002361575847156928, "actor_loss": -17.4481967715323, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.404954433441162, "step": 128000}
{"episode_reward": 1.9550149641144339, "episode": 129.0, "batch_reward": 0.007003326628007926, "critic_loss": 0.0021754679518489867, "actor_loss": -17.186908160313966, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.36449122428894, "step": 129000}
{"episode_reward": 2.8372792711702006, "episode": 130.0, "batch_reward": 0.006712867065099999, "critic_loss": 0.0034134968902144467, "actor_loss": -17.34143316723406, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.24828839302063, "step": 130000}
{"episode_reward": 2.3237686862394913, "episode": 131.0, "batch_reward": 0.00672371431440115, "critic_loss": 0.003307609376461187, "actor_loss": -16.031004456460476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.94772410392761, "step": 131000}
{"episode_reward": 2.51834543130966, "episode": 132.0, "batch_reward": 0.006732900123810396, "critic_loss": 0.0027905045241277547, "actor_loss": -17.61509133809805, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.36813259124756, "step": 132000}
{"episode_reward": 2.5789001995896097, "episode": 133.0, "batch_reward": 0.006680641784565523, "critic_loss": 0.001874526376624999, "actor_loss": -16.198223284035922, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.212610006332397, "step": 133000}
{"episode_reward": 2.2322063918247284, "episode": 134.0, "batch_reward": 0.006949401217396371, "critic_loss": 0.002413665597254294, "actor_loss": -16.81842859506607, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.158939599990845, "step": 134000}
{"episode_reward": 3.6117934717829625, "episode": 135.0, "batch_reward": 0.006464360478683375, "critic_loss": 0.001921884678216884, "actor_loss": -17.71382842896879, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.367275953292847, "step": 135000}
{"episode_reward": 2.584493584462396, "episode": 136.0, "batch_reward": 0.00675020480027888, "critic_loss": 0.0027912362292481703, "actor_loss": -17.674419565811753, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.709166049957275, "step": 136000}
{"episode_reward": 2.5170457549235463, "episode": 137.0, "batch_reward": 0.006539085505413823, "critic_loss": 0.0020664108814817156, "actor_loss": -17.0280109603107, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.293573141098022, "step": 137000}
{"episode_reward": 2.8029401810566186, "episode": 138.0, "batch_reward": 0.006571520389639773, "critic_loss": 0.0032082345857634208, "actor_loss": -17.32309732605517, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.89115333557129, "step": 138000}
{"episode_reward": 1.566047261379159, "episode": 139.0, "batch_reward": 0.0064201832170365375, "critic_loss": 0.0017732777583187272, "actor_loss": -16.7558765258193, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.394598484039307, "step": 139000}
{"episode_reward": 4.237261678459232, "episode": 140.0, "batch_reward": 0.006732018625363707, "critic_loss": 0.0027834495386487106, "actor_loss": -18.062883448570968, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.228294849395752, "step": 140000}
{"episode_reward": 3.5357221475697096, "episode": 141.0, "batch_reward": 0.006453711508656852, "critic_loss": 0.0020256503736200103, "actor_loss": -16.91660764785111, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.78159761428833, "step": 141000}
{"episode_reward": 2.756243366365015, "episode": 142.0, "batch_reward": 0.006481847542105242, "critic_loss": 0.002772182338809216, "actor_loss": -16.863885449767114, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.369324445724487, "step": 142000}
{"episode_reward": 2.1497711376070274, "episode": 143.0, "batch_reward": 0.006610132246976718, "critic_loss": 0.00280858186945261, "actor_loss": -16.413855440095066, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.1887845993042, "step": 143000}
{"episode_reward": 5.167981016439812, "episode": 144.0, "batch_reward": 0.006437864650972187, "critic_loss": 0.001484641109193035, "actor_loss": -17.59136719878018, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.257137537002563, "step": 144000}
{"episode_reward": 1.9597893721823225, "episode": 145.0, "batch_reward": 0.006241237420705147, "critic_loss": 0.001631877726824314, "actor_loss": -17.759412733227016, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.393635272979736, "step": 145000}
{"episode_reward": 3.3689081557354212, "episode": 146.0, "batch_reward": 0.006254342882544734, "critic_loss": 0.0028575538576915277, "actor_loss": -16.234633836865424, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.32462763786316, "step": 146000}
{"episode_reward": 2.0628330778060135, "episode": 147.0, "batch_reward": 0.006443943338934332, "critic_loss": 0.0025746909101508207, "actor_loss": -17.14208534581959, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.11679744720459, "step": 147000}
{"episode_reward": 2.0482133968889586, "episode": 148.0, "batch_reward": 0.006229819243191741, "critic_loss": 0.0020695520356275665, "actor_loss": -17.41571144321561, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.82384753227234, "step": 148000}
{"episode_reward": 1.995715939187759, "episode": 149.0, "batch_reward": 0.006295254702679813, "critic_loss": 0.003074666467073257, "actor_loss": -17.133213714256883, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.359103441238403, "step": 149000}
{"episode_reward": 3.6488091103476075, "episode": 150.0, "batch_reward": 0.006044530992978253, "critic_loss": 0.0022072589674717166, "actor_loss": -17.121065739646554, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
