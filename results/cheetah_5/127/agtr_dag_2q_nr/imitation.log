{"mmd_loss": 0.0855717808008194, "episode": 2.0, "episode_reward": 0.6388590497847214, "step": 1000}
{"batch_reward": -0.23875275461583048, "critic_loss": 0.20792193075192644, "actor_loss": 1.0529401776210996, "actor_target_entropy": -6.0, "actor_entropy": 4.9230409757885525, "alpha_loss": 0.7781863659321665, "alpha_value": 0.09758840514160358, "mmd_loss": 0.07669103145599365, "episode": 3.0, "episode_reward": 2.0984740209136525, "step": 2000}
{"batch_reward": -0.22212699572741984, "critic_loss": 0.00884368513058871, "actor_loss": 0.416346849322319, "actor_target_entropy": -6.0, "actor_entropy": 7.258379556655884, "alpha_loss": 0.9075530196428299, "alpha_value": 0.09234540987083141, "mmd_loss": 0.07380999624729156, "episode": 4.0, "episode_reward": 4.31538381557989, "step": 3000}
{"batch_reward": -0.21619093483686447, "critic_loss": 0.004117669221712276, "actor_loss": -0.1942329287547618, "actor_target_entropy": -6.0, "actor_entropy": 7.609134114265442, "alpha_loss": 0.8720838186740876, "alpha_value": 0.08774797910327292, "mmd_loss": 0.07110075652599335, "episode": 5.0, "episode_reward": 4.512962641611757, "step": 4000}
{"batch_reward": -0.20980558133125304, "critic_loss": 0.0032981149522820488, "actor_loss": -0.7245547816157341, "actor_target_entropy": -6.0, "actor_entropy": 7.7189466114044185, "alpha_loss": 0.8351641491651535, "alpha_value": 0.08355593023074787, "mmd_loss": 0.06883729994297028, "episode": 6.0, "episode_reward": 4.652174581594296, "step": 5000}
{"batch_reward": -0.204177532017231, "critic_loss": 0.0027536923161242156, "actor_loss": -1.2169424706697465, "actor_target_entropy": -6.0, "actor_entropy": 7.76046757888794, "alpha_loss": 0.7986393733024597, "alpha_value": 0.07965797445010309, "mmd_loss": 0.06789537519216537, "episode": 7.0, "episode_reward": 1.5960980318127977, "step": 6000}
{"batch_reward": -0.20076199799776076, "critic_loss": 0.002433671073173173, "actor_loss": -1.6544125504493714, "actor_target_entropy": -6.0, "actor_entropy": 7.770906639099121, "alpha_loss": 0.7633777995109559, "alpha_value": 0.07600003875518092, "mmd_loss": 0.06585748493671417, "episode": 8.0, "episode_reward": 8.227301264271738, "step": 7000}
{"batch_reward": -0.19553541985154152, "critic_loss": 0.002185295430419501, "actor_loss": -2.038570735216141, "actor_target_entropy": -6.0, "actor_entropy": 7.767397601127625, "alpha_loss": 0.7290927920341491, "alpha_value": 0.07254958837633821, "mmd_loss": 0.06539211422204971, "episode": 9.0, "episode_reward": 4.367161076529113, "step": 8000}
{"batch_reward": -0.19370699326694013, "critic_loss": 0.0020411211255704984, "actor_loss": -2.3583599109649658, "actor_target_entropy": -6.0, "actor_entropy": 7.789586668014526, "alpha_loss": 0.6964571191072464, "alpha_value": 0.06928228391544823, "mmd_loss": 0.06469619274139404, "episode": 10.0, "episode_reward": 5.465697981528283, "step": 9000}
{"batch_reward": -0.19116947601735593, "critic_loss": 0.0018929441657382995, "actor_loss": -2.617627320766449, "actor_target_entropy": -6.0, "actor_entropy": 7.7816361045837406, "alpha_loss": 0.6648945519924164, "alpha_value": 0.06618082356717207, "mmd_loss": 0.06325751543045044, "episode": 11.0, "episode_reward": 4.406355561216639, "step": 10000}
{"batch_reward": -0.18712230321764947, "critic_loss": 0.0018793057732982561, "actor_loss": -2.8289188981056212, "actor_target_entropy": -6.0, "actor_entropy": 7.763916702270508, "alpha_loss": 0.6344833402633667, "alpha_value": 0.06323142752899938, "mmd_loss": 0.062165144830942154, "episode": 12.0, "episode_reward": 7.23551535968868, "step": 11000}
{"batch_reward": -0.18403881250321866, "critic_loss": 0.0018362615581136196, "actor_loss": -2.9903842496871946, "actor_target_entropy": -6.0, "actor_entropy": 7.775789192199707, "alpha_loss": 0.6056092079877854, "alpha_value": 0.060422584265024774, "mmd_loss": 0.0605010911822319, "episode": 13.0, "episode_reward": 6.542332622302444, "step": 12000}
{"batch_reward": -0.17943223717808723, "critic_loss": 0.001875763086718507, "actor_loss": -3.114047887802124, "actor_target_entropy": -6.0, "actor_entropy": 7.79075360584259, "alpha_loss": 0.5779518319368362, "alpha_value": 0.05774462204840933, "mmd_loss": 0.058311186730861664, "episode": 14.0, "episode_reward": 15.01452971534149, "step": 13000}
{"batch_reward": -0.17356264433264731, "critic_loss": 0.0018443070418434218, "actor_loss": -3.208670213699341, "actor_target_entropy": -6.0, "actor_entropy": 7.772727047920227, "alpha_loss": 0.5510445932149887, "alpha_value": 0.055190390719891055, "mmd_loss": 0.05500955879688263, "episode": 15.0, "episode_reward": 22.402481995242706, "step": 14000}
{"batch_reward": -0.16504932712018489, "critic_loss": 0.0018558125951094553, "actor_loss": -3.286813481807709, "actor_target_entropy": -6.0, "actor_entropy": 7.775660394668579, "alpha_loss": 0.5248149621486664, "alpha_value": 0.05275494577248924, "mmd_loss": 0.05200553685426712, "episode": 16.0, "episode_reward": 13.257127790238666, "step": 15000}
{"batch_reward": -0.15647609105706214, "critic_loss": 0.0018159907106892205, "actor_loss": -3.351039339542389, "actor_target_entropy": -6.0, "actor_entropy": 7.754754886627198, "alpha_loss": 0.4998142719268799, "alpha_value": 0.0504309303852635, "mmd_loss": 0.04898027330636978, "episode": 17.0, "episode_reward": 24.74900404611096, "step": 16000}
{"batch_reward": -0.14828545290231704, "critic_loss": 0.0018476315708248875, "actor_loss": -3.4051719937324525, "actor_target_entropy": -6.0, "actor_entropy": 7.746452858924866, "alpha_loss": 0.4760649502873421, "alpha_value": 0.04821247182368947, "mmd_loss": 0.04567466303706169, "episode": 18.0, "episode_reward": 19.41316515763016, "step": 17000}
{"batch_reward": -0.13886623634397985, "critic_loss": 0.0018903871722286568, "actor_loss": -3.457621980190277, "actor_target_entropy": -6.0, "actor_entropy": 7.742464243888855, "alpha_loss": 0.4533007188439369, "alpha_value": 0.046093491033892736, "mmd_loss": 0.042331770062446594, "episode": 19.0, "episode_reward": 42.8628710291872, "step": 18000}
{"batch_reward": -0.12947545368224384, "critic_loss": 0.002118141755578108, "actor_loss": -3.5120456519126892, "actor_target_entropy": -6.0, "actor_entropy": 7.726793274879456, "alpha_loss": 0.4319034895300865, "alpha_value": 0.04406850187990183, "mmd_loss": 0.038806602358818054, "episode": 20.0, "episode_reward": 43.225636089564354, "step": 19000}
{"batch_reward": -0.11867839883267879, "critic_loss": 0.0023543004042003305, "actor_loss": -3.570406363964081, "actor_target_entropy": -6.0, "actor_entropy": 7.709589820861816, "alpha_loss": 0.4113354562520981, "alpha_value": 0.04213308174642804, "mmd_loss": 0.03502612188458443, "episode": 21.0, "episode_reward": 34.637328678022286, "step": 20000}
{"batch_reward": -0.1068508245125413, "critic_loss": 0.002704481154331006, "actor_loss": -3.6382142233848573, "actor_target_entropy": -6.0, "actor_entropy": 7.676065438270569, "alpha_loss": 0.39121212941408157, "alpha_value": 0.040284387828902465, "mmd_loss": 0.031529586762189865, "episode": 22.0, "episode_reward": 59.90593818046252, "step": 21000}
{"batch_reward": -0.09580493804067373, "critic_loss": 0.0029045106299454347, "actor_loss": -3.725020502567291, "actor_target_entropy": -6.0, "actor_entropy": 7.6507324686050415, "alpha_loss": 0.3722401288151741, "alpha_value": 0.03851877069950139, "mmd_loss": 0.028461584821343422, "episode": 23.0, "episode_reward": 53.6670344987567, "step": 22000}
{"batch_reward": -0.0853224252909422, "critic_loss": 0.00264274723676499, "actor_loss": -3.8155333495140074, "actor_target_entropy": -6.0, "actor_entropy": 7.624455818176269, "alpha_loss": 0.3547085562944412, "alpha_value": 0.03682992687709041, "mmd_loss": 0.025796376168727875, "episode": 24.0, "episode_reward": 56.810876001287475, "step": 23000}
{"batch_reward": -0.07502628741413354, "critic_loss": 0.00311492625111714, "actor_loss": -3.915003027439117, "actor_target_entropy": -6.0, "actor_entropy": 7.624553967475891, "alpha_loss": 0.33888429987430574, "alpha_value": 0.03521244038589118, "mmd_loss": 0.02218019962310791, "episode": 25.0, "episode_reward": 38.15062885438625, "step": 24000}
{"batch_reward": -0.06073049965500832, "critic_loss": 0.0027928451641928403, "actor_loss": -4.02945752954483, "actor_target_entropy": -6.0, "actor_entropy": 7.609326547622681, "alpha_loss": 0.3251097814440727, "alpha_value": 0.03366045057809348, "mmd_loss": 0.020456060767173767, "episode": 26.0, "episode_reward": 77.65102245191201, "step": 25000}
