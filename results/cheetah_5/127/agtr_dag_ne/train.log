{"episode": 1.0, "duration": 17.14920425415039, "episode_reward": 4.859792814687425, "step": 1000}
{"episode": 2.0, "duration": 1.470780611038208, "episode_reward": 550.1572824113056, "step": 2000}
{"episode": 3.0, "batch_reward": 0.2879692185378226, "critic_loss": 0.43404677076493153, "actor_loss": -42.99588389280272, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 77.92423415184021, "episode_reward": 501.35722308256624, "step": 3000}
{"episode": 4.0, "batch_reward": 0.3228751415759325, "critic_loss": 0.5540145380496979, "actor_loss": -41.50063208770752, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.081135511398315, "episode_reward": 81.04787067232115, "step": 4000}
{"episode": 5.0, "batch_reward": 0.2941833015829325, "critic_loss": 0.7055705586671829, "actor_loss": -37.11928771209717, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.29235291481018, "episode_reward": 416.5666835932549, "step": 5000}
{"episode": 6.0, "batch_reward": 0.31977749687433243, "critic_loss": 0.9310956181585789, "actor_loss": -39.06581578826904, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.91685390472412, "episode_reward": 471.19771570356505, "step": 6000}
{"episode": 7.0, "batch_reward": 0.35004984909296033, "critic_loss": 0.9555574678182602, "actor_loss": -41.43985759735107, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.17699408531189, "episode_reward": 564.7181690962971, "step": 7000}
{"episode": 8.0, "batch_reward": 0.36601170951128004, "critic_loss": 1.0635938942432404, "actor_loss": -42.99197814178467, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.783707857131958, "episode_reward": 417.53177430006525, "step": 8000}
{"episode": 9.0, "batch_reward": 0.37245917159318925, "critic_loss": 1.039988039314747, "actor_loss": -43.817791412353515, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.1578848361969, "episode_reward": 436.0637345975958, "step": 9000}
{"episode": 10.0, "batch_reward": 0.3837473895549774, "critic_loss": 1.2077659022808076, "actor_loss": -38.44483836364746, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 3996.4761786460876, "episode_reward": 433.13419765532177, "step": 10000}
{"episode": 11.0, "batch_reward": 0.3858497708439827, "critic_loss": 1.2707001107931137, "actor_loss": -38.42628839874268, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.68574523925781, "episode_reward": 297.3991744993347, "step": 11000}
{"episode": 12.0, "batch_reward": 0.38451290038228036, "critic_loss": 1.2972280416488648, "actor_loss": -33.103363674163816, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 427.48826456069946, "episode_reward": 538.0649299236794, "step": 12000}
{"episode": 13.0, "batch_reward": 0.3954579834342003, "critic_loss": 1.2668484107851983, "actor_loss": -34.02962725830078, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.41226625442505, "episode_reward": 510.1477719110241, "step": 13000}
{"episode": 14.0, "batch_reward": 0.4040600728690624, "critic_loss": 1.2607134726643563, "actor_loss": -31.21491612625122, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 470.1704704761505, "episode_reward": 550.4914660533054, "step": 14000}
{"episode": 15.0, "batch_reward": 0.40839314904809, "critic_loss": 1.2873400587439536, "actor_loss": -31.656580291748046, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.42936134338379, "episode_reward": 430.50024985349984, "step": 15000}
{"episode": 16.0, "batch_reward": 0.4161863106191158, "critic_loss": 1.3092903338670732, "actor_loss": -29.370528392791748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 474.105539560318, "episode_reward": 517.916272230577, "step": 16000}
{"episode": 17.0, "batch_reward": 0.4218843255937099, "critic_loss": 1.3324362362623214, "actor_loss": -29.80030662536621, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.24514627456665, "episode_reward": 531.2421007500085, "step": 17000}
{"episode": 18.0, "batch_reward": 0.4317825608551502, "critic_loss": 1.2839298632740974, "actor_loss": -28.14141879272461, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 470.08387780189514, "episode_reward": 654.8802690093326, "step": 18000}
{"episode": 19.0, "batch_reward": 0.43888355338573454, "critic_loss": 1.1677318599820137, "actor_loss": -28.542039291381837, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.590025901794434, "episode_reward": 467.58823736912154, "step": 19000}
{"episode": 20.0, "batch_reward": 0.4362276728451252, "critic_loss": 1.1236784121394157, "actor_loss": -26.385328491210938, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 487.32501339912415, "episode_reward": 189.44136960890083, "step": 20000}
{"episode": 21.0, "batch_reward": 0.4288077545166016, "critic_loss": 1.0122819008827209, "actor_loss": -25.80275532531738, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.297924280166626, "episode_reward": 540.6624742535761, "step": 21000}
{"episode": 22.0, "batch_reward": 0.434750103443861, "critic_loss": 1.0077941282391547, "actor_loss": -24.92309708404541, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 465.72790932655334, "episode_reward": 487.2552117460501, "step": 22000}
{"episode": 23.0, "batch_reward": 0.43564656791090967, "critic_loss": 1.0112954983711242, "actor_loss": -24.896609050750733, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.70961093902588, "episode_reward": 506.0374550107102, "step": 23000}
