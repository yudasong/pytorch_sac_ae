{"episode_reward": 0.0, "episode": 1.0, "duration": 19.68104100227356, "step": 1000}
{"episode_reward": 4.859792814687425, "episode": 2.0, "duration": 1.63541579246521, "step": 2000}
{"episode_reward": 550.1572824113056, "episode": 3.0, "batch_reward": 0.2634925477333409, "critic_loss": 0.054921434321150396, "actor_loss": -21.196439737959782, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 66.8766028881073, "step": 3000}
{"episode_reward": 40.2889332391176, "episode": 4.0, "batch_reward": 0.17335588918626307, "critic_loss": 0.0411438090223819, "actor_loss": -13.502756506979466, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.423229932785034, "step": 4000}
{"episode_reward": 8.192293222761588, "episode": 5.0, "batch_reward": 0.135236581787467, "critic_loss": 0.030563013741746546, "actor_loss": -13.656863604605197, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.494151830673218, "step": 5000}
{"episode_reward": 5.424167312033844, "episode": 6.0, "batch_reward": 0.11277795624360443, "critic_loss": 0.03153684461675584, "actor_loss": -14.81251202455163, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.175663232803345, "step": 6000}
{"episode_reward": 36.123116590255584, "episode": 7.0, "batch_reward": 0.10364432575553656, "critic_loss": 0.038950460293330255, "actor_loss": -13.556878529712558, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.957223176956177, "step": 7000}
{"episode_reward": 67.08045315333251, "episode": 8.0, "batch_reward": 0.10449571055546403, "critic_loss": 0.048776946723461154, "actor_loss": -13.584368156909942, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.992949724197388, "step": 8000}
{"episode_reward": 143.4257269594056, "episode": 9.0, "batch_reward": 0.10804229299351573, "critic_loss": 0.05685200334340334, "actor_loss": -15.065304540708661, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.108477115631104, "step": 9000}
{"episode_reward": 123.35438032716584, "episode": 10.0, "batch_reward": 0.11102277844399214, "critic_loss": 0.06585808147117496, "actor_loss": -15.364801709458233, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.664822340011597, "step": 10000}
{"episode_reward": 157.8930982429839, "episode": 11.0, "batch_reward": 0.11508233973383904, "critic_loss": 0.07265238997898996, "actor_loss": -15.205746583074331, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.93813490867615, "step": 11000}
{"episode_reward": 93.37431726221594, "episode": 12.0, "batch_reward": 0.11458584101498127, "critic_loss": 0.07035584333725274, "actor_loss": -14.931191138118505, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.292840480804443, "step": 12000}
{"episode_reward": 216.99442348655703, "episode": 13.0, "batch_reward": 0.12143089324235916, "critic_loss": 0.08976705720648169, "actor_loss": -15.299140753507615, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.647501707077026, "step": 13000}
{"episode_reward": 150.14223018000507, "episode": 14.0, "batch_reward": 0.12426175782084466, "critic_loss": 0.0900997572503984, "actor_loss": -15.75689376449585, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.723087072372437, "step": 14000}
{"episode_reward": 165.53069592610967, "episode": 15.0, "batch_reward": 0.12389538256078958, "critic_loss": 0.10412168642506003, "actor_loss": -16.5779969124794, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.410207271575928, "step": 15000}
{"episode_reward": 40.423652111487065, "episode": 16.0, "batch_reward": 0.12307330473512411, "critic_loss": 0.09863444934412836, "actor_loss": -16.22100545310974, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.04086709022522, "step": 16000}
{"episode_reward": 212.8507332971693, "episode": 17.0, "batch_reward": 0.12891199054569005, "critic_loss": 0.11902411810308695, "actor_loss": -16.213289353847504, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.679237127304077, "step": 17000}
{"episode_reward": 182.7214018610095, "episode": 18.0, "batch_reward": 0.12937687775492668, "critic_loss": 0.12312480726093054, "actor_loss": -16.25408024263382, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.503617763519287, "step": 18000}
{"episode_reward": 186.95716873950389, "episode": 19.0, "batch_reward": 0.13464980355650186, "critic_loss": 0.1427773230075836, "actor_loss": -16.515123673439025, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.164063453674316, "step": 19000}
{"episode_reward": 143.76591983836684, "episode": 20.0, "batch_reward": 0.13384700666368007, "critic_loss": 0.13591851790249349, "actor_loss": -17.135246174335478, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.776407718658447, "step": 20000}
{"episode_reward": 98.7609129297502, "episode": 21.0, "batch_reward": 0.1318886674195528, "critic_loss": 0.14385545923560858, "actor_loss": -15.421280067443847, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.89001226425171, "step": 21000}
{"episode_reward": 190.1396225114015, "episode": 22.0, "batch_reward": 0.13489150857925414, "critic_loss": 0.15333501978218556, "actor_loss": -17.198011996269226, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.921119451522827, "step": 22000}
{"episode_reward": 109.44191906063124, "episode": 23.0, "batch_reward": 0.13612934533506632, "critic_loss": 0.15266813085228204, "actor_loss": -16.65053595352173, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.303919076919556, "step": 23000}
{"episode_reward": 183.423562971498, "episode": 24.0, "batch_reward": 0.1379087726250291, "critic_loss": 0.18507507959008218, "actor_loss": -16.67482045841217, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.97279644012451, "step": 24000}
{"episode_reward": 226.3911196769248, "episode": 25.0, "batch_reward": 0.14151387990266084, "critic_loss": 0.1946394876614213, "actor_loss": -17.08523728466034, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.23966121673584, "step": 25000}
{"episode_reward": 175.2308942477355, "episode": 26.0, "batch_reward": 0.1429100502729416, "critic_loss": 0.20377875757962466, "actor_loss": -16.783109718322756, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.61391592025757, "step": 26000}
{"episode_reward": 200.92494948851785, "episode": 27.0, "batch_reward": 0.1477590097784996, "critic_loss": 0.21729814353585244, "actor_loss": -16.92274276828766, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.14067792892456, "step": 27000}
{"episode_reward": 406.2341014202423, "episode": 28.0, "batch_reward": 0.1537021166831255, "critic_loss": 0.23466907639801501, "actor_loss": -17.80641081237793, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.2830708026886, "step": 28000}
{"episode_reward": 119.80994922644425, "episode": 29.0, "batch_reward": 0.15024653159826995, "critic_loss": 0.21580304165184497, "actor_loss": -17.50726444530487, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.15001630783081, "step": 29000}
{"episode_reward": 71.95327097253855, "episode": 30.0, "batch_reward": 0.1501979914084077, "critic_loss": 0.20319924253225327, "actor_loss": -16.89054622268677, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.159848928451538, "step": 30000}
{"episode_reward": 134.8088583310307, "episode": 31.0, "batch_reward": 0.14905872792750596, "critic_loss": 0.2132044086754322, "actor_loss": -17.38512520980835, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.19544005393982, "step": 31000}
{"episode_reward": 187.03837023708078, "episode": 32.0, "batch_reward": 0.1512722580358386, "critic_loss": 0.21064596482366324, "actor_loss": -18.103301210403444, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.322361946105957, "step": 32000}
{"episode_reward": 282.88042063585294, "episode": 33.0, "batch_reward": 0.1559021954536438, "critic_loss": 0.23296826408803464, "actor_loss": -18.38833251953125, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.984489679336548, "step": 33000}
{"episode_reward": 302.89898828879905, "episode": 34.0, "batch_reward": 0.15879624491930008, "critic_loss": 0.21921863777190448, "actor_loss": -18.307081861495973, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.400795459747314, "step": 34000}
{"episode_reward": 165.91918174600838, "episode": 35.0, "batch_reward": 0.1593112007677555, "critic_loss": 0.23215014055371286, "actor_loss": -18.70175019836426, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.97854495048523, "step": 35000}
{"episode_reward": 165.0948389599605, "episode": 36.0, "batch_reward": 0.16090435575693846, "critic_loss": 0.2325695891007781, "actor_loss": -18.90201781463623, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.269006490707397, "step": 36000}
{"episode_reward": 286.0871499221114, "episode": 37.0, "batch_reward": 0.16108861209452152, "critic_loss": 0.23389744075387717, "actor_loss": -18.49091934776306, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.752245903015137, "step": 37000}
{"episode_reward": 66.10050252949033, "episode": 38.0, "batch_reward": 0.1597490073442459, "critic_loss": 0.23641435234993696, "actor_loss": -18.615046895980836, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.73377537727356, "step": 38000}
{"episode_reward": 257.69176013726064, "episode": 39.0, "batch_reward": 0.16219498347491026, "critic_loss": 0.233893515445292, "actor_loss": -18.793074281692505, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.82891607284546, "step": 39000}
{"episode_reward": 177.32851964993145, "episode": 40.0, "batch_reward": 0.1637287105843425, "critic_loss": 0.2201312725916505, "actor_loss": -18.917999486923218, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.500842094421387, "step": 40000}
{"episode_reward": 259.04021548963783, "episode": 41.0, "batch_reward": 0.16508619023114443, "critic_loss": 0.22439271560311316, "actor_loss": -18.890327487945555, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.3105046749115, "step": 41000}
{"episode_reward": 168.70628862135075, "episode": 42.0, "batch_reward": 0.1648757639080286, "critic_loss": 0.21078412082791328, "actor_loss": -18.955758722305298, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.68684959411621, "step": 42000}
{"episode_reward": 73.37510629028849, "episode": 43.0, "batch_reward": 0.16361731988191605, "critic_loss": 0.23242362938821315, "actor_loss": -19.129430053710937, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.88161015510559, "step": 43000}
{"episode_reward": 174.174188635105, "episode": 44.0, "batch_reward": 0.16387236236035824, "critic_loss": 0.22272786056995392, "actor_loss": -19.48023250579834, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.464178800582886, "step": 44000}
{"episode_reward": 222.41376297190624, "episode": 45.0, "batch_reward": 0.16574468751251697, "critic_loss": 0.22137731324881316, "actor_loss": -19.162449001312257, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.590721130371094, "step": 45000}
{"episode_reward": 271.0456381843429, "episode": 46.0, "batch_reward": 0.16715870589017867, "critic_loss": 0.2519847237095237, "actor_loss": -19.062802995681764, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.548582553863525, "step": 46000}
{"episode_reward": 182.00381094356192, "episode": 47.0, "batch_reward": 0.16875953720510006, "critic_loss": 0.23162671227008105, "actor_loss": -19.342457517623902, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.979218244552612, "step": 47000}
{"episode_reward": 372.9599148741608, "episode": 48.0, "batch_reward": 0.17324901789426803, "critic_loss": 0.2587872915118933, "actor_loss": -19.647149602890014, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.746094942092896, "step": 48000}
{"episode_reward": 242.3791895116962, "episode": 49.0, "batch_reward": 0.17421282704174518, "critic_loss": 0.2508016683012247, "actor_loss": -20.2997458114624, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.958353757858276, "step": 49000}
{"episode_reward": 310.47937576364956, "episode": 50.0, "batch_reward": 0.17688912692666053, "critic_loss": 0.24357180228829384, "actor_loss": -20.144455600738524, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.623953342437744, "step": 50000}
{"episode_reward": 237.9740203329825, "episode": 51.0, "batch_reward": 0.17853206768631935, "critic_loss": 0.24183483154326677, "actor_loss": -19.940119173049926, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 43.85562872886658, "step": 51000}
{"episode_reward": 360.5136742759077, "episode": 52.0, "batch_reward": 0.1815919626057148, "critic_loss": 0.2626579811424017, "actor_loss": -20.24699440383911, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.607235193252563, "step": 52000}
{"episode_reward": 289.5413486465531, "episode": 53.0, "batch_reward": 0.18444214494526387, "critic_loss": 0.2586407542452216, "actor_loss": -20.954047550201416, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.717116832733154, "step": 53000}
{"episode_reward": 369.3563537167038, "episode": 54.0, "batch_reward": 0.18526824364066125, "critic_loss": 0.26395989122986796, "actor_loss": -21.137469175338744, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.247573614120483, "step": 54000}
{"episode_reward": 75.63791204107721, "episode": 55.0, "batch_reward": 0.1838495568782091, "critic_loss": 0.2687289583235979, "actor_loss": -20.384914251327515, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.168846607208252, "step": 55000}
{"episode_reward": 104.86135139640305, "episode": 56.0, "batch_reward": 0.18425358234345912, "critic_loss": 0.2693319888114929, "actor_loss": -20.55265471076965, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.544761896133423, "step": 56000}
{"episode_reward": 380.6287885288596, "episode": 57.0, "batch_reward": 0.18755276480317115, "critic_loss": 0.28929340364038947, "actor_loss": -20.771514318466185, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.162709712982178, "step": 57000}
{"episode_reward": 396.1972181981677, "episode": 58.0, "batch_reward": 0.19143423587083816, "critic_loss": 0.29589331153035164, "actor_loss": -21.45396406173706, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.75018000602722, "step": 58000}
{"episode_reward": 280.44731984525873, "episode": 59.0, "batch_reward": 0.19229954795539378, "critic_loss": 0.2872698385566473, "actor_loss": -21.293387516021728, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.901490926742554, "step": 59000}
{"episode_reward": 252.9156211793405, "episode": 60.0, "batch_reward": 0.19275004681944846, "critic_loss": 0.28867048959434033, "actor_loss": -21.554639553070068, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.55998468399048, "step": 60000}
{"episode_reward": 115.70412110233329, "episode": 61.0, "batch_reward": 0.19210955084860326, "critic_loss": 0.293324980661273, "actor_loss": -21.281416990280153, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.9438111782074, "step": 61000}
{"episode_reward": 251.3498282524188, "episode": 62.0, "batch_reward": 0.19442344461381436, "critic_loss": 0.3030931997746229, "actor_loss": -21.854820676803588, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.396032571792603, "step": 62000}
{"episode_reward": 375.2017965174339, "episode": 63.0, "batch_reward": 0.19472854743897916, "critic_loss": 0.3208731806129217, "actor_loss": -21.28301062774658, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.22364044189453, "step": 63000}
{"episode_reward": 131.6824821380495, "episode": 64.0, "batch_reward": 0.19544153068959713, "critic_loss": 0.3398180291056633, "actor_loss": -21.710968618392943, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.095799446105957, "step": 64000}
{"episode_reward": 358.6366455213864, "episode": 65.0, "batch_reward": 0.1969707851409912, "critic_loss": 0.36088643120229247, "actor_loss": -21.68736293029785, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.914573669433594, "step": 65000}
{"episode_reward": 75.89162669803872, "episode": 66.0, "batch_reward": 0.19524237409234046, "critic_loss": 0.31638746631145476, "actor_loss": -21.622880809783936, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.88842248916626, "step": 66000}
{"episode_reward": 235.37778469556082, "episode": 67.0, "batch_reward": 0.1962125944942236, "critic_loss": 0.32533580808341506, "actor_loss": -21.95288086128235, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.872215747833252, "step": 67000}
{"episode_reward": 163.46184725859274, "episode": 68.0, "batch_reward": 0.19608935080468654, "critic_loss": 0.31082501463592055, "actor_loss": -21.989780374526976, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.640118837356567, "step": 68000}
{"episode_reward": 439.05321475610623, "episode": 69.0, "batch_reward": 0.19969180622696878, "critic_loss": 0.3103558700978756, "actor_loss": -21.841524082183838, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.105576276779175, "step": 69000}
{"episode_reward": 402.08704982704853, "episode": 70.0, "batch_reward": 0.20226587049663067, "critic_loss": 0.3122708854824305, "actor_loss": -22.1610223236084, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.802387237548828, "step": 70000}
{"episode_reward": 304.13828481103167, "episode": 71.0, "batch_reward": 0.20389967021346092, "critic_loss": 0.3175621317625046, "actor_loss": -22.124716373443604, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.92277431488037, "step": 71000}
{"episode_reward": 193.46661915262294, "episode": 72.0, "batch_reward": 0.20336561211943627, "critic_loss": 0.3264078550636768, "actor_loss": -22.05475987815857, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.39941668510437, "step": 72000}
{"episode_reward": 325.8933394670512, "episode": 73.0, "batch_reward": 0.2050331755578518, "critic_loss": 0.3295022303014994, "actor_loss": -22.379918972015382, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.874359607696533, "step": 73000}
{"episode_reward": 124.01151140827457, "episode": 74.0, "batch_reward": 0.20414705011248588, "critic_loss": 0.28271510475873946, "actor_loss": -22.28805340576172, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.28694987297058, "step": 74000}
{"episode_reward": 286.67584157903775, "episode": 75.0, "batch_reward": 0.2061051228493452, "critic_loss": 0.30996307589113714, "actor_loss": -22.35031122779846, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.67776608467102, "step": 75000}
{"episode_reward": 297.65531452840986, "episode": 76.0, "batch_reward": 0.20581715840101242, "critic_loss": 0.28896706260740757, "actor_loss": -22.19799049949646, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.60733413696289, "step": 76000}
{"episode_reward": 126.40552853650499, "episode": 77.0, "batch_reward": 0.20671678023040294, "critic_loss": 0.2920064560174942, "actor_loss": -22.366731889724733, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.261740922927856, "step": 77000}
{"episode_reward": 316.0506520113084, "episode": 78.0, "batch_reward": 0.2058991770297289, "critic_loss": 0.306769115626812, "actor_loss": -22.1470380821228, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.21013641357422, "step": 78000}
{"episode_reward": 201.51688037860805, "episode": 79.0, "batch_reward": 0.20753831027448177, "critic_loss": 0.29454098127782347, "actor_loss": -22.68484730529785, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.63096284866333, "step": 79000}
{"episode_reward": 380.7017091006286, "episode": 80.0, "batch_reward": 0.20952033686637878, "critic_loss": 0.2920192001760006, "actor_loss": -22.752642726898195, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.186603546142578, "step": 80000}
{"episode_reward": 433.51507458968507, "episode": 81.0, "batch_reward": 0.21286651428043843, "critic_loss": 0.3021792192608118, "actor_loss": -23.042530452728272, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.19644546508789, "step": 81000}
{"episode_reward": 451.8827036481226, "episode": 82.0, "batch_reward": 0.21461899682879448, "critic_loss": 0.3110799724161625, "actor_loss": -22.840522331237793, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.647117853164673, "step": 82000}
{"episode_reward": 450.4943837426052, "episode": 83.0, "batch_reward": 0.2191783288270235, "critic_loss": 0.3072903964072466, "actor_loss": -23.436693660736083, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.057467460632324, "step": 83000}
{"episode_reward": 470.7455252450152, "episode": 84.0, "batch_reward": 0.22146842269599437, "critic_loss": 0.31399103854596616, "actor_loss": -23.750403915405272, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.66513156890869, "step": 84000}
{"episode_reward": 430.736117977633, "episode": 85.0, "batch_reward": 0.22316727359592914, "critic_loss": 0.29928362596035, "actor_loss": -23.514496604919433, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.603812217712402, "step": 85000}
{"episode_reward": 441.10192134360483, "episode": 86.0, "batch_reward": 0.22645070554316044, "critic_loss": 0.3061735631525517, "actor_loss": -23.80188743209839, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.289881706237793, "step": 86000}
{"episode_reward": 277.78865660632835, "episode": 87.0, "batch_reward": 0.22617917604744434, "critic_loss": 0.303198046579957, "actor_loss": -23.912123851776123, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.255648851394653, "step": 87000}
{"episode_reward": 395.6631056414545, "episode": 88.0, "batch_reward": 0.22851866473257543, "critic_loss": 0.2982623242586851, "actor_loss": -24.079396324157713, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.574378967285156, "step": 88000}
{"episode_reward": 470.40596567722247, "episode": 89.0, "batch_reward": 0.23246571071445943, "critic_loss": 0.3058308518230915, "actor_loss": -24.053114894866944, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.67487621307373, "step": 89000}
{"episode_reward": 446.39153746513824, "episode": 90.0, "batch_reward": 0.2349825351834297, "critic_loss": 0.3126013081073761, "actor_loss": -24.27558060836792, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.121258974075317, "step": 90000}
{"episode_reward": 456.5960704258947, "episode": 91.0, "batch_reward": 0.23540664876997472, "critic_loss": 0.30275233352184294, "actor_loss": -24.127686931610107, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.266040325164795, "step": 91000}
{"episode_reward": 244.48461370912494, "episode": 92.0, "batch_reward": 0.2367128650993109, "critic_loss": 0.32445366407930853, "actor_loss": -24.775460735321044, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.745376586914062, "step": 92000}
{"episode_reward": 350.71410578026274, "episode": 93.0, "batch_reward": 0.23731782993674277, "critic_loss": 0.3228465431779623, "actor_loss": -24.38279597091675, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.70385456085205, "step": 93000}
{"episode_reward": 493.62029581235936, "episode": 94.0, "batch_reward": 0.24149960416555405, "critic_loss": 0.31929894344508647, "actor_loss": -25.117275527954103, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.923821926116943, "step": 94000}
{"episode_reward": 489.8702728690548, "episode": 95.0, "batch_reward": 0.2427409520149231, "critic_loss": 0.3172237620949745, "actor_loss": -25.510938999176027, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.48279333114624, "step": 95000}
{"episode_reward": 447.50785286216916, "episode": 96.0, "batch_reward": 0.24575989758968353, "critic_loss": 0.3241351164281368, "actor_loss": -25.223369373321532, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.596256732940674, "step": 96000}
{"episode_reward": 424.7910079436449, "episode": 97.0, "batch_reward": 0.2464680196195841, "critic_loss": 0.3340255606770515, "actor_loss": -25.26073666381836, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.764553546905518, "step": 97000}
{"episode_reward": 425.051441182525, "episode": 98.0, "batch_reward": 0.24840036988258363, "critic_loss": 0.3426905470341444, "actor_loss": -25.906046554565428, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.732550621032715, "step": 98000}
{"episode_reward": 466.9596723749467, "episode": 99.0, "batch_reward": 0.2515812983959913, "critic_loss": 0.3192441357523203, "actor_loss": -26.124822937011718, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.497321128845215, "step": 99000}
{"episode_reward": 460.78758947487626, "episode": 100.0, "batch_reward": 0.2538583557903767, "critic_loss": 0.32792895248532294, "actor_loss": -26.19123988723755, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.171764612197876, "step": 100000}
{"episode_reward": 410.19680075079395, "episode": 101.0, "batch_reward": 0.25460765455663203, "critic_loss": 0.34361211873590947, "actor_loss": -26.10312142562866, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.09407186508179, "step": 101000}
{"episode_reward": 370.31211116768594, "episode": 102.0, "batch_reward": 0.25649068373441697, "critic_loss": 0.3510536976158619, "actor_loss": -26.47980163192749, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.07031559944153, "step": 102000}
{"episode_reward": 431.650252469499, "episode": 103.0, "batch_reward": 0.2569540213495493, "critic_loss": 0.3465691680461168, "actor_loss": -26.584955947875976, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.47959613800049, "step": 103000}
{"episode_reward": 252.18530604735523, "episode": 104.0, "batch_reward": 0.2565961236655712, "critic_loss": 0.3843859637230635, "actor_loss": -26.03222862625122, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.7674241065979, "step": 104000}
{"episode_reward": 115.08197424697522, "episode": 105.0, "batch_reward": 0.25651074378192423, "critic_loss": 0.370394876986742, "actor_loss": -26.273522155761718, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.406181812286377, "step": 105000}
{"episode_reward": 420.0758060424694, "episode": 106.0, "batch_reward": 0.25760763032734396, "critic_loss": 0.35914309152960777, "actor_loss": -26.217966423034667, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.792688131332397, "step": 106000}
{"episode_reward": 271.44968144980714, "episode": 107.0, "batch_reward": 0.2578712473362684, "critic_loss": 0.39179087993502615, "actor_loss": -26.280994075775148, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.472845315933228, "step": 107000}
{"episode_reward": 401.68029371093536, "episode": 108.0, "batch_reward": 0.259732747271657, "critic_loss": 0.39949640388786795, "actor_loss": -26.947653858184815, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.019764184951782, "step": 108000}
{"episode_reward": 311.348267933915, "episode": 109.0, "batch_reward": 0.25979535768926143, "critic_loss": 0.3794769355803728, "actor_loss": -26.66669143295288, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.423673629760742, "step": 109000}
{"episode_reward": 409.1843274078835, "episode": 110.0, "batch_reward": 0.2607302562892437, "critic_loss": 0.4126602783948183, "actor_loss": -26.87521203994751, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.977290391921997, "step": 110000}
{"episode_reward": 159.91645057029373, "episode": 111.0, "batch_reward": 0.2598221166431904, "critic_loss": 0.4047305276691914, "actor_loss": -26.55203067779541, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.340328216552734, "step": 111000}
{"episode_reward": 413.6000037135035, "episode": 112.0, "batch_reward": 0.2615219482779503, "critic_loss": 0.41488243426382543, "actor_loss": -27.046620944976805, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.24972891807556, "step": 112000}
{"episode_reward": 385.82533191370175, "episode": 113.0, "batch_reward": 0.26317975226044654, "critic_loss": 0.4265898036509752, "actor_loss": -26.695861877441406, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.649651527404785, "step": 113000}
{"episode_reward": 466.9906595590113, "episode": 114.0, "batch_reward": 0.2640372500270605, "critic_loss": 0.42616540676355363, "actor_loss": -27.176798904418945, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.22530961036682, "step": 114000}
{"episode_reward": 260.6699490835655, "episode": 115.0, "batch_reward": 0.2652283169776201, "critic_loss": 0.42557503607869146, "actor_loss": -27.077335899353027, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.391348838806152, "step": 115000}
{"episode_reward": 506.87883639859433, "episode": 116.0, "batch_reward": 0.2661949300467968, "critic_loss": 0.4421971479952335, "actor_loss": -27.121496643066408, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.674100637435913, "step": 116000}
{"episode_reward": 185.92415988675532, "episode": 117.0, "batch_reward": 0.2667169401496649, "critic_loss": 0.3997462972253561, "actor_loss": -26.998855865478514, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.254180669784546, "step": 117000}
{"episode_reward": 500.90400112782044, "episode": 118.0, "batch_reward": 0.2670679663270712, "critic_loss": 0.41393298643827436, "actor_loss": -27.044819442749024, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.956791877746582, "step": 118000}
{"episode_reward": 171.8694412495646, "episode": 119.0, "batch_reward": 0.26682279554009436, "critic_loss": 0.40943734158575534, "actor_loss": -27.03069217300415, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.8296320438385, "step": 119000}
{"episode_reward": 255.36846679492962, "episode": 120.0, "batch_reward": 0.26704655261337756, "critic_loss": 0.4195540882349014, "actor_loss": -27.075236835479735, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.20052146911621, "step": 120000}
{"episode_reward": 503.28776381116273, "episode": 121.0, "batch_reward": 0.26805322432518003, "critic_loss": 0.4165179612338543, "actor_loss": -27.058792446136476, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.75130033493042, "step": 121000}
{"episode_reward": 445.73895877862674, "episode": 122.0, "batch_reward": 0.27118827736377715, "critic_loss": 0.4199994629919529, "actor_loss": -27.661586620330812, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.653236865997314, "step": 122000}
{"episode_reward": 519.6190617748741, "episode": 123.0, "batch_reward": 0.2727079995125532, "critic_loss": 0.4159578582048416, "actor_loss": -27.77568786239624, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.472731590270996, "step": 123000}
{"episode_reward": 386.1315857808788, "episode": 124.0, "batch_reward": 0.27432709009945394, "critic_loss": 0.43751001614332197, "actor_loss": -28.088006938934328, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.48134756088257, "step": 124000}
{"episode_reward": 427.94546279441977, "episode": 125.0, "batch_reward": 0.27530945439636706, "critic_loss": 0.44793421567976477, "actor_loss": -27.889098251342773, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.602020740509033, "step": 125000}
{"episode_reward": 491.5767695229841, "episode": 126.0, "batch_reward": 0.27655746655166147, "critic_loss": 0.44573127235472204, "actor_loss": -28.22461770248413, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.40526533126831, "step": 126000}
{"episode_reward": 544.5726448806856, "episode": 127.0, "batch_reward": 0.278238090261817, "critic_loss": 0.4434372383952141, "actor_loss": -28.330697277069092, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.102400541305542, "step": 127000}
{"episode_reward": 543.4067416248222, "episode": 128.0, "batch_reward": 0.28109793858230114, "critic_loss": 0.42792596158385277, "actor_loss": -28.600644138336182, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.16935443878174, "step": 128000}
{"episode_reward": 561.5252500782271, "episode": 129.0, "batch_reward": 0.2836353971362114, "critic_loss": 0.40749835638701914, "actor_loss": -28.618686515808104, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.57948350906372, "step": 129000}
{"episode_reward": 524.5029919487696, "episode": 130.0, "batch_reward": 0.28503578358888626, "critic_loss": 0.40068167351186273, "actor_loss": -28.695633598327635, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.225016117095947, "step": 130000}
{"episode_reward": 550.9096752892972, "episode": 131.0, "batch_reward": 0.2872472119629383, "critic_loss": 0.4046031980365515, "actor_loss": -28.738208156585692, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 44.38869261741638, "step": 131000}
{"episode_reward": 284.9395198803425, "episode": 132.0, "batch_reward": 0.28691277547180655, "critic_loss": 0.3958076454102993, "actor_loss": -28.876109661102294, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.760613679885864, "step": 132000}
{"episode_reward": 499.3836466178932, "episode": 133.0, "batch_reward": 0.2887986176609993, "critic_loss": 0.4364577583372593, "actor_loss": -28.732323246002196, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.735000371932983, "step": 133000}
{"episode_reward": 501.1571120568148, "episode": 134.0, "batch_reward": 0.28953676228225234, "critic_loss": 0.45527946144342424, "actor_loss": -28.94615390777588, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.20432209968567, "step": 134000}
{"episode_reward": 545.4885818593074, "episode": 135.0, "batch_reward": 0.29169127529859545, "critic_loss": 0.43681699702143667, "actor_loss": -29.409986015319824, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.556389093399048, "step": 135000}
{"episode_reward": 569.1039416501259, "episode": 136.0, "batch_reward": 0.29370037658512593, "critic_loss": 0.4311080592423677, "actor_loss": -29.504639736175537, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.110610485076904, "step": 136000}
{"episode_reward": 505.76465555513846, "episode": 137.0, "batch_reward": 0.2959930391013622, "critic_loss": 0.4542267336398363, "actor_loss": -29.566254093170166, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.117281198501587, "step": 137000}
{"episode_reward": 320.3517190062972, "episode": 138.0, "batch_reward": 0.2954533308446407, "critic_loss": 0.4178013838529587, "actor_loss": -29.574470462799074, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.57310128211975, "step": 138000}
{"episode_reward": 511.476633779126, "episode": 139.0, "batch_reward": 0.2973255979567766, "critic_loss": 0.4401723114252091, "actor_loss": -29.66418769836426, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.30277180671692, "step": 139000}
{"episode_reward": 492.0401815007009, "episode": 140.0, "batch_reward": 0.2987785037904978, "critic_loss": 0.40955496519804, "actor_loss": -29.84704261779785, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.24050521850586, "step": 140000}
{"episode_reward": 517.8419989384043, "episode": 141.0, "batch_reward": 0.30075696159899235, "critic_loss": 0.40944201515614986, "actor_loss": -30.006249340057373, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.93721318244934, "step": 141000}
{"episode_reward": 580.1528783020394, "episode": 142.0, "batch_reward": 0.3018791626095772, "critic_loss": 0.41487656778097154, "actor_loss": -30.069550609588624, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.332746028900146, "step": 142000}
{"episode_reward": 205.9798525708649, "episode": 143.0, "batch_reward": 0.30198643539845943, "critic_loss": 0.4076290612816811, "actor_loss": -30.089522006988524, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.545564889907837, "step": 143000}
{"episode_reward": 551.329205837479, "episode": 144.0, "batch_reward": 0.30282637825608255, "critic_loss": 0.45483743435144425, "actor_loss": -30.158840286254883, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.849291801452637, "step": 144000}
{"episode_reward": 543.7345687891401, "episode": 145.0, "batch_reward": 0.30517261764407155, "critic_loss": 0.3925678669065237, "actor_loss": -30.523452964782713, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.211166620254517, "step": 145000}
{"episode_reward": 271.1197009062764, "episode": 146.0, "batch_reward": 0.3039601939022541, "critic_loss": 0.451737248390913, "actor_loss": -30.093687175750734, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.849212169647217, "step": 146000}
{"episode_reward": 546.002568669013, "episode": 147.0, "batch_reward": 0.30483412835001944, "critic_loss": 0.4396730532646179, "actor_loss": -30.379654350280763, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.23106861114502, "step": 147000}
{"episode_reward": 189.2096787550207, "episode": 148.0, "batch_reward": 0.3061316432952881, "critic_loss": 0.45185578003525734, "actor_loss": -30.397282825469972, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.6426842212677, "step": 148000}
{"episode_reward": 505.59569631256886, "episode": 149.0, "batch_reward": 0.3074885711669922, "critic_loss": 0.4337769439667463, "actor_loss": -30.527671733856202, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.180477380752563, "step": 149000}
{"episode_reward": 491.72767995656466, "episode": 150.0, "batch_reward": 0.3079881167858839, "critic_loss": 0.4532007115036249, "actor_loss": -30.785101444244386, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
