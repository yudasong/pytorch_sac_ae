{"episode_reward": 0.0, "episode": 1.0, "duration": 21.375974416732788, "step": 1000}
{"episode_reward": 4.859792814687425, "episode": 2.0, "duration": 1.8605971336364746, "step": 2000}
{"episode_reward": 550.1572824113056, "episode": 3.0, "batch_reward": 0.2602174842047588, "critic_loss": 0.019817975666401918, "actor_loss": -18.637129292002363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 69.16558599472046, "step": 3000}
{"episode_reward": 1.9619256572169976, "episode": 4.0, "batch_reward": 0.16136953739076854, "critic_loss": 0.010617383972625249, "actor_loss": -16.16426751756668, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.53527021408081, "step": 4000}
{"episode_reward": 2.274813605244189, "episode": 5.0, "batch_reward": 0.12504177561402322, "critic_loss": 0.0123489475667011, "actor_loss": -15.469612342596054, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.57512640953064, "step": 5000}
{"episode_reward": 2.3037607616405076, "episode": 6.0, "batch_reward": 0.10166879936307668, "critic_loss": 0.010039564695674926, "actor_loss": -15.672451946735382, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.772148609161377, "step": 6000}
{"episode_reward": 2.0219090558463844, "episode": 7.0, "batch_reward": 0.08688324178010225, "critic_loss": 0.012115206770366057, "actor_loss": -14.060549753665924, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.644436597824097, "step": 7000}
{"episode_reward": 2.3192093070017425, "episode": 8.0, "batch_reward": 0.07577311819791793, "critic_loss": 0.01367922797135543, "actor_loss": -14.396352416992187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.672309637069702, "step": 8000}
{"episode_reward": 2.40363886489979, "episode": 9.0, "batch_reward": 0.06681781996786594, "critic_loss": 0.014250016934936867, "actor_loss": -14.58381154370308, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.066417455673218, "step": 9000}
{"episode_reward": 1.922052597857428, "episode": 10.0, "batch_reward": 0.06035730339959264, "critic_loss": 0.014589580539264716, "actor_loss": -14.270942116498947, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.73850440979004, "step": 10000}
{"episode_reward": 1.970125956624261, "episode": 11.0, "batch_reward": 0.055661933176219466, "critic_loss": 0.010225084369885735, "actor_loss": -14.433845982789993, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 45.20830059051514, "step": 11000}
{"episode_reward": 3.0086167021326213, "episode": 12.0, "batch_reward": 0.04944990525394678, "critic_loss": 0.013381317289196885, "actor_loss": -14.036708955526352, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.057166814804077, "step": 12000}
{"episode_reward": 3.1580277529637413, "episode": 13.0, "batch_reward": 0.04611955846566707, "critic_loss": 0.017401768980489576, "actor_loss": -12.917258415937424, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.97834610939026, "step": 13000}
{"episode_reward": 2.874966311904074, "episode": 14.0, "batch_reward": 0.043426127299666405, "critic_loss": 0.011693565307330574, "actor_loss": -13.701165487289428, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.698172330856323, "step": 14000}
{"episode_reward": 3.685153309295484, "episode": 15.0, "batch_reward": 0.0401037487545982, "critic_loss": 0.012117218715429772, "actor_loss": -14.03489864397049, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.404687881469727, "step": 15000}
{"episode_reward": 2.3400546454059463, "episode": 16.0, "batch_reward": 0.03786746003711596, "critic_loss": 0.012694949485128745, "actor_loss": -12.807660564541816, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.757116556167603, "step": 16000}
{"episode_reward": 1.9434229376028864, "episode": 17.0, "batch_reward": 0.0355568102102261, "critic_loss": 0.012228718109137844, "actor_loss": -13.233165825366974, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.0972638130188, "step": 17000}
{"episode_reward": 2.0581698266775854, "episode": 18.0, "batch_reward": 0.03455299812462181, "critic_loss": 0.014172418928297702, "actor_loss": -13.116752410292625, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.255718231201172, "step": 18000}
{"episode_reward": 3.4503660406116405, "episode": 19.0, "batch_reward": 0.03237503326032311, "critic_loss": 0.00906493409111863, "actor_loss": -13.004520231366158, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.760765075683594, "step": 19000}
{"episode_reward": 2.465723104583102, "episode": 20.0, "batch_reward": 0.030775462478399275, "critic_loss": 0.013495969327865169, "actor_loss": -13.287609525084495, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.56999897956848, "step": 20000}
{"episode_reward": 1.9052691269759856, "episode": 21.0, "batch_reward": 0.028592591036343946, "critic_loss": 0.012439309361740016, "actor_loss": -11.990112645745278, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.26490044593811, "step": 21000}
{"episode_reward": 3.0171880754681153, "episode": 22.0, "batch_reward": 0.028263373216381296, "critic_loss": 0.011309732025751145, "actor_loss": -14.323398319005966, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.001824140548706, "step": 22000}
{"episode_reward": 3.7329416935279895, "episode": 23.0, "batch_reward": 0.026868768128566443, "critic_loss": 0.011550990852585529, "actor_loss": -13.111911491632462, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.146156072616577, "step": 23000}
{"episode_reward": 2.0364467034299523, "episode": 24.0, "batch_reward": 0.025746839643456042, "critic_loss": 0.010541998324071756, "actor_loss": -13.042743430256843, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.077251195907593, "step": 24000}
{"episode_reward": 2.2051791281715314, "episode": 25.0, "batch_reward": 0.025377833981649018, "critic_loss": 0.01638481418020092, "actor_loss": -12.941891501903534, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.764329195022583, "step": 25000}
{"episode_reward": 2.990168527562307, "episode": 26.0, "batch_reward": 0.02411522146174684, "critic_loss": 0.011321005663252436, "actor_loss": -12.13952053964138, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.90630793571472, "step": 26000}
{"episode_reward": 2.7348833385474176, "episode": 27.0, "batch_reward": 0.023880410740850493, "critic_loss": 0.013010586232107017, "actor_loss": -11.506469211220741, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.248289346694946, "step": 27000}
{"episode_reward": 2.6500154777241587, "episode": 28.0, "batch_reward": 0.022907718617236242, "critic_loss": 0.008539558593489345, "actor_loss": -13.165674477577209, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.80559229850769, "step": 28000}
{"episode_reward": 2.286445631564961, "episode": 29.0, "batch_reward": 0.02162356056261342, "critic_loss": 0.011419693149175146, "actor_loss": -12.491621166944503, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.937129259109497, "step": 29000}
{"episode_reward": 2.6702386594453875, "episode": 30.0, "batch_reward": 0.02130017662420869, "critic_loss": 0.008247839071525959, "actor_loss": -11.18181358397007, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.793673992156982, "step": 30000}
{"episode_reward": 3.071525371100039, "episode": 31.0, "batch_reward": 0.02050398870802019, "critic_loss": 0.007140751567014376, "actor_loss": -12.398507917642593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.25202226638794, "step": 31000}
{"episode_reward": 2.417275830566748, "episode": 32.0, "batch_reward": 0.0200550330420956, "critic_loss": 0.010292210284678732, "actor_loss": -12.666956963181496, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.567142963409424, "step": 32000}
{"episode_reward": 2.4164902281141103, "episode": 33.0, "batch_reward": 0.019545235272031277, "critic_loss": 0.009011885841158801, "actor_loss": -13.031955831646918, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.751566171646118, "step": 33000}
{"episode_reward": 2.6215303311508547, "episode": 34.0, "batch_reward": 0.019179408890893684, "critic_loss": 0.008062164969276637, "actor_loss": -11.432642466664314, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.721042156219482, "step": 34000}
{"episode_reward": 2.719833147317731, "episode": 35.0, "batch_reward": 0.017875404889695346, "critic_loss": 0.007786465463039348, "actor_loss": -12.775926321923732, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.336347579956055, "step": 35000}
{"episode_reward": 1.8692118626580023, "episode": 36.0, "batch_reward": 0.01761214339302387, "critic_loss": 0.00901231048983027, "actor_loss": -12.973449461519719, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.658491849899292, "step": 36000}
{"episode_reward": 2.9293213679868186, "episode": 37.0, "batch_reward": 0.017673774182097986, "critic_loss": 0.008898880069908046, "actor_loss": -12.07343368166685, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.086665630340576, "step": 37000}
{"episode_reward": 2.9683602051577385, "episode": 38.0, "batch_reward": 0.01700766158197075, "critic_loss": 0.005353123625412991, "actor_loss": -12.543755955398083, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.71663999557495, "step": 38000}
{"episode_reward": 2.2575200073140946, "episode": 39.0, "batch_reward": 0.01690201584994793, "critic_loss": 0.008176395853748545, "actor_loss": -12.372328871667385, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.29069685935974, "step": 39000}
{"episode_reward": 1.907519446522637, "episode": 40.0, "batch_reward": 0.016419437995762565, "critic_loss": 0.006104525724775158, "actor_loss": -11.844335399329662, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.934380054473877, "step": 40000}
{"episode_reward": 2.7281834424013462, "episode": 41.0, "batch_reward": 0.016021262252354063, "critic_loss": 0.0073523643687804, "actor_loss": -9.863429852128029, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.506183385849, "step": 41000}
{"episode_reward": 2.3123499472319082, "episode": 42.0, "batch_reward": 0.015774656068650073, "critic_loss": 0.00578681947835139, "actor_loss": -11.889359365820885, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.374780416488647, "step": 42000}
{"episode_reward": 2.5745810221519987, "episode": 43.0, "batch_reward": 0.015243279356858694, "critic_loss": 0.00993518436137674, "actor_loss": -12.252504615426064, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.56012511253357, "step": 43000}
{"episode_reward": 2.111405030020732, "episode": 44.0, "batch_reward": 0.0152715576387709, "critic_loss": 0.004930542252659507, "actor_loss": -13.057352214396, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.122992515563965, "step": 44000}
{"episode_reward": 3.0136334609828523, "episode": 45.0, "batch_reward": 0.014666449867771008, "critic_loss": 0.00637251541268779, "actor_loss": -12.939472570896148, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.80834412574768, "step": 45000}
{"episode_reward": 3.351420810578337, "episode": 46.0, "batch_reward": 0.014272521086386405, "critic_loss": 0.006024362787291466, "actor_loss": -11.048536989867687, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.409088373184204, "step": 46000}
{"episode_reward": 2.620955117737939, "episode": 47.0, "batch_reward": 0.014123633189941757, "critic_loss": 0.007555460613613832, "actor_loss": -11.552803993582726, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.603108882904053, "step": 47000}
{"episode_reward": 2.9261789991242275, "episode": 48.0, "batch_reward": 0.014064175941399299, "critic_loss": 0.007878495753240714, "actor_loss": -12.707911264181137, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.121469259262085, "step": 48000}
{"episode_reward": 2.8178012211957846, "episode": 49.0, "batch_reward": 0.014044021876063198, "critic_loss": 0.005252678198892681, "actor_loss": -12.988440238893032, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.58698582649231, "step": 49000}
{"episode_reward": 2.680019216995761, "episode": 50.0, "batch_reward": 0.013633027155068703, "critic_loss": 0.008580026124429423, "actor_loss": -11.566802924215793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.320525407791138, "step": 50000}
{"episode_reward": 3.3917442410920655, "episode": 51.0, "batch_reward": 0.013435966784250922, "critic_loss": 0.004643252467620186, "actor_loss": -10.867095646202564, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.87706637382507, "step": 51000}
{"episode_reward": 2.33845305637397, "episode": 52.0, "batch_reward": 0.013298872380750254, "critic_loss": 0.006211126527843589, "actor_loss": -10.749584325015546, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.425727367401123, "step": 52000}
{"episode_reward": 1.6314000115121012, "episode": 53.0, "batch_reward": 0.013069768691319041, "critic_loss": 0.005011797722479969, "actor_loss": -12.638177579462528, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.94288945198059, "step": 53000}
{"episode_reward": 1.8148608459034157, "episode": 54.0, "batch_reward": 0.012789264811668545, "critic_loss": 0.005838201047510665, "actor_loss": -11.700556714653969, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.72848415374756, "step": 54000}
{"episode_reward": 2.291137126121344, "episode": 55.0, "batch_reward": 0.012878820705111139, "critic_loss": 0.006269594071993197, "actor_loss": -11.928849620223046, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.180350065231323, "step": 55000}
{"episode_reward": 2.484918370218459, "episode": 56.0, "batch_reward": 0.012311328479438088, "critic_loss": 0.0069077608979714565, "actor_loss": -11.331769657969474, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.60423493385315, "step": 56000}
{"episode_reward": 2.083576597842594, "episode": 57.0, "batch_reward": 0.012263645538827405, "critic_loss": 0.006252735554320679, "actor_loss": -11.89942572313547, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.162148237228394, "step": 57000}
{"episode_reward": 3.560711975742037, "episode": 58.0, "batch_reward": 0.012001607011421583, "critic_loss": 0.004624392436664493, "actor_loss": -12.360716278761625, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.81882357597351, "step": 58000}
{"episode_reward": 3.3298065553024476, "episode": 59.0, "batch_reward": 0.011828624113462865, "critic_loss": 0.006071585851801501, "actor_loss": -11.9466516802907, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.125027179718018, "step": 59000}
{"episode_reward": 2.2072556889819075, "episode": 60.0, "batch_reward": 0.011919861645321361, "critic_loss": 0.0048189038353448265, "actor_loss": -12.17254069596529, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.27640390396118, "step": 60000}
{"episode_reward": 2.0288801767955524, "episode": 61.0, "batch_reward": 0.011851714864722453, "critic_loss": 0.007505442818997835, "actor_loss": -11.203544010967017, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.18407344818115, "step": 61000}
{"episode_reward": 1.7768095879477668, "episode": 62.0, "batch_reward": 0.011791494863689878, "critic_loss": 0.0055310596126219025, "actor_loss": -12.423103986740113, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.09080743789673, "step": 62000}
{"episode_reward": 2.9884513557712324, "episode": 63.0, "batch_reward": 0.011287550266017205, "critic_loss": 0.0059369895332056334, "actor_loss": -10.653414348572493, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.24549674987793, "step": 63000}
{"episode_reward": 1.5676918923614078, "episode": 64.0, "batch_reward": 0.011090657474822364, "critic_loss": 0.005628259585908381, "actor_loss": -11.938919942975044, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.380132913589478, "step": 64000}
{"episode_reward": 1.3503716677516617, "episode": 65.0, "batch_reward": 0.010898943099891766, "critic_loss": 0.007751493968768045, "actor_loss": -11.208511489480735, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.09432601928711, "step": 65000}
{"episode_reward": 2.8713427516745025, "episode": 66.0, "batch_reward": 0.010751491860602983, "critic_loss": 0.004619859785758308, "actor_loss": -11.65451551514864, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.392367124557495, "step": 66000}
{"episode_reward": 1.871230642921349, "episode": 67.0, "batch_reward": 0.010848234988516196, "critic_loss": 0.005566113022534409, "actor_loss": -13.289500947058201, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.377703428268433, "step": 67000}
{"episode_reward": 1.8573028735275483, "episode": 68.0, "batch_reward": 0.010541308047948405, "critic_loss": 0.007045423166404362, "actor_loss": -12.703753971189261, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.796753406524658, "step": 68000}
{"episode_reward": 3.129438294993311, "episode": 69.0, "batch_reward": 0.010581526111462153, "critic_loss": 0.0051089184884403945, "actor_loss": -11.053069024592638, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.058555841445923, "step": 69000}
{"episode_reward": 1.9493759103471882, "episode": 70.0, "batch_reward": 0.010475363040342927, "critic_loss": 0.005308624760778912, "actor_loss": -12.185514597296715, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.066778421401978, "step": 70000}
{"episode_reward": 2.69331027766417, "episode": 71.0, "batch_reward": 0.010394816741463728, "critic_loss": 0.00659825434673985, "actor_loss": -11.617036150455474, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.580954790115356, "step": 71000}
{"episode_reward": 2.003632300382943, "episode": 72.0, "batch_reward": 0.010315225902246311, "critic_loss": 0.005622857259120792, "actor_loss": -11.222492222100497, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.95143699645996, "step": 72000}
{"episode_reward": 2.4565852939855066, "episode": 73.0, "batch_reward": 0.01018166299816221, "critic_loss": 0.007826293547797831, "actor_loss": -11.551987980902195, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.42131781578064, "step": 73000}
{"episode_reward": 1.7632737288220468, "episode": 74.0, "batch_reward": 0.009747107006493025, "critic_loss": 0.0051405020521924595, "actor_loss": -11.813958934903145, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.092487335205078, "step": 74000}
{"episode_reward": 2.772635274343989, "episode": 75.0, "batch_reward": 0.00958484343381133, "critic_loss": 0.0032374363224371336, "actor_loss": -11.16094391438365, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.41122341156006, "step": 75000}
{"episode_reward": 2.2052967441879145, "episode": 76.0, "batch_reward": 0.009928859708830715, "critic_loss": 0.00637391909235157, "actor_loss": -10.948901791214944, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.6361186504364, "step": 76000}
{"episode_reward": 3.0353807318094406, "episode": 77.0, "batch_reward": 0.009967673406703398, "critic_loss": 0.007236898464878323, "actor_loss": -11.739879915505648, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.770172595977783, "step": 77000}
{"episode_reward": 2.8184291025225923, "episode": 78.0, "batch_reward": 0.010030458854162134, "critic_loss": 0.004891682690918969, "actor_loss": -10.933109053641557, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.703338146209717, "step": 78000}
{"episode_reward": 3.0524267601238497, "episode": 79.0, "batch_reward": 0.009416670997743494, "critic_loss": 0.003960998131820816, "actor_loss": -11.942042295753955, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.511160850524902, "step": 79000}
{"episode_reward": 2.991673080565975, "episode": 80.0, "batch_reward": 0.00949261856963858, "critic_loss": 0.004825463963468792, "actor_loss": -12.322710729330778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.42237091064453, "step": 80000}
{"episode_reward": 2.369823523754639, "episode": 81.0, "batch_reward": 0.009055452989414334, "critic_loss": 0.004320251602905046, "actor_loss": -12.53728090712428, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.04133939743042, "step": 81000}
{"episode_reward": 3.1026532337153236, "episode": 82.0, "batch_reward": 0.009078136050491594, "critic_loss": 0.003735108772649255, "actor_loss": -11.242808059901, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.871858596801758, "step": 82000}
{"episode_reward": 2.2558142515249435, "episode": 83.0, "batch_reward": 0.009175561452633702, "critic_loss": 0.0036215309767721918, "actor_loss": -12.365701482266187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.3028621673584, "step": 83000}
{"episode_reward": 2.1657318767042595, "episode": 84.0, "batch_reward": 0.008977669390267692, "critic_loss": 0.00579792653118784, "actor_loss": -12.22548470133543, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.455295085906982, "step": 84000}
{"episode_reward": 3.373628560789961, "episode": 85.0, "batch_reward": 0.008967666813521647, "critic_loss": 0.004113249164998706, "actor_loss": -11.562764623761177, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.38626194000244, "step": 85000}
{"episode_reward": 2.5479684840342616, "episode": 86.0, "batch_reward": 0.009084195832605474, "critic_loss": 0.0036652114556418383, "actor_loss": -11.948285208374262, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.79806137084961, "step": 86000}
{"episode_reward": 3.0030257414302746, "episode": 87.0, "batch_reward": 0.00890017305710353, "critic_loss": 0.0026835456764601985, "actor_loss": -12.33439392581582, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.52818250656128, "step": 87000}
{"episode_reward": 3.047201420599913, "episode": 88.0, "batch_reward": 0.00845841702260077, "critic_loss": 0.002862771547130251, "actor_loss": -12.14106225028634, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.91724133491516, "step": 88000}
{"episode_reward": 3.0435403354881236, "episode": 89.0, "batch_reward": 0.008828780435607768, "critic_loss": 0.0036323954483377745, "actor_loss": -10.794556717962026, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.821933269500732, "step": 89000}
{"episode_reward": 2.4227682376605313, "episode": 90.0, "batch_reward": 0.00883181098732166, "critic_loss": 0.0034517549819211126, "actor_loss": -10.860867973715067, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.37970542907715, "step": 90000}
{"episode_reward": 2.8132489415436135, "episode": 91.0, "batch_reward": 0.008612426409614272, "critic_loss": 0.0025999148708797295, "actor_loss": -10.623373703032732, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.42730903625488, "step": 91000}
{"episode_reward": 2.1302786820752537, "episode": 92.0, "batch_reward": 0.008453534047934226, "critic_loss": 0.004050580703289597, "actor_loss": -11.766792065292597, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.101345539093018, "step": 92000}
{"episode_reward": 3.292401830124011, "episode": 93.0, "batch_reward": 0.008365168489515781, "critic_loss": 0.002911003522000101, "actor_loss": -10.622722986280918, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.4991192817688, "step": 93000}
{"episode_reward": 2.838700440810163, "episode": 94.0, "batch_reward": 0.0083011895037489, "critic_loss": 0.004321719118612236, "actor_loss": -11.469666826307774, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.165867567062378, "step": 94000}
{"episode_reward": 3.277650101952145, "episode": 95.0, "batch_reward": 0.008168235451681539, "critic_loss": 0.003111239429381385, "actor_loss": -12.546075154274702, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.79418444633484, "step": 95000}
{"episode_reward": 3.3048608558308588, "episode": 96.0, "batch_reward": 0.008529048628173769, "critic_loss": 0.0045908112810429886, "actor_loss": -11.01865583154559, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.177536725997925, "step": 96000}
{"episode_reward": 2.7063816143247297, "episode": 97.0, "batch_reward": 0.008322655301890336, "critic_loss": 0.004308705380528409, "actor_loss": -10.881076014250517, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.963131189346313, "step": 97000}
{"episode_reward": 2.667299353945528, "episode": 98.0, "batch_reward": 0.008008678319863976, "critic_loss": 0.003329470592434518, "actor_loss": -12.916399007961154, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.581427335739136, "step": 98000}
{"episode_reward": 2.872904211790585, "episode": 99.0, "batch_reward": 0.008034935915842652, "critic_loss": 0.0029530129985068923, "actor_loss": -11.669478098720312, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.329325914382935, "step": 99000}
{"episode_reward": 2.192822816991359, "episode": 100.0, "batch_reward": 0.008121925080544315, "critic_loss": 0.003829548442889063, "actor_loss": -12.289935531765222, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.77164578437805, "step": 100000}
{"episode_reward": 2.8327374096624425, "episode": 101.0, "batch_reward": 0.008181848308769986, "critic_loss": 0.0037045929727974, "actor_loss": -10.83478289423883, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.71707487106323, "step": 101000}
{"episode_reward": 2.712812847923359, "episode": 102.0, "batch_reward": 0.008199287777766585, "critic_loss": 0.004118513122270087, "actor_loss": -12.021131797388197, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.24306344985962, "step": 102000}
{"episode_reward": 2.674712195059109, "episode": 103.0, "batch_reward": 0.008209900312009268, "critic_loss": 0.002484562776480743, "actor_loss": -11.970362184986472, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.026429176330566, "step": 103000}
{"episode_reward": 2.152997033847646, "episode": 104.0, "batch_reward": 0.008056440548971295, "critic_loss": 0.003831717997840315, "actor_loss": -10.999812900230289, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.561699390411377, "step": 104000}
{"episode_reward": 2.8343931206495094, "episode": 105.0, "batch_reward": 0.007906159461126662, "critic_loss": 0.0028895527194690656, "actor_loss": -11.45259434095025, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.929420232772827, "step": 105000}
{"episode_reward": 1.92326423536411, "episode": 106.0, "batch_reward": 0.0074661677266703915, "critic_loss": 0.002523777251619322, "actor_loss": -11.371629133075476, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.672341346740723, "step": 106000}
{"episode_reward": 3.2595089506117754, "episode": 107.0, "batch_reward": 0.0077486401675269006, "critic_loss": 0.0032602578500445815, "actor_loss": -10.658382820233703, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.560450792312622, "step": 107000}
{"episode_reward": 2.6866379442245263, "episode": 108.0, "batch_reward": 0.008004653395619243, "critic_loss": 0.0030960690984138638, "actor_loss": -12.471159270197154, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.37691044807434, "step": 108000}
{"episode_reward": 2.495434688537789, "episode": 109.0, "batch_reward": 0.007437592203612439, "critic_loss": 0.0038283318497706205, "actor_loss": -10.853729889437556, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.37528657913208, "step": 109000}
{"episode_reward": 3.1130549940319496, "episode": 110.0, "batch_reward": 0.007850372002227232, "critic_loss": 0.003006507444919407, "actor_loss": -12.456106211423874, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.232004165649414, "step": 110000}
{"episode_reward": 2.160202337870946, "episode": 111.0, "batch_reward": 0.007504138563526794, "critic_loss": 0.0026899967730423667, "actor_loss": -11.366553031340242, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.82004237174988, "step": 111000}
{"episode_reward": 2.009288988561818, "episode": 112.0, "batch_reward": 0.0075793168762465935, "critic_loss": 0.003878028120852832, "actor_loss": -12.498104044392704, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.26893901824951, "step": 112000}
{"episode_reward": 3.038818560481954, "episode": 113.0, "batch_reward": 0.007521460889372974, "critic_loss": 0.0024257227071357193, "actor_loss": -10.818604817658663, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.60593605041504, "step": 113000}
{"episode_reward": 2.7979150782076294, "episode": 114.0, "batch_reward": 0.007557459173956886, "critic_loss": 0.0026881211478466866, "actor_loss": -12.392938027054072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.040380477905273, "step": 114000}
{"episode_reward": 3.449010317545242, "episode": 115.0, "batch_reward": 0.007130690756021067, "critic_loss": 0.0030219942992480357, "actor_loss": -12.084688721776008, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.46847701072693, "step": 115000}
{"episode_reward": 2.519831340007467, "episode": 116.0, "batch_reward": 0.007405054510105401, "critic_loss": 0.0026427588944243326, "actor_loss": -11.601340022668243, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.869936227798462, "step": 116000}
{"episode_reward": 1.9765381640735105, "episode": 117.0, "batch_reward": 0.007209474944975227, "critic_loss": 0.002802674979724543, "actor_loss": -10.756052692949773, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.13295269012451, "step": 117000}
{"episode_reward": 1.85472532360094, "episode": 118.0, "batch_reward": 0.007106279443949461, "critic_loss": 0.0025662450315358, "actor_loss": -10.965169255495072, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.378472328186035, "step": 118000}
{"episode_reward": 2.227486272175402, "episode": 119.0, "batch_reward": 0.007417162412777543, "critic_loss": 0.003958939148065838, "actor_loss": -10.917161238878965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.28949284553528, "step": 119000}
{"episode_reward": 2.2948546672599943, "episode": 120.0, "batch_reward": 0.007370023637195118, "critic_loss": 0.0031606358495046153, "actor_loss": -10.75165981335938, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.679487705230713, "step": 120000}
{"episode_reward": 2.2997639692472838, "episode": 121.0, "batch_reward": 0.00697570275363978, "critic_loss": 0.004059876721301407, "actor_loss": -10.16365881346166, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.87225008010864, "step": 121000}
{"episode_reward": 2.5762673037117194, "episode": 122.0, "batch_reward": 0.0069972687311237675, "critic_loss": 0.0031942096849670635, "actor_loss": -12.261134400963783, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.685821533203125, "step": 122000}
{"episode_reward": 3.6838905036299527, "episode": 123.0, "batch_reward": 0.007078724523074925, "critic_loss": 0.0033082035599436496, "actor_loss": -11.518544795408845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.22755789756775, "step": 123000}
{"episode_reward": 2.3716052294311813, "episode": 124.0, "batch_reward": 0.00727659750613384, "critic_loss": 0.003153192036523251, "actor_loss": -11.868220193654299, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.535497188568115, "step": 124000}
{"episode_reward": 2.544964154718511, "episode": 125.0, "batch_reward": 0.0068270188861060885, "critic_loss": 0.0023010142938655917, "actor_loss": -11.662095634549857, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.857197523117065, "step": 125000}
{"episode_reward": 1.8725186101293383, "episode": 126.0, "batch_reward": 0.007063526640064083, "critic_loss": 0.00329078995634336, "actor_loss": -11.85662494136393, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.37481999397278, "step": 126000}
{"episode_reward": 4.112794993566306, "episode": 127.0, "batch_reward": 0.006922365228529088, "critic_loss": 0.002400099794183916, "actor_loss": -12.8683557767123, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.701040029525757, "step": 127000}
{"episode_reward": 2.3678211085586827, "episode": 128.0, "batch_reward": 0.006908717903774232, "critic_loss": 0.0021143808909400833, "actor_loss": -11.844742299720645, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.618895530700684, "step": 128000}
{"episode_reward": 1.9550149641144339, "episode": 129.0, "batch_reward": 0.007003326099482365, "critic_loss": 0.002954075769703195, "actor_loss": -11.914237435728312, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.113873720169067, "step": 129000}
{"episode_reward": 2.8372792711702006, "episode": 130.0, "batch_reward": 0.006712866718880832, "critic_loss": 0.0020752691484412934, "actor_loss": -11.615403714060783, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.60096001625061, "step": 130000}
{"episode_reward": 2.3237686862394913, "episode": 131.0, "batch_reward": 0.006723713693325408, "critic_loss": 0.0035751193562755363, "actor_loss": -11.497629490286112, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.50019669532776, "step": 131000}
{"episode_reward": 2.51834543130966, "episode": 132.0, "batch_reward": 0.006732899754191749, "critic_loss": 0.0018449342214153148, "actor_loss": -11.607183245241641, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.319573163986206, "step": 132000}
{"episode_reward": 2.5789001995896097, "episode": 133.0, "batch_reward": 0.006680641278857365, "critic_loss": 0.0018634827246423811, "actor_loss": -9.906799297392368, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 26.136550664901733, "step": 133000}
{"episode_reward": 2.2322063918247284, "episode": 134.0, "batch_reward": 0.006949400680488907, "critic_loss": 0.0015395019503557706, "actor_loss": -11.49180038447678, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.9853458404541, "step": 134000}
{"episode_reward": 3.6117934717829625, "episode": 135.0, "batch_reward": 0.006464359900914133, "critic_loss": 0.0019938029284385264, "actor_loss": -12.274592897623778, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.183441162109375, "step": 135000}
{"episode_reward": 2.584493584462396, "episode": 136.0, "batch_reward": 0.006750204233569093, "critic_loss": 0.0025319594919055817, "actor_loss": -11.894588432729245, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.798684120178223, "step": 136000}
{"episode_reward": 2.5170457549235463, "episode": 137.0, "batch_reward": 0.006539084951626137, "critic_loss": 0.001855382074449153, "actor_loss": -11.985145083755254, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.197493076324463, "step": 137000}
{"episode_reward": 2.8029401810566186, "episode": 138.0, "batch_reward": 0.006571519926656038, "critic_loss": 0.0023262645743379836, "actor_loss": -11.319568855196238, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.86192512512207, "step": 138000}
{"episode_reward": 1.566047261379159, "episode": 139.0, "batch_reward": 0.006420182811329141, "critic_loss": 0.0016535177675104933, "actor_loss": -11.3710117007792, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.155941247940063, "step": 139000}
{"episode_reward": 4.237261678459232, "episode": 140.0, "batch_reward": 0.006732018449692987, "critic_loss": 0.002112899870124238, "actor_loss": -11.20669274623692, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.47606062889099, "step": 140000}
{"episode_reward": 3.5357221475697096, "episode": 141.0, "batch_reward": 0.006453711017151363, "critic_loss": 0.0015148583840418723, "actor_loss": -12.437110033467413, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.7532377243042, "step": 141000}
{"episode_reward": 2.756243366365015, "episode": 142.0, "batch_reward": 0.006481847122893669, "critic_loss": 0.002534407498613291, "actor_loss": -11.653549050286411, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.280057191848755, "step": 142000}
{"episode_reward": 2.1497711376070274, "episode": 143.0, "batch_reward": 0.006610131965600886, "critic_loss": 0.0024640250444572303, "actor_loss": -11.427094134867191, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.40234398841858, "step": 143000}
{"episode_reward": 5.167981016439812, "episode": 144.0, "batch_reward": 0.0064378640891518445, "critic_loss": 0.001172384804685862, "actor_loss": -11.811294583573938, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.33988332748413, "step": 144000}
{"episode_reward": 1.9597893721823225, "episode": 145.0, "batch_reward": 0.006241236685193143, "critic_loss": 0.0014745992083699093, "actor_loss": -12.092497562989593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.719757318496704, "step": 145000}
{"episode_reward": 3.3689081557354212, "episode": 146.0, "batch_reward": 0.006254342445754446, "critic_loss": 0.001710744013715157, "actor_loss": -10.915801138579845, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.33802580833435, "step": 146000}
{"episode_reward": 2.0628330778060135, "episode": 147.0, "batch_reward": 0.006443943158956245, "critic_loss": 0.002560711779537087, "actor_loss": -11.306519896894693, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.046464204788208, "step": 147000}
{"episode_reward": 2.0482133968889586, "episode": 148.0, "batch_reward": 0.006229818712337874, "critic_loss": 0.0027774465251713992, "actor_loss": -11.234455667406321, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.09903645515442, "step": 148000}
{"episode_reward": 1.995715939187759, "episode": 149.0, "batch_reward": 0.006295254262746312, "critic_loss": 0.0016876265300525119, "actor_loss": -11.74943060964346, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.711636066436768, "step": 149000}
{"episode_reward": 3.6488091103476075, "episode": 150.0, "batch_reward": 0.006044530709506944, "critic_loss": 0.0015435950710016187, "actor_loss": -12.870951595947146, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
