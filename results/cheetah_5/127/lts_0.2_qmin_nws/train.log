{"episode_reward": 0.0, "episode": 1.0, "duration": 19.567805290222168, "step": 1000}
{"episode_reward": 4.859792814687425, "episode": 2.0, "duration": 1.516643762588501, "step": 2000}
{"episode_reward": 550.1572824113056, "episode": 3.0, "batch_reward": 0.26267871854403846, "critic_loss": 0.031460396537265956, "actor_loss": -9.434263616739749, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 69.10990285873413, "step": 3000}
{"episode_reward": 34.293677692556805, "episode": 4.0, "batch_reward": 0.17971750389784574, "critic_loss": 0.03245238695014268, "actor_loss": -7.631149581909179, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.281986951828003, "step": 4000}
{"episode_reward": 42.50168550946625, "episode": 5.0, "batch_reward": 0.1609030412659049, "critic_loss": 0.04742357262037694, "actor_loss": -6.314131858348847, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.98385977745056, "step": 5000}
{"episode_reward": 206.30977882710351, "episode": 6.0, "batch_reward": 0.15760984316468238, "critic_loss": 0.055750813018530604, "actor_loss": -7.562385452747345, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.02983784675598, "step": 6000}
{"episode_reward": 36.90662389613181, "episode": 7.0, "batch_reward": 0.15037289509177207, "critic_loss": 0.06524628894589841, "actor_loss": -7.642799762248993, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.388735055923462, "step": 7000}
{"episode_reward": 169.4112434784666, "episode": 8.0, "batch_reward": 0.14891730786859989, "critic_loss": 0.06815398850291968, "actor_loss": -8.487535778999328, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.2620849609375, "step": 8000}
{"episode_reward": 98.55494561639917, "episode": 9.0, "batch_reward": 0.14693293756991624, "critic_loss": 0.07784897232428194, "actor_loss": -9.230737195014953, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.448356866836548, "step": 9000}
{"episode_reward": 174.92881818082594, "episode": 10.0, "batch_reward": 0.14525685846060515, "critic_loss": 0.08470897680893541, "actor_loss": -10.098947315216064, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.897133588790894, "step": 10000}
{"episode_reward": 90.23094441417801, "episode": 11.0, "batch_reward": 0.14134260687232017, "critic_loss": 0.0867568130530417, "actor_loss": -10.375180166244506, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.055480003356934, "step": 11000}
{"episode_reward": 168.97767333330822, "episode": 12.0, "batch_reward": 0.13966749347001314, "critic_loss": 0.09863440188765525, "actor_loss": -10.97436444568634, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.266314029693604, "step": 12000}
{"episode_reward": 54.00615176072253, "episode": 13.0, "batch_reward": 0.1411637210547924, "critic_loss": 0.13564778576791287, "actor_loss": -10.980799585342407, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.10581374168396, "step": 13000}
{"episode_reward": 306.3900648774913, "episode": 14.0, "batch_reward": 0.1469471005871892, "critic_loss": 0.14504064384847878, "actor_loss": -11.944438983917236, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.298365354537964, "step": 14000}
{"episode_reward": 49.350258527688055, "episode": 15.0, "batch_reward": 0.14821814339607953, "critic_loss": 0.1651530631557107, "actor_loss": -12.35253858947754, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.928120851516724, "step": 15000}
{"episode_reward": 370.59338879126545, "episode": 16.0, "batch_reward": 0.16060095851123332, "critic_loss": 0.17443245243281125, "actor_loss": -13.467643707275391, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.54491901397705, "step": 16000}
{"episode_reward": 300.58757219926895, "episode": 17.0, "batch_reward": 0.1642304538041353, "critic_loss": 0.174640871219337, "actor_loss": -14.086235656738282, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.335821866989136, "step": 17000}
{"episode_reward": 61.91803630820796, "episode": 18.0, "batch_reward": 0.16004849620908498, "critic_loss": 0.1950070263594389, "actor_loss": -14.212910934448242, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.80899477005005, "step": 18000}
{"episode_reward": 111.4167501070941, "episode": 19.0, "batch_reward": 0.15627399496734143, "critic_loss": 0.19531845765560865, "actor_loss": -14.39887813949585, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.421138286590576, "step": 19000}
{"episode_reward": 79.37063138938063, "episode": 20.0, "batch_reward": 0.15760214482992888, "critic_loss": 0.21394207543879748, "actor_loss": -14.836132169723511, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.041301727294922, "step": 20000}
{"episode_reward": 337.8683099114617, "episode": 21.0, "batch_reward": 0.16283943550288676, "critic_loss": 0.19667861637473105, "actor_loss": -15.509116285324097, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.93843626976013, "step": 21000}
{"episode_reward": 106.02160968888627, "episode": 22.0, "batch_reward": 0.16197839518636464, "critic_loss": 0.20947176453471184, "actor_loss": -15.93652141571045, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.719743013381958, "step": 22000}
{"episode_reward": 174.1379912705935, "episode": 23.0, "batch_reward": 0.1638304829597473, "critic_loss": 0.20397510573267938, "actor_loss": -16.122777109146117, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.914124488830566, "step": 23000}
{"episode_reward": 211.09524412954005, "episode": 24.0, "batch_reward": 0.16308476281911136, "critic_loss": 0.20134414287656546, "actor_loss": -16.506386880874633, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.475892066955566, "step": 24000}
{"episode_reward": 136.46395092868505, "episode": 25.0, "batch_reward": 0.1630463666766882, "critic_loss": 0.21725919961184265, "actor_loss": -16.65735169029236, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.204671382904053, "step": 25000}
{"episode_reward": 130.41392776612292, "episode": 26.0, "batch_reward": 0.16245835388451815, "critic_loss": 0.22404624205827714, "actor_loss": -16.789369888305664, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.30656099319458, "step": 26000}
{"episode_reward": 176.97675582278026, "episode": 27.0, "batch_reward": 0.16320320881903172, "critic_loss": 0.21854081536829473, "actor_loss": -17.1076692943573, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.94976282119751, "step": 27000}
{"episode_reward": 205.20211847754263, "episode": 28.0, "batch_reward": 0.16692810982465744, "critic_loss": 0.24217017751932143, "actor_loss": -17.655555347442625, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.5822970867157, "step": 28000}
{"episode_reward": 390.50589732624655, "episode": 29.0, "batch_reward": 0.16990595158934593, "critic_loss": 0.2512270857542753, "actor_loss": -18.014391468048096, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.515701293945312, "step": 29000}
{"episode_reward": 92.26039455935992, "episode": 30.0, "batch_reward": 0.1683273315280676, "critic_loss": 0.23284753683954476, "actor_loss": -18.014880542755126, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.894818544387817, "step": 30000}
{"episode_reward": 81.83250387504837, "episode": 31.0, "batch_reward": 0.16537183606624603, "critic_loss": 0.2511687917560339, "actor_loss": -18.004845987319946, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.05511975288391, "step": 31000}
{"episode_reward": 98.82641261941947, "episode": 32.0, "batch_reward": 0.16300153383612634, "critic_loss": 0.2790366768911481, "actor_loss": -17.98214925956726, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.71949005126953, "step": 32000}
{"episode_reward": 96.00744772016228, "episode": 33.0, "batch_reward": 0.1634547342285514, "critic_loss": 0.29761279599368573, "actor_loss": -18.129664144515992, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.87194299697876, "step": 33000}
{"episode_reward": 373.0427127995812, "episode": 34.0, "batch_reward": 0.16822422733157874, "critic_loss": 0.28594887737184765, "actor_loss": -18.56829341506958, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.37835168838501, "step": 34000}
{"episode_reward": 167.92706274175856, "episode": 35.0, "batch_reward": 0.1678802096247673, "critic_loss": 0.28115456859767435, "actor_loss": -18.633894233703614, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.700366973876953, "step": 35000}
{"episode_reward": 179.7045505864102, "episode": 36.0, "batch_reward": 0.17022759433835744, "critic_loss": 0.3223887022435665, "actor_loss": -18.808595371246337, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.9166738986969, "step": 36000}
{"episode_reward": 223.32427674967062, "episode": 37.0, "batch_reward": 0.1688622048944235, "critic_loss": 0.3341599149405956, "actor_loss": -18.794043479919434, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.9095618724823, "step": 37000}
{"episode_reward": 141.35755801954022, "episode": 38.0, "batch_reward": 0.16971029891073705, "critic_loss": 0.3413223444372416, "actor_loss": -18.9999465637207, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.634754180908203, "step": 38000}
{"episode_reward": 259.54728421628374, "episode": 39.0, "batch_reward": 0.17210165940970182, "critic_loss": 0.36005428126454353, "actor_loss": -19.204836742401124, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.55648684501648, "step": 39000}
{"episode_reward": 236.74162959484872, "episode": 40.0, "batch_reward": 0.17244741163402796, "critic_loss": 0.35469260880351067, "actor_loss": -19.341708116531372, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.24726366996765, "step": 40000}
{"episode_reward": 102.87300952216474, "episode": 41.0, "batch_reward": 0.17241118766367436, "critic_loss": 0.3310019198358059, "actor_loss": -19.32934310913086, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.96628522872925, "step": 41000}
{"episode_reward": 283.02981891811186, "episode": 42.0, "batch_reward": 0.1773619296848774, "critic_loss": 0.38439000184834005, "actor_loss": -19.803790241241455, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.01139736175537, "step": 42000}
{"episode_reward": 431.3161967402168, "episode": 43.0, "batch_reward": 0.17891283895075322, "critic_loss": 0.3885529305785894, "actor_loss": -19.885633039474488, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.812511682510376, "step": 43000}
{"episode_reward": 114.54823456310046, "episode": 44.0, "batch_reward": 0.1788718199580908, "critic_loss": 0.39074686454236507, "actor_loss": -19.98260474205017, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.499086380004883, "step": 44000}
{"episode_reward": 290.218280557268, "episode": 45.0, "batch_reward": 0.1819268562346697, "critic_loss": 0.4112478589266539, "actor_loss": -20.31783260536194, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.523975133895874, "step": 45000}
{"episode_reward": 300.9483095500282, "episode": 46.0, "batch_reward": 0.18458008988201619, "critic_loss": 0.41599936009943483, "actor_loss": -20.582066818237305, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.52453637123108, "step": 46000}
{"episode_reward": 334.0216002610222, "episode": 47.0, "batch_reward": 0.18749236349761486, "critic_loss": 0.45627535420656207, "actor_loss": -20.713018939971924, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.834417581558228, "step": 47000}
{"episode_reward": 307.9909250290071, "episode": 48.0, "batch_reward": 0.18924499233067035, "critic_loss": 0.4280706714242697, "actor_loss": -20.998132137298583, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.8380343914032, "step": 48000}
{"episode_reward": 176.95140241871297, "episode": 49.0, "batch_reward": 0.19119797672331335, "critic_loss": 0.46061281056702136, "actor_loss": -21.218969062805176, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.73125457763672, "step": 49000}
{"episode_reward": 392.25073945226893, "episode": 50.0, "batch_reward": 0.19261627599596978, "critic_loss": 0.43601728220283986, "actor_loss": -21.36453318786621, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.60641860961914, "step": 50000}
{"episode_reward": 54.14903918788556, "episode": 51.0, "batch_reward": 0.1899678852111101, "critic_loss": 0.41727257204055784, "actor_loss": -21.153914419174193, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.6625862121582, "step": 51000}
{"episode_reward": 113.87591303069054, "episode": 52.0, "batch_reward": 0.18956934781372548, "critic_loss": 0.4241087341606617, "actor_loss": -21.104634998321533, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.24872064590454, "step": 52000}
{"episode_reward": 134.45380377949374, "episode": 53.0, "batch_reward": 0.18917450727522372, "critic_loss": 0.4377675961703062, "actor_loss": -20.981064603805542, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.820045232772827, "step": 53000}
{"episode_reward": 378.7819176629165, "episode": 54.0, "batch_reward": 0.19322786708176137, "critic_loss": 0.4231108974963427, "actor_loss": -21.4784497756958, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.931663036346436, "step": 54000}
{"episode_reward": 381.9441733830205, "episode": 55.0, "batch_reward": 0.19307427372038363, "critic_loss": 0.4335166425853968, "actor_loss": -21.42171131515503, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.90511918067932, "step": 55000}
{"episode_reward": 71.73096838389762, "episode": 56.0, "batch_reward": 0.19495258815586566, "critic_loss": 0.45705539202690126, "actor_loss": -21.5197845993042, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.262660026550293, "step": 56000}
{"episode_reward": 244.62495955629328, "episode": 57.0, "batch_reward": 0.19468131192028523, "critic_loss": 0.44077968433499337, "actor_loss": -21.351020362854005, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.786231994628906, "step": 57000}
{"episode_reward": 186.81794261510674, "episode": 58.0, "batch_reward": 0.19346174184978007, "critic_loss": 0.4282662541866302, "actor_loss": -21.39585813522339, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.585981607437134, "step": 58000}
{"episode_reward": 137.92198573334412, "episode": 59.0, "batch_reward": 0.19305163131654263, "critic_loss": 0.4186551982760429, "actor_loss": -21.258700674057007, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.834495067596436, "step": 59000}
{"episode_reward": 312.13622069282764, "episode": 60.0, "batch_reward": 0.19516391338407993, "critic_loss": 0.45370182786881924, "actor_loss": -21.47967530441284, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.087082624435425, "step": 60000}
{"episode_reward": 329.6536276677548, "episode": 61.0, "batch_reward": 0.1975892136991024, "critic_loss": 0.45361194032430646, "actor_loss": -21.708601192474365, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.394736528396606, "step": 61000}
{"episode_reward": 161.67362678024082, "episode": 62.0, "batch_reward": 0.1975633660107851, "critic_loss": 0.44483839711546896, "actor_loss": -21.707154752731324, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.220205307006836, "step": 62000}
{"episode_reward": 252.47660984290746, "episode": 63.0, "batch_reward": 0.19672999404370783, "critic_loss": 0.44079470792412756, "actor_loss": -21.594482460021972, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.576716423034668, "step": 63000}
{"episode_reward": 142.46163364017647, "episode": 64.0, "batch_reward": 0.19669282734394072, "critic_loss": 0.4174770492166281, "actor_loss": -21.522825634002686, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.120211839675903, "step": 64000}
{"episode_reward": 366.95807556957215, "episode": 65.0, "batch_reward": 0.20053688274323941, "critic_loss": 0.4193926007151604, "actor_loss": -21.782169174194337, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.682678937911987, "step": 65000}
{"episode_reward": 292.99326964228766, "episode": 66.0, "batch_reward": 0.20069247445464133, "critic_loss": 0.4439348583519459, "actor_loss": -21.74026390838623, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.48225688934326, "step": 66000}
{"episode_reward": 274.55323616363034, "episode": 67.0, "batch_reward": 0.20136470717191696, "critic_loss": 0.456514566808939, "actor_loss": -21.650939853668213, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.351449012756348, "step": 67000}
{"episode_reward": 110.15401671602201, "episode": 68.0, "batch_reward": 0.20113971680402756, "critic_loss": 0.426489001005888, "actor_loss": -21.612106384277343, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.58493709564209, "step": 68000}
{"episode_reward": 365.42659831541096, "episode": 69.0, "batch_reward": 0.20244882912933826, "critic_loss": 0.453784033536911, "actor_loss": -21.732111110687256, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.45481514930725, "step": 69000}
{"episode_reward": 306.5669628305481, "episode": 70.0, "batch_reward": 0.20394973941147326, "critic_loss": 0.48008940859138965, "actor_loss": -21.822200481414797, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.68451762199402, "step": 70000}
{"episode_reward": 116.53682707978939, "episode": 71.0, "batch_reward": 0.20356570659577847, "critic_loss": 0.42181025256216526, "actor_loss": -21.714765399932862, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.67392706871033, "step": 71000}
{"episode_reward": 273.6732556442391, "episode": 72.0, "batch_reward": 0.2049797595143318, "critic_loss": 0.44527423165738583, "actor_loss": -21.819549556732177, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.731067895889282, "step": 72000}
{"episode_reward": 291.33814204020683, "episode": 73.0, "batch_reward": 0.20529897902905941, "critic_loss": 0.4568325921744108, "actor_loss": -21.7943035736084, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.302425384521484, "step": 73000}
{"episode_reward": 113.7202098900215, "episode": 74.0, "batch_reward": 0.20451799269020557, "critic_loss": 0.44887002317607405, "actor_loss": -21.735361295700073, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.877520322799683, "step": 74000}
{"episode_reward": 373.88115415087304, "episode": 75.0, "batch_reward": 0.2061466995626688, "critic_loss": 0.45175318130850795, "actor_loss": -21.9052481842041, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.044147968292236, "step": 75000}
{"episode_reward": 180.55068111726106, "episode": 76.0, "batch_reward": 0.20691891807317733, "critic_loss": 0.42917517451941967, "actor_loss": -21.78705997085571, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.101688861846924, "step": 76000}
{"episode_reward": 267.40119855910416, "episode": 77.0, "batch_reward": 0.20856121338903905, "critic_loss": 0.4450751348137856, "actor_loss": -21.8850036239624, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.260132551193237, "step": 77000}
{"episode_reward": 423.1757029171289, "episode": 78.0, "batch_reward": 0.20917509332299233, "critic_loss": 0.4397201098948717, "actor_loss": -22.034510063171385, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.901721477508545, "step": 78000}
{"episode_reward": 136.33622265398293, "episode": 79.0, "batch_reward": 0.2091974236369133, "critic_loss": 0.4450893911421299, "actor_loss": -22.006111476898194, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.467782735824585, "step": 79000}
{"episode_reward": 268.3725359694492, "episode": 80.0, "batch_reward": 0.20995099267363548, "critic_loss": 0.49286917434632777, "actor_loss": -21.932879920959472, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.96609616279602, "step": 80000}
{"episode_reward": 192.01517305170816, "episode": 81.0, "batch_reward": 0.21046660500764847, "critic_loss": 0.4883926560431719, "actor_loss": -21.95772960662842, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.34217667579651, "step": 81000}
{"episode_reward": 231.69513949347655, "episode": 82.0, "batch_reward": 0.21009810389578343, "critic_loss": 0.4700970935970545, "actor_loss": -21.980705074310304, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.13290524482727, "step": 82000}
{"episode_reward": 397.2136605855906, "episode": 83.0, "batch_reward": 0.21290145464241506, "critic_loss": 0.487488326087594, "actor_loss": -22.008930603027345, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.879843711853027, "step": 83000}
{"episode_reward": 218.52566709591324, "episode": 84.0, "batch_reward": 0.21293765634298326, "critic_loss": 0.5187372312545776, "actor_loss": -22.08021206665039, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.04110288619995, "step": 84000}
{"episode_reward": 469.53289683503374, "episode": 85.0, "batch_reward": 0.214356062784791, "critic_loss": 0.5064493037909269, "actor_loss": -22.15042293167114, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.83423686027527, "step": 85000}
{"episode_reward": 373.6190312044129, "episode": 86.0, "batch_reward": 0.21694837838411332, "critic_loss": 0.47809821915626527, "actor_loss": -22.416410781860353, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.64250421524048, "step": 86000}
{"episode_reward": 193.14202726761087, "episode": 87.0, "batch_reward": 0.21773874029517173, "critic_loss": 0.45113898211717607, "actor_loss": -22.501630447387694, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.763214349746704, "step": 87000}
{"episode_reward": 470.97546928966597, "episode": 88.0, "batch_reward": 0.22002883003652096, "critic_loss": 0.5211965395361186, "actor_loss": -22.64442734146118, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.967342376708984, "step": 88000}
{"episode_reward": 237.53905019443826, "episode": 89.0, "batch_reward": 0.22064447118341923, "critic_loss": 0.47470134776830675, "actor_loss": -22.60547412109375, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.545152187347412, "step": 89000}
{"episode_reward": 428.75990682323294, "episode": 90.0, "batch_reward": 0.2225275091677904, "critic_loss": 0.5086036536842584, "actor_loss": -22.783524959564208, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.95329713821411, "step": 90000}
{"episode_reward": 103.41337340619224, "episode": 91.0, "batch_reward": 0.22079632924497128, "critic_loss": 0.48382276444137096, "actor_loss": -22.715026054382324, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.46650791168213, "step": 91000}
{"episode_reward": 115.74280763780455, "episode": 92.0, "batch_reward": 0.21910611952841283, "critic_loss": 0.4834768224656582, "actor_loss": -22.50433974838257, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.87105393409729, "step": 92000}
{"episode_reward": 102.71670811678776, "episode": 93.0, "batch_reward": 0.2178977521955967, "critic_loss": 0.45315095637738706, "actor_loss": -22.456115901947022, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.376950979232788, "step": 93000}
{"episode_reward": 506.8874108704548, "episode": 94.0, "batch_reward": 0.2210279564410448, "critic_loss": 0.5034824847131968, "actor_loss": -22.728088661193848, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.46162486076355, "step": 94000}
{"episode_reward": 305.0896819738041, "episode": 95.0, "batch_reward": 0.22183797903358937, "critic_loss": 0.4777433357089758, "actor_loss": -22.789028068542482, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.16586399078369, "step": 95000}
{"episode_reward": 173.00485725550146, "episode": 96.0, "batch_reward": 0.2225085759907961, "critic_loss": 0.4891833650022745, "actor_loss": -22.742207637786866, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.666218996047974, "step": 96000}
{"episode_reward": 190.61719037901602, "episode": 97.0, "batch_reward": 0.2209876846820116, "critic_loss": 0.4781051379591227, "actor_loss": -22.639619243621826, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.093165636062622, "step": 97000}
{"episode_reward": 109.23217827936959, "episode": 98.0, "batch_reward": 0.2207384002059698, "critic_loss": 0.482650129199028, "actor_loss": -22.60024080657959, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.639150381088257, "step": 98000}
{"episode_reward": 267.71312158936445, "episode": 99.0, "batch_reward": 0.2219912292510271, "critic_loss": 0.4902592552304268, "actor_loss": -22.607548488616942, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.703048706054688, "step": 99000}
{"episode_reward": 436.5925364526708, "episode": 100.0, "batch_reward": 0.22239231561124326, "critic_loss": 0.451800654143095, "actor_loss": -22.58896807098389, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.773279190063477, "step": 100000}
{"episode_reward": 126.44178928180537, "episode": 101.0, "batch_reward": 0.22173500683903694, "critic_loss": 0.4604853256344795, "actor_loss": -22.58787559890747, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.92631196975708, "step": 101000}
{"episode_reward": 356.58064316575286, "episode": 102.0, "batch_reward": 0.22353148613870144, "critic_loss": 0.4863406133502722, "actor_loss": -22.538525768280028, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.170755863189697, "step": 102000}
{"episode_reward": 142.82023636265234, "episode": 103.0, "batch_reward": 0.22357287451624872, "critic_loss": 0.48718525967001913, "actor_loss": -22.62295378112793, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.740859746932983, "step": 103000}
{"episode_reward": 521.7730100689149, "episode": 104.0, "batch_reward": 0.22673990082740783, "critic_loss": 0.5027282304614782, "actor_loss": -22.844955265045165, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.65659475326538, "step": 104000}
{"episode_reward": 520.011736370091, "episode": 105.0, "batch_reward": 0.2293072427213192, "critic_loss": 0.48556860925257206, "actor_loss": -23.090088439941407, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.008159399032593, "step": 105000}
{"episode_reward": 381.4128330871221, "episode": 106.0, "batch_reward": 0.2311244452893734, "critic_loss": 0.4871320704370737, "actor_loss": -23.238914806365965, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.702018976211548, "step": 106000}
{"episode_reward": 507.64038848762874, "episode": 107.0, "batch_reward": 0.23231645016372204, "critic_loss": 0.44195305538177493, "actor_loss": -23.29537441253662, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.24752163887024, "step": 107000}
{"episode_reward": 445.35473050052525, "episode": 108.0, "batch_reward": 0.23427347639203072, "critic_loss": 0.4953652525693178, "actor_loss": -23.48301142501831, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.24757742881775, "step": 108000}
{"episode_reward": 138.8675221085078, "episode": 109.0, "batch_reward": 0.23459921495616437, "critic_loss": 0.45245071132481096, "actor_loss": -23.471057296752928, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.978550672531128, "step": 109000}
{"episode_reward": 534.5497649660726, "episode": 110.0, "batch_reward": 0.23626969388127328, "critic_loss": 0.4649957187771797, "actor_loss": -23.548562854766846, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.460811376571655, "step": 110000}
{"episode_reward": 223.69774338608218, "episode": 111.0, "batch_reward": 0.23619046758115292, "critic_loss": 0.4544139346331358, "actor_loss": -23.534038173675537, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 43.38807487487793, "step": 111000}
{"episode_reward": 288.72186593898846, "episode": 112.0, "batch_reward": 0.2379948451370001, "critic_loss": 0.4697340287417173, "actor_loss": -23.509911796569824, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.43428611755371, "step": 112000}
{"episode_reward": 554.9066216682174, "episode": 113.0, "batch_reward": 0.24082854963839054, "critic_loss": 0.48536923663318154, "actor_loss": -23.817799018859862, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.707339763641357, "step": 113000}
{"episode_reward": 249.47903932654629, "episode": 114.0, "batch_reward": 0.24067814004421234, "critic_loss": 0.4848676359206438, "actor_loss": -23.801401096343994, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.395174264907837, "step": 114000}
{"episode_reward": 473.0543807261062, "episode": 115.0, "batch_reward": 0.24303192022442818, "critic_loss": 0.4476353475004435, "actor_loss": -23.866654537200926, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.110316038131714, "step": 115000}
{"episode_reward": 491.8347847181479, "episode": 116.0, "batch_reward": 0.2429721312224865, "critic_loss": 0.4409609491080046, "actor_loss": -23.84561640548706, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.08410358428955, "step": 116000}
{"episode_reward": 258.00813012736756, "episode": 117.0, "batch_reward": 0.24372047935426236, "critic_loss": 0.4619815057069063, "actor_loss": -24.014330913543702, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.2795352935791, "step": 117000}
{"episode_reward": 221.65429608518895, "episode": 118.0, "batch_reward": 0.24413585996627807, "critic_loss": 0.4531279619038105, "actor_loss": -24.010485580444335, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.399991512298584, "step": 118000}
{"episode_reward": 432.55292042247873, "episode": 119.0, "batch_reward": 0.2462252518236637, "critic_loss": 0.47574383175373075, "actor_loss": -24.165101039886476, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.618285655975342, "step": 119000}
{"episode_reward": 398.3566924818323, "episode": 120.0, "batch_reward": 0.24687190374732018, "critic_loss": 0.4888977245539427, "actor_loss": -24.205092632293702, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.922298908233643, "step": 120000}
{"episode_reward": 543.9376934103816, "episode": 121.0, "batch_reward": 0.24940464006364346, "critic_loss": 0.47767219552397727, "actor_loss": -24.326339767456055, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.88261961936951, "step": 121000}
{"episode_reward": 427.5264130072821, "episode": 122.0, "batch_reward": 0.25200341430306433, "critic_loss": 0.4963261752873659, "actor_loss": -24.625751430511475, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.718358039855957, "step": 122000}
{"episode_reward": 416.6309195745886, "episode": 123.0, "batch_reward": 0.2532883930057287, "critic_loss": 0.5118542433679104, "actor_loss": -24.666210845947266, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.874558210372925, "step": 123000}
{"episode_reward": 570.9379576454241, "episode": 124.0, "batch_reward": 0.2552786054611206, "critic_loss": 0.5048188143819571, "actor_loss": -24.887668132781982, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.224172353744507, "step": 124000}
{"episode_reward": 298.89868105609924, "episode": 125.0, "batch_reward": 0.25518600396811963, "critic_loss": 0.5403662053197622, "actor_loss": -24.787229473114014, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.145827293395996, "step": 125000}
{"episode_reward": 587.6625811655365, "episode": 126.0, "batch_reward": 0.25859265148639676, "critic_loss": 0.4711076998114586, "actor_loss": -25.085510116577147, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.75541090965271, "step": 126000}
{"episode_reward": 304.5655535774885, "episode": 127.0, "batch_reward": 0.25769965644180776, "critic_loss": 0.4844401033371687, "actor_loss": -24.987646961212157, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.911287307739258, "step": 127000}
{"episode_reward": 650.6103988215227, "episode": 128.0, "batch_reward": 0.2626484838575125, "critic_loss": 0.46743385449051855, "actor_loss": -25.423960552215576, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.19741463661194, "step": 128000}
{"episode_reward": 516.2921671078639, "episode": 129.0, "batch_reward": 0.26283871273696424, "critic_loss": 0.4796474972963333, "actor_loss": -25.40402823257446, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.501587867736816, "step": 129000}
{"episode_reward": 231.39286832019044, "episode": 130.0, "batch_reward": 0.2631497761756182, "critic_loss": 0.4770100226700306, "actor_loss": -25.514053897857664, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.984158039093018, "step": 130000}
{"episode_reward": 256.87848988298964, "episode": 131.0, "batch_reward": 0.26365388196706774, "critic_loss": 0.5051709511876106, "actor_loss": -25.396769207000734, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.419190406799316, "step": 131000}
{"episode_reward": 105.8378114417369, "episode": 132.0, "batch_reward": 0.2630668078660965, "critic_loss": 0.48956564436852934, "actor_loss": -25.49650361633301, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.892002820968628, "step": 132000}
{"episode_reward": 556.6869440875171, "episode": 133.0, "batch_reward": 0.2642993201315403, "critic_loss": 0.5127037249505519, "actor_loss": -25.50871901321411, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.28115463256836, "step": 133000}
{"episode_reward": 549.2207830919234, "episode": 134.0, "batch_reward": 0.26578066556155683, "critic_loss": 0.4902335801422596, "actor_loss": -25.573030086517335, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.221943378448486, "step": 134000}
{"episode_reward": 248.80358846836225, "episode": 135.0, "batch_reward": 0.265711909532547, "critic_loss": 0.5258673142492771, "actor_loss": -25.72104474258423, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.411900281906128, "step": 135000}
{"episode_reward": 401.3633056851817, "episode": 136.0, "batch_reward": 0.26759724850952626, "critic_loss": 0.5146395159959793, "actor_loss": -25.818081775665284, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.865907669067383, "step": 136000}
{"episode_reward": 440.32676650360764, "episode": 137.0, "batch_reward": 0.26877769745886326, "critic_loss": 0.5137048556655646, "actor_loss": -25.91246766281128, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.38951063156128, "step": 137000}
{"episode_reward": 507.223210713701, "episode": 138.0, "batch_reward": 0.2698172358572483, "critic_loss": 0.512368706330657, "actor_loss": -25.964740745544432, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.875667095184326, "step": 138000}
{"episode_reward": 451.39959027283794, "episode": 139.0, "batch_reward": 0.2720355716198683, "critic_loss": 0.5293006579875946, "actor_loss": -26.229311347961424, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.880218029022217, "step": 139000}
{"episode_reward": 507.97539263368077, "episode": 140.0, "batch_reward": 0.27325499592721464, "critic_loss": 0.5413487160801888, "actor_loss": -26.435580764770506, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.59855031967163, "step": 140000}
{"episode_reward": 646.6925207585404, "episode": 141.0, "batch_reward": 0.27637265805900096, "critic_loss": 0.5369895898401738, "actor_loss": -26.430196044921875, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.601977825164795, "step": 141000}
{"episode_reward": 448.70055311666545, "episode": 142.0, "batch_reward": 0.27730149403214455, "critic_loss": 0.5506653461009264, "actor_loss": -26.492356727600097, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.47359800338745, "step": 142000}
{"episode_reward": 503.4057606892085, "episode": 143.0, "batch_reward": 0.2787769550085068, "critic_loss": 0.5223879481554031, "actor_loss": -26.770085918426513, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.551641941070557, "step": 143000}
{"episode_reward": 557.7711853115616, "episode": 144.0, "batch_reward": 0.28023680666089057, "critic_loss": 0.533393733099103, "actor_loss": -26.939349239349365, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.52111840248108, "step": 144000}
{"episode_reward": 122.42624855580662, "episode": 145.0, "batch_reward": 0.2790401826351881, "critic_loss": 0.5504707453250886, "actor_loss": -26.771181529998778, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.02696442604065, "step": 145000}
{"episode_reward": 368.74655266458814, "episode": 146.0, "batch_reward": 0.27963723419606684, "critic_loss": 0.5052390243262053, "actor_loss": -26.836191570281983, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.263177156448364, "step": 146000}
{"episode_reward": 600.7799671275974, "episode": 147.0, "batch_reward": 0.28223436178267003, "critic_loss": 0.5387519825994969, "actor_loss": -27.12150305938721, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.636640787124634, "step": 147000}
{"episode_reward": 323.14741668322074, "episode": 148.0, "batch_reward": 0.2831621533930302, "critic_loss": 0.5613165096789599, "actor_loss": -27.07837091064453, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.083914279937744, "step": 148000}
{"episode_reward": 411.636273458889, "episode": 149.0, "batch_reward": 0.2833524684906006, "critic_loss": 0.5290978764891624, "actor_loss": -27.100525455474852, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.083364009857178, "step": 149000}
{"episode_reward": 314.1546860589612, "episode": 150.0, "batch_reward": 0.2840321119129658, "critic_loss": 0.5674314826875925, "actor_loss": -27.054555335998536, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
