{"episode_reward": 0.0, "episode": 1.0, "duration": 17.735324144363403, "step": 1000}
{"episode_reward": 7.300100444933085, "episode": 2.0, "duration": 1.5081322193145752, "step": 2000}
{"episode_reward": 521.6090064697321, "episode": 3.0, "batch_reward": 0.2508338014109209, "critic_loss": 0.039425491569014086, "actor_loss": -30.944551273288965, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 68.1164972782135, "step": 3000}
{"episode_reward": 31.171651792347177, "episode": 4.0, "batch_reward": 0.1660551529750228, "critic_loss": 0.05260929913632572, "actor_loss": -18.513643908500672, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.12137508392334, "step": 4000}
{"episode_reward": 25.296453814280603, "episode": 5.0, "batch_reward": 0.14516701114177705, "critic_loss": 0.06872389713488519, "actor_loss": -14.897051421076059, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.678019523620605, "step": 5000}
{"episode_reward": 164.6371764838518, "episode": 6.0, "batch_reward": 0.14363932149112224, "critic_loss": 0.08879393080808222, "actor_loss": -12.483603264644742, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.036144018173218, "step": 6000}
{"episode_reward": 121.35375474665602, "episode": 7.0, "batch_reward": 0.13729347467422484, "critic_loss": 0.06634901147708297, "actor_loss": -13.756996943503617, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.427054405212402, "step": 7000}
{"episode_reward": 38.02128744824538, "episode": 8.0, "batch_reward": 0.12527446346729995, "critic_loss": 0.057149428334087135, "actor_loss": -16.329059139192104, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.295610189437866, "step": 8000}
{"episode_reward": 43.76383939914824, "episode": 9.0, "batch_reward": 0.11488226006925106, "critic_loss": 0.05827325962483883, "actor_loss": -16.14939852361381, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.79966688156128, "step": 9000}
{"episode_reward": 42.45128392644109, "episode": 10.0, "batch_reward": 0.10939690552651882, "critic_loss": 0.06005194515734911, "actor_loss": -16.72782214342058, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.99480128288269, "step": 10000}
{"episode_reward": 101.9650825487849, "episode": 11.0, "batch_reward": 0.10840508326143027, "critic_loss": 0.07135886374488473, "actor_loss": -17.57551055574417, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.72283220291138, "step": 11000}
{"episode_reward": 68.36431567807114, "episode": 12.0, "batch_reward": 0.10372781367599965, "critic_loss": 0.07170350096747279, "actor_loss": -16.804179069578648, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.22937846183777, "step": 12000}
{"episode_reward": 28.90928433557589, "episode": 13.0, "batch_reward": 0.10067435909807682, "critic_loss": 0.07454229084774852, "actor_loss": -16.831609663560986, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.224403142929077, "step": 13000}
{"episode_reward": 108.55627203559031, "episode": 14.0, "batch_reward": 0.10383582214266061, "critic_loss": 0.09165992726013064, "actor_loss": -17.024482415005565, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.529226779937744, "step": 14000}
{"episode_reward": 197.78211756455931, "episode": 15.0, "batch_reward": 0.10654305437207222, "critic_loss": 0.08736022824048996, "actor_loss": -16.24684204518795, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.664331674575806, "step": 15000}
{"episode_reward": 135.0818498747364, "episode": 16.0, "batch_reward": 0.10697056601196528, "critic_loss": 0.10931503010168672, "actor_loss": -18.369752040326595, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.78389310836792, "step": 16000}
{"episode_reward": 47.7610540835265, "episode": 17.0, "batch_reward": 0.10784871236979962, "critic_loss": 0.11422104201093317, "actor_loss": -17.72232784321904, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.38354253768921, "step": 17000}
{"episode_reward": 145.9200985856104, "episode": 18.0, "batch_reward": 0.10871460888534784, "critic_loss": 0.12863129613921045, "actor_loss": -17.940749569773676, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.089508533477783, "step": 18000}
{"episode_reward": 154.98502347036725, "episode": 19.0, "batch_reward": 0.11192528155446052, "critic_loss": 0.13916518894955515, "actor_loss": -18.616961397767067, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.721895694732666, "step": 19000}
{"episode_reward": 163.98043952182763, "episode": 20.0, "batch_reward": 0.11701349111646414, "critic_loss": 0.14273026827350258, "actor_loss": -17.950783633470536, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.31139349937439, "step": 20000}
{"episode_reward": 260.30021160992976, "episode": 21.0, "batch_reward": 0.12003995805978775, "critic_loss": 0.14999912977963686, "actor_loss": -18.76359629034996, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 43.76298236846924, "step": 21000}
{"episode_reward": 83.44215597795424, "episode": 22.0, "batch_reward": 0.11859801960736513, "critic_loss": 0.15820633383095264, "actor_loss": -18.460731664180756, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.845370292663574, "step": 22000}
{"episode_reward": 147.43779080605557, "episode": 23.0, "batch_reward": 0.1187833170965314, "critic_loss": 0.16269939974695444, "actor_loss": -19.028508839607237, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.24979877471924, "step": 23000}
{"episode_reward": 48.54226387361809, "episode": 24.0, "batch_reward": 0.11765560323745013, "critic_loss": 0.17047945330291986, "actor_loss": -19.05303513431549, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.210567951202393, "step": 24000}
{"episode_reward": 111.71449249970702, "episode": 25.0, "batch_reward": 0.11919477409124374, "critic_loss": 0.17900235529243946, "actor_loss": -18.81907894563675, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.579328060150146, "step": 25000}
{"episode_reward": 287.29374848266406, "episode": 26.0, "batch_reward": 0.12386036098748446, "critic_loss": 0.1862925150617957, "actor_loss": -19.030839546203612, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.648029088974, "step": 26000}
{"episode_reward": 98.85758911990712, "episode": 27.0, "batch_reward": 0.12316757521033286, "critic_loss": 0.20346984983980657, "actor_loss": -18.678222202301026, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.0444495677948, "step": 27000}
{"episode_reward": 82.65100418599881, "episode": 28.0, "batch_reward": 0.1199200933277607, "critic_loss": 0.20939343342930078, "actor_loss": -18.851994899749755, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.662079334259033, "step": 28000}
{"episode_reward": 75.43699268707195, "episode": 29.0, "batch_reward": 0.12056049522012473, "critic_loss": 0.21600825515389444, "actor_loss": -18.36483003234863, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.580455541610718, "step": 29000}
{"episode_reward": 243.2372094252482, "episode": 30.0, "batch_reward": 0.1254082127586007, "critic_loss": 0.22203233349323273, "actor_loss": -18.77243645477295, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.3110089302063, "step": 30000}
{"episode_reward": 249.32295725528337, "episode": 31.0, "batch_reward": 0.12980653878301382, "critic_loss": 0.2338294124007225, "actor_loss": -19.964336594581603, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.44973659515381, "step": 31000}
{"episode_reward": 279.81325927810093, "episode": 32.0, "batch_reward": 0.13495718669891357, "critic_loss": 0.23907464028149844, "actor_loss": -20.25791692352295, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.68688726425171, "step": 32000}
{"episode_reward": 265.01930162779155, "episode": 33.0, "batch_reward": 0.13865905921906233, "critic_loss": 0.22138029281049967, "actor_loss": -20.37374457836151, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.0939679145813, "step": 33000}
{"episode_reward": 220.92457193692562, "episode": 34.0, "batch_reward": 0.14063050524145365, "critic_loss": 0.22335633181780576, "actor_loss": -21.20343459892273, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.41759991645813, "step": 34000}
{"episode_reward": 290.4468068867614, "episode": 35.0, "batch_reward": 0.14501088075339794, "critic_loss": 0.21572530392557382, "actor_loss": -21.03932832336426, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.190876960754395, "step": 35000}
{"episode_reward": 223.27862400005017, "episode": 36.0, "batch_reward": 0.14573832628130912, "critic_loss": 0.22447615599632262, "actor_loss": -22.144436668395997, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.32657027244568, "step": 36000}
{"episode_reward": 93.963922502286, "episode": 37.0, "batch_reward": 0.1464804123416543, "critic_loss": 0.24002633948624133, "actor_loss": -21.56531294155121, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.237138509750366, "step": 37000}
{"episode_reward": 262.33716077701513, "episode": 38.0, "batch_reward": 0.1497429204136133, "critic_loss": 0.24380399343371392, "actor_loss": -21.377460987091066, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.54452872276306, "step": 38000}
{"episode_reward": 276.97195854816783, "episode": 39.0, "batch_reward": 0.15346655633300543, "critic_loss": 0.25223564337193966, "actor_loss": -22.29349866104126, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.066638469696045, "step": 39000}
{"episode_reward": 203.0503170963194, "episode": 40.0, "batch_reward": 0.1540468722805381, "critic_loss": 0.2548176563978195, "actor_loss": -22.470061317443847, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.088916063308716, "step": 40000}
{"episode_reward": 328.7976891461484, "episode": 41.0, "batch_reward": 0.1578648591786623, "critic_loss": 0.2790315606445074, "actor_loss": -23.19895354747772, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.79152512550354, "step": 41000}
{"episode_reward": 183.8415307824769, "episode": 42.0, "batch_reward": 0.15916351622343064, "critic_loss": 0.2974443056732416, "actor_loss": -22.501751518249513, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.784934043884277, "step": 42000}
{"episode_reward": 338.93899619443147, "episode": 43.0, "batch_reward": 0.16404227893054485, "critic_loss": 0.302842484280467, "actor_loss": -23.414442915916442, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.273324251174927, "step": 43000}
{"episode_reward": 295.7487759042354, "episode": 44.0, "batch_reward": 0.1658830295652151, "critic_loss": 0.3189140662252903, "actor_loss": -23.20622006034851, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.63731050491333, "step": 44000}
{"episode_reward": 256.5456937668034, "episode": 45.0, "batch_reward": 0.16634185843914748, "critic_loss": 0.3172020174562931, "actor_loss": -22.986859934806823, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.439589023590088, "step": 45000}
{"episode_reward": 82.57956957297867, "episode": 46.0, "batch_reward": 0.16527477777004243, "critic_loss": 0.31964900957047937, "actor_loss": -23.191258646011352, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.83612823486328, "step": 46000}
{"episode_reward": 123.28967045893596, "episode": 47.0, "batch_reward": 0.16742250108718873, "critic_loss": 0.32082232102751734, "actor_loss": -23.646591596603393, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.074026346206665, "step": 47000}
{"episode_reward": 383.1016518209878, "episode": 48.0, "batch_reward": 0.17048003602027892, "critic_loss": 0.319548435151577, "actor_loss": -23.671640714645385, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.678329706192017, "step": 48000}
{"episode_reward": 291.61114099540214, "episode": 49.0, "batch_reward": 0.17273878793418407, "critic_loss": 0.3291213880777359, "actor_loss": -24.183409395217897, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.439207792282104, "step": 49000}
{"episode_reward": 295.8515152329738, "episode": 50.0, "batch_reward": 0.1752175048738718, "critic_loss": 0.3572762771844864, "actor_loss": -23.81333042526245, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.68588900566101, "step": 50000}
{"episode_reward": 257.07910009783217, "episode": 51.0, "batch_reward": 0.1765547908395529, "critic_loss": 0.3393154958486557, "actor_loss": -24.208919857025148, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.4230592250824, "step": 51000}
{"episode_reward": 282.17015646491757, "episode": 52.0, "batch_reward": 0.17790623612701892, "critic_loss": 0.3564624663889408, "actor_loss": -24.76149959373474, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.166422367095947, "step": 52000}
{"episode_reward": 120.43602172923141, "episode": 53.0, "batch_reward": 0.1784407439082861, "critic_loss": 0.3429063699692488, "actor_loss": -23.726371704101563, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.224668264389038, "step": 53000}
{"episode_reward": 268.3289558666156, "episode": 54.0, "batch_reward": 0.18031541594862938, "critic_loss": 0.3467217670232058, "actor_loss": -25.37841025543213, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.605716228485107, "step": 54000}
{"episode_reward": 314.9499598694537, "episode": 55.0, "batch_reward": 0.18239842341840268, "critic_loss": 0.32565670350193976, "actor_loss": -25.32669252204895, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.012019157409668, "step": 55000}
{"episode_reward": 376.4337476541418, "episode": 56.0, "batch_reward": 0.18497425170242787, "critic_loss": 0.3552791640013456, "actor_loss": -24.588694986343384, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.422401189804077, "step": 56000}
{"episode_reward": 275.52411365361894, "episode": 57.0, "batch_reward": 0.1866132022589445, "critic_loss": 0.35832833686470983, "actor_loss": -25.377668977737425, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.504109859466553, "step": 57000}
{"episode_reward": 311.7502080397191, "episode": 58.0, "batch_reward": 0.1896572804003954, "critic_loss": 0.36908599753677845, "actor_loss": -25.46315888404846, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.244415998458862, "step": 58000}
{"episode_reward": 359.4155027947666, "episode": 59.0, "batch_reward": 0.19063148637115956, "critic_loss": 0.40423410503566265, "actor_loss": -25.95993576812744, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.049548625946045, "step": 59000}
{"episode_reward": 107.90978571916169, "episode": 60.0, "batch_reward": 0.19068385492265225, "critic_loss": 0.38995579083263876, "actor_loss": -25.37540120315552, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.917242765426636, "step": 60000}
{"episode_reward": 360.48526989486254, "episode": 61.0, "batch_reward": 0.1937579049319029, "critic_loss": 0.3878391278088093, "actor_loss": -26.217338760375977, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.56137228012085, "step": 61000}
{"episode_reward": 312.6199942898176, "episode": 62.0, "batch_reward": 0.19621561454236508, "critic_loss": 0.38301023849844934, "actor_loss": -25.554030561447142, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.40618658065796, "step": 62000}
{"episode_reward": 393.83104498121673, "episode": 63.0, "batch_reward": 0.19882331645488738, "critic_loss": 0.38324171063303947, "actor_loss": -26.09334871292114, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.84096884727478, "step": 63000}
{"episode_reward": 420.3482420995973, "episode": 64.0, "batch_reward": 0.20203302754461766, "critic_loss": 0.38515690331161023, "actor_loss": -26.75028909111023, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.65509819984436, "step": 64000}
{"episode_reward": 181.42927785757155, "episode": 65.0, "batch_reward": 0.2014094312787056, "critic_loss": 0.3964600184261799, "actor_loss": -25.967477029800413, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.31416392326355, "step": 65000}
{"episode_reward": 177.6176084403612, "episode": 66.0, "batch_reward": 0.20229073593020439, "critic_loss": 0.3893037437349558, "actor_loss": -26.187628774642945, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.922373056411743, "step": 66000}
{"episode_reward": 422.0697945649221, "episode": 67.0, "batch_reward": 0.20450539490580558, "critic_loss": 0.3829810870587826, "actor_loss": -26.19834053993225, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.01790475845337, "step": 67000}
{"episode_reward": 157.52344961318767, "episode": 68.0, "batch_reward": 0.20239218038320542, "critic_loss": 0.4024079918861389, "actor_loss": -26.71463774681091, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.06391215324402, "step": 68000}
{"episode_reward": 35.85219545200174, "episode": 69.0, "batch_reward": 0.20220024159550667, "critic_loss": 0.4072270160168409, "actor_loss": -26.447860872268677, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.318630933761597, "step": 69000}
{"episode_reward": 401.08265986077055, "episode": 70.0, "batch_reward": 0.2038964204788208, "critic_loss": 0.43453892558813095, "actor_loss": -26.960627174377443, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.327340126037598, "step": 70000}
{"episode_reward": 100.25760753162211, "episode": 71.0, "batch_reward": 0.20287074536085128, "critic_loss": 0.40938624078035357, "actor_loss": -25.574070428848266, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 38.06726431846619, "step": 71000}
{"episode_reward": 253.40362702693875, "episode": 72.0, "batch_reward": 0.2036085509955883, "critic_loss": 0.405150890275836, "actor_loss": -26.47464770889282, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.813767194747925, "step": 72000}
{"episode_reward": 228.08797019215226, "episode": 73.0, "batch_reward": 0.2043703625202179, "critic_loss": 0.4463181662112474, "actor_loss": -26.460672763824462, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.85087513923645, "step": 73000}
{"episode_reward": 371.1457668060828, "episode": 74.0, "batch_reward": 0.2069228142350912, "critic_loss": 0.4387747513651848, "actor_loss": -26.664432695388793, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.662402153015137, "step": 74000}
{"episode_reward": 285.54638568753427, "episode": 75.0, "batch_reward": 0.2076148507297039, "critic_loss": 0.4563291953355074, "actor_loss": -26.794662921905516, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.45023202896118, "step": 75000}
{"episode_reward": 236.30736151783495, "episode": 76.0, "batch_reward": 0.20847035297751426, "critic_loss": 0.453104357406497, "actor_loss": -26.67699129676819, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.833608627319336, "step": 76000}
{"episode_reward": 280.19733216528914, "episode": 77.0, "batch_reward": 0.20895360662043094, "critic_loss": 0.48339502047002314, "actor_loss": -26.4597671585083, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.117812395095825, "step": 77000}
{"episode_reward": 383.3848954248377, "episode": 78.0, "batch_reward": 0.2094711693227291, "critic_loss": 0.41173662266135214, "actor_loss": -26.694925497055053, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.69188404083252, "step": 78000}
{"episode_reward": 51.80529598417896, "episode": 79.0, "batch_reward": 0.20872561021149158, "critic_loss": 0.43338006787002087, "actor_loss": -26.465532691955566, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.249175786972046, "step": 79000}
{"episode_reward": 169.7636957018265, "episode": 80.0, "batch_reward": 0.20920733574032785, "critic_loss": 0.44326023231446743, "actor_loss": -26.669558197021484, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.186212301254272, "step": 80000}
{"episode_reward": 433.7562256973712, "episode": 81.0, "batch_reward": 0.21125119872391224, "critic_loss": 0.45885164698958397, "actor_loss": -26.287885782241823, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 37.893173694610596, "step": 81000}
{"episode_reward": 256.5832837524731, "episode": 82.0, "batch_reward": 0.21027098619937898, "critic_loss": 0.44066798335313795, "actor_loss": -26.306708646774293, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.880077123641968, "step": 82000}
{"episode_reward": 154.53091502922243, "episode": 83.0, "batch_reward": 0.21106470416486264, "critic_loss": 0.4529324000030756, "actor_loss": -27.18888987159729, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.453907251358032, "step": 83000}
{"episode_reward": 393.8499257077233, "episode": 84.0, "batch_reward": 0.2113399302661419, "critic_loss": 0.43242395941913125, "actor_loss": -27.08806529045105, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.593082904815674, "step": 84000}
{"episode_reward": 95.54474589837804, "episode": 85.0, "batch_reward": 0.2121040226817131, "critic_loss": 0.4014045811742544, "actor_loss": -26.419920413970946, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.38365626335144, "step": 85000}
{"episode_reward": 406.76067641909066, "episode": 86.0, "batch_reward": 0.21521236601471902, "critic_loss": 0.4035748108327389, "actor_loss": -26.94514285850525, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.273597955703735, "step": 86000}
{"episode_reward": 379.7700702947424, "episode": 87.0, "batch_reward": 0.21691101291775702, "critic_loss": 0.40367174151539803, "actor_loss": -27.200588806152343, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.19338059425354, "step": 87000}
{"episode_reward": 399.23671446131783, "episode": 88.0, "batch_reward": 0.2185170311331749, "critic_loss": 0.43330324190855024, "actor_loss": -27.579498607635497, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.91654872894287, "step": 88000}
{"episode_reward": 206.18638874007078, "episode": 89.0, "batch_reward": 0.21874898447096347, "critic_loss": 0.4104158022850752, "actor_loss": -26.87887935256958, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.522545337677002, "step": 89000}
{"episode_reward": 191.94371284301667, "episode": 90.0, "batch_reward": 0.21913252334296704, "critic_loss": 0.41438279531896116, "actor_loss": -27.36304916191101, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.3753080368042, "step": 90000}
{"episode_reward": 390.87220031627714, "episode": 91.0, "batch_reward": 0.220539731323719, "critic_loss": 0.4548938953578472, "actor_loss": -27.04813038825989, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 39.27342867851257, "step": 91000}
{"episode_reward": 475.85897370299165, "episode": 92.0, "batch_reward": 0.22326417647302152, "critic_loss": 0.4176225959211588, "actor_loss": -27.631909198760987, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.969890117645264, "step": 92000}
{"episode_reward": 450.2447048712797, "episode": 93.0, "batch_reward": 0.2251846392005682, "critic_loss": 0.42894405153393744, "actor_loss": -27.84773056793213, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.44549536705017, "step": 93000}
{"episode_reward": 200.8418298574461, "episode": 94.0, "batch_reward": 0.22450966715812684, "critic_loss": 0.42472666612267496, "actor_loss": -28.332857580184935, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.40543007850647, "step": 94000}
{"episode_reward": 130.60092241725837, "episode": 95.0, "batch_reward": 0.22304586593806744, "critic_loss": 0.48047636984288694, "actor_loss": -28.005781969070433, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.03347945213318, "step": 95000}
{"episode_reward": 82.03199718311856, "episode": 96.0, "batch_reward": 0.22153643801808356, "critic_loss": 0.4280534453690052, "actor_loss": -27.675920101165772, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.92705464363098, "step": 96000}
{"episode_reward": 190.4040791596298, "episode": 97.0, "batch_reward": 0.22171689639985562, "critic_loss": 0.3986471980065107, "actor_loss": -27.48186836051941, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.587341785430908, "step": 97000}
{"episode_reward": 427.0926073896037, "episode": 98.0, "batch_reward": 0.22415757310390472, "critic_loss": 0.39766461995244023, "actor_loss": -27.538728466033934, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.448071479797363, "step": 98000}
{"episode_reward": 174.98238297739695, "episode": 99.0, "batch_reward": 0.22257020580768586, "critic_loss": 0.39192907501757146, "actor_loss": -27.587528739929198, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.41878366470337, "step": 99000}
{"episode_reward": 53.81182648839845, "episode": 100.0, "batch_reward": 0.22127422343194486, "critic_loss": 0.3773778756707907, "actor_loss": -27.564049964904786, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.9825336933136, "step": 100000}
{"episode_reward": 127.30633454825814, "episode": 101.0, "batch_reward": 0.22241404378414154, "critic_loss": 0.36538799698650837, "actor_loss": -27.39819023513794, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.28733825683594, "step": 101000}
{"episode_reward": 395.0643505864462, "episode": 102.0, "batch_reward": 0.22203446307778357, "critic_loss": 0.39538066370785235, "actor_loss": -28.101030309677125, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.58394765853882, "step": 102000}
{"episode_reward": 239.73656384428514, "episode": 103.0, "batch_reward": 0.2235871202647686, "critic_loss": 0.4208263240158558, "actor_loss": -27.265611705780028, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.306349992752075, "step": 103000}
{"episode_reward": 432.1453283417532, "episode": 104.0, "batch_reward": 0.22490657812356948, "critic_loss": 0.4366467763334513, "actor_loss": -27.844224714279175, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.98321294784546, "step": 104000}
{"episode_reward": 82.85399073920225, "episode": 105.0, "batch_reward": 0.2225838211774826, "critic_loss": 0.41356202675402165, "actor_loss": -27.060571783065797, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.361181497573853, "step": 105000}
{"episode_reward": 185.8481813411022, "episode": 106.0, "batch_reward": 0.22179761651158333, "critic_loss": 0.4218268192410469, "actor_loss": -27.79433200263977, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.749236583709717, "step": 106000}
{"episode_reward": 171.52738925720934, "episode": 107.0, "batch_reward": 0.22212990295886995, "critic_loss": 0.40794371484220027, "actor_loss": -27.311920602798462, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.03410768508911, "step": 107000}
{"episode_reward": 99.73589022750441, "episode": 108.0, "batch_reward": 0.2199275454133749, "critic_loss": 0.43800166592001916, "actor_loss": -26.540047889709474, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.54848885536194, "step": 108000}
{"episode_reward": 70.92019737568438, "episode": 109.0, "batch_reward": 0.22045088873803614, "critic_loss": 0.43949952605366704, "actor_loss": -27.272778276443482, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.947482347488403, "step": 109000}
{"episode_reward": 511.24751915232974, "episode": 110.0, "batch_reward": 0.2233495669066906, "critic_loss": 0.4401686164736748, "actor_loss": -27.140743646621704, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.049296617507935, "step": 110000}
{"episode_reward": 394.2023813561404, "episode": 111.0, "batch_reward": 0.22489248698949812, "critic_loss": 0.4564986215233803, "actor_loss": -26.856290658950805, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.53363084793091, "step": 111000}
{"episode_reward": 513.447082373262, "episode": 112.0, "batch_reward": 0.22759869889914988, "critic_loss": 0.4379079728126526, "actor_loss": -27.61513614845276, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.056976079940796, "step": 112000}
{"episode_reward": 473.4802594976488, "episode": 113.0, "batch_reward": 0.22990506590902807, "critic_loss": 0.4803835306465626, "actor_loss": -27.966681802749633, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.19392156600952, "step": 113000}
{"episode_reward": 434.8293532677725, "episode": 114.0, "batch_reward": 0.22998811264336108, "critic_loss": 0.46660565109550955, "actor_loss": -28.266404817581176, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.551653146743774, "step": 114000}
{"episode_reward": 238.16813766689378, "episode": 115.0, "batch_reward": 0.2303720767945051, "critic_loss": 0.47833853052556513, "actor_loss": -27.737413787841795, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.723870992660522, "step": 115000}
{"episode_reward": 443.6945982841551, "episode": 116.0, "batch_reward": 0.23349803221225737, "critic_loss": 0.4734467858374119, "actor_loss": -27.98452614593506, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.014333248138428, "step": 116000}
{"episode_reward": 447.9786885966364, "episode": 117.0, "batch_reward": 0.23380871841311454, "critic_loss": 0.49570582492649556, "actor_loss": -27.65842776107788, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.60727858543396, "step": 117000}
{"episode_reward": 91.03593952062117, "episode": 118.0, "batch_reward": 0.233802877292037, "critic_loss": 0.4688340518325567, "actor_loss": -27.97304000854492, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.517051935195923, "step": 118000}
{"episode_reward": 447.4195746162734, "episode": 119.0, "batch_reward": 0.2348065951615572, "critic_loss": 0.4869354297220707, "actor_loss": -28.092868389129638, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.81815505027771, "step": 119000}
{"episode_reward": 208.89143624868163, "episode": 120.0, "batch_reward": 0.2334955896884203, "critic_loss": 0.49905284121632576, "actor_loss": -27.654527376174926, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.756166219711304, "step": 120000}
{"episode_reward": 62.864651883849184, "episode": 121.0, "batch_reward": 0.23373581084609032, "critic_loss": 0.451480163320899, "actor_loss": -27.536627265930175, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 40.282227754592896, "step": 121000}
{"episode_reward": 531.1224465288677, "episode": 122.0, "batch_reward": 0.23591194838285445, "critic_loss": 0.4516272930800915, "actor_loss": -28.053028528213503, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.069286823272705, "step": 122000}
{"episode_reward": 500.24432736263543, "episode": 123.0, "batch_reward": 0.23748088029026984, "critic_loss": 0.4200456854104996, "actor_loss": -28.689378717422485, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.181525468826294, "step": 123000}
{"episode_reward": 204.49716652238732, "episode": 124.0, "batch_reward": 0.23701120573282242, "critic_loss": 0.41581190095841886, "actor_loss": -28.46031824874878, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.43425941467285, "step": 124000}
{"episode_reward": 234.43870503955915, "episode": 125.0, "batch_reward": 0.2386930064111948, "critic_loss": 0.4519194248020649, "actor_loss": -28.33404744720459, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.891045570373535, "step": 125000}
{"episode_reward": 492.693459203051, "episode": 126.0, "batch_reward": 0.24058517277240754, "critic_loss": 0.4309779485017061, "actor_loss": -28.256006443023683, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.374876022338867, "step": 126000}
{"episode_reward": 451.2819965781708, "episode": 127.0, "batch_reward": 0.24190886043012141, "critic_loss": 0.4281843661814928, "actor_loss": -28.35291827201843, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.3743999004364, "step": 127000}
{"episode_reward": 433.6953535974937, "episode": 128.0, "batch_reward": 0.24261639986932276, "critic_loss": 0.4332297358363867, "actor_loss": -28.61601670074463, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 24.649505615234375, "step": 128000}
{"episode_reward": 274.4579457402321, "episode": 129.0, "batch_reward": 0.2438969336897135, "critic_loss": 0.44826788145303725, "actor_loss": -28.402417068481444, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.63077139854431, "step": 129000}
{"episode_reward": 538.0119498936458, "episode": 130.0, "batch_reward": 0.2464493509978056, "critic_loss": 0.4150695224702358, "actor_loss": -28.711507152557374, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.61066198348999, "step": 130000}
{"episode_reward": 523.085028991226, "episode": 131.0, "batch_reward": 0.2483430999815464, "critic_loss": 0.41070702171325685, "actor_loss": -28.990205118179322, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 42.680379152297974, "step": 131000}
{"episode_reward": 470.9089573971907, "episode": 132.0, "batch_reward": 0.2498304099291563, "critic_loss": 0.4245392825603485, "actor_loss": -29.467791053771972, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.102133989334106, "step": 132000}
{"episode_reward": 527.8732084560896, "episode": 133.0, "batch_reward": 0.25173651772737504, "critic_loss": 0.429913563773036, "actor_loss": -29.283088886260987, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.670936822891235, "step": 133000}
{"episode_reward": 513.8407727626247, "episode": 134.0, "batch_reward": 0.2547284713834524, "critic_loss": 0.43291278825700286, "actor_loss": -29.218884647369386, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.548546314239502, "step": 134000}
{"episode_reward": 508.0678413093276, "episode": 135.0, "batch_reward": 0.2549269642829895, "critic_loss": 0.45598398193717005, "actor_loss": -29.57060697937012, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 25.054892539978027, "step": 135000}
{"episode_reward": 193.7725747408825, "episode": 136.0, "batch_reward": 0.25554280607402324, "critic_loss": 0.46107683500647545, "actor_loss": -28.77901671218872, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.246787786483765, "step": 136000}
{"episode_reward": 496.62927160637486, "episode": 137.0, "batch_reward": 0.25694881415367127, "critic_loss": 0.44593711525201796, "actor_loss": -29.980461246490478, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.638056755065918, "step": 137000}
{"episode_reward": 507.53024564767844, "episode": 138.0, "batch_reward": 0.2601891026496887, "critic_loss": 0.45823718124628066, "actor_loss": -30.26675640487671, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.670060873031616, "step": 138000}
{"episode_reward": 555.8993392797046, "episode": 139.0, "batch_reward": 0.2622660632133484, "critic_loss": 0.457084194034338, "actor_loss": -30.179843326568605, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.99047875404358, "step": 139000}
{"episode_reward": 290.254702330715, "episode": 140.0, "batch_reward": 0.26272195468842985, "critic_loss": 0.47615518715977667, "actor_loss": -30.031795749664308, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.715564489364624, "step": 140000}
{"episode_reward": 315.0744025580038, "episode": 141.0, "batch_reward": 0.2622243448346853, "critic_loss": 0.48045889267325403, "actor_loss": -30.08661762237549, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 41.487247705459595, "step": 141000}
{"episode_reward": 538.1353204681652, "episode": 142.0, "batch_reward": 0.2627078475356102, "critic_loss": 0.47440695159137247, "actor_loss": -29.6154919090271, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.51144003868103, "step": 142000}
{"episode_reward": 483.72019993249324, "episode": 143.0, "batch_reward": 0.26620641674101353, "critic_loss": 0.49054437045753, "actor_loss": -30.274142223358155, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.377946138381958, "step": 143000}
{"episode_reward": 574.9370322840908, "episode": 144.0, "batch_reward": 0.2677229878753424, "critic_loss": 0.5121142608970404, "actor_loss": -30.453747665405274, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.54810667037964, "step": 144000}
{"episode_reward": 599.2170547411075, "episode": 145.0, "batch_reward": 0.27048233085870743, "critic_loss": 0.49885509672760964, "actor_loss": -30.990211723327636, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 23.112913608551025, "step": 145000}
{"episode_reward": 510.6857031279099, "episode": 146.0, "batch_reward": 0.2710875546783209, "critic_loss": 0.4737928252220154, "actor_loss": -31.070749347686768, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.91194224357605, "step": 146000}
{"episode_reward": 509.93954763405014, "episode": 147.0, "batch_reward": 0.2734865937530994, "critic_loss": 0.5048045217692853, "actor_loss": -31.291693328857423, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.016647338867188, "step": 147000}
{"episode_reward": 194.72835986702853, "episode": 148.0, "batch_reward": 0.27265358772873877, "critic_loss": 0.49802524723112584, "actor_loss": -31.394143379211425, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.879440784454346, "step": 148000}
{"episode_reward": 425.5852309044373, "episode": 149.0, "batch_reward": 0.272979413330555, "critic_loss": 0.4626171048283577, "actor_loss": -30.7359874420166, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.93228054046631, "step": 149000}
{"episode_reward": 518.983495473437, "episode": 150.0, "batch_reward": 0.27539401446282863, "critic_loss": 0.4642755949795246, "actor_loss": -31.454286136627196, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
