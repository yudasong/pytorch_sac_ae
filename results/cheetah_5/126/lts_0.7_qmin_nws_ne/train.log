{"episode_reward": 0.0, "episode": 1.0, "duration": 17.882166385650635, "step": 1000}
{"episode_reward": 7.300100444933085, "episode": 2.0, "duration": 1.529783010482788, "step": 2000}
{"episode_reward": 521.6090064697321, "episode": 3.0, "batch_reward": 0.2482080795231085, "critic_loss": 0.01725633690662703, "actor_loss": -31.760157736940307, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 69.139151096344, "step": 3000}
{"episode_reward": 5.977588209481898, "episode": 4.0, "batch_reward": 0.15543446247279644, "critic_loss": 0.011884720808593556, "actor_loss": -27.567994550704956, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.716066598892212, "step": 4000}
{"episode_reward": 7.932380812542933, "episode": 5.0, "batch_reward": 0.12298278130218386, "critic_loss": 0.013667573394952342, "actor_loss": -25.60982242012024, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.376919984817505, "step": 5000}
{"episode_reward": 10.543905507765128, "episode": 6.0, "batch_reward": 0.10222232919186354, "critic_loss": 0.013850268992595375, "actor_loss": -24.765090257644655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.557353734970093, "step": 6000}
{"episode_reward": 9.323491269892079, "episode": 7.0, "batch_reward": 0.08802224099636077, "critic_loss": 0.00989881927962415, "actor_loss": -23.79325770187378, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.64272451400757, "step": 7000}
{"episode_reward": 7.170052120172145, "episode": 8.0, "batch_reward": 0.07692262258008123, "critic_loss": 0.014743981554405764, "actor_loss": -24.661911847114563, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.093693733215332, "step": 8000}
{"episode_reward": 8.429162227646522, "episode": 9.0, "batch_reward": 0.06831256237253547, "critic_loss": 0.008954671362414957, "actor_loss": -23.32056184768677, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.564852714538574, "step": 9000}
{"episode_reward": 10.029624830045435, "episode": 10.0, "batch_reward": 0.06267597572691738, "critic_loss": 0.012490103938151151, "actor_loss": -23.021928318023683, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.707643747329712, "step": 10000}
{"episode_reward": 7.2339140708656435, "episode": 11.0, "batch_reward": 0.05699052691832185, "critic_loss": 0.011464501653332263, "actor_loss": -22.890809473514558, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.09133791923523, "step": 11000}
{"episode_reward": 7.531856804799757, "episode": 12.0, "batch_reward": 0.05358435118384659, "critic_loss": 0.011151124257477931, "actor_loss": -22.64526479625702, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.186936378479004, "step": 12000}
{"episode_reward": 9.509122594270126, "episode": 13.0, "batch_reward": 0.04969084564037621, "critic_loss": 0.01214632985217031, "actor_loss": -22.555760619163514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.46759867668152, "step": 13000}
{"episode_reward": 8.066403884083376, "episode": 14.0, "batch_reward": 0.045766575334593655, "critic_loss": 0.007941667685983703, "actor_loss": -22.123026731491088, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.96574592590332, "step": 14000}
{"episode_reward": 7.7526690150885935, "episode": 15.0, "batch_reward": 0.043851737443357706, "critic_loss": 0.01152832705155015, "actor_loss": -20.496128573894502, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.314717769622803, "step": 15000}
{"episode_reward": 7.96467486936288, "episode": 16.0, "batch_reward": 0.040837012698873874, "critic_loss": 0.010350395900430158, "actor_loss": -22.515757670879363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.438119411468506, "step": 16000}
{"episode_reward": 9.011635452946779, "episode": 17.0, "batch_reward": 0.03933637439273298, "critic_loss": 0.00915278307022527, "actor_loss": -21.8368464140892, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.02207589149475, "step": 17000}
{"episode_reward": 10.297699812037411, "episode": 18.0, "batch_reward": 0.037374412822537124, "critic_loss": 0.007576037909486331, "actor_loss": -21.890867010116576, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.451938152313232, "step": 18000}
{"episode_reward": 7.610319892141546, "episode": 19.0, "batch_reward": 0.036616974234115335, "critic_loss": 0.012239764532539993, "actor_loss": -22.0490948882103, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.022225856781006, "step": 19000}
{"episode_reward": 13.285540031791374, "episode": 20.0, "batch_reward": 0.035260138649493455, "critic_loss": 0.007094357435125858, "actor_loss": -20.628754342556, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.034878730773926, "step": 20000}
{"episode_reward": 8.228257817481172, "episode": 21.0, "batch_reward": 0.03309591605188325, "critic_loss": 0.010546808164333924, "actor_loss": -20.97894150876999, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 39.906628370285034, "step": 21000}
{"episode_reward": 10.406125166318436, "episode": 22.0, "batch_reward": 0.032300034759100524, "critic_loss": 0.0077644414730602875, "actor_loss": -20.78371748495102, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.680776119232178, "step": 22000}
{"episode_reward": 11.00295095709115, "episode": 23.0, "batch_reward": 0.031343839399516585, "critic_loss": 0.006656098601641134, "actor_loss": -21.10930594444275, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.64422869682312, "step": 23000}
{"episode_reward": 6.5499791237131495, "episode": 24.0, "batch_reward": 0.030612794192507863, "critic_loss": 0.008109415225335396, "actor_loss": -21.41414002585411, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.303433895111084, "step": 24000}
{"episode_reward": 7.409283492720344, "episode": 25.0, "batch_reward": 0.029617883284110576, "critic_loss": 0.007755213817057666, "actor_loss": -20.868636543273926, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.966917753219604, "step": 25000}
{"episode_reward": 8.329841405248526, "episode": 26.0, "batch_reward": 0.029164474637247622, "critic_loss": 0.007080041124834679, "actor_loss": -20.762273745059968, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.51127052307129, "step": 26000}
{"episode_reward": 10.771137985980529, "episode": 27.0, "batch_reward": 0.02846857483452186, "critic_loss": 0.0075785300465067845, "actor_loss": -20.20905868244171, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.40092134475708, "step": 27000}
{"episode_reward": 9.087160071410176, "episode": 28.0, "batch_reward": 0.026791485361754894, "critic_loss": 0.009045329992310143, "actor_loss": -20.86872011733055, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.207584381103516, "step": 28000}
{"episode_reward": 9.169397030843932, "episode": 29.0, "batch_reward": 0.026339170487597586, "critic_loss": 0.004025426501699258, "actor_loss": -20.042073486089706, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.48397397994995, "step": 29000}
{"episode_reward": 10.611577426118027, "episode": 30.0, "batch_reward": 0.026533679897896945, "critic_loss": 0.007868454118841328, "actor_loss": -19.9563122651577, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.581487894058228, "step": 30000}
{"episode_reward": 8.245032376090796, "episode": 31.0, "batch_reward": 0.025506897185463458, "critic_loss": 0.006030678637442179, "actor_loss": -20.62324983215332, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.62804079055786, "step": 31000}
{"episode_reward": 7.426418946924758, "episode": 32.0, "batch_reward": 0.02491208848915994, "critic_loss": 0.006089008300448768, "actor_loss": -20.348919896125793, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.26207160949707, "step": 32000}
{"episode_reward": 6.234739569101739, "episode": 33.0, "batch_reward": 0.024907036036718636, "critic_loss": 0.008555009939649609, "actor_loss": -19.934454882860184, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.56367826461792, "step": 33000}
{"episode_reward": 10.332743655689871, "episode": 34.0, "batch_reward": 0.023911403241101654, "critic_loss": 0.005747996669088025, "actor_loss": -20.724773546695708, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.562362670898438, "step": 34000}
{"episode_reward": 10.053389554106204, "episode": 35.0, "batch_reward": 0.02350309984013438, "critic_loss": 0.004946856674883747, "actor_loss": -19.78375415635109, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.477909803390503, "step": 35000}
{"episode_reward": 8.15765334374669, "episode": 36.0, "batch_reward": 0.023244046116713435, "critic_loss": 0.005524248144443846, "actor_loss": -21.22796338045597, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.650657176971436, "step": 36000}
{"episode_reward": 11.66142057117396, "episode": 37.0, "batch_reward": 0.022794891241472215, "critic_loss": 0.006776593097485602, "actor_loss": -20.38339046084881, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.32524085044861, "step": 37000}
{"episode_reward": 8.820161198695798, "episode": 38.0, "batch_reward": 0.02254102248325944, "critic_loss": 0.013215640405454906, "actor_loss": -19.410021487236023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.400312185287476, "step": 38000}
{"episode_reward": 7.723875969389872, "episode": 39.0, "batch_reward": 0.02245350262382999, "critic_loss": 0.0035758869995479474, "actor_loss": -20.293896873116495, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.23435378074646, "step": 39000}
{"episode_reward": 7.123924206565693, "episode": 40.0, "batch_reward": 0.021469464445486666, "critic_loss": 0.00439527781988727, "actor_loss": -20.37486642551422, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.18859601020813, "step": 40000}
{"episode_reward": 7.341407311416065, "episode": 41.0, "batch_reward": 0.021429087847005576, "critic_loss": 0.006909985561243957, "actor_loss": -20.967296168804168, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 40.52518343925476, "step": 41000}
{"episode_reward": 8.410970322619992, "episode": 42.0, "batch_reward": 0.020952023038174956, "critic_loss": 0.005066347928310279, "actor_loss": -19.883280232429506, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.045125484466553, "step": 42000}
{"episode_reward": 7.418084391472881, "episode": 43.0, "batch_reward": 0.020575624532531946, "critic_loss": 0.0066393096881511154, "actor_loss": -20.55496494424343, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.76281452178955, "step": 43000}
{"episode_reward": 9.400301447076643, "episode": 44.0, "batch_reward": 0.02024619207298383, "critic_loss": 0.004256185762875247, "actor_loss": -19.773427770853043, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.7526113986969, "step": 44000}
{"episode_reward": 8.536035538845349, "episode": 45.0, "batch_reward": 0.020109084142372013, "critic_loss": 0.004517050768074114, "actor_loss": -19.479200075387954, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.39774465560913, "step": 45000}
{"episode_reward": 9.142703780560883, "episode": 46.0, "batch_reward": 0.020061187242157757, "critic_loss": 0.005250060187623603, "actor_loss": -20.070024984240533, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.151044368743896, "step": 46000}
{"episode_reward": 10.169289515807465, "episode": 47.0, "batch_reward": 0.019761872057337315, "critic_loss": 0.005688062480156077, "actor_loss": -20.385570546507836, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.44870090484619, "step": 47000}
{"episode_reward": 7.606318941929153, "episode": 48.0, "batch_reward": 0.01938034411892295, "critic_loss": 0.004019073764880887, "actor_loss": -20.181152584671974, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.956169366836548, "step": 48000}
{"episode_reward": 7.217105461362922, "episode": 49.0, "batch_reward": 0.019671429408714174, "critic_loss": 0.00407670914530172, "actor_loss": -20.25077868425846, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.586745262145996, "step": 49000}
{"episode_reward": 9.06847803386471, "episode": 50.0, "batch_reward": 0.019312113164924084, "critic_loss": 0.0036930657297780272, "actor_loss": -19.513020361423493, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.283439874649048, "step": 50000}
{"episode_reward": 12.177893512979553, "episode": 51.0, "batch_reward": 0.018954362192656844, "critic_loss": 0.0054655627229367384, "actor_loss": -19.863196870625018, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.31586790084839, "step": 51000}
{"episode_reward": 11.31256594487652, "episode": 52.0, "batch_reward": 0.019010276520159096, "critic_loss": 0.004507890322100138, "actor_loss": -20.412858086228372, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.797415018081665, "step": 52000}
{"episode_reward": 9.83833384091636, "episode": 53.0, "batch_reward": 0.018754733317065984, "critic_loss": 0.004468052747572074, "actor_loss": -18.808775646209718, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.257662534713745, "step": 53000}
{"episode_reward": 8.014494030630242, "episode": 54.0, "batch_reward": 0.018857057849876582, "critic_loss": 0.004044630987540586, "actor_loss": -21.200430409252643, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.04636836051941, "step": 54000}
{"episode_reward": 8.661618915542103, "episode": 55.0, "batch_reward": 0.0181835277187638, "critic_loss": 0.004288752400869271, "actor_loss": -20.514861352145672, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.737106800079346, "step": 55000}
{"episode_reward": 7.99430363596811, "episode": 56.0, "batch_reward": 0.0178618554091081, "critic_loss": 0.005449997646152041, "actor_loss": -19.119761962473394, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.21991515159607, "step": 56000}
{"episode_reward": 8.374019479183396, "episode": 57.0, "batch_reward": 0.018039142514579, "critic_loss": 0.0030493122583138757, "actor_loss": -20.202656332433225, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.566412210464478, "step": 57000}
{"episode_reward": 8.717777643484403, "episode": 58.0, "batch_reward": 0.017777960235718638, "critic_loss": 0.005257021161698503, "actor_loss": -19.93106141960621, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.997204303741455, "step": 58000}
{"episode_reward": 10.253060870077451, "episode": 59.0, "batch_reward": 0.0174527510330081, "critic_loss": 0.0026965529432636686, "actor_loss": -20.331019832730295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.091105937957764, "step": 59000}
{"episode_reward": 12.480607536659297, "episode": 60.0, "batch_reward": 0.017572137624491006, "critic_loss": 0.005245872642874019, "actor_loss": -19.45688741928339, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.38417148590088, "step": 60000}
{"episode_reward": 15.946651063558406, "episode": 61.0, "batch_reward": 0.01730046194884926, "critic_loss": 0.0041181203963060395, "actor_loss": -20.37504903203249, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.99004864692688, "step": 61000}
{"episode_reward": 8.341154599232596, "episode": 62.0, "batch_reward": 0.017045064783189446, "critic_loss": 0.003641555676731514, "actor_loss": -19.041980600595473, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.840921878814697, "step": 62000}
{"episode_reward": 11.758313715646226, "episode": 63.0, "batch_reward": 0.01711109602171928, "critic_loss": 0.004084769995097303, "actor_loss": -19.458429326832295, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.863286018371582, "step": 63000}
{"episode_reward": 11.208925216278843, "episode": 64.0, "batch_reward": 0.017022575379814952, "critic_loss": 0.00351817677760846, "actor_loss": -20.021765114068984, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.58632493019104, "step": 64000}
{"episode_reward": 12.208889667218, "episode": 65.0, "batch_reward": 0.017134891434106975, "critic_loss": 0.004385326709365472, "actor_loss": -18.959424119591713, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.215319871902466, "step": 65000}
{"episode_reward": 8.390593034101638, "episode": 66.0, "batch_reward": 0.016648683141451327, "critic_loss": 0.0030028128383273725, "actor_loss": -19.280605085730553, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.423522233963013, "step": 66000}
{"episode_reward": 7.335883136687344, "episode": 67.0, "batch_reward": 0.016592002579476683, "critic_loss": 0.003896771117229946, "actor_loss": -18.811777934849264, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.56212878227234, "step": 67000}
{"episode_reward": 7.026054898603834, "episode": 68.0, "batch_reward": 0.016730261627584696, "critic_loss": 0.004005918243492488, "actor_loss": -20.08851998516917, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.135652780532837, "step": 68000}
{"episode_reward": 6.314273421309433, "episode": 69.0, "batch_reward": 0.016616553467232734, "critic_loss": 0.0035564998240733984, "actor_loss": -19.76781754595041, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.79088068008423, "step": 69000}
{"episode_reward": 12.591858339167471, "episode": 70.0, "batch_reward": 0.016291437342762945, "critic_loss": 0.0035073388700257056, "actor_loss": -20.400862923532724, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.909928560256958, "step": 70000}
{"episode_reward": 8.026938969144659, "episode": 71.0, "batch_reward": 0.016228679405059666, "critic_loss": 0.0034741277343127877, "actor_loss": -18.359012882053854, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.491783142089844, "step": 71000}
{"episode_reward": 7.729968951164117, "episode": 72.0, "batch_reward": 0.016063663371838628, "critic_loss": 0.0036774736095103434, "actor_loss": -19.727995398879052, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.484301328659058, "step": 72000}
{"episode_reward": 6.380870010788216, "episode": 73.0, "batch_reward": 0.016108303751796484, "critic_loss": 0.0033020337348862085, "actor_loss": -19.833526389807464, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.763524770736694, "step": 73000}
{"episode_reward": 8.496936458773744, "episode": 74.0, "batch_reward": 0.015896065106149763, "critic_loss": 0.003843537819542689, "actor_loss": -19.638828677773475, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.027958154678345, "step": 74000}
{"episode_reward": 8.272578658659116, "episode": 75.0, "batch_reward": 0.016141749683301895, "critic_loss": 0.004339973841531901, "actor_loss": -19.975142620384695, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.79511070251465, "step": 75000}
{"episode_reward": 8.029111368812458, "episode": 76.0, "batch_reward": 0.015597626217175276, "critic_loss": 0.003008864539064234, "actor_loss": -19.670495017915965, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.91527509689331, "step": 76000}
{"episode_reward": 7.315586351846342, "episode": 77.0, "batch_reward": 0.015359228926245123, "critic_loss": 0.0035945846265822185, "actor_loss": -19.222332988381385, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.676711320877075, "step": 77000}
{"episode_reward": 11.472288792118572, "episode": 78.0, "batch_reward": 0.015674846529960634, "critic_loss": 0.004094440249813488, "actor_loss": -19.522033332139255, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.8149094581604, "step": 78000}
{"episode_reward": 7.270328606147149, "episode": 79.0, "batch_reward": 0.015346544972155244, "critic_loss": 0.0023942029343306786, "actor_loss": -19.261101623803377, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.136250257492065, "step": 79000}
{"episode_reward": 8.169872555157793, "episode": 80.0, "batch_reward": 0.015431673146318645, "critic_loss": 0.0037610734777408653, "actor_loss": -19.434403828531504, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.42941665649414, "step": 80000}
{"episode_reward": 7.807819571791945, "episode": 81.0, "batch_reward": 0.01534937695786357, "critic_loss": 0.0025637386973830873, "actor_loss": -18.61571572354436, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.76617503166199, "step": 81000}
{"episode_reward": 7.208767116904792, "episode": 82.0, "batch_reward": 0.015074186524376273, "critic_loss": 0.003217436110106064, "actor_loss": -18.87906468614936, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.96152663230896, "step": 82000}
{"episode_reward": 7.00855119008684, "episode": 83.0, "batch_reward": 0.014966968534514308, "critic_loss": 0.004038520606321981, "actor_loss": -20.229140785843136, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.38703465461731, "step": 83000}
{"episode_reward": 7.1927523411433905, "episode": 84.0, "batch_reward": 0.014630945703480393, "critic_loss": 0.0026863265862048136, "actor_loss": -20.293520110964774, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.246617555618286, "step": 84000}
{"episode_reward": 6.496106851419371, "episode": 85.0, "batch_reward": 0.015170014503411948, "critic_loss": 0.004113513022253755, "actor_loss": -18.82087242856622, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.71585702896118, "step": 85000}
{"episode_reward": 4.985995054053279, "episode": 86.0, "batch_reward": 0.01478093726048246, "critic_loss": 0.003647008670028299, "actor_loss": -19.23571296864748, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.88050365447998, "step": 86000}
{"episode_reward": 9.504204940307588, "episode": 87.0, "batch_reward": 0.015051491116639226, "critic_loss": 0.0027138310757, "actor_loss": -19.51502973562479, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.729859352111816, "step": 87000}
{"episode_reward": 8.24348401833136, "episode": 88.0, "batch_reward": 0.014794188380707056, "critic_loss": 0.0038387973062781384, "actor_loss": -19.95072657507658, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.253397464752197, "step": 88000}
{"episode_reward": 6.691062978340157, "episode": 89.0, "batch_reward": 0.014545682271476835, "critic_loss": 0.002675425095396349, "actor_loss": -18.842311459973455, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.29407501220703, "step": 89000}
{"episode_reward": 11.79156901517135, "episode": 90.0, "batch_reward": 0.01440014118514955, "critic_loss": 0.0030780409612489166, "actor_loss": -19.657924326196312, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.906383275985718, "step": 90000}
{"episode_reward": 8.943101357524636, "episode": 91.0, "batch_reward": 0.014511183951981366, "critic_loss": 0.004403425369178877, "actor_loss": -19.048284906610846, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.82095813751221, "step": 91000}
{"episode_reward": 10.216232914321315, "episode": 92.0, "batch_reward": 0.014466250328812748, "critic_loss": 0.0035150549110257996, "actor_loss": -19.424542449086903, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.009111881256104, "step": 92000}
{"episode_reward": 9.28500227457898, "episode": 93.0, "batch_reward": 0.014489395502023399, "critic_loss": 0.003708030934110866, "actor_loss": -19.74605260141194, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.115499019622803, "step": 93000}
{"episode_reward": 9.731142333852457, "episode": 94.0, "batch_reward": 0.014144045865628868, "critic_loss": 0.0024391578290669713, "actor_loss": -20.677724517807363, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.618754625320435, "step": 94000}
{"episode_reward": 8.69049999581609, "episode": 95.0, "batch_reward": 0.014129530261736363, "critic_loss": 0.004079468248441117, "actor_loss": -20.512765438213943, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.24762749671936, "step": 95000}
{"episode_reward": 9.83928482025667, "episode": 96.0, "batch_reward": 0.014220186423975975, "critic_loss": 0.0025717011763481423, "actor_loss": -20.07836854314804, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.6271231174469, "step": 96000}
{"episode_reward": 6.362512092375104, "episode": 97.0, "batch_reward": 0.014027069022413343, "critic_loss": 0.0033140552329568892, "actor_loss": -19.948390608355403, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.387521505355835, "step": 97000}
{"episode_reward": 5.045407202301223, "episode": 98.0, "batch_reward": 0.013926817621104419, "critic_loss": 0.002760850578692043, "actor_loss": -19.565322858408095, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.285881519317627, "step": 98000}
{"episode_reward": 9.162818840187098, "episode": 99.0, "batch_reward": 0.014077108844183386, "critic_loss": 0.003983647283079336, "actor_loss": -20.000451984494923, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.29007601737976, "step": 99000}
{"episode_reward": 7.427601971530432, "episode": 100.0, "batch_reward": 0.01378977259900421, "critic_loss": 0.002514139588238322, "actor_loss": -20.05418038767576, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.143771409988403, "step": 100000}
{"episode_reward": 8.4831898129583, "episode": 101.0, "batch_reward": 0.01373638401646167, "critic_loss": 0.0021296764825092396, "actor_loss": -19.76632269074023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 43.746636152267456, "step": 101000}
{"episode_reward": 9.463680315445847, "episode": 102.0, "batch_reward": 0.0136283024167642, "critic_loss": 0.0036643275890965014, "actor_loss": -21.128859979823233, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.238035917282104, "step": 102000}
{"episode_reward": 6.180297736856311, "episode": 103.0, "batch_reward": 0.013816428358200938, "critic_loss": 0.002157449920588988, "actor_loss": -19.559381904169918, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.807113647460938, "step": 103000}
{"episode_reward": 6.0487873044465, "episode": 104.0, "batch_reward": 0.013950844686478376, "critic_loss": 0.0027637646414805203, "actor_loss": -20.32428803293407, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.421397924423218, "step": 104000}
{"episode_reward": 6.982166468423846, "episode": 105.0, "batch_reward": 0.013527869445737452, "critic_loss": 0.0026626854387286586, "actor_loss": -19.253625772759317, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.809447526931763, "step": 105000}
{"episode_reward": 7.744034666016148, "episode": 106.0, "batch_reward": 0.013670504949986935, "critic_loss": 0.00256343574897619, "actor_loss": -20.865900903970005, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.042802572250366, "step": 106000}
{"episode_reward": 10.911019276014876, "episode": 107.0, "batch_reward": 0.01352500564372167, "critic_loss": 0.002388073729627649, "actor_loss": -20.014485814869403, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.53178119659424, "step": 107000}
{"episode_reward": 10.183724111764377, "episode": 108.0, "batch_reward": 0.013267841257154942, "critic_loss": 0.002297233159188181, "actor_loss": -18.845638117060066, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.61496591567993, "step": 108000}
{"episode_reward": 9.82995362332113, "episode": 109.0, "batch_reward": 0.01348183654434979, "critic_loss": 0.0031069867078040263, "actor_loss": -20.46663621082902, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.702643632888794, "step": 109000}
{"episode_reward": 8.295169085842451, "episode": 110.0, "batch_reward": 0.013253405258525162, "critic_loss": 0.002614285827992717, "actor_loss": -19.62353886666894, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.94746470451355, "step": 110000}
{"episode_reward": 10.217697827071573, "episode": 111.0, "batch_reward": 0.013355427131056785, "critic_loss": 0.0024900107852445217, "actor_loss": -19.02612553358078, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 44.02219104766846, "step": 111000}
{"episode_reward": 7.491469105737339, "episode": 112.0, "batch_reward": 0.013453349262010306, "critic_loss": 0.0028359149367752252, "actor_loss": -20.17568374504149, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.366413116455078, "step": 112000}
{"episode_reward": 7.855147328172014, "episode": 113.0, "batch_reward": 0.01331708997907117, "critic_loss": 0.0025087158821261255, "actor_loss": -19.904580277174713, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.25147581100464, "step": 113000}
{"episode_reward": 7.508317957291304, "episode": 114.0, "batch_reward": 0.013386334272101522, "critic_loss": 0.002296022846363485, "actor_loss": -20.674089236691593, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.32004451751709, "step": 114000}
{"episode_reward": 7.426813212091537, "episode": 115.0, "batch_reward": 0.012937244896311312, "critic_loss": 0.002083469179473468, "actor_loss": -19.775858091577888, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.694834232330322, "step": 115000}
{"episode_reward": 9.764629569047766, "episode": 116.0, "batch_reward": 0.013068952255416662, "critic_loss": 0.003751329182719928, "actor_loss": -19.990603301361205, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.492915630340576, "step": 116000}
{"episode_reward": 10.77007500938524, "episode": 117.0, "batch_reward": 0.013466571459081023, "critic_loss": 0.0017890579178347253, "actor_loss": -18.91154381299019, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.242144346237183, "step": 117000}
{"episode_reward": 7.390392166578284, "episode": 118.0, "batch_reward": 0.013008977495133876, "critic_loss": 0.0025958408592705383, "actor_loss": -19.606569863066078, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.415839433670044, "step": 118000}
{"episode_reward": 8.871339913814117, "episode": 119.0, "batch_reward": 0.012984371162950993, "critic_loss": 0.0023349367782648186, "actor_loss": -19.81890674698353, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.08112120628357, "step": 119000}
{"episode_reward": 7.292554666214803, "episode": 120.0, "batch_reward": 0.012979955133516342, "critic_loss": 0.002429411266683019, "actor_loss": -19.103374823078514, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.57718825340271, "step": 120000}
{"episode_reward": 10.3156812111789, "episode": 121.0, "batch_reward": 0.01298188499500975, "critic_loss": 0.002059487770020496, "actor_loss": -18.95039145851135, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 41.5593798160553, "step": 121000}
{"episode_reward": 6.135593988081927, "episode": 122.0, "batch_reward": 0.013047217869199813, "critic_loss": 0.0031159379990422166, "actor_loss": -19.50064603839815, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.329188585281372, "step": 122000}
{"episode_reward": 7.9861447871632505, "episode": 123.0, "batch_reward": 0.012652628179173916, "critic_loss": 0.001833637492833077, "actor_loss": -20.580272062137723, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.31814193725586, "step": 123000}
{"episode_reward": 8.212998747569833, "episode": 124.0, "batch_reward": 0.012822225279174745, "critic_loss": 0.0023293388167658123, "actor_loss": -20.189210515841843, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.261255979537964, "step": 124000}
{"episode_reward": 10.593786105456118, "episode": 125.0, "batch_reward": 0.012874792399350554, "critic_loss": 0.0029666401941503863, "actor_loss": -19.86421496592462, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.82574200630188, "step": 125000}
{"episode_reward": 8.574229230278654, "episode": 126.0, "batch_reward": 0.012885910260025411, "critic_loss": 0.0023559216578432824, "actor_loss": -19.273624947682023, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.021179914474487, "step": 126000}
{"episode_reward": 8.073290364731854, "episode": 127.0, "batch_reward": 0.012711470519658178, "critic_loss": 0.0015535256578004919, "actor_loss": -19.304754089452327, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.580594062805176, "step": 127000}
{"episode_reward": 7.411251103789497, "episode": 128.0, "batch_reward": 0.012314094066154212, "critic_loss": 0.0024978382677072657, "actor_loss": -19.846766669817267, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.506314992904663, "step": 128000}
{"episode_reward": 10.847059919665876, "episode": 129.0, "batch_reward": 0.012916811308823526, "critic_loss": 0.0018024555498413974, "actor_loss": -19.205699327334763, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.782196044921875, "step": 129000}
{"episode_reward": 6.08449686530827, "episode": 130.0, "batch_reward": 0.012400091538671405, "critic_loss": 0.0019915559680084697, "actor_loss": -19.23614871926606, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.869348764419556, "step": 130000}
{"episode_reward": 10.373384022464709, "episode": 131.0, "batch_reward": 0.012581952893175185, "critic_loss": 0.002089102354788338, "actor_loss": -19.54691586613655, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 38.59961152076721, "step": 131000}
{"episode_reward": 8.360590570989443, "episode": 132.0, "batch_reward": 0.01256553271226585, "critic_loss": 0.0025326934560434894, "actor_loss": -20.124889610871673, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.116365671157837, "step": 132000}
{"episode_reward": 6.563056210323642, "episode": 133.0, "batch_reward": 0.012370241646654904, "critic_loss": 0.0022611038955801634, "actor_loss": -19.613817440986633, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.636418342590332, "step": 133000}
{"episode_reward": 7.380478336617393, "episode": 134.0, "batch_reward": 0.012610982734244316, "critic_loss": 0.0018254594540921972, "actor_loss": -19.04100251404941, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.940165281295776, "step": 134000}
{"episode_reward": 8.93646773766219, "episode": 135.0, "batch_reward": 0.012445434853900223, "critic_loss": 0.001771643076906912, "actor_loss": -19.561004463881254, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 25.063669681549072, "step": 135000}
{"episode_reward": 10.648556847464588, "episode": 136.0, "batch_reward": 0.012379161909222603, "critic_loss": 0.0019167081233172213, "actor_loss": -17.908206742361187, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.015676975250244, "step": 136000}
{"episode_reward": 8.532095150025121, "episode": 137.0, "batch_reward": 0.012391627468634397, "critic_loss": 0.002483058100377093, "actor_loss": -19.950507414355876, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 20.89117956161499, "step": 137000}
{"episode_reward": 7.124560122525719, "episode": 138.0, "batch_reward": 0.012358275428414344, "critic_loss": 0.0016330732404167066, "actor_loss": -20.32823386479914, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.414023876190186, "step": 138000}
{"episode_reward": 8.225802663400152, "episode": 139.0, "batch_reward": 0.012266489997971803, "critic_loss": 0.0022984664188552416, "actor_loss": -19.610283760547638, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 24.952150106430054, "step": 139000}
{"episode_reward": 7.2192487514116435, "episode": 140.0, "batch_reward": 0.012242572360206396, "critic_loss": 0.002723154823150253, "actor_loss": -19.28752634577453, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.586002349853516, "step": 140000}
{"episode_reward": 8.1376672760039, "episode": 141.0, "batch_reward": 0.012475883315782994, "critic_loss": 0.002053875608791714, "actor_loss": -19.56337080632895, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 42.22587847709656, "step": 141000}
{"episode_reward": 9.300726714633504, "episode": 142.0, "batch_reward": 0.012291600348427892, "critic_loss": 0.0018344345905497903, "actor_loss": -18.461037597849966, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.508991718292236, "step": 142000}
{"episode_reward": 7.205365514239099, "episode": 143.0, "batch_reward": 0.012455465420149267, "critic_loss": 0.0018583224551694001, "actor_loss": -19.078785479784013, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 21.048556089401245, "step": 143000}
{"episode_reward": 10.446832326050101, "episode": 144.0, "batch_reward": 0.01202775195427239, "critic_loss": 0.002547893570284941, "actor_loss": -19.337943860352038, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.54412341117859, "step": 144000}
{"episode_reward": 11.728162138546214, "episode": 145.0, "batch_reward": 0.012118170173838734, "critic_loss": 0.0029950302554061638, "actor_loss": -19.990660789191722, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.603880882263184, "step": 145000}
{"episode_reward": 11.970964990472844, "episode": 146.0, "batch_reward": 0.012273228013888, "critic_loss": 0.0022451552745478692, "actor_loss": -20.112546412095426, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.521878004074097, "step": 146000}
{"episode_reward": 7.918200281277292, "episode": 147.0, "batch_reward": 0.01208842993946746, "critic_loss": 0.0018683085347729503, "actor_loss": -19.8385999417454, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.472414016723633, "step": 147000}
{"episode_reward": 10.787772602574472, "episode": 148.0, "batch_reward": 0.01219756545079872, "critic_loss": 0.002321714525402058, "actor_loss": -20.32608814676106, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 22.24867296218872, "step": 148000}
{"episode_reward": 7.654654850098393, "episode": 149.0, "batch_reward": 0.012145876068156213, "critic_loss": 0.0022169345585862175, "actor_loss": -19.054862091943622, "actor_target_entropy": -6.0, "alpha_value": 0.0, "duration": 23.49494433403015, "step": 149000}
{"episode_reward": 8.015494454568074, "episode": 150.0, "batch_reward": 0.012542224532924593, "critic_loss": 0.0019863052561704536, "actor_loss": -19.735358019962906, "actor_target_entropy": -6.0, "alpha_value": 0.0, "step": 150000}
