{"episode_reward": 0.0, "episode": 1.0, "duration": 12.123013734817505, "step": 1000}
{"episode_reward": 7.300100444933085, "episode": 2.0, "duration": 0.9909193515777588, "step": 2000}
{"episode_reward": 521.6090064697321, "episode": 3.0, "batch_reward": 0.25178138564908137, "critic_loss": 0.05552794520500035, "actor_loss": -38.690982226313665, "actor_target_entropy": -6.0, "alpha_value": 0.010000000000000443, "duration": 67.42054533958435, "step": 3000}
{"episode_reward": 43.515714293875625, "episode": 4.0, "batch_reward": 0.16602409199625254, "critic_loss": 0.05717254717648029, "actor_loss": -27.294087520599366, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.370309829711914, "step": 4000}
{"episode_reward": 6.1310912426237465, "episode": 5.0, "batch_reward": 0.13805052895843983, "critic_loss": 0.0641874340530485, "actor_loss": -25.890830223560332, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.902857303619385, "step": 5000}
{"episode_reward": 69.65879599964569, "episode": 6.0, "batch_reward": 0.13042710914462805, "critic_loss": 0.06572041483968497, "actor_loss": -25.082003510475158, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.24897313117981, "step": 6000}
{"episode_reward": 127.68060997909569, "episode": 7.0, "batch_reward": 0.1261552654132247, "critic_loss": 0.056218763617798685, "actor_loss": -24.799248422622682, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.496144771575928, "step": 7000}
{"episode_reward": 52.83251675707272, "episode": 8.0, "batch_reward": 0.11680089465528727, "critic_loss": 0.05225967403315008, "actor_loss": -20.741905603408814, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.44064712524414, "step": 8000}
{"episode_reward": 108.02117550116078, "episode": 9.0, "batch_reward": 0.11415457125008106, "critic_loss": 0.053549440409988165, "actor_loss": -21.251760566234587, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.012288331985474, "step": 9000}
{"episode_reward": 82.493310247897, "episode": 10.0, "batch_reward": 0.11048291317373514, "critic_loss": 0.051088508076965806, "actor_loss": -20.859363090515135, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.18997049331665, "step": 10000}
{"episode_reward": 51.68126647620124, "episode": 11.0, "batch_reward": 0.10457903846353292, "critic_loss": 0.04980756383575499, "actor_loss": -20.920208057403563, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 32.79906225204468, "step": 11000}
{"episode_reward": 64.8350205323466, "episode": 12.0, "batch_reward": 0.1049206308349967, "critic_loss": 0.054828753395006063, "actor_loss": -19.443088439464567, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 20.901217460632324, "step": 12000}
{"episode_reward": 111.0435560113087, "episode": 13.0, "batch_reward": 0.10709172087907791, "critic_loss": 0.08566983774676919, "actor_loss": -20.852388958215712, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.823517084121704, "step": 13000}
{"episode_reward": 224.194905667473, "episode": 14.0, "batch_reward": 0.11352091379463673, "critic_loss": 0.09807398230209946, "actor_loss": -20.592317254424096, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.85670757293701, "step": 14000}
{"episode_reward": 137.01541130227577, "episode": 15.0, "batch_reward": 0.11497476561367512, "critic_loss": 0.10656970877572894, "actor_loss": -22.161642207026482, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.622668504714966, "step": 15000}
{"episode_reward": 76.11947878186396, "episode": 16.0, "batch_reward": 0.11372279962897301, "critic_loss": 0.13210630624368785, "actor_loss": -17.18838030320406, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.87009835243225, "step": 16000}
{"episode_reward": 155.2341999438216, "episode": 17.0, "batch_reward": 0.11498392702639103, "critic_loss": 0.15223358104377985, "actor_loss": -17.631953486919404, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.616865634918213, "step": 17000}
{"episode_reward": 128.15303241307203, "episode": 18.0, "batch_reward": 0.11863247130066157, "critic_loss": 0.20430900374799968, "actor_loss": -17.980353733241557, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.314398765563965, "step": 18000}
{"episode_reward": 213.67974842509614, "episode": 19.0, "batch_reward": 0.12385882627964019, "critic_loss": 0.23067606097459792, "actor_loss": -17.928007329180836, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.827826976776123, "step": 19000}
{"episode_reward": 170.70050496346752, "episode": 20.0, "batch_reward": 0.12790906559675932, "critic_loss": 0.22721958573907614, "actor_loss": -19.165750875145196, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.501651763916016, "step": 20000}
{"episode_reward": 303.88230935802073, "episode": 21.0, "batch_reward": 0.13427025041729213, "critic_loss": 0.23618513940274716, "actor_loss": -18.798961474061013, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 32.95751214027405, "step": 21000}
{"episode_reward": 223.3608908203809, "episode": 22.0, "batch_reward": 0.13723552194982766, "critic_loss": 0.2607625782713294, "actor_loss": -19.565261555671693, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.459033966064453, "step": 22000}
{"episode_reward": 86.82353916058744, "episode": 23.0, "batch_reward": 0.13791514724493026, "critic_loss": 0.27425299176573753, "actor_loss": -18.560796682834624, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.440896034240723, "step": 23000}
{"episode_reward": 288.52254283926356, "episode": 24.0, "batch_reward": 0.14439881434291602, "critic_loss": 0.25505186744779346, "actor_loss": -19.22385208082199, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.50571608543396, "step": 24000}
{"episode_reward": 264.2403601978028, "episode": 25.0, "batch_reward": 0.14823773680627347, "critic_loss": 0.23688353810459375, "actor_loss": -19.218707627773284, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.480826377868652, "step": 25000}
{"episode_reward": 271.84442636015547, "episode": 26.0, "batch_reward": 0.15287606161087752, "critic_loss": 0.24347949006408454, "actor_loss": -18.785870170593263, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.686059713363647, "step": 26000}
{"episode_reward": 188.5608768097169, "episode": 27.0, "batch_reward": 0.15581203746050595, "critic_loss": 0.2357152803465724, "actor_loss": -19.715590695381163, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.97601866722107, "step": 27000}
{"episode_reward": 309.28595938698845, "episode": 28.0, "batch_reward": 0.16010557585954666, "critic_loss": 0.23974317990243435, "actor_loss": -18.962241456985474, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.388427019119263, "step": 28000}
{"episode_reward": 208.59244433118909, "episode": 29.0, "batch_reward": 0.16151746118068694, "critic_loss": 0.2522736704200506, "actor_loss": -19.927032664299013, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.389960289001465, "step": 29000}
{"episode_reward": 150.96108247983437, "episode": 30.0, "batch_reward": 0.16097845876216887, "critic_loss": 0.23512888135015964, "actor_loss": -19.642224294662476, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.72747039794922, "step": 30000}
{"episode_reward": 127.33617253369442, "episode": 31.0, "batch_reward": 0.16138430380821228, "critic_loss": 0.23418897267431021, "actor_loss": -18.95330673980713, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 32.977168560028076, "step": 31000}
{"episode_reward": 232.39398149557033, "episode": 32.0, "batch_reward": 0.16319087405502797, "critic_loss": 0.259018098667264, "actor_loss": -19.12971697425842, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.65731644630432, "step": 32000}
{"episode_reward": 180.53908872727575, "episode": 33.0, "batch_reward": 0.16519253772497178, "critic_loss": 0.2813961365073919, "actor_loss": -19.400199432373046, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.85927653312683, "step": 33000}
{"episode_reward": 321.85485843211706, "episode": 34.0, "batch_reward": 0.16848716139793396, "critic_loss": 0.2854043248444796, "actor_loss": -18.690930292129515, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.48473286628723, "step": 34000}
{"episode_reward": 205.8849887302319, "episode": 35.0, "batch_reward": 0.16735732753574847, "critic_loss": 0.26237808546423913, "actor_loss": -19.35728274154663, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.09074068069458, "step": 35000}
{"episode_reward": 91.7349396209218, "episode": 36.0, "batch_reward": 0.1677832249701023, "critic_loss": 0.276918602488935, "actor_loss": -18.405745914459228, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.334327697753906, "step": 36000}
{"episode_reward": 335.13053887175295, "episode": 37.0, "batch_reward": 0.17259861052036285, "critic_loss": 0.2980892932862043, "actor_loss": -19.37416622161865, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.531579732894897, "step": 37000}
{"episode_reward": 232.54052640169974, "episode": 38.0, "batch_reward": 0.17262956941127777, "critic_loss": 0.29236132180690766, "actor_loss": -19.917882331848144, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.699018239974976, "step": 38000}
{"episode_reward": 148.62313315330297, "episode": 39.0, "batch_reward": 0.1729279192686081, "critic_loss": 0.31759567036479713, "actor_loss": -19.373452907562257, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.03126358985901, "step": 39000}
{"episode_reward": 197.1482300602927, "episode": 40.0, "batch_reward": 0.17428132167458535, "critic_loss": 0.27999796536564825, "actor_loss": -19.40913419151306, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.859076261520386, "step": 40000}
{"episode_reward": 374.3417976670378, "episode": 41.0, "batch_reward": 0.17831863854825497, "critic_loss": 0.30404686817526816, "actor_loss": -19.38994429397583, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 33.45305061340332, "step": 41000}
{"episode_reward": 174.07258433334593, "episode": 42.0, "batch_reward": 0.17900220604240893, "critic_loss": 0.28435905092954633, "actor_loss": -20.070222677230834, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.511367797851562, "step": 42000}
{"episode_reward": 242.9237911561854, "episode": 43.0, "batch_reward": 0.1796084357202053, "critic_loss": 0.2916250339448452, "actor_loss": -19.51287059402466, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.94454312324524, "step": 43000}
{"episode_reward": 139.25917168739633, "episode": 44.0, "batch_reward": 0.17807384627312423, "critic_loss": 0.3055265349149704, "actor_loss": -19.52986956977844, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.651228666305542, "step": 44000}
{"episode_reward": 139.76314043103653, "episode": 45.0, "batch_reward": 0.17781062692403793, "critic_loss": 0.29411495476216076, "actor_loss": -19.722944591522218, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.83172869682312, "step": 45000}
{"episode_reward": 127.88219222234812, "episode": 46.0, "batch_reward": 0.17640732534229756, "critic_loss": 0.271301221549511, "actor_loss": -19.823625467300413, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.992302894592285, "step": 46000}
{"episode_reward": 170.59401488036417, "episode": 47.0, "batch_reward": 0.17592734862864018, "critic_loss": 0.2889652867019176, "actor_loss": -19.52831067657471, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.52769708633423, "step": 47000}
{"episode_reward": 127.09214778158682, "episode": 48.0, "batch_reward": 0.17600373010337353, "critic_loss": 0.2613873017057776, "actor_loss": -19.54190499305725, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.992398738861084, "step": 48000}
{"episode_reward": 305.1323227178228, "episode": 49.0, "batch_reward": 0.17866524147987367, "critic_loss": 0.2761144525930285, "actor_loss": -19.839239795684815, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.873186588287354, "step": 49000}
{"episode_reward": 189.56156356763628, "episode": 50.0, "batch_reward": 0.17983245031535625, "critic_loss": 0.3077831575125456, "actor_loss": -20.275558921813964, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.708115816116333, "step": 50000}
{"episode_reward": 420.9023214890368, "episode": 51.0, "batch_reward": 0.18419829991459846, "critic_loss": 0.33815844129025935, "actor_loss": -20.397250255584716, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 33.3862042427063, "step": 51000}
{"episode_reward": 258.0987408273229, "episode": 52.0, "batch_reward": 0.18560828717052938, "critic_loss": 0.31855974743515253, "actor_loss": -20.69298013496399, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.3244366645813, "step": 52000}
{"episode_reward": 236.13748724549427, "episode": 53.0, "batch_reward": 0.18682106214761734, "critic_loss": 0.28950275062024594, "actor_loss": -21.1590298538208, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.44011092185974, "step": 53000}
{"episode_reward": 242.06769126609834, "episode": 54.0, "batch_reward": 0.1874996954202652, "critic_loss": 0.2965127196162939, "actor_loss": -21.138717308044434, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.068276166915894, "step": 54000}
{"episode_reward": 184.24936745806923, "episode": 55.0, "batch_reward": 0.18710441666841507, "critic_loss": 0.27573122236132624, "actor_loss": -21.434130672454835, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.989877462387085, "step": 55000}
{"episode_reward": 265.8533287283546, "episode": 56.0, "batch_reward": 0.18790872685611248, "critic_loss": 0.3148394072055817, "actor_loss": -21.846234336853026, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.933431386947632, "step": 56000}
{"episode_reward": 370.41055722700037, "episode": 57.0, "batch_reward": 0.1926937564909458, "critic_loss": 0.32200081487745047, "actor_loss": -21.94886248397827, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.478080987930298, "step": 57000}
{"episode_reward": 407.0789328911832, "episode": 58.0, "batch_reward": 0.19432424974441528, "critic_loss": 0.3194753228127956, "actor_loss": -22.362433002471924, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.545084714889526, "step": 58000}
{"episode_reward": 92.7745646466793, "episode": 59.0, "batch_reward": 0.19504722794890403, "critic_loss": 0.28839325172454117, "actor_loss": -22.313227199554444, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.88091230392456, "step": 59000}
{"episode_reward": 490.4278174788462, "episode": 60.0, "batch_reward": 0.19860829223692417, "critic_loss": 0.30655221170932057, "actor_loss": -22.991909198760986, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.665071725845337, "step": 60000}
{"episode_reward": 172.27069185410213, "episode": 61.0, "batch_reward": 0.19723121131956578, "critic_loss": 0.2987690592855215, "actor_loss": -23.05453990936279, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 33.442091941833496, "step": 61000}
{"episode_reward": 101.4350236618663, "episode": 62.0, "batch_reward": 0.19585759967565536, "critic_loss": 0.29382648001611233, "actor_loss": -23.029478176116942, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.207093000411987, "step": 62000}
{"episode_reward": 143.4849772706377, "episode": 63.0, "batch_reward": 0.19473580956459047, "critic_loss": 0.287915675483644, "actor_loss": -22.845959819793702, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.846270322799683, "step": 63000}
{"episode_reward": 69.40074681146912, "episode": 64.0, "batch_reward": 0.19199986985325812, "critic_loss": 0.28442963457852605, "actor_loss": -22.77615609741211, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.08260703086853, "step": 64000}
{"episode_reward": 35.845270121545155, "episode": 65.0, "batch_reward": 0.19150751711428166, "critic_loss": 0.2849146916717291, "actor_loss": -22.84944290161133, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.444764614105225, "step": 65000}
{"episode_reward": 98.01344996436478, "episode": 66.0, "batch_reward": 0.19026349745690824, "critic_loss": 0.3120831807106733, "actor_loss": -22.6459280128479, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.54370617866516, "step": 66000}
{"episode_reward": 352.4444647002881, "episode": 67.0, "batch_reward": 0.19243265374004842, "critic_loss": 0.29680981451272964, "actor_loss": -22.956375427246094, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.160719394683838, "step": 67000}
{"episode_reward": 192.04390249409414, "episode": 68.0, "batch_reward": 0.19263892631232737, "critic_loss": 0.307614564999938, "actor_loss": -22.89556679916382, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.854487419128418, "step": 68000}
{"episode_reward": 311.913100094919, "episode": 69.0, "batch_reward": 0.19304037299752236, "critic_loss": 0.31476092134416106, "actor_loss": -22.97986333847046, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.266515016555786, "step": 69000}
{"episode_reward": 67.91042413542662, "episode": 70.0, "batch_reward": 0.1924354167729616, "critic_loss": 0.31861744338274, "actor_loss": -22.835061939239502, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.117477893829346, "step": 70000}
{"episode_reward": 251.19658946279947, "episode": 71.0, "batch_reward": 0.19314459036290646, "critic_loss": 0.34344414942711593, "actor_loss": -23.327553619384766, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 32.85911178588867, "step": 71000}
{"episode_reward": 358.36215553797973, "episode": 72.0, "batch_reward": 0.19521032062172888, "critic_loss": 0.34082703544199466, "actor_loss": -23.2105719909668, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.133216857910156, "step": 72000}
{"episode_reward": 165.92283149354546, "episode": 73.0, "batch_reward": 0.19558425475656988, "critic_loss": 0.3401873031705618, "actor_loss": -23.27899487686157, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.034189462661743, "step": 73000}
{"episode_reward": 204.31732247545764, "episode": 74.0, "batch_reward": 0.19543998335301876, "critic_loss": 0.3334691536128521, "actor_loss": -23.43680869293213, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.860363483428955, "step": 74000}
{"episode_reward": 217.6104416631229, "episode": 75.0, "batch_reward": 0.1958292225599289, "critic_loss": 0.35201302433013915, "actor_loss": -23.45352756881714, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.994354486465454, "step": 75000}
{"episode_reward": 441.71509862853765, "episode": 76.0, "batch_reward": 0.19997746044397355, "critic_loss": 0.366770600810647, "actor_loss": -23.786560287475584, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.79012942314148, "step": 76000}
{"episode_reward": 525.6688032992012, "episode": 77.0, "batch_reward": 0.20292661249637603, "critic_loss": 0.36065663884580135, "actor_loss": -23.907082897186278, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.021024465560913, "step": 77000}
{"episode_reward": 159.97224488282654, "episode": 78.0, "batch_reward": 0.2024998794347048, "critic_loss": 0.34721789820492266, "actor_loss": -24.085399398803713, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.702017545700073, "step": 78000}
{"episode_reward": 121.24485006869214, "episode": 79.0, "batch_reward": 0.20205846112966538, "critic_loss": 0.37539145965874193, "actor_loss": -23.979710487365722, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.942590951919556, "step": 79000}
{"episode_reward": 266.97300242143365, "episode": 80.0, "batch_reward": 0.2019924574494362, "critic_loss": 0.39562872324883935, "actor_loss": -23.968532077789305, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.876932621002197, "step": 80000}
{"episode_reward": 228.81628298984697, "episode": 81.0, "batch_reward": 0.20351720145344734, "critic_loss": 0.4104697843939066, "actor_loss": -24.05329824447632, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 32.857365131378174, "step": 81000}
{"episode_reward": 249.45540362770257, "episode": 82.0, "batch_reward": 0.20172556868195535, "critic_loss": 0.3860268523395062, "actor_loss": -23.98831574630737, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.81490445137024, "step": 82000}
{"episode_reward": 155.90725448971688, "episode": 83.0, "batch_reward": 0.20213451328873636, "critic_loss": 0.42427052192389964, "actor_loss": -24.028998935699462, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.991885900497437, "step": 83000}
{"episode_reward": 246.78362265369637, "episode": 84.0, "batch_reward": 0.20400777553021907, "critic_loss": 0.4569580864906311, "actor_loss": -24.17494648742676, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.048811435699463, "step": 84000}
{"episode_reward": 516.5651804365551, "episode": 85.0, "batch_reward": 0.2076252359598875, "critic_loss": 0.3933836139142513, "actor_loss": -24.613400054931642, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.314706087112427, "step": 85000}
{"episode_reward": 506.64251091551574, "episode": 86.0, "batch_reward": 0.21061220346391202, "critic_loss": 0.41805768194794657, "actor_loss": -24.782785396575928, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.559550285339355, "step": 86000}
{"episode_reward": 442.3589051671813, "episode": 87.0, "batch_reward": 0.21416945193707942, "critic_loss": 0.3940112787038088, "actor_loss": -25.2020301322937, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.400785446166992, "step": 87000}
{"episode_reward": 397.8692638078111, "episode": 88.0, "batch_reward": 0.21539286097884178, "critic_loss": 0.45015710657835006, "actor_loss": -25.121155334472657, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.331679105758667, "step": 88000}
{"episode_reward": 170.14325944198544, "episode": 89.0, "batch_reward": 0.2159388658106327, "critic_loss": 0.4307155102491379, "actor_loss": -25.452793102264405, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.613184928894043, "step": 89000}
{"episode_reward": 323.6394252114584, "episode": 90.0, "batch_reward": 0.21603146086633204, "critic_loss": 0.45871583685278894, "actor_loss": -25.293154308319092, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.01784896850586, "step": 90000}
{"episode_reward": 201.1571803470476, "episode": 91.0, "batch_reward": 0.2151161728054285, "critic_loss": 0.4716295222491026, "actor_loss": -25.3239349861145, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 33.35107946395874, "step": 91000}
{"episode_reward": 336.46937194639037, "episode": 92.0, "batch_reward": 0.21750281836092472, "critic_loss": 0.47736857901513574, "actor_loss": -25.77116893005371, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.186963319778442, "step": 92000}
{"episode_reward": 478.31429934719984, "episode": 93.0, "batch_reward": 0.22099132707715036, "critic_loss": 0.4838639710992575, "actor_loss": -25.868527038574218, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.239144563674927, "step": 93000}
{"episode_reward": 523.2761578155219, "episode": 94.0, "batch_reward": 0.22317511261999606, "critic_loss": 0.5219553802013397, "actor_loss": -26.224056011199952, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.968255281448364, "step": 94000}
{"episode_reward": 401.9478168677721, "episode": 95.0, "batch_reward": 0.22463115338981152, "critic_loss": 0.5096183868050576, "actor_loss": -26.45450676345825, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.75606393814087, "step": 95000}
{"episode_reward": 259.2966730251241, "episode": 96.0, "batch_reward": 0.22432496842741967, "critic_loss": 0.5199187319725752, "actor_loss": -26.449696681976317, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.300423860549927, "step": 96000}
{"episode_reward": 297.8354283875883, "episode": 97.0, "batch_reward": 0.22498295271396637, "critic_loss": 0.5204526328146458, "actor_loss": -26.814071079254152, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.94121289253235, "step": 97000}
{"episode_reward": 312.60876621923404, "episode": 98.0, "batch_reward": 0.2250456935465336, "critic_loss": 0.5095325820147991, "actor_loss": -26.961591709136965, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.516575574874878, "step": 98000}
{"episode_reward": 4.601185772085378, "episode": 99.0, "batch_reward": 0.22281878265738486, "critic_loss": 0.4996391723304987, "actor_loss": -27.124187099456787, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.37135124206543, "step": 99000}
{"episode_reward": 10.831723626633266, "episode": 100.0, "batch_reward": 0.22049812059104443, "critic_loss": 0.5127076187729835, "actor_loss": -27.229687610626222, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.592004537582397, "step": 100000}
{"episode_reward": 30.504910495921994, "episode": 101.0, "batch_reward": 0.2195665436834097, "critic_loss": 0.4753253327012062, "actor_loss": -27.40324467086792, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 33.631040811538696, "step": 101000}
{"episode_reward": 9.470924205264925, "episode": 102.0, "batch_reward": 0.2180577121824026, "critic_loss": 0.43777675546705724, "actor_loss": -27.49272707748413, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.74736523628235, "step": 102000}
{"episode_reward": 47.01377063259755, "episode": 103.0, "batch_reward": 0.2155334197282791, "critic_loss": 0.4101934414356947, "actor_loss": -27.40450524520874, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.873811721801758, "step": 103000}
{"episode_reward": 16.794637304390438, "episode": 104.0, "batch_reward": 0.21332740817964077, "critic_loss": 0.4041034779399633, "actor_loss": -27.391218200683593, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.10245108604431, "step": 104000}
{"episode_reward": 21.62562216429631, "episode": 105.0, "batch_reward": 0.21230579198896884, "critic_loss": 0.392716643884778, "actor_loss": -27.535977909088135, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.607539892196655, "step": 105000}
{"episode_reward": 7.532441419325169, "episode": 106.0, "batch_reward": 0.21046063756942748, "critic_loss": 0.3890354683101177, "actor_loss": -27.483034675598145, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.921215534210205, "step": 106000}
{"episode_reward": 5.880228740635942, "episode": 107.0, "batch_reward": 0.2086932340413332, "critic_loss": 0.3710929181724787, "actor_loss": -27.446432025909424, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.857895612716675, "step": 107000}
{"episode_reward": 5.346246214366799, "episode": 108.0, "batch_reward": 0.20531466452777386, "critic_loss": 0.35899355459213256, "actor_loss": -27.223016921997072, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.566835165023804, "step": 108000}
{"episode_reward": 56.02837246284166, "episode": 109.0, "batch_reward": 0.20528396578133107, "critic_loss": 0.36027073515951635, "actor_loss": -27.360895572662354, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.810096740722656, "step": 109000}
{"episode_reward": 214.75970909558131, "episode": 110.0, "batch_reward": 0.20653630481660365, "critic_loss": 0.3544814595133066, "actor_loss": -27.51812361907959, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.6444149017334, "step": 110000}
{"episode_reward": 470.1253880737058, "episode": 111.0, "batch_reward": 0.20859982332587243, "critic_loss": 0.34283956065773963, "actor_loss": -27.758451057434083, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 33.07235074043274, "step": 111000}
{"episode_reward": 495.49398173320805, "episode": 112.0, "batch_reward": 0.2096997199356556, "critic_loss": 0.3379121326953173, "actor_loss": -27.850687194824218, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.82599902153015, "step": 112000}
{"episode_reward": 10.468916303257807, "episode": 113.0, "batch_reward": 0.211482574775815, "critic_loss": 0.32487198317050936, "actor_loss": -27.991331974029542, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.708782196044922, "step": 113000}
{"episode_reward": 466.8520897002423, "episode": 114.0, "batch_reward": 0.21205223037302495, "critic_loss": 0.3283989809602499, "actor_loss": -27.80987899017334, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.152987241744995, "step": 114000}
{"episode_reward": 521.4001358448547, "episode": 115.0, "batch_reward": 0.21307297679781914, "critic_loss": 0.32423768343031406, "actor_loss": -27.736368564605712, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.537378072738647, "step": 115000}
{"episode_reward": 187.57269023204262, "episode": 116.0, "batch_reward": 0.21412580479681492, "critic_loss": 0.3171390988379717, "actor_loss": -27.57429974746704, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.87680411338806, "step": 116000}
{"episode_reward": 489.1802667673936, "episode": 117.0, "batch_reward": 0.21663074232637883, "critic_loss": 0.34141006974875926, "actor_loss": -27.947878997802736, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.30559515953064, "step": 117000}
{"episode_reward": 388.09784627466325, "episode": 118.0, "batch_reward": 0.2183764449506998, "critic_loss": 0.35751028373837473, "actor_loss": -27.638780979156493, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.630481004714966, "step": 118000}
{"episode_reward": 500.0231566085403, "episode": 119.0, "batch_reward": 0.22058274628221988, "critic_loss": 0.3306855491548777, "actor_loss": -27.668717243194582, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.557796716690063, "step": 119000}
{"episode_reward": 579.3922837396905, "episode": 120.0, "batch_reward": 0.22350718615949153, "critic_loss": 0.3558736718595028, "actor_loss": -27.80548143005371, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.744011163711548, "step": 120000}
{"episode_reward": 582.5837735316742, "episode": 121.0, "batch_reward": 0.22661359657347202, "critic_loss": 0.3801956608071923, "actor_loss": -28.03076140213013, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 32.91853046417236, "step": 121000}
{"episode_reward": 575.4860588396446, "episode": 122.0, "batch_reward": 0.22995777191221714, "critic_loss": 0.4018751471936703, "actor_loss": -28.05696947860718, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.999631881713867, "step": 122000}
{"episode_reward": 565.0194170858977, "episode": 123.0, "batch_reward": 0.2312581999003887, "critic_loss": 0.38104608476161955, "actor_loss": -28.2287834815979, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.587843894958496, "step": 123000}
{"episode_reward": 437.3327954970481, "episode": 124.0, "batch_reward": 0.23198760053515435, "critic_loss": 0.37013473626971244, "actor_loss": -28.28549427032471, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.75415349006653, "step": 124000}
{"episode_reward": 94.99231152822804, "episode": 125.0, "batch_reward": 0.23279926221072675, "critic_loss": 0.3828448890000582, "actor_loss": -28.38698230743408, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.362294673919678, "step": 125000}
{"episode_reward": 494.8939053373125, "episode": 126.0, "batch_reward": 0.23553985711932182, "critic_loss": 0.39151167911291124, "actor_loss": -28.525340812683105, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.913108348846436, "step": 126000}
{"episode_reward": 624.3423384945239, "episode": 127.0, "batch_reward": 0.23874508625268936, "critic_loss": 0.41231321029365064, "actor_loss": -28.76253024673462, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.727150917053223, "step": 127000}
{"episode_reward": 567.7610473637856, "episode": 128.0, "batch_reward": 0.24142459915578365, "critic_loss": 0.42297282998263835, "actor_loss": -29.135927291870118, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.916560411453247, "step": 128000}
{"episode_reward": 568.0417520880872, "episode": 129.0, "batch_reward": 0.24309466332197188, "critic_loss": 0.3959181696176529, "actor_loss": -29.149922035217283, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.424971342086792, "step": 129000}
{"episode_reward": 543.6352228509367, "episode": 130.0, "batch_reward": 0.24550506833195687, "critic_loss": 0.42082630161941054, "actor_loss": -29.30013808441162, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.077082633972168, "step": 130000}
{"episode_reward": 522.327305814218, "episode": 131.0, "batch_reward": 0.2484564890265465, "critic_loss": 0.39676136630773545, "actor_loss": -29.621824756622313, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 32.89435958862305, "step": 131000}
{"episode_reward": 541.8193882267361, "episode": 132.0, "batch_reward": 0.24940230436623095, "critic_loss": 0.39176497431099416, "actor_loss": -29.660601234436037, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.840278148651123, "step": 132000}
{"episode_reward": 563.2795583808576, "episode": 133.0, "batch_reward": 0.25226885336637495, "critic_loss": 0.40914808623492716, "actor_loss": -29.805086448669435, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.08924913406372, "step": 133000}
{"episode_reward": 573.6587645258442, "episode": 134.0, "batch_reward": 0.2538243133276701, "critic_loss": 0.41445494145154954, "actor_loss": -30.05484214782715, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.388241052627563, "step": 134000}
{"episode_reward": 515.9571618425723, "episode": 135.0, "batch_reward": 0.25679860772192475, "critic_loss": 0.3999480288475752, "actor_loss": -30.2058038482666, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.5145525932312, "step": 135000}
{"episode_reward": 624.9143083887585, "episode": 136.0, "batch_reward": 0.25979615922272203, "critic_loss": 0.39981108690798284, "actor_loss": -30.458905467987062, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.2480947971344, "step": 136000}
{"episode_reward": 588.1542222126385, "episode": 137.0, "batch_reward": 0.26141746778786185, "critic_loss": 0.4043632084578276, "actor_loss": -30.630798355102538, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.771446466445923, "step": 137000}
{"episode_reward": 374.0585080809555, "episode": 138.0, "batch_reward": 0.26340490029752256, "critic_loss": 0.40170699334144594, "actor_loss": -30.723395542144775, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.799910306930542, "step": 138000}
{"episode_reward": 569.2687609147837, "episode": 139.0, "batch_reward": 0.26535327719151974, "critic_loss": 0.38462566851079466, "actor_loss": -30.8626728515625, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.341041088104248, "step": 139000}
{"episode_reward": 298.7986167473091, "episode": 140.0, "batch_reward": 0.2660896870493889, "critic_loss": 0.357724322527647, "actor_loss": -30.8535274848938, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.99272894859314, "step": 140000}
{"episode_reward": 577.3256638188294, "episode": 141.0, "batch_reward": 0.26693632134795187, "critic_loss": 0.45508443769812584, "actor_loss": -30.926502281188966, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 33.47177219390869, "step": 141000}
{"episode_reward": 597.0595723520543, "episode": 142.0, "batch_reward": 0.26847509601712227, "critic_loss": 0.39945608519017695, "actor_loss": -31.23110834503174, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.55731987953186, "step": 142000}
{"episode_reward": 601.6005031490969, "episode": 143.0, "batch_reward": 0.27127050141990183, "critic_loss": 0.4117032473981381, "actor_loss": -31.42522624206543, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.200835704803467, "step": 143000}
{"episode_reward": 345.5264303753336, "episode": 144.0, "batch_reward": 0.2713490244299173, "critic_loss": 0.4284788431823254, "actor_loss": -31.397040397644044, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.529868602752686, "step": 144000}
{"episode_reward": 202.97081267778907, "episode": 145.0, "batch_reward": 0.2723319923728704, "critic_loss": 0.4077148748934269, "actor_loss": -31.446711128234863, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.443663120269775, "step": 145000}
{"episode_reward": 581.855484372235, "episode": 146.0, "batch_reward": 0.2748003118336201, "critic_loss": 0.41067757292091844, "actor_loss": -31.54940617752075, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 22.360534191131592, "step": 146000}
{"episode_reward": 609.7608576992717, "episode": 147.0, "batch_reward": 0.2769683215767145, "critic_loss": 0.4012048362791538, "actor_loss": -31.877966632843016, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.727113246917725, "step": 147000}
{"episode_reward": 633.0053360146106, "episode": 148.0, "batch_reward": 0.27902681949734687, "critic_loss": 0.40093273228406906, "actor_loss": -32.1882689819336, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.61456847190857, "step": 148000}
{"episode_reward": 592.1911061870773, "episode": 149.0, "batch_reward": 0.2808515410274267, "critic_loss": 0.375617544978857, "actor_loss": -32.338878696441654, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "duration": 21.949738264083862, "step": 149000}
{"episode_reward": 634.1332173936178, "episode": 150.0, "batch_reward": 0.2834829639494419, "critic_loss": 0.38660269671678543, "actor_loss": -32.53439866638183, "actor_target_entropy": -6.0, "alpha_value": 0.009999999999999875, "step": 150000}
